{
    "main_text": "Samples of What Neural Networks Can Do\n3. Samples of What Neural Networks Can Do\n\nConnectionists have made significant progress in demonstrating the\npower of neural networks to master cognitive tasks. Here are three\nwell-known experiments that have encouraged connectionists to believe\nthat neural networks are good models of human intelligence. One of the\nmost attractive of these efforts is Sejnowski and Rosenberg\u2019s\n1987 work on a net that can read English text called NETtalk. The\ntraining set for NETtalk was a large data base consisting of English\ntext coupled with its corresponding phonetic output, written in a code\nsuitable for use with a speech synthesizer. Tapes of NETtalk\u2019s\nperformance at different stages of its training are very interesting\nlistening. At first the output is random noise. Later, the net sounds\nlike it is babbling, and later still as though it is speaking English\ndouble-talk (speech that is formed of sounds that resemble English\nwords). At the end of training, NETtalk does a fairly good job of\npronouncing the text given to it. Furthermore, this ability\ngeneralizes fairly well to text that was not presented in the training\nset.\n\nAnother influential early connectionist model was a net trained by\nRumelhart and McClelland (1986) to predict the past tense of English\nverbs. The task is interesting because although most of the verbs in\nEnglish (the regular verbs) form the past tense by adding the suffix\n\u201c-ed\u201d, many of the most frequently verbs are irregular\n(\u201cis\u201d / \u201cwas\u201d, \u201ccome\u201d /\n\u201ccame\u201d, \u201cgo\u201d / \u201cwent\u201d). The net\nwas first trained on a set containing a large number of irregular\nverbs, and later on a set of 460 verbs containing mostly regulars. The\nnet learned the past tenses of the 460 verbs in about 200 rounds of\ntraining, and it generalized fairly well to verbs not in the training\nset. It even showed a good appreciation of \u201cregularities\u201d\nto be found among the irregular verbs (\u201csend\u201d /\n\u201csent\u201d, \u201cbuild\u201d / \u201cbuilt\u201d;\n\u201cblow\u201d / \u201cblew\u201d, \u201cfly\u201d /\n\u201cflew\u201d). During learning, as the system was exposed to the\ntraining set containing more regular verbs, it had a tendency to\noverregularize, i.e., to combine both irregular and regular forms:\n(\u201cbreak\u201d / \u201cbroked\u201d, instead of\n\u201cbreak\u201d / \u201cbroke\u201d). This was corrected with\nmore training. It is interesting to note that children are known to\nexhibit the same tendency to overregularize during language learning.\nHowever, there is hot debate over whether Rumelhart and\nMcClelland\u2019s is a good model of how humans actually learn and\nprocess verb endings. For example, Pinker and Prince (1988) point out\nthat the model does a poor job of generalizing to some novel regular\nverbs. They believe that this is a sign of a basic failing in\nconnectionist models. Nets may be good at making associations and\nmatching patterns, but they have fundamental limitations in mastering\ngeneral rules such as the formation of the regular past tense. These\ncomplaints raise an important issue for connectionist modelers, namely\nwhether nets can generalize properly to master cognitive tasks\ninvolving rules. Despite Pinker and Prince\u2019s objections, many\nconnectionists believe that generalization of the right kind is still\npossible (Niklasson & van Gelder 1994).\n\nElman\u2019s 1991 work on nets that can appreciate grammatical\nstructure has important implications for the debate about whether\nneural networks can learn to master rules. Elman trained a simple\nrecurrent network to predict the next word in a large corpus of\nEnglish sentences. The sentences were formed from a simple vocabulary\nof 23 words using a subset of English grammar. The grammar, though\nsimple, posed a hard test for linguistic awareness. It allowed\nunlimited formation of relative clauses while demanding agreement\nbetween the head noun and the verb. So for example, in the\nsentence\n\n\nAny man that chases dogs that chase cats \u2026\nruns.\n\n\nthe singular \u201cman\u201d must agree with the\nverb \u201cruns\u201d despite the intervening\nplural nouns (\u201cdogs\u201d, \u201ccats\u201d) which might\ncause the selection of \u201crun\u201d. One of the important\nfeatures of Elman\u2019s model is the use of recurrent connections.\nThe values at the hidden units are saved in a set of so called context\nunits, to be sent back to the input level for the next round of\nprocessing. This looping back from hidden to input layers provides the\nnet with a rudimentary form of memory of the sequence of words in the\ninput sentence. Elman\u2019s nets displayed an appreciation of the\ngrammatical structure of sentences that were not in the training set.\nThe net\u2019s command of syntax was measured in the following way.\nPredicting the next word in an English sentence is, of course, an\nimpossible task. However, these nets succeeded, at least by the\nfollowing measure. At a given point in an input sentence, the output\nunits for words that are grammatical continuations of the sentence at\nthat point should be active and output units for all other words\nshould be inactive. After intensive training, Elman was able to\nproduce nets that displayed perfect performance on this measure\nincluding sentences not in the training set. The work of Christiansen\nand Chater (1999a) and Morris, Cottrell, and Elman (2000) extends this\nresearch to more complex grammars. For a broader view of progress in\nconnectionist natural language processing see summaries by\nChristiansen and Chater (1999b), and Rohde and Plaut (2003).\n\nAlthough this performance is impressive, there is still a long way to\ngo in training nets that can process a language like English.\nFurthermore, doubts have been raised about the significance of\nElman\u2019s results. For example, Marcus (1998, 2001) argues that\nElman\u2019s nets are not able to generalize this performance to\nsentences formed from a novel vocabulary. This, he claims, is a sign\nthat connectionist models merely associate instances, and are unable\nto truly master abstract rules. On the other hand, Phillips (2002)\nargues that classical architectures are no better off in this respect.\nThe purported inability of connectionist models to generalize\nperformance in this way has become an important theme in the\nsystematicity debate. (See Section 7 below.)\n\nA somewhat different concern about the adequacy of connectionist\nlanguage processing focuses on tasks that mimic infant learning of\nsimple artificial grammars. Data on reaction time confirms that\ninfants can learn to distinguish well-formed from ill-formed sentences\nin a novel language created by experimenters. Shultz and Bale (2001)\nreport success in training neural nets on the same task. Vilcu and\nHadley (2005) object that this work fails to demonstrate true\nacquisition of the grammar, but see Shultz and Bale (2006) for a\ndetailed reply.\n",
    "section_title": "3. Samples of What Neural Networks Can Do",
    "entry_title": "Connectionism",
    "hierarchy_title": "Connectionism || Samples of What Neural Networks Can Do",
    "tokenized_text": [
        "sample",
        "neural",
        "network",
        "sample",
        "neural",
        "network",
        "connectionists",
        "made",
        "significant",
        "progress",
        "demonstrating",
        "power",
        "neural",
        "network",
        "master",
        "cognitive",
        "task",
        "three",
        "wellknown",
        "experiment",
        "encouraged",
        "connectionists",
        "believe",
        "neural",
        "network",
        "good",
        "model",
        "human",
        "intelligence",
        "one",
        "attractive",
        "effort",
        "sejnowski",
        "rosenberg",
        "work",
        "net",
        "read",
        "english",
        "text",
        "called",
        "nettalk",
        "training",
        "set",
        "nettalk",
        "large",
        "data",
        "base",
        "consisting",
        "english",
        "text",
        "coupled",
        "corresponding",
        "phonetic",
        "output",
        "written",
        "code",
        "suitable",
        "use",
        "speech",
        "synthesizer",
        "tape",
        "nettalk",
        "performance",
        "different",
        "stage",
        "training",
        "interesting",
        "listening",
        "first",
        "output",
        "random",
        "noise",
        "later",
        "net",
        "sound",
        "like",
        "babbling",
        "later",
        "still",
        "though",
        "speaking",
        "english",
        "doubletalk",
        "speech",
        "formed",
        "sound",
        "resemble",
        "english",
        "word",
        "end",
        "training",
        "nettalk",
        "fairly",
        "good",
        "job",
        "pronouncing",
        "text",
        "given",
        "furthermore",
        "ability",
        "generalizes",
        "fairly",
        "well",
        "text",
        "presented",
        "training",
        "set",
        "another",
        "influential",
        "early",
        "connectionist",
        "model",
        "net",
        "trained",
        "rumelhart",
        "mcclelland",
        "predict",
        "past",
        "tense",
        "english",
        "verb",
        "task",
        "interesting",
        "although",
        "verb",
        "english",
        "regular",
        "verb",
        "form",
        "past",
        "tense",
        "adding",
        "suffix",
        "ed",
        "many",
        "frequently",
        "verb",
        "irregular",
        "come",
        "came",
        "go",
        "went",
        "net",
        "first",
        "trained",
        "set",
        "containing",
        "large",
        "number",
        "irregular",
        "verb",
        "later",
        "set",
        "verb",
        "containing",
        "mostly",
        "regular",
        "net",
        "learned",
        "past",
        "tense",
        "verb",
        "round",
        "training",
        "generalized",
        "fairly",
        "well",
        "verb",
        "training",
        "set",
        "even",
        "showed",
        "good",
        "appreciation",
        "regularity",
        "found",
        "among",
        "irregular",
        "verb",
        "send",
        "sent",
        "build",
        "built",
        "blow",
        "blew",
        "fly",
        "flew",
        "learning",
        "system",
        "exposed",
        "training",
        "set",
        "containing",
        "regular",
        "verb",
        "tendency",
        "overregularize",
        "ie",
        "combine",
        "irregular",
        "regular",
        "form",
        "break",
        "broked",
        "instead",
        "break",
        "broke",
        "corrected",
        "training",
        "interesting",
        "note",
        "child",
        "known",
        "exhibit",
        "tendency",
        "overregularize",
        "language",
        "learning",
        "however",
        "hot",
        "debate",
        "whether",
        "rumelhart",
        "mcclelland",
        "good",
        "model",
        "human",
        "actually",
        "learn",
        "process",
        "verb",
        "ending",
        "example",
        "pinker",
        "prince",
        "point",
        "model",
        "poor",
        "job",
        "generalizing",
        "novel",
        "regular",
        "verb",
        "believe",
        "sign",
        "basic",
        "failing",
        "connectionist",
        "model",
        "net",
        "may",
        "good",
        "making",
        "association",
        "matching",
        "pattern",
        "fundamental",
        "limitation",
        "mastering",
        "general",
        "rule",
        "formation",
        "regular",
        "past",
        "tense",
        "complaint",
        "raise",
        "important",
        "issue",
        "connectionist",
        "modeler",
        "namely",
        "whether",
        "net",
        "generalize",
        "properly",
        "master",
        "cognitive",
        "task",
        "involving",
        "rule",
        "despite",
        "pinker",
        "prince",
        "objection",
        "many",
        "connectionists",
        "believe",
        "generalization",
        "right",
        "kind",
        "still",
        "possible",
        "niklasson",
        "van",
        "gelder",
        "elman",
        "work",
        "net",
        "appreciate",
        "grammatical",
        "structure",
        "important",
        "implication",
        "debate",
        "whether",
        "neural",
        "network",
        "learn",
        "master",
        "rule",
        "elman",
        "trained",
        "simple",
        "recurrent",
        "network",
        "predict",
        "next",
        "word",
        "large",
        "corpus",
        "english",
        "sentence",
        "sentence",
        "formed",
        "simple",
        "vocabulary",
        "word",
        "using",
        "subset",
        "english",
        "grammar",
        "grammar",
        "though",
        "simple",
        "posed",
        "hard",
        "test",
        "linguistic",
        "awareness",
        "allowed",
        "unlimited",
        "formation",
        "relative",
        "clause",
        "demanding",
        "agreement",
        "head",
        "noun",
        "verb",
        "example",
        "sentence",
        "man",
        "chase",
        "dog",
        "chase",
        "cat",
        "run",
        "singular",
        "man",
        "must",
        "agree",
        "verb",
        "run",
        "despite",
        "intervening",
        "plural",
        "noun",
        "dog",
        "cat",
        "might",
        "cause",
        "selection",
        "run",
        "one",
        "important",
        "feature",
        "elman",
        "model",
        "use",
        "recurrent",
        "connection",
        "value",
        "hidden",
        "unit",
        "saved",
        "set",
        "called",
        "context",
        "unit",
        "sent",
        "back",
        "input",
        "level",
        "next",
        "round",
        "processing",
        "looping",
        "back",
        "hidden",
        "input",
        "layer",
        "provides",
        "net",
        "rudimentary",
        "form",
        "memory",
        "sequence",
        "word",
        "input",
        "sentence",
        "elman",
        "net",
        "displayed",
        "appreciation",
        "grammatical",
        "structure",
        "sentence",
        "training",
        "set",
        "net",
        "command",
        "syntax",
        "measured",
        "following",
        "way",
        "predicting",
        "next",
        "word",
        "english",
        "sentence",
        "course",
        "impossible",
        "task",
        "however",
        "net",
        "succeeded",
        "least",
        "following",
        "measure",
        "given",
        "point",
        "input",
        "sentence",
        "output",
        "unit",
        "word",
        "grammatical",
        "continuation",
        "sentence",
        "point",
        "active",
        "output",
        "unit",
        "word",
        "inactive",
        "intensive",
        "training",
        "elman",
        "able",
        "produce",
        "net",
        "displayed",
        "perfect",
        "performance",
        "measure",
        "including",
        "sentence",
        "training",
        "set",
        "work",
        "christiansen",
        "chater",
        "a",
        "morris",
        "cottrell",
        "elman",
        "extends",
        "research",
        "complex",
        "grammar",
        "broader",
        "view",
        "progress",
        "connectionist",
        "natural",
        "language",
        "processing",
        "see",
        "summary",
        "christiansen",
        "chater",
        "b",
        "rohde",
        "plaut",
        "although",
        "performance",
        "impressive",
        "still",
        "long",
        "way",
        "go",
        "training",
        "net",
        "process",
        "language",
        "like",
        "english",
        "furthermore",
        "doubt",
        "raised",
        "significance",
        "elman",
        "result",
        "example",
        "marcus",
        "argues",
        "elman",
        "net",
        "able",
        "generalize",
        "performance",
        "sentence",
        "formed",
        "novel",
        "vocabulary",
        "claim",
        "sign",
        "connectionist",
        "model",
        "merely",
        "associate",
        "instance",
        "unable",
        "truly",
        "master",
        "abstract",
        "rule",
        "hand",
        "phillips",
        "argues",
        "classical",
        "architecture",
        "better",
        "respect",
        "purported",
        "inability",
        "connectionist",
        "model",
        "generalize",
        "performance",
        "way",
        "become",
        "important",
        "theme",
        "systematicity",
        "debate",
        "see",
        "section",
        "somewhat",
        "different",
        "concern",
        "adequacy",
        "connectionist",
        "language",
        "processing",
        "focus",
        "task",
        "mimic",
        "infant",
        "learning",
        "simple",
        "artificial",
        "grammar",
        "data",
        "reaction",
        "time",
        "confirms",
        "infant",
        "learn",
        "distinguish",
        "wellformed",
        "illformed",
        "sentence",
        "novel",
        "language",
        "created",
        "experimenter",
        "shultz",
        "bale",
        "report",
        "success",
        "training",
        "neural",
        "net",
        "task",
        "vilcu",
        "hadley",
        "object",
        "work",
        "fails",
        "demonstrate",
        "true",
        "acquisition",
        "grammar",
        "see",
        "shultz",
        "bale",
        "detailed",
        "reply"
    ]
}