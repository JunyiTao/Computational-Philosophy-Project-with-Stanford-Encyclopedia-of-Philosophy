{
    "url": "logic-classical",
    "title": "Classical Logic",
    "authorship": {
        "year": "Copyright \u00a9 2022",
        "author_text": "Stewart Shapiro\n<shapiro.4@osu.edu>\nTeresa Kouri Kissel\n<tkouri@odu.edu>",
        "author_links": [
            {
                "https://philosophy.osu.edu/people/shapiro.4/": "Stewart Shapiro"
            },
            {
                "mailto:shapiro%2e4%40osu%2eedu": "shapiro.4@osu.edu"
            },
            {
                "https://sites.google.com/site/teresakouri/": "Teresa Kouri Kissel"
            },
            {
                "mailto:tkouri%40odu%2eedu": "tkouri@odu.edu"
            }
        ],
        "raw_html": "<div id=\"article-copyright\">\n<p>\n<a href=\"../../info.html#c\">Copyright \u00a9 2022</a> by\n\n<br/>\n<a href=\"https://philosophy.osu.edu/people/shapiro.4/\" target=\"other\">Stewart Shapiro</a>\n&lt;<a href=\"mailto:shapiro%2e4%40osu%2eedu\"><em>shapiro<abbr title=\" dot \">.</abbr>4<abbr title=\" at \">@</abbr>osu<abbr title=\" dot \">.</abbr>edu</em></a>&gt;<br/>\n<a href=\"https://sites.google.com/site/teresakouri/\" target=\"other\">Teresa Kouri Kissel</a>\n&lt;<a href=\"mailto:tkouri%40odu%2eedu\"><em>tkouri<abbr title=\" at \">@</abbr>odu<abbr title=\" dot \">.</abbr>edu</em></a>&gt;\n    </p>\n</div>"
    },
    "pubinfo": [
        "First published Sat Sep 16, 2000",
        "substantive revision Wed Jun 29, 2022"
    ],
    "preamble": "\n\nTypically, a logic consists of a formal or informal language\ntogether with a deductive system and/or a model-theoretic semantics.\nThe language has components that correspond to a part of a natural\nlanguage like English or Greek. The deductive system is to capture,\ncodify, or simply record arguments that are valid\nfor the given language, and the semantics is to capture, codify, or\nrecord the meanings, or truth-conditions for at least part of the\nlanguage.\n\nThe following sections provide the basics of a typical logic,\nsometimes called \u201cclassical elementary logic\u201d or\n\u201cclassical first-order logic\u201d. Section 2 develops a formal\nlanguage, with a rigorous syntax and grammar. The formal language is a\nrecursively defined collection of strings on a fixed alphabet. As\nsuch, it has no meaning, or perhaps better, the meaning of its\nformulas is given by the deductive system and the semantics. Some of\nthe symbols have counterparts in ordinary language. We define an\nargument to be a non-empty collection of sentences in the\nformal language, one of which is designated to be the conclusion. The\nother sentences (if any) in an argument are its premises. Section 3\nsets up a deductive system for the language, in the spirit of natural\ndeduction. An argument is derivable if there is a deduction\nfrom some or all of its premises to its conclusion. Section 4 provides\na model-theoretic semantics. An argument is valid if there is\nno interpretation (in the semantics) in which its premises are all\ntrue and its conclusion false. This reflects the longstanding view\nthat a valid argument is truth-preserving.\n\nIn Section 5, we turn to relationships between the deductive system\nand the semantics, and in particular, the relationship between\nderivability and validity. We show that an argument is derivable only\nif it is valid. This pleasant feature, called soundness,\nentails that no deduction takes one from true premises to a false\nconclusion. Thus, deductions preserve truth. Then we establish a\nconverse, called completeness, that an argument is valid only\nif it is derivable. This shows that the deductive system is rich\nenough to provide a deduction for every valid argument. So there are\nenough deductions: all and only valid arguments are derivable. We\nbriefly indicate other features of the logic, some of which are\ncorollaries to soundness and completeness.\n\nThe final section, Section 6, is devoted to the a brief examination of\nthe philosophical position that classical logic is \u201cthe one\nright logic\u201d.\n",
    "toc": [
        {
            "#Intr": "1. Introduction "
        },
        {
            "#Lang": "2. Language"
        },
        {
            "#BuilBloc": "2.1 Building blocks"
        },
        {
            "#AtomForm": "2.2 Atomic formulas "
        },
        {
            "#CompForm": "2.3 Compound formulas"
        },
        {
            "#FeatSynt": "2.4 Features of the syntax "
        },
        {
            "#Dedu": "3. Deduction"
        },
        {
            "#Sema": "4. Semantics"
        },
        {
            "#MetaTheo": "5. Meta-theory"
        },
        {
            "#OneRighLogi": "6. The One Right Logic? "
        },
        {
            "#RivaClasFirsOrdeLogi": "6.1 Rivals to classical, first-order logic"
        },
        {
            "#SublClasFirsOrdeLogi": "6.2 Sublogics of classical, first-order logic"
        },
        {
            "#Bib": "Bibliography"
        },
        {
            "#FurtRead": "Further Reading"
        },
        {
            "#Aca": "Academic Tools"
        },
        {
            "#Oth": "Other Internet Resources"
        },
        {
            "#Rel": "Related Entries"
        }
    ],
    "main_text": "\n1. Introduction\n\nToday, logic is a branch of mathematics and a branch of philosophy. In\nmost large universities, both departments offer courses in logic, and\nthere is usually a lot of overlap between them. Formal languages,\ndeductive systems, and model-theoretic semantics are mathematical\nobjects and, as such, the logician is interested in their mathematical\nproperties and relations. Soundness, completeness, and most of the\nother results reported below are typical examples. Philosophically,\nlogic is at least closely related to the study of correct\nreasoning. Reasoning is an epistemic, mental activity. So logic\nis at least closely allied with epistemology. Logic is also a central\nbranch of computer science, due, in part, to interesting computational\nrelations in logical systems, and, in part, to the close connection\nbetween formal deductive argumentation and reasoning (see the entries\non\n recursive functions,\n computability and complexity, and\n philosophy of computer science).\n\nThis raises questions concerning the philosophical relevance of the\nvarious mathematical aspects of logic. How do deducibility and\nvalidity, as properties of formal languages \u2013 sets of strings on\na fixed alphabet \u2013 relate to correct reasoning? What do the\nmathematical results reported below have to do with the original\nphilosophical issues concerning valid reasoning? This is an instance\nof the philosophical problem of explaining how mathematics applies to\nnon-mathematical reality.\n\nTypically, ordinary deductive reasoning takes place in a natural\nlanguage, or perhaps a natural language augmented with some\nmathematical symbols. So our question begins with the relationship\nbetween a natural language and a formal language. Without attempting\nto be comprehensive, it may help to sketch several options on this\nmatter.\n\nOne view is that the formal languages accurately exhibit actual\nfeatures of certain fragments of a natural language. Some philosophers\nclaim that declarative sentences of natural language have underlying\nlogical forms and that these forms are displayed by formulas\nof a formal language. Other writers hold that (successful) declarative\nsentences express propositions; and formulas of formal\nlanguages somehow display the forms of these propositions. On views\nlike this, the components of a logic provide the underlying deep\nstructure of correct reasoning. A chunk of reasoning in natural\nlanguage is correct if the forms underlying the sentences constitute a\nvalid or deducible argument. See for example, Montague [1974],\nDavidson [1984], Lycan [1984] (and the entry on\n logical form).\n\nAnother view, held at least in part by Gottlob Frege and Wilhelm\nLeibniz, is that because natural languages are fraught with vagueness\nand ambiguity, they should be replaced by formal languages. A\nsimilar view, held by W. V. O. Quine (e.g., [1960], [1986]), is that a\nnatural language should be regimented, cleaned up for serious\nscientific and metaphysical work. One desideratum of the enterprise is\nthat the logical structures in the regimented language should be\ntransparent. It should be easy to \u201cread off\u201d the logical\nproperties of each sentence. A regimented language is similar to a\nformal language regarding, for example, the explicitly presented rigor\nof its syntax and its truth conditions.\n\nOn a view like this, deducibility and validity represent\nidealizations of correct reasoning in natural language. A\nchunk of reasoning is correct to the extent that it corresponds to, or\ncan be regimented by, a valid or deducible argument in a formal\nlanguage.\n\nWhen mathematicians and many philosophers engage in deductive\nreasoning, they occasionally invoke formulas in a formal language to\nhelp disambiguate, or otherwise clarify what they mean. In other\nwords, sometimes formulas in a formal language are used in\nordinary reasoning. This suggests that one might think of a formal\nlanguage as an addendum to a natural language. Then our\npresent question concerns the relationship between this addendum and\nthe original language. What do deducibility and validity, as sharply\ndefined on the addendum, tell us about correct deductive reasoning in\ngeneral?\n\nAnother view is that a formal language is a mathematical\nmodel of a natural language in roughly the same sense as, say, a\ncollection of point masses is a model of a system of physical objects,\nand the Bohr construction is a model of an atom. In other words, a\nformal language displays certain features of natural languages, or\nidealizations thereof, while ignoring or simplifying other features.\nThe purpose of mathematical models is to shed light on what they are\nmodels of, without claiming that the model is accurate in all respects\nor that the model should replace what it is a model of. On a view like\nthis, deducibility and validity represent mathematical models of\n(perhaps different aspects of) correct reasoning in natural languages.\nCorrect chunks of deductive reasoning correspond, more or less, to\nvalid or deducible arguments; incorrect chunks of reasoning roughly\ncorrespond to invalid or non-deducible arguments. See, for example,\nCorcoran [1973], Shapiro [1998], and Cook [2002].\n\nThere is no need to adjudicate this matter here. Perhaps the truth\nlies in a combination of the above options, or maybe some other option\nis the correct, or most illuminating one. We raise the matter only to\nlend some philosophical perspective to the formal treatment that\nfollows.\n2. Language\n\nHere we develop the basics of a formal language, or to be precise, a\nclass of formal languages. Again, a formal language is a recursively\ndefined set of strings on a fixed alphabet. Some aspects of the formal\nlanguages correspond to, or have counterparts in, natural languages\nlike English. Technically, this \u201ccounterpart relation\u201d is\nnot part of the formal development, but we will mention it from time\nto time, to motivate some of the features and results.\n2.1 Building blocks\n\nWe begin with analogues of singular terms, linguistic items\nwhose function is to denote a person or object. We call these\nterms. We assume a stock of individual constants.\nThese are lower-case letters, near the beginning of the Roman\nalphabet, with or without numerical subscripts: \n\\[\na, a_1, b_{23}, c, d_{22}, \\text{etc}.\n\\]\n\n\nWe envisage a potential infinity of individual constants. In the\npresent system each constant is a single character, and so individual\nconstants do not have an internal syntax. Thus we have an infinite\nalphabet. This could be avoided by taking a constant like \\(d_{22}\\),\nfor example, to consist of three characters, a lowercase\n\u201c\\(d\\)\u201d followed by a pair of subscript\n\u201c2\u201ds.\n\nWe also assume a stock of individual variables. These are\nlower-case letters, near the end of the alphabet, with or without\nnumerical subscripts: \n\\[\nw, x, y_{12}, z,  z_4, \\text{etc}. \n\\]\n\n\nIn ordinary mathematical reasoning, there are two functions terms need\nto fulfill. We need to be able to denote specific, but unspecified (or\narbitrary) objects, and sometimes we need to express generality. In\nour system, we use some constants in the role of unspecified reference\nand variables to express generality. Both uses are recapitulated in\nthe formal treatment below. Some logicians employ different symbols\nfor unspecified objects (sometimes called \u201cindividual\nparameters\u201d) and variables used to express generality. \n\nConstants and variables are the only terms in our formal language, so\nall of our terms are simple, corresponding to proper names and some\nuses of pronouns. We call a term closed if it is not a variable. In\ngeneral, we use \\(v\\) to represent variables, and \\(t\\) to represent a\nclosed term, an individual constant. Some authors also introduce\nfunction letters, which allow complex terms corresponding to:\n\u201c\\(7+4\\)\u201d and \u201cthe wife of Bill Clinton\u201d, or\ncomplex terms containing variables, like \u201cthe father of\n\\(x\\)\u201d and \u201c\\(x/y\\)\u201d. Logic books aimed at\nmathematicians are likely to contain function letters, probably due to\nthe centrality of functions in mathematical discourse. Books aimed at\na more general audience (or at philosophy students), may leave out\nfunction letters, since it simplifies the syntax and theory. We follow\nthe latter route here. This is an instance of a general tradeoff\nbetween presenting a system with greater expressive resources, at the\ncost of making its formal treatment more complex. \n\nFor each natural number \\(n\\), we introduce a stock of \\(n\\)-place\npredicate letters. These are upper-case letters at the\nbeginning or middle of the alphabet. A superscript indicates the\nnumber of places, and there may or may not be a subscript. For\nexample,  \n\\[\nA^3, B^{3}_2,  P^3, \\text{etc}.\n\\]\n\n\nare three-place predicate letters. We often omit the superscript, when\nno confusion will result. We also add a special two-place predicate\nsymbol \u201c\\(=\\)\u201d for identity.\n\nZero-place predicate letters are sometimes called \u201csentence\nletters\u201d. They correspond to free-standing sentences whose\ninternal structure does not matter. One-place predicate letters,\ncalled \u201cmonadic predicate letters\u201d, correspond to\nlinguistic items denoting properties, like \u201cbeing a man\u201d,\n\u201cbeing red\u201d, or \u201cbeing a prime number\u201d.\nTwo-place predicate letters, called \u201cbinary predicate\nletters\u201d, correspond to linguistic items denoting binary\nrelations, like \u201cis a parent of\u201d or \u201cis greater\nthan\u201d. Three-place predicate letters correspond to three-place\nrelations, like \u201clies on a straight line between\u201d. And so\non. \n\nThe non-logical terminology of the language consists of its\nindividual constants and predicate letters. The symbol\n\u201c\\(=\\)\u201d, for identity, is not a non-logical symbol. In\ntaking identity to be logical, we provide explicit treatment for it in\nthe deductive system and in the model-theoretic semantics. Most\nauthors do the same, but there is some controversy over the issue\n(Quine [1986, Chapter 5]). If \\(K\\) is a set of constants and\npredicate letters, then we give the fundamentals of a language\n\\(\\LKe\\) built on this set of non-logical terminology. It may be\ncalled the first-order language with identity on \\(K\\). A\nsimilar language that lacks the symbol for identity (or which takes\nidentity to be non-logical) may be called \\(\\mathcal{L}1K\\), the\nfirst-order language without identity on \\(K\\).\n2.2 Atomic formulas\n\nIf \\(V\\) is an \\(n\\)-place predicate letter in \\(K\\), and \\(t_1,\n\\ldots,t_n\\) are terms of \\(K\\), then \\(Vt_1 \\ldots t_n\\) is an\natomic formula of \\(\\LKe\\). Notice that the terms \\(t_1,\n\\ldots,t_n\\) need not be distinct. Examples of atomic formulas\ninclude: \n\\[\nP^4 xaab, C^1 x, C^1 a, D^0, A^3 abc.\n\\]\n\n\nThe last one is an analogue of a statement that a certain relation\n\\((A)\\) holds between three objects \\((a, b, c)\\). If \\(t_1\\) and\n\\(t_2\\) are terms, then \\(t_1 =t_2\\) is also an atomic formula of\n\\(\\LKe\\). It corresponds to an assertion that \\(t_1\\) is identical to\n\\(t_2\\).\n\nIf an atomic formula has no variables, then it is called an atomic\nsentence. If it does have variables, it is called open.\nIn the above list of examples, the first and second are open; the rest\nare sentences. \n2.3 Compound formulas\n\nWe now introduce the final items of the lexicon: \n\\[\n\\neg, \\amp, \\vee, \\rightarrow, \\forall, \\exists, (, )\n\\]\n\n\nWe give a recursive definition of a formula of \\(\\LKe\\):\n\nAll atomic formulas of \\(\\LKe\\) are formulas of \\(\\LKe\\).\nIf \\(\\theta\\) is a formula of \\(\\LKe\\), then so is \\(\\neg\n\\theta\\).\n\n\nA formula corresponding to \\(\\neg \\theta\\) thus says that it is not\nthe case that \\(\\theta\\). The symbol \u201c\\(\\neg\\)\u201d is called\n\u201cnegation\u201d, and is a unary connective.\n\nIf \\(\\theta\\) and \\(\\psi\\) are formulas of \\(\\LKe\\), then so is\n\\((\\theta \\amp \\psi)\\).\n\n\nThe ampersand \u201c\\(\\amp\\)\u201d corresponds to the English\n\u201cand\u201d (when \u201cand\u201d is used to connect\nsentences). So \\((\\theta \\amp \\psi)\\) can be read \u201c\\(\\theta\\)\nand \\(\\psi\\)\u201d. The formula \\((\\theta \\amp \\psi)\\) is called the\n\u201cconjunction\u201d of \\(\\theta\\) and \\(\\psi\\).\n\nIf \\(\\theta\\) and \\(\\psi\\) are formulas of \\(\\LKe\\), then so is\n\\((\\theta \\vee \\psi)\\). \n\n\nThe symbol \u201c\\(\\vee\\)\u201d corresponds to \u201ceither\n\u2026 or \u2026 or both\u201d, so \\((\\theta \\vee \\psi)\\) can be\nread \u201c\\(\\theta\\) or \\(\\psi\\)\u201d. The formula \\((\\theta \\vee\n\\psi)\\) is called the \u201cdisjunction\u201d of \\(\\theta\\) and\n\\(\\psi\\).\n\n If \\(\\theta\\) and \\(\\psi\\) are formulas of \\(\\LKe\\), then so is\n\\((\\theta \\rightarrow \\psi)\\).\n\n\nThe arrow \u201c\\(\\rightarrow\\)\u201d roughly corresponds to\n\u201cif \u2026 then \u2026 \u201d, so \\((\\theta \\rightarrow\n\\psi)\\) can be read \u201cif \\(\\theta\\) then \\(\\psi\\)\u201d or\n\u201c\\(\\theta\\) only if \\(\\psi\\)\u201d.\n\nThe symbols \u201c\\(\\amp\\)\u201d, \u201c\\(\\vee\\)\u201d, and\n\u201c\\(\\rightarrow\\)\u201d are called \u201cbinary\nconnectives\u201d, since they serve to \u201cconnect\u201d two\nformulas into one. Some authors introduce \\((\\theta \\leftrightarrow\n\\psi)\\) as an abbreviation of \\(((\\theta \\rightarrow \\psi) \\amp(\\psi\n\\rightarrow \\theta))\\). The symbol \u201c\\(\\leftrightarrow\\)\u201d\nis an analogue of the locution \u201cif and only if\u201d.\n\nIf \\(\\theta\\) is a formula of \\(\\LKe\\) and \\(v\\) is a variable,\nthen \\(\\forall v \\theta\\) is a formula of \\(\\LKe\\).\n\n\nThe symbol \u201c\\(\\forall\\)\u201d is called a universal\nquantifier, and is an analogue of \u201cfor all\u201d; so\n\\(\\forall v\\theta\\) can be read \u201cfor all \\(v,\n\\theta\\)\u201d.\n\nIf \\(\\theta\\) is a formula of \\(\\LKe\\) and \\(v\\) is a variable,\nthen \\(\\exists v \\theta\\) is a formula of \\(\\LKe\\).\n\n\nThe symbol \u201c\\(\\exists\\)\u201d is called an existential\nquantifier, and is an analogue of \u201cthere exists\u201d or\n\u201cthere is\u201d; so \\(\\exists v \\theta\\) can be read\n\u201cthere is a \\(v\\) such that \\(\\theta\\)\u201d.\n\nThat\u2019s all folks. That is, all formulas are constructed in\naccordance with rules (1)\u2013(7).\n\n\nClause (8) allows us to do inductions on the complexity of formulas.\nIf a certain property holds of the atomic formulas and is closed under\nthe operations presented in clauses (2)\u2013(7), then the property\nholds of all formulas. Here is a simple example:\n\n\nTheorem 1. Every formula of \\(\\LKe\\) has the same\nnumber of left and right parentheses. Moreover, each left parenthesis\ncorresponds to a unique right parenthesis, which occurs to the right\nof the left parenthesis. Similarly, each right parenthesis corresponds\nto a unique left parenthesis, which occurs to the left of the given\nright parenthesis. If a parenthesis occurs between a matched pair of\nparentheses, then its mate also occurs within that matched pair. In\nother words, parentheses that occur within a matched pair are\nthemselves matched.\n\nProof: By clause (8), every formula is built up from\nthe atomic formulas using clauses (2)\u2013(7). The atomic formulas\nhave no parentheses. Parentheses are introduced only in clauses\n(3)\u2013(5), and each time they are introduced as a matched set. So\nat any stage in the construction of a formula, the parentheses are\npaired off.\n\n\nWe next define the notion of an occurrence of a variable being\nfree or bound in a formula. A variable that\nimmediately follows a quantifier (as in \u201c\\(\\forall x\\)\u201d\nand \u201c\\(\\exists y\\)\u201d) is neither free nor bound. We do not\neven think of those as occurrences of the variable. All variables that\noccur in an atomic formula are free. If a variable occurs free (or\nbound) in \\(\\theta\\) or in \\(\\psi\\), then that same occurrence is free\n(or bound) in \\(\\neg \\theta, (\\theta \\amp \\psi), (\\theta \\vee \\psi)\\),\nand \\((\\theta \\rightarrow \\psi)\\). That is, the (unary and binary)\nconnectives do not change the status of variables that occur in them.\nAll occurrences of the variable \\(v\\) in \\(\\theta\\) are bound in\n\\(\\forall v \\theta\\) and \\(\\exists v \\theta\\). Any free\noccurrences of \\(v\\) in \\(\\theta\\) are bound by the initial\nquantifier. All other variables that occur in \\(\\theta\\) are free or\nbound in \\(\\forall v \\theta\\) and \\(\\exists v \\theta\\), as they are in\n\\(\\theta\\).\n\nFor example, in the formula \\((\\forall\\)x(Axy \\(\\vee Bx) \\amp\nBx)\\), the occurrences of \u201c\\(x\\)\u201d in Axy and in\nthe first \\(Bx\\) are bound by the quantifier. The occurrence of\n\u201c\\(y\\)\u201d and last occurrence of \u201c\\(x\\)\u201d are\nfree. In \\(\\forall x(Ax \\rightarrow \\exists\\)xBx), the\n\u201c\\(x\\)\u201d in \\(Ax\\) is bound by the initial universal\nquantifier, while the other occurrence of \\(x\\) is bound by the\nexistential quantifier. The above syntax allows this\n\u201cdouble-binding\u201d. Although it does not create any\nambiguities (see below), we will avoid such formulas, as a matter of\ntaste and clarity.\n\nThe syntax also allows so-called vacuous binding, as in\n\\(\\forall\\)x\\(Bc\\). These, too, will be avoided in what follows. Some\ntreatments of logic rule out vacuous binding and double binding as a\nmatter of syntax. That simplifies some of the treatments below, and\ncomplicates others.\n\nFree variables correspond to place-holders, while bound variables are\nused to express generality. If a formula has no free variables, then\nit is called a sentence. If a formula has free variables, it\nis called open. \n2.4 Features of the syntax\n\nBefore turning to the deductive system and semantics, we mention a few\nfeatures of the language, as developed so far. This helps draw the\ncontrast between formal languages and natural languages like\nEnglish.\n\nWe assume at the outset that all of the categories are disjoint. For\nexample, no connective is also a quantifier or a variable, and the\nnon-logical terms are not also parentheses or connectives. Also, the\nitems within each category are distinct. For example, the sign for\ndisjunction does not do double-duty as the negation symbol, and\nperhaps more significantly, no two-place predicate is also a one-place\npredicate.\n\nOne difference between natural languages like English and formal\nlanguages like \\(\\LKe\\) is that the latter are not supposed to have\nany ambiguities. The policy that the different categories of symbols\ndo not overlap, and that no symbol does double-duty, avoids the kind\nof ambiguity, sometimes called \u201cequivocation\u201d, that occurs\nwhen a single word has two meanings: \u201cI\u2019ll meet you at the\nbank.\u201d But there are other kinds of ambiguity. Consider the\nEnglish sentence:\n\nJohn is married, and Mary is single, or Joe is crazy. \n\nIt can mean that John is married and either Mary is single or Joe is\ncrazy, or else it can mean that either both John is married and Mary\nis single, or else Joe is crazy. An ambiguity like this, due to\ndifferent ways to parse the same sentence, is sometimes called an\n\u201camphiboly\u201d. If our formal language did not have the\nparentheses in it, it would have amphibolies. For example, there would\nbe a \u201cformula\u201d \\(A \\amp B \\vee\\) C. Is this\nsupposed to be \\(((A \\amp B) \\vee C)\\), or is it \\((A \\amp(B \\vee\nC))\\)? The parentheses resolve what would be an amphiboly.\n\nCan we be sure that there are no other amphibolies in our language?\nThat is, can we be sure that each formula of \\(\\LKe\\) can be put\ntogether in only one way? Our next task is to answer this\nquestion.\n\nLet us temporarily use the term \u201cunary marker\u201d for the\nnegation symbol \\((\\neg)\\) or a quantifier followed by a variable\n(e.g., \\(\\forall x, \\exists z)\\).\n\n\nLemma 2. Each formula consists of a string of zero or\nmore unary markers followed by either an atomic formula or a formula\nproduced using a binary connective, via one of clauses\n(3)\u2013(5).\n\nProof: We proceed by induction on the complexity of\nthe formula or, in other words, on the number of formation rules that\nare applied. The Lemma clearly holds for atomic formulas. Let \\(n\\) be\na natural number, and suppose that the Lemma holds for any formula\nconstructed from \\(n\\) or fewer instances of clauses (2)\u2013(7).\nLet \\(\\theta\\) be a formula constructed from \\(n+1\\) instances. The\nLemma holds if the last clause used to construct \\(\\theta\\) was either\n(3), (4), or (5). If the last clause used to construct \\(\\theta\\) was\n(2), then \\(\\theta\\) is \\(\\neg \\psi\\). Since \\(\\psi\\) was constructed\nwith \\(n\\) instances of the rule, the Lemma holds for \\(\\psi\\) (by the\ninduction hypothesis), and so it holds for \\(\\theta\\). Similar\nreasoning shows the Lemma to hold for \\(\\theta\\) if the last clause\nwas (6) or (7). By clause (8), this exhausts the cases, and so the\nLemma holds for \\(\\theta\\), by induction.\n\nLemma 3. If a formula \\(\\theta\\) contains a left\nparenthesis, then it ends with a right parenthesis, which matches the\nleftmost left parenthesis in \\(\\theta\\).\n\nProof: Here we also proceed by induction on the\nnumber of instances of (2)\u2013(7) used to construct the formula.\nClearly, the Lemma holds for atomic formulas, since they have no\nparentheses. Suppose, then, that the Lemma holds for formulas\nconstructed with \\(n\\) or fewer instances of (2)\u2013(7), and let\n\\(\\theta\\) be constructed with \\(n+1\\) instances. If the last clause\napplied was (3)\u2013(5), then the Lemma holds since \\(\\theta\\)\nitself begins with a left parenthesis and ends with the matching right\nparenthesis. If the last clause applied was (2), then \\(\\theta\\) is\n\\(\\neg \\psi\\), and the induction hypothesis applies to \\(\\psi\\).\nSimilarly, if the last clause applied was (6) or (7), then \\(\\theta\\)\nconsists of a quantifier, a variable, and a formula to which we can\napply the induction hypothesis. It follows that the Lemma holds for\n\\(\\theta\\).\n\nLemma 4. Each formula contains at least one atomic\nformula.\n\n\nThe proof proceeds by induction on the number of instances of\n(2)\u2013(7) used to construct the formula, and we leave it as an\nexercise. \n\n\nTheorem 5. Let \\(\\alpha, \\beta\\) be nonempty\nsequences of characters on our alphabet, such that \\(\\alpha \\beta\\)\n(i.e \\(\\alpha\\) followed by \\(\\beta)\\) is a formula. Then \\(\\alpha\\)\nis not a formula.\n\nProof: By Theorem 1 and Lemma 3, if \\(\\alpha\\)\ncontains a left parenthesis, then the right parenthesis that matches\nthe leftmost left parenthesis in \\(\\alpha \\beta\\) comes at the end of\n\\(\\alpha \\beta\\), and so the matching right parenthesis is in\n\\(\\beta\\). So, \\(\\alpha\\) has more left parentheses than right\nparentheses. By Theorem \\(1, \\alpha\\) is not a formula. So now suppose\nthat \\(\\alpha\\) does not contain any left parentheses. By Lemma \\(2,\n\\alpha \\beta\\) consists of a string of zero or more unary markers\nfollowed by either an atomic formula or a formula produced using a\nbinary connective, via one of clauses (3)\u2013(5). If the latter\nformula was produced via one of clauses (3)\u2013(5), then it begins\nwith a left parenthesis. Since \\(\\alpha\\) does not contain any\nparentheses, it must be a string of unary markers. But then \\(\\alpha\\)\ndoes not contain any atomic formulas, and so by Lemma \\(4, \\alpha\\) is\nnot a formula. The only case left is where \\(\\alpha \\beta\\) consists\nof a string of unary markers followed by an atomic formula, either in\nthe form \\(t_1 =t_2\\) or \\(Pt_1 \\ldots t_n\\). Again, if \\(\\alpha\\)\njust consisted of unary markers, it would not be a formula, and so\n\\(\\alpha\\) must consist of the unary markers that start \\(\\alpha\n\\beta\\), followed by either \\(t_1\\) by itself, \\(t_1 =\\) by itself, or\nthe predicate letter \\(P\\), and perhaps some (but not all) of the\nterms \\(t_1, \\ldots,t_n\\). In the first two cases, \\(\\alpha\\) does not\ncontain an atomic formula, by the policy that the categories do not\noverlap. Since \\(P\\) is an \\(n\\)-place predicate letter, by the policy\nthat the predicate letters are distinct, \\(P\\) is not an \\(m\\)-place\npredicate letter for any \\(m \\ne n\\). So the part of \\(\\alpha\\) that\nconsists of \\(P\\) followed by the terms is not an atomic formula. In\nall of these cases, then, \\(\\alpha\\) does not contain an atomic\nformula. By Lemma \\(4, \\alpha\\) is not a formula.\n\n\nWe are finally in position to show that there is no amphiboly in our\nlanguage.\n\n\nTheorem 6. Let \\(\\theta\\) be any formula of \\(\\LKe\\).\nIf \\(\\theta\\) is not atomic, then there is one and only one among\n(2)\u2013(7) that was the last clause applied to construct\n\\(\\theta\\). That is, \\(\\theta\\) could not be produced by two different\nclauses. Moreover, no formula produced by clauses (2)\u2013(7) is\natomic.\n\nProof: By Clause (8), either \\(\\theta\\) is atomic or\nit was produced by one of clauses (2)\u2013(7). Thus, the first\nsymbol in \\(\\theta\\) must be either a predicate letter, a term, a\nunary marker, or a left parenthesis. If the first symbol in \\(\\theta\\)\nis a predicate letter or term, then \\(\\theta\\) is atomic. In this\ncase, \\(\\theta\\) was not produced by any of (2)\u2013(7), since all\nsuch formulas begin with something other than a predicate letter or\nterm. If the first symbol in \\(\\theta\\) is a negation sign\n\u201c\\(\\neg\\)\u201d, then was \\(\\theta\\) produced by clause (2),\nand not by any other clause (since the other clauses produce formulas\nthat begin with either a quantifier or a left parenthesis). Similarly,\nif \\(\\theta\\) begins with a universal quantifier, then it was produced\nby clause (6), and not by any other clause, and if \\(\\theta\\) begins\nwith an existential quantifier, then it was produced by clause (7),\nand not by any other clause. The only case left is where \\(\\theta\\)\nbegins with a left parenthesis. In this case, it must have been\nproduced by one of (3)\u2013(5), and not by any other clause. We only\nneed to rule out the possibility that \\(\\theta\\) was produced by more\nthan one of (3)\u2013(5). To take an example, suppose that \\(\\theta\\)\nwas produced by (3) and (4). Then \\(\\theta\\) is \\((\\psi_1 \\amp\n\\psi_2)\\) and \\(\\theta\\) is also \\((\\psi_3 \\vee \\psi_4)\\), where\n\\(\\psi_1, \\psi_2, \\psi_3\\), and \\(\\psi_4\\) are themselves formulas.\nThat is, \\((\\psi_1 \\amp \\psi_2)\\) is the very same formula as\n\\((\\psi_3 \\vee \\psi_4)\\). By Theorem \\(5, \\psi_1\\) cannot be a proper\npart of \\(\\psi_3\\), nor can \\(\\psi_3\\) be a proper part of \\(\\psi_1\\).\nSo \\(\\psi_1\\) must be the same formula as \\(\\psi_3\\). But then\n\u201c\\(\\amp\\)\u201d must be the same symbol as\n\u201c\\(\\vee\\)\u201d, and this contradicts the policy that all of\nthe symbols are different. So \\(\\theta\\) was not produced by both\nClause (3) and Clause (4). Similar reasoning takes care of the other\ncombinations.\n\n\nThis result is sometimes called \u201cunique readability\u201d. It\nshows that each formula is produced from the atomic formulas via the\nvarious clauses in exactly one way. If \\(\\theta\\) was produced by\nclause (2), then its main connective is the initial\n\u201c\\(\\neg\\)\u201d. If \\(\\theta\\) was produced by clauses (3),\n(4), or (5), then its main connective is the introduced\n\u201c\\(\\amp\\)\u201d, \u201c\\(\\vee\\)\u201d, or\n\u201c\\(\\rightarrow\\)\u201d, respectively. If \\(\\theta\\) was\nproduced by clauses (6) or (7), then its main connective is\nthe initial quantifier. We apologize for the tedious details. We\nincluded them to indicate the level of precision and rigor for the\nsyntax.\n3. Deduction\n\nWe now introduce a deductive system, \\(D\\), for our\nlanguages. As above, we define an argument to be a non-empty\ncollection of sentences in the formal language, one of which is\ndesignated to be the conclusion. If there are any other\nsentences in the argument, they are its\n premises.[1]\n By convention, we use \u201c\\(\\Gamma\\)\u201d,\n\u201c\\(\\Gamma'\\)\u201d, \u201c\\(\\Gamma_1\\)\u201d, etc, to range\nover sets of sentences, and we use the letters \u201c\\(\\phi\\)\u201d,\n\u201c\\(\\psi\\)\u201d, \u201c\\(\\theta\\)\u201d, uppercase or\nlowercase, with or without subscripts, to range over single sentences.\nWe write \u201c\\(\\Gamma, \\Gamma'\\)\u201d for the union of \\(\\Gamma\\)\nand \\(\\Gamma'\\), and \u201c\\(\\Gamma, \\phi\\)\u201d for the union of\n\\(\\Gamma\\) with \\(\\{\\phi\\}\\).\n\nWe write an argument in the form \\(\\langle \\Gamma, \\phi \\rangle\\),\nwhere \\(\\Gamma\\) is a set of sentences, the premises, and \\(\\phi\\) is\na single sentence, the conclusion. Remember that \\(\\Gamma\\) may be\nempty. We write \\(\\Gamma \\vdash \\phi\\) to indicate that \\(\\phi\\) is\ndeducible from \\(\\Gamma\\), or, in other words, that the argument\n\\(\\langle \\Gamma, \\phi \\rangle\\) is deducible in \\(D\\). We may write\n\\(\\Gamma \\vdash_D \\phi\\) to emphasize the deductive system \\(D\\). We\nwrite \\(\\vdash \\phi\\) or \\(\\vdash_D \\phi\\) to indicate that \\(\\phi\\)\ncan be deduced (in \\(D)\\) from the empty set of premises.\n\nThe rules in \\(D\\) are chosen to match logical relations concerning\nthe English analogues of the logical terminology in the language.\nAgain, we define the deducibility relation by recursion. We start with\na rule of assumptions: \n\n(As)\nIf \\(\\phi\\) is a member of \\(\\Gamma\\), then \\(\\Gamma \\vdash\n\\phi\\).\n\n\nWe thus have that \\(\\{\\phi \\}\\vdash \\phi\\); each premise follows from\nitself. We next present two clauses for each connective and\nquantifier. The clauses indicate how to \u201cintroduce\u201d and\n\u201celiminate\u201d sentences in which each symbol is the main\nconnective.\n\nFirst, recall that \u201c\\(\\amp\\)\u201d is an analogue of the\nEnglish connective \u201cand\u201d. Intuitively, one can deduce a\nsentence in the form \\((\\theta \\amp \\psi)\\) if one has deduced\n\\(\\theta\\) and one has deduced \\(\\psi\\). Conversely, one can deduce\n\\(\\theta\\) from \\((\\theta \\amp \\psi)\\) and one can deduce \\(\\psi\\)\nfrom \\((\\theta \\amp \\psi)\\):\n\n\\((\\amp \\mathrm{I})\\)\nIf \\(\\Gamma_1 \\vdash \\theta\\) and \\(\\Gamma_2 \\vdash \\psi\\), then\n\\(\\Gamma_1, \\Gamma_2 \\vdash(\\theta \\amp \\psi)\\). \n\\((\\amp \\mathrm{E})\\)\nIf \\(\\Gamma \\vdash(\\theta \\amp \\psi)\\) then \\(\\Gamma \\vdash\n\\theta\\); and if \\(\\Gamma \\vdash(\\theta \\amp \\psi)\\) then \\(\\Gamma\n\\vdash \\psi\\).\n\n\nThe name \u201c&I\u201d stands for\n\u201c&-introduction\u201d; \u201c&E\u201d stands for\n\u201c&-elimination\u201d.\n\nSince, the symbol \u201c\\(\\vee\\)\u201d corresponds to the English\n\u201cor\u201d, \\((\\theta \\vee \\psi)\\) should be deducible from\n\\(\\theta\\), and \\((\\theta \\vee \\psi)\\) should also be deducible from\n\\(\\psi\\): \n\n\\((\\vee \\mathrm{I})\\) \nIf \\(\\Gamma \\vdash \\theta\\) then \\(\\Gamma \\vdash(\\theta \\vee\n\\psi)\\); if \\(\\Gamma \\vdash \\psi\\) then \\(\\Gamma \\vdash(\\theta \\vee\n\\psi)\\).\n\n\nThe elimination rule is a bit more complicated. Suppose that\n\u201c\\(\\theta\\) or \\(\\psi\\)\u201d is true. Suppose also that\n\\(\\phi\\) follows from \\(\\theta\\) and that \\(\\phi\\) follows from\n\\(\\psi\\). One can reason that if \\(\\theta\\) is true, then \\(\\phi\\) is\ntrue. If instead \\(\\psi\\) is true, we still have that \\(\\phi\\) is\ntrue. So either way, \\(\\phi\\) must be true.\n\n\\((\\vee \\mathrm{E})\\) \nIf \\(\\Gamma_1 \\vdash(\\theta \\vee \\psi), \\Gamma_2, \\theta \\vdash\n\\phi\\) and \\(\\Gamma_3, \\psi \\vdash \\phi\\), then \\(\\Gamma_1, \\Gamma_2,\n\\Gamma_3 \\vdash \\phi\\). \n\n\nFor the next clauses, recall that the symbol,\n\u201c\\(\\rightarrow\\)\u201d, is an analogue of the English \u201cif\n\u2026 then \u2026 \u201d construction. If one knows, or assumes\n\\((\\theta \\rightarrow \\psi)\\) and also knows, or assumes \\(\\theta\\),\nthen one can conclude \\(\\psi\\). Conversely, if one deduces \\(\\psi\\)\nfrom an assumption \\(\\theta\\), then one can conclude that \\((\\theta\n\\rightarrow \\psi)\\).\n\n\\(({\\rightarrow}\\mathrm{I})\\) \nIf \\(\\Gamma, \\theta \\vdash \\psi\\), then \\(\\Gamma \\vdash(\\theta\n\\rightarrow \\psi)\\). \n\\(({\\rightarrow}\\mathrm{E})\\) \nIf \\(\\Gamma_1 \\vdash(\\theta \\rightarrow \\psi)\\) and \\(\\Gamma_2\n\\vdash \\theta\\), then \\(\\Gamma_1, \\Gamma_2 \\vdash \\psi\\).\n\n\nThis elimination rule is sometimes called \u201cmodus ponens\u201d.\nIn some logic texts, the introduction rule is proved as a\n\u201cdeduction theorem\u201d.\n\nOur next clauses are for the negation sign, \u201c\\(\\neg\\)\u201d.\nThe underlying idea is that a sentence \\(\\psi\\) is inconsistent with\nits negation \\(\\neg \\psi\\). They cannot both be true. We call a pair\nof sentences \\(\\psi, \\neg \\psi\\) contradictory opposites. If\none can deduce such a pair from an assumption \\(\\theta\\), then one can\nconclude that \\(\\theta\\) is false, or, in other words, one can\nconclude \\(\\neg \\theta\\).\n\n\\((\\neg \\mathrm{I})\\)\nIf \\(\\Gamma_1, \\theta \\vdash \\psi\\) and \\(\\Gamma_2, \\theta \\vdash\n\\neg \\psi\\), then \\(\\Gamma_1, \\Gamma_2 \\vdash \\neg \\theta\\). \n\n\nBy (As), we have that \\(\\{A,\\neg A\\}\\vdash A\\) and\n\\(\\{\\)A,\\(\\neg\\)A\\(\\}\\vdash \\neg A\\). So by \\(\\neg\\)I we have\nthat \\(\\{A\\}\\vdash \\neg \\neg A\\). However, we do not have the converse\nyet. Intuitively, \\(\\neg \\neg \\theta\\) corresponds to \u201cit is not\nthe case that it is not the case that\u201d . One might think that\nthis last is equivalent to \\(\\theta\\), and we have a rule to that\neffect: \n\n(DNE)\nIf \\(\\Gamma \\vdash \\neg \\neg \\theta\\), then \\(\\Gamma \\vdash\n\\theta\\).\n\n\nThe name DNE stands for \u201cdouble-negation elimination\u201d.\nThere is some controversy over this inference. It is rejected by\nphilosophers and mathematicians who do not hold that each meaningful\nsentence is either true or not true. Intuitionistic logic\ndoes not sanction the inference in question (see, for example Dummett\n[2000], or the entry on\n intuitionistic logic,\n or\n history of intuitionistic logic),\n but, again, classical logic does.\n\nTo illustrate the parts of the deductive system \\(D\\) presented thus\nfar, we show that \\(\\vdash(A \\vee \\neg A)\\):\n\n\\(\\{\\neg(A \\vee \\neg A), A\\}\\vdash \\neg(A \\vee \\neg A)\\), by\n(As)\n\\(\\{\\neg(A \\vee \\neg A), A\\}\\vdash A\\), by (As).\n\\(\\{\\neg(A \\vee \\neg A), A\\}\\vdash(A \\vee \\neg A)\\), by\n\\((\\vee\\)I), from (ii).\n\\(\\{\\neg(A \\vee \\neg A)\\}\\vdash \\neg A\\), by \\((\\neg\\)I), from (i)\nand (iii).\n\\(\\{\\neg(A \\vee \\neg A), \\neg A\\}\\vdash \\neg(A \\vee \\neg A)\\), by\n(As)\n\\(\\{\\neg(A \\vee \\neg A), \\neg A\\}\\vdash \\neg A\\), by (As)\n\\(\\{\\neg(A \\vee \\neg A), \\neg A\\}\\vdash(A \\vee \\neg A)\\), by\n\\((\\vee\\)I), from (vi).\n\\(\\{\\neg(A \\vee \\neg A)\\}\\vdash \\neg \\neg A\\), by \\((\\neg\\)I),\nfrom (v) and (vii).\n\\(\\vdash \\neg \\neg(A \\vee \\neg A)\\), by \\((\\neg\\)I), from (iv) and\n(viii).\n\\(\\vdash(A \\vee \\neg A)\\), by (DNE), from (ix).\n\n\nThe principle \\((\\theta \\vee \\neg \\theta)\\) is sometimes called the\nlaw of excluded middle. It is not valid in intuitionistic\nlogic.\n\nLet \\(\\theta, \\neg \\theta\\) be a pair of contradictory opposites, and\nlet \\(\\psi\\) be any sentence at all. By (As) we have \\(\\{\\theta, \\neg\n\\theta, \\neg \\psi \\}\\vdash \\theta\\) and \\(\\{\\theta, \\neg \\theta, \\neg\n\\psi \\}\\vdash \\neg \\theta\\). So by \\((\\neg\\)I), \\(\\{\\theta, \\neg\n\\theta \\}\\vdash \\neg \\neg \\psi\\). So, by (DNE) we have \\(\\{\\theta ,\n\\neg \\theta \\}\\vdash \\psi\\) . That is, anything at all follows from a\npair of contradictory opposites. Some logicians introduce a rule to\ncodify a similar inference:\n\nIf \\(\\Gamma_1 \\vdash \\theta\\) and \\(\\Gamma_2 \\vdash \\neg \\theta\\),\nthen for any sentence \\(\\psi, \\Gamma_1, \\Gamma_2 \\vdash \\psi\\) \n\nThe inference is sometimes called ex falso quodlibet or, more\ncolorfully, explosion. Some call it\n\u201c\\(\\neg\\)-elimination\u201d, but perhaps this stretches the\nnotion of \u201celimination\u201d a bit. We do not officially\ninclude ex falso quodlibet as a separate rule in \\(D\\), but\nas will be shown below (Theorem 10), each instance of it is derivable\nin our system \\(D\\).\n\nSome logicians object to ex falso quodlibet, on the ground\nthat the sentence \\(\\psi\\) may be irrelevant to any of the\npremises in \\(\\Gamma\\). Suppose, for example, that one starts with\nsome premises \\(\\Gamma\\) about human nature and facts about certain\npeople, and then deduces both the sentence \u201cClinton had\nextra-marital sexual relations\u201d and \u201cClinton did not have\nextra-marital sexual relations\u201d. One can perhaps conclude that\nthere is something wrong with the premises \\(\\Gamma\\). But should we\nbe allowed to then deduce anything at all from \\(\\Gamma\\)?\nShould we be allowed to deduce \u201cThe economy is sound\u201d?\n\nA small minority of logicians, called dialetheists, hold that\nsome contradictions are actually true. For them, ex falso\nquodlibet is not truth-preserving.\n\nDeductive systems that demur from ex falso quodlibet are\ncalled paraconsistent. Most relevant logics are\nparaconsistent. See the entries on\n relevance logic,\n paraconsistent logic, and\n dialetheism.\n Or see Anderson and Belnap [1975], Anderson, Belnap, and Dunn [1992],\nand Tennant [1997] for fuller overviews of relevant logic; and Priest\n[2006a,b], for dialetheism. Deep philosophical issues concerning the\nnature of\n logical consequence\n are involved. Far be it for an article in a philosophy encyclopedia\nto avoid philosophical issues, but space considerations preclude a\nfuller treatment of this issue here. Suffice it to note that the\ninference ex falso quodlibet is sanctioned in systems of\nclassical logic, the subject of this article. It is essential\nto establishing the balance between the deductive system and the\nsemantics (see \u00a75 below).\n\nThe next pieces of \\(D\\) are the clauses for the quantifiers. Let\n\\(\\theta\\) be a formula, \\(v\\) a variable, and \\(t\\) a term (i.e., a\nvariable or a constant). Then define \\(\\theta(v|t)\\) to be the result\nof substituting \\(t\\) for each free occurrence of \\(v\\) in\n\\(\\theta\\). So, if \\(\\theta\\) is \\((Qx \\amp \\exists\\)xPxy),\nthen \\(\\theta(x|c)\\) is \\((Qc \\amp \\exists\\)xPxy). The last\noccurrence of \\(x\\) is not free.\n\nA sentence in the form \\(\\forall v \\theta\\) is an analogue of the\nEnglish \u201cfor every \\(v, \\theta\\) holds\u201d. So one should be\nable to infer \\(\\theta(v|t)\\) from \\(\\forall v \\theta\\) for any closed\nterm \\(t\\). Recall that the only closed terms in our system are\nconstants.\n\n\\((\\forall \\mathrm{E})\\)\nIf \\(\\Gamma \\vdash \\forall v \\theta\\), then \\(\\Gamma \\vdash\n\\theta(v|t)\\), for any closed term \\(t\\).\n\n\nThe idea here is that if \\(\\forall v \\theta\\) is true, then \\(\\theta\\)\nshould hold of \\(t\\), no matter what \\(t\\) is.\n\nThe introduction clause for the universal quantifier is a bit more\ncomplicated. Suppose that a sentence \\(\\theta\\) contains a closed term\n\\(t\\), and that \\(\\theta\\) has been deduced from a set of premises\n\\(\\Gamma\\). If the closed term \\(t\\) does not occur in any member of\n\\(\\Gamma\\), then \\(\\theta\\) will hold no matter which object \\(t\\) may\ndenote. That is, \\(\\forall v \\theta\\) follows. \n\n\\((\\forall \\mathrm{I})\\) \nFor any closed term \\(t\\), if \\(\\Gamma\\vdash\\theta (v|t)\\), then\n\\(\\Gamma\\vdash\\forall v\\theta\\) provided that \\(t\\) is not in\n\\(\\Gamma\\) or \\(\\theta\\). \n\n\nThis rule \\((\\forall \\mathbf{I})\\) corresponds to a common inference\nin mathematics. Suppose that a mathematician says \u201clet \\(n\\) be\na natural number\u201d and goes on to show that \\(n\\) has a certain\nproperty \\(P\\), without assuming anything about \\(n\\) (except that it\nis a natural number). She then reminds the reader that \\(n\\) is\n\u201carbitrary\u201d, and concludes that \\(P\\) holds for\nall natural numbers. The condition that the term \\(t\\) not\noccur in any premise is what guarantees that it is indeed\n\u201carbitrary\u201d. It could be any object, and so anything we\nconclude about it holds for all objects. \n\nThe existential quantifier is an analogue of the English expression\n\u201cthere exists\u201d, or perhaps just \u201cthere is\u201d. If\nwe have established (or assumed) that a given object \\(t\\) has a given\nproperty, then it follows that there is something that has that\nproperty. \n\n\\((\\exists \\mathrm{I})\\)\nFor any closed term \\(t\\), if \\(\\Gamma\\vdash\\theta (v|t)\\) then\n\\(\\Gamma\\vdash\\exists v\\theta\\). \n\n\nThe elimination rule for \\(\\exists\\) is not quite as simple:\n\n\\((\\exists \\mathrm{E})\\)\nFor any closed term \\(t\\), if \\(\\Gamma_1\\vdash\\exists v\\theta\\)\nand \\(\\Gamma_2, \\theta(v|t)\\vdash\\phi\\), then \\(\\Gamma_1\n,\\Gamma_2\\vdash\\phi\\), provided that \\(t\\) does not occur in \\(\\phi\\),\n\\(\\Gamma_2\\) or \\(\\theta\\).\n\n\nThis elimination rule also corresponds to a common inference. Suppose\nthat a mathematician assumes or somehow concludes that there is a\nnatural number with a given property \\(P\\). She then says \u201clet\n\\(n\\) be such a natural number, so that \\(Pn\\)\u201d, and goes on to\nestablish a sentence \\(\\phi\\), which does not mention the number\n\\(n\\). If the derivation of \\(\\phi\\) does not invoke anything about\n\\(n\\) (other than the assumption that it has the given property\n\\(P)\\), then \\(n\\) could have been any number that has the property\n\\(P\\). That is, \\(n\\) is an arbitrary number with property\n\\(P\\). It does not matter which number \\(n\\) is. Since \\(\\phi\\) does\nnot mention \\(n\\), it follows from the assertion that something has\nproperty \\(P\\). The provisions added to \\((\\exists\\)E) are to\nguarantee that \\(t\\) is \u201carbitrary\u201d.\n\nThe final items are the rules for the identity sign \u201c=\u201d.\nThe introduction rule is about a simple as can be:\n\n\\(({=}\\mathrm{I})\\)\n\\(\\Gamma \\vdash t=t\\), where \\(t\\) is any closed term. \n\n\nThis \u201cinference\u201d corresponds to the truism that everything\nis identical to itself. The elimination rule corresponds to a\nprinciple that if \\(a\\) is identical to \\(b\\), then anything true of\n\\(a\\) is also true of \\(b\\). \n\n\\(({=}\\mathrm{E})\\)\nFor any closed terms \\(t_1\\) and \\(t_2\\), if \\(\\Gamma_1 \\vdash t_1\n=t_2\\) and \\(\\Gamma_2 \\vdash \\theta\\), then \\(\\Gamma_1, \\Gamma_2\n\\vdash \\theta'\\), where \\(\\theta'\\) is obtained from \\(\\theta\\) by\nreplacing one or more occurances of \\(t_1\\) with \\(t_2\\). \n\n\nThe rule \\(({=}\\mathrm{E})\\) indicates a certain restriction in the\nexpressive resources of our language. Suppose, for example, that Harry\nis identical to Donald (since his mischievous parents gave him two\nnames). According to most people\u2019s intuitions, it would not\nfollow from this and \u201cDick knows that Harry is wicked\u201d\nthat \u201cDick knows that Donald is wicked\u201d, for the reason\nthat Dick might not know that Harry is identical to Donald. Contexts\nlike this, in which identicals cannot safely be substituted for each\nother, are called \u201copaque\u201d. We assume that our language\n\\(\\LKe\\) has no opaque contexts.\n\nOne final clause completes the description of the deductive system\n\\(D\\):\n\n(*)\nThat\u2019s all folks. \\(\\Gamma \\vdash \\theta\\) only if\n\\(\\theta\\) follows from members of \\(\\Gamma\\) by the above rules.\n\n\nAgain, this clause allows proofs by induction on the rules used to\nestablish an argument. If a property of arguments holds of all\ninstances of (As) and \\(({=}\\mathrm{I})\\), and if the other rules\npreserve the property, then every argument that is deducible in \\(D\\)\nenjoys the property in question.\n\nBefore moving on to the model theory for \\(\\LKe\\), we pause to note a\nfew features of the deductive system. To illustrate the level of\nrigor, we begin with a lemma that if a sentence does not contain a\nparticular closed term, we can make small changes to the set of\nsentences we prove it from without problems. We allow ourselves the\nliberty here of extending some previous notation: for any terms \\(t\\)\nand \\(t'\\), and any formula \\(\\theta\\), we say that \\(\\theta(t|t')\\)\nis the result of replacing all free occurrences of \\(t\\) in \\(\\theta\\)\nwith \\(t'\\).\n\n\nLemma 7. If \\(\\Gamma_1\\) and \\(\\Gamma_2\\) differ only\nin that wherever \\(\\Gamma_1\\) contains \\(\\theta\\), \\(\\Gamma_2\\)\ncontains \\(\\theta(t|t')\\), then for any sentence \\(\\phi\\) not\ncontaining \\(t\\) or \\(t'\\), if \\(\\Gamma_1\\vdash\\phi\\) then\n\\(\\Gamma_2\\vdash\\phi\\). \n\nProof: The proof proceeds by induction on the number\nof steps in the proof of \\(\\phi\\). Crucial to this proof is the fact\nthat \\(\\theta=\\theta(t|t')\\) whenever \\(\\theta\\) does not contain\n\\(t\\) or \\(t'\\). When the number of steps in the proof of \\(\\phi\\) is\none, this means that the last (and only) rule applied is (As) or (=I).\nThen, since \\(\\phi\\) does not contain \\(t\\) or \\(t'\\), if\n\\(\\Gamma_1\\vdash\\phi\\) we simply apply the same rule ((As) or (=I)) to\n\\(\\Gamma_2\\) to get \\(\\Gamma_2\\vdash\\phi\\). Assume that there are\n\\(n>1\\) steps in the proof of \\(\\phi\\), and that Lemma 7 holds for any\nproof with less than \\(n\\) steps. Suppose that the \\(n^{th}\\) rule\napplied to \\(\\Gamma_1\\) was (\\(\\amp I\\)). Then \\(\\phi\\) is\n\\(\\psi\\amp\\chi\\), and \\(\\Gamma_1\\vdash\\phi\\amp\\chi\\). But then we know\nthat previous steps in the proof include \\(\\Gamma_1\\vdash\\psi\\) and\n\\(\\Gamma_1\\vdash\\chi\\), and by induction, we have\n\\(\\Gamma_2\\vdash\\psi\\) and \\(\\Gamma_2\\vdash\\chi\\), since neither\n\\(\\psi\\) nor \\(\\chi\\) contain \\(t\\) or \\(t'\\). So, we simply apply\n(\\(\\amp I\\)) to \\(\\Gamma_2\\) to get \\(\\Gamma_2\\vdash\\psi\\amp\\chi\\) as\nrequired. Suppose now that the last step applied in the proof of\n\\(\\Gamma_1\\vdash\\phi\\) was (\\(\\amp E\\)). Then, at a previous step in\nthe proof of \\(\\phi\\), we know \\(\\Gamma_1\\vdash\\phi\\amp\\psi\\) for some\nsentence \\(\\psi\\). If \\(\\psi\\) does not contain \\(t\\), then we simply\napply (\\(\\amp E\\)) to \\(\\Gamma_2\\) to obtain the desired result. The\nonly complication is if \\(\\psi\\) contains \\(t\\). Then we would have\nthat \\(\\Gamma_2\\vdash (\\phi\\amp\\psi)(t|t')\\). But, since\n\\((\\phi\\amp\\psi)(t|t')\\) is \\(\\phi(t|t')\\amp\\psi(t|t')\\), and\n\\(\\phi(t|t')\\) is just \\(\\phi\\), we can just apply (\\(\\amp E\\)) to get\n\\(\\Gamma_2\\vdash\\phi\\) as required. The cases for the other rules are\nsimilar.\n\nTheorem 8. The rule of Weakening. If \\(\\Gamma_1\n\\vdash \\phi\\) and \\(\\Gamma_1 \\subseteq \\Gamma_2\\), then \\(\\Gamma_2\n\\vdash \\phi\\).\n\nProof: Again, we proceed by induction on the number\nof rules that were used to arrive at \\(\\Gamma_1 \\vdash \\phi\\). Suppose\nthat \\(n\\gt 0\\) is a natural number, and that the theorem holds for\nany argument that was derived using fewer than \\(n\\) rules. Suppose\nthat \\(\\Gamma_1 \\vdash \\phi\\) using exactly \\(n\\) rules. If \\(n=1\\),\nthen the rule is either (As) or \\((=\\)I). In these cases, \\(\\Gamma_2\n\\vdash \\phi\\) by the same rule. If the last rule applied was (&I),\nthen \\(\\phi\\) has the form \\((\\theta \\amp \\psi)\\), and we have\n\\(\\Gamma_3 \\vdash \\theta\\) and \\(\\Gamma_4 \\vdash \\psi\\), with\n\\(\\Gamma_1 = \\Gamma_3, \\Gamma_4\\). We apply the induction hypothesis\nto the deductions of \\(\\theta\\) and \\(\\psi\\), to get \\(\\Gamma_2 \\vdash\n\\theta\\) and \\(\\Gamma_2 \\vdash \\psi\\). and then apply (&I) to the\nresult to get \\(\\Gamma_2 \\vdash \\phi\\). Most of the other cases are\nexactly like this. Slight complications arise only in the rules\n\\((\\forall\\)I) and \\((\\exists\\)E), because there we have to pay\nattention to the conditions for the rules.\n\nSuppose that the last rule applied to get \\(\\Gamma_1 \\vdash \\phi\\) is\n\\((\\forall\\)I). So \\(\\phi\\) is a sentence of the form \\(\\forall\nv\\theta\\), and we have \\(\\Gamma_1 \\vdash \\theta (v|t)\\) and \\(t\\) does\nnot occur in any member of \\(\\Gamma_1\\) or in \\(\\theta\\). The problem\nis that \\(t\\) may occur in a member of \\(\\Gamma_2\\), and so we cannot\njust invoke the induction hypothesis and apply \\((\\forall\\)I) to the\nresult. So, let \\(t'\\) be a term not occurring in any sentence in\n\\(\\Gamma_2\\). Let \\(\\Gamma'\\) be the result of substituting \\(t'\\) for\nall \\(t\\) in \\(\\Gamma_2\\). Then, since \\(t\\) does not occur in\n\\(\\Gamma_1\\), \\(\\Gamma_1\\subseteq\\Gamma'\\). So, the induction\nhypothesis gives us \\(\\Gamma'\\vdash\\theta (v|t)\\), and we know that\n\\(\\Gamma'\\) does not contain \\(t\\), so we can apply (\\(\\forall I\\)) to\nget \\(\\Gamma'\\vdash\\forall v\\theta\\). But \\(\\forall v\\theta\\) does not\ncontain \\(t\\) or \\(t'\\), so \\(\\Gamma_2\\vdash\\forall v\\theta\\) by Lemma\n7.\n\nSuppose that the last rule applied was \\((\\exists\\)E), we have\n\\(\\Gamma_3 \\vdash \\exists v\\theta\\) and \\(\\Gamma_4, \\theta (v|t)\n\\vdash \\phi\\), with \\(\\Gamma_1\\) being \\(\\Gamma_3, \\Gamma_4\\), and\n\\(t\\) not in \\(\\phi\\), \\(\\Gamma_4\\) or \\(\\theta\\). If \\(t\\) does not\noccur free in \\(\\Gamma_2\\), we apply the induction hypothesis to get\n\\(\\Gamma_2 \\vdash \\exists v\\theta\\), and then \\((\\exists\\)E) to end up\nwith \\(\\Gamma_2 \\vdash \\phi\\). If \\(t\\) does occur free in\n\\(\\Gamma_2\\), then we follow a similar proceedure to \\(\\forall I\\),\nusing Lemma 7. \n\n\nTheorem 8 allows us to add on premises at will. It follows that\n\\(\\Gamma \\vdash \\phi\\) if and only if there is a subset\n\\(\\Gamma'\\subseteq \\Gamma\\) such that \\(\\Gamma'\\vdash \\phi\\). Some\nsystems of relevant logic do not have weakening, nor does\nsubstructural logic (See the entries on\n relevance logic,\n substructural logics, and\n linear logic).\n\nBy clause (*), all derivations are established in a finite number of\nsteps. So we have\n\n\nTheorem 9. \\(\\Gamma \\vdash \\phi\\) if and only if\nthere is a finite \\(\\Gamma'\\subseteq \\Gamma\\) such that\n\\(\\Gamma'\\vdash \\phi\\).\n\nTheorem 10. The rule of ex falso quodlibet\nis a \u201cderived rule\u201d of \\(D\\): if \\(\\Gamma_1 \\vdash\n\\theta\\) and \\(\\Gamma_2 \\vdash \\neg \\theta\\), then \\(\\Gamma_1,\\Gamma_2\n\\vdash \\psi\\), for any sentence \\(\\psi\\).\n\nProof: Suppose that \\(\\Gamma_1 \\vdash \\theta\\) and\n\\(\\Gamma_2 \\vdash \\neg \\theta\\). Then by Theorem \\(8, \\Gamma_1,\\neg\n\\psi \\vdash \\theta\\), and \\(\\Gamma_2,\\neg \\psi \\vdash \\neg \\theta\\).\nSo by \\((\\neg\\)I), \\(\\Gamma_1, \\Gamma_2 \\vdash \\neg \\neg \\psi\\). By\n(DNE), \\(\\Gamma_1, \\Gamma_2 \\vdash \\psi\\). \n\nTheorem 11. The rule of Cut. If \\(\\Gamma_1 \\vdash\n\\psi\\) and \\(\\Gamma_2, \\psi \\vdash \\theta\\), then \\(\\Gamma_1, \\Gamma_2\n\\vdash \\theta\\).\n\nProof: Suppose \\(\\Gamma_1 \\vdash \\psi\\) and\n\\(\\Gamma_2, \\psi \\vdash \\theta\\). We proceed by induction on the\nnumber of rules used to establish \\(\\Gamma_2, \\psi \\vdash \\theta\\).\nSuppose that \\(n\\) is a natural number, and that the theorem holds for\nany argument that was derived using fewer than \\(n\\) rules. Suppose\nthat \\(\\Gamma_2, \\psi \\vdash \\theta\\) was derived using exactly \\(n\\)\nrules. If the last rule used was \\((=\\)I), then \\(\\Gamma_1, \\Gamma_2\n\\vdash \\theta\\) is also an instance of \\((=\\)I). If \\(\\Gamma_2, \\psi\n\\vdash \\theta\\) is an instance of (As), then either \\(\\theta\\) is\n\\(\\psi\\), or \\(\\theta\\) is a member of \\(\\Gamma_2\\). In the former\ncase, we have \\(\\Gamma_1 \\vdash \\theta\\) by supposition, and get\n\\(\\Gamma_1, \\Gamma_2 \\vdash \\theta\\) by Weakening (Theorem 8). In the\nlatter case, \\(\\Gamma_1, \\Gamma_2 \\vdash \\theta\\) is itself an\ninstance of (As). Suppose that \\(\\Gamma_2, \\psi \\vdash \\theta\\) was\nobtained using (&E). Then we have \\(\\Gamma_2, \\psi \\vdash(\\theta\n\\amp \\phi)\\). The induction hypothesis gives us \\(\\Gamma_1, \\Gamma_2\n\\vdash(\\theta \\amp \\phi)\\), and (&E) produces \\(\\Gamma_1, \\Gamma_2\n\\vdash \\theta\\). The remaining cases are similar. \n\n\nTheorem 11 allows us to chain together inferences. This fits the\npractice of establishing theorems and lemmas and then using those\ntheorems and lemmas later, at will. The cut principle is, some think,\nessential to reasoning. In some logical systems, the cut principle is\na deep theorem; in others it is invalid. The system here was designed,\nin part, to make the proof of Theorem 11 straightforward.\n\nIf \\(\\Gamma \\vdash_D \\theta\\), then we say that the sentence\n\\(\\theta\\) is a deductive consequence of the set of sentences\n\\(\\Gamma\\), and that the argument \\(\\langle \\Gamma,\\theta \\rangle\\) is\ndeductively valid. A sentence \\(\\theta\\) is a logical\ntheorem, or a deductive logical truth, if \\(\\vdash_D\n\\theta\\). That is, \\(\\theta\\) is a logical theorem if it is a\ndeductive consequence of the empty set. A set \\(\\Gamma\\) of sentences\nis consistent if there is no sentence \\(\\theta\\) such that\n\\(\\Gamma \\vdash_D \\theta\\) and \\(\\Gamma \\vdash_D \\neg \\theta\\). That\nis, a set is consistent if it does not entail a pair of contradictory\nopposite sentencess.\n\n\nTheorem 12. A set \\(\\Gamma\\) is consistent if and\nonly if there is a sentence \\(\\theta\\) such that it is not the case\nthat \\(\\Gamma \\vdash \\theta\\).\n\nProof: Suppose that \\(\\Gamma\\) is consistent and let\n\\(\\theta\\) be any sentence. Then either it is not the case that\n\\(\\Gamma \\vdash \\theta\\) or it is not the case that \\(\\Gamma \\vdash\n\\neg \\theta\\). For the converse, suppose that \\(\\Gamma\\) is\ninconsistent and let \\(\\psi\\) be any sentence. We have that there is a\nsentence such that both \\(\\Gamma \\vdash \\theta\\) and \\(\\Gamma \\vdash\n\\neg \\theta\\). By ex falso quodlibet (Theorem 10), \\(\\Gamma\n\\vdash \\psi\\). \n\n\nDefine a set \\(\\Gamma\\) of sentences of the language \\(\\LKe\\) to be\nmaximally consistent if \\(\\Gamma\\) is consistent and for\nevery sentence \\(\\theta\\) of \\(\\LKe\\), if \\(\\theta\\) is not in\n\\(\\Gamma\\), then \\(\\Gamma,\\theta\\) is inconsistent. In other words,\n\\(\\Gamma\\) is maximally consistent if \\(\\Gamma\\) is consistent, and\nadding any sentence in the language not already in \\(\\Gamma\\) renders\nit inconsistent. Notice that if \\(\\Gamma\\) is maximally consistent\nthen \\(\\Gamma \\vdash \\theta\\) if and only if \\(\\theta\\) is in\n\\(\\Gamma\\).\n\n\nTheorem 13. The Lindenbaum Lemma. Let \\(\\Gamma\\) be\nany consistent set of sentences of \\(\\LKe .\\) Then there is a set\n\\(\\Gamma'\\) of sentences of \\(\\LKe\\) such that \\(\\Gamma \\subseteq\n\\Gamma'\\) and \\(\\Gamma'\\) is maximally consistent.\n\nProof: Although this theorem holds in general, we\nassume here that the set \\(K\\) of non-logical terminology is either\nfinite or denumerably infinite (i.e., the size of the natural numbers,\nusually called \\(\\aleph_0)\\). It follows that there is an enumeration\n\\(\\theta_0, \\theta_1,\\ldots\\) of the sentences of \\(\\LKe\\), such that\nevery sentence of \\(\\LKe\\) eventually occurs in the list. Define a\nsequence of sets of sentences, by recursion, as follows: \\(\\Gamma_0\\)\nis \\(\\Gamma\\); for each natural number \\(n\\), if \\(\\Gamma_n,\n\\theta_n\\) is consistent, then let \\(\\Gamma_{n+1} = \\Gamma_n,\n\\theta_n\\). Otherwise, let \\(\\Gamma_{n+1} = \\Gamma_n\\). Let\n\\(\\Gamma'\\) be the union of all of the sets \\(\\Gamma_n\\). Intuitively,\nthe idea is to go through the sentences of \\(\\LKe\\), throwing each one\ninto \\(\\Gamma'\\) if doing so produces a consistent set. Notice that\neach \\(\\Gamma_n\\) is consistent. Suppose that \\(\\Gamma'\\) is\ninconsistent. Then there is a sentence \\(\\theta\\) such that\n\\(\\Gamma'\\vdash \\theta\\) and \\(\\Gamma'\\vdash \\neg \\theta\\). By Theorem\n9 and Weakening (Theorem 8), there is finite subset \\(\\Gamma''\\) of\n\\(\\Gamma'\\) such that \\(\\Gamma''\\vdash \\theta\\) and \\(\\Gamma''\\vdash\n\\neg \\theta\\). Because \\(\\Gamma''\\) is finite, there is a natural\nnumber \\(n\\) such that every member of \\(\\Gamma''\\) is in\n\\(\\Gamma_n\\). So, by Weakening again, \\(\\Gamma_n \\vdash \\theta\\) and\n\\(\\Gamma_n \\vdash \\neg \\theta\\). So \\(\\Gamma_n\\) is inconsistent,\nwhich contradicts the construction. So \\(\\Gamma'\\) is consistent. Now\nsuppose that a sentence \\(\\theta\\) is not in \\(\\Gamma'\\). We have to\nshow that \\(\\Gamma', \\theta\\) is inconsistent. The sentence \\(\\theta\\)\nmust occur in the aforementioned list of sentences; say that\n\\(\\theta\\) is \\(\\theta_m\\). Since \\(\\theta_m\\) is not in \\(\\Gamma'\\),\nthen it is not in \\(\\Gamma_{m+1}\\). This happens only if \\(\\Gamma_m,\n\\theta_m\\) is inconsistent. So a pair of contradictory opposites can\nbe deduced from \\(\\Gamma_m,\\theta_m\\). By Weakening, a pair of\ncontradictory opposites can be deduced from \\(\\Gamma', \\theta_m\\). So\n\\(\\Gamma', \\theta_m\\) is inconsistent. Thus, \\(\\Gamma'\\) is maximally\nconsistent.\n\n\nNotice that this proof uses a principle corresponding to the law of\nexcluded middle. In the construction of \\(\\Gamma'\\), we assumed that,\nat each stage, either \\(\\Gamma_n\\) is consistent or it is not.\nIntuitionists, who demur from excluded middle, do not accept the\nLindenbaum lemma.\n4. Semantics\n\nLet \\(K\\) be a set of non-logical terminology. An\ninterpretation for the language \\(\\LKe\\) is a structure \\(M =\n\\langle d,I\\rangle\\), where \\(d\\) is a non-empty set, called the\ndomain-of-discourse, or simply the domain, of the\ninterpretation, and \\(I\\) is an interpretation function.\nInformally, the domain is what we interpret the language \\(\\LKe\\) to\nbe about. It is what the variables range over. The interpretation\nfunction assigns appropriate extensions to the non-logical terms. In\nparticular,\n\nIf \\(c\\) is a constant in \\(K\\), then \\(I(c)\\) is a member of the\ndomain \\(d\\). \n\nThus we assume that every constant denotes something. Systems where\nthis is not assumed are called free logics (see the entry on\n free logic).\n Continuing,\n\n\nIf \\(P^0\\) is a zero-place predicate letter in \\(K\\), then \\(I(P)\\) is\na truth value, either truth or falsehood.\n\nIf \\(Q^1\\) is a one-place predicate letter in \\(K\\), then \\(I(Q)\\) is\na subset of \\(d\\). Intuitively, \\(I(Q)\\) is the set of members of the\ndomain that the predicate \\(Q\\) holds of. For example, \\(I(Q)\\) might\nbe the set of red members of the domain.\n\nIf \\(R^2\\) is a two-place predicate letter in \\(K\\), then \\(I(R)\\) is\na set of ordered pairs of members of \\(d\\). Intuitively, \\(I(R)\\) is\nthe set of pairs of members of the domain that the relation \\(R\\)\nholds between. For example, \\(I(R)\\) might be the set of pairs\n\\(\\langle a,b\\rangle\\) such that \\(a\\) and \\(b\\) are the members of\nthe domain for which \\(a\\) loves \\(b\\).\n\nIn general, if S\\(^n\\) is an \\(n\\)-place predicate letter in\n\\(K\\), then \\(I(S)\\) is a set of ordered \\(n\\)-tuples of members of\n\\(d\\).\n\n\nDefine \\(s\\) to be a variable-assignment, or simply an\nassignment, on an interpretation \\(M\\), if \\(s\\) is a\nfunction from the variables to the domain \\(d\\) of \\(M\\). The role of\nvariable-assignments is to assign denotations to the free\nvariables of open formulas. (In a sense, the quantifiers determine the\n\u201cmeaning\u201d of the bound variables.) \n\nLet \\(t\\) be a term of \\(\\LKe\\). We define the denotation of\n\\(t\\) in \\(M\\) under \\(s\\), in terms of the interpretation function\nand variable-assignment:\n\nIf \\(t\\) is a constant, then \\(D_{M,s}(t)\\) is \\(I(t)\\), and if \\(t\\)\nis a variable, then \\(D_{M,s}(t)\\) is \\(s(t)\\). \n\nThat is, the interpretation \\(M\\) assigns denotations to the\nconstants, while the variable-assignment assigns denotations to the\n(free) variables. If the language contained function symbols, the\ndenotation function would be defined by recursion.\n\nWe now define a relation of satisfaction between\ninterpretations, variable-assignments, and formulas of \\(\\LKe\\). If\n\\(\\phi\\) is a formula of \\(\\LKe, M\\) is an interpretation for\n\\(\\LKe\\), and \\(s\\) is a variable-assignment on \\(M\\), then we write\n\\(M,s\\vDash \\phi\\) for \\(M\\) satisfies \\(\\phi\\) under the\nassignment \\(s\\). The idea is that \\(M,s\\vDash \\phi\\) is an\nanalogue of \u201c\\(\\phi\\) comes out true when interpreted as in\n\\(M\\) via \\(s\\)\u201d.\n\nWe proceed by recursion on the complexity of the formulas of\n\\(\\LKe\\).\n\nIf \\(t_1\\) and \\(t_2\\) are terms, then \\(M,s\\vDash t_1 =t_2\\) if and\nonly if \\(D_{M,s}(t_1)\\) is the same as \\(D_{M,s}(t_2)\\). \n\nThis is about as straightforward as it gets. An identity \\(t_1 =t_2\\)\ncomes out true if and only if the terms \\(t_1\\) and \\(t_2\\) denote the\nsame thing.\n\nIf \\(P^0\\) is a zero-place predicate letter in \\(K\\), then \\(M,s\\vDash\nP\\) if and only if \\(I(P)\\) is truth. \n\nIf S\\(^n\\) is an \\(n\\)-place predicate letter in \\(K\\) and\n\\(t_1, \\ldots,t_n\\) are terms, then \\(M,s\\vDash St_1 \\ldots t_n\\) if\nand only if the \\(n\\)-tuple \\(\\langle D_{M,s}(t_1),\n\\ldots,D_{M,s}(t_n)\\rangle\\) is in \\(I(S)\\).\n\nThis takes care of the atomic formulas. We now proceed to the compound\nformulas of the language, more or less following the meanings of the\nEnglish counterparts of the logical terminology.\n\n\n\\(M,s\\vDash \\neg \\theta\\) if and only if it is not the case that\n\\(M,s\\vDash \\theta\\).\n\n\\(M,s\\vDash(\\theta \\amp \\psi)\\) if and only if both \\(M,s\\vDash\n\\theta\\) and \\(M,s\\vDash \\psi\\).\n\n\\(M,s\\vDash(\\theta \\vee \\psi)\\) if and only if either \\(M,s\\vDash\n\\theta\\) or \\(M,s\\vDash \\psi\\).\n\n\\(M,s\\vDash(\\theta \\rightarrow \\psi)\\) if and only if either it is not\nthe case that \\(M,s\\vDash \\theta\\), or \\(M,s\\vDash \\psi\\).\n\n\\(M,s\\vDash \\forall v\\theta\\) if and only if \\(M,s'\\vDash \\theta\\),\nfor every assignment \\(s'\\) that agrees with \\(s\\) except possibly at\nthe variable \\(v\\).\n\n\nThe idea here is that \\(\\forall v\\theta\\) comes out true if and only\nif \\(\\theta\\) comes out true no matter what is assigned to the\nvariable \\(v\\). The final clause is similar.\n\n\\(M,s\\vDash \\exists v\\theta\\) if and only if \\(M,s'\\vDash \\theta\\),\nfor some assignment \\(s'\\) that agrees with \\(s\\) except possibly at\nthe variable \\(v\\). \n\nSo \\(\\exists v\\theta\\) comes out true if there is an assignment to\n\\(v\\) that makes \\(\\theta\\) true.\n\nTheorem 6, unique readability, assures us that this definition is\ncoherent. At each stage in breaking down a formula, there is exactly\none clause to be applied, and so we never get contradictory verdicts\nconcerning satisfaction.\n\nAs indicated, the role of variable-assignments is to give denotations\nto the free variables. We now show that variable-assignments play no\nother role.\n\n\nTheorem 14. For any formula \\(\\theta\\), if \\(s_1\\)\nand \\(s_2\\) agree on the free variables in \\(\\theta\\), then \\(M,s_1\n\\vDash \\theta\\) if and only if \\(M,s_2 \\vDash \\theta\\).\n\nProof: We proceed by induction on the complexity of\nthe formula \\(\\theta\\). The theorem clearly holds if \\(\\theta\\) is\natomic, since in those cases only the values of the\nvariable-assignments at the variables in \\(\\theta\\) figure in the\ndefinition. Assume, then, that the theorem holds for all formulas less\ncomplex than \\(\\theta\\). And suppose that \\(s_1\\) and \\(s_2\\) agree on\nthe free variables of \\(\\theta\\). Assume, first, that \\(\\theta\\) is a\nnegation, \\(\\neg \\psi\\). Then, by the induction hypothesis, \\(M,s_1\n\\vDash \\psi\\) if and only if \\(M,s_2 \\vDash \\psi\\). So, by the clause\nfor negation, \\(M,s_1 \\vDash \\neg \\psi\\) if and only if \\(M,s_2 \\vDash\n\\neg \\psi\\). The cases where the main connective in \\(\\theta\\) is\nbinary are also straightforward. Suppose that \\(\\theta\\) is \\(\\exists\nv\\psi\\), and that \\(M,s_1 \\vDash \\exists v\\psi\\). Then there is an\nassignment \\(s_1'\\) that agrees with \\(s_1\\) except possibly at \\(v\\)\nsuch that \\(M,s_1'\\vDash \\psi\\). Let \\(s_2'\\) be the assignment that\nagrees with \\(s_2\\) on the free variables not in \\(\\psi\\) and agrees\nwith \\(s_1'\\) on the others. Then, by the induction hypothesis,\n\\(M,s_2'\\vDash \\psi\\). Notice that \\(s_2'\\) agrees with \\(s_2\\) on\nevery variable except possibly \\(v\\). So \\(M,s_2 \\vDash \\exists\nv\\psi\\). The converse is the same, and the case where \\(\\theta\\)\nbegins with a universal quantifier is similar.\n\n\nBy Theorem 14, if \\(\\theta\\) is a sentence, and \\(s_1, s_2\\), are any\ntwo variable-assignments, then \\(M,s_1 \\vDash \\theta\\) if and only if\n\\(M,s_2 \\vDash \\theta\\). So we can just write \\(M\\vDash \\theta\\) if\n\\(M,s\\vDash \\theta\\) for some, or all, variable-assignments \\(s\\). So\nwe define \n\n\\(M\\vDash \\theta\\) where \\(\\theta\\) is a sentence just in case\n\\(M,s\\vDash\\theta\\) for all variable assignments \\(s\\). \n\nIn this case, we call \\(M\\) a model of \\(\\theta\\).\n\nSuppose that \\(K'\\subseteq K\\) are two sets of non-logical terms. If\n\\(M = \\langle d,I\\rangle\\) is an interpretation of \\(\\LKe\\), then we\ndefine the restriction of \\(M\\) to \\(\\mathcal{L}1K'{=}\\) to\nbe the interpretation \\(M'=\\langle d,I'\\rangle\\) such that \\(I'\\) is\nthe restriction of \\(I\\) to \\(K'\\). That is, \\(M\\) and \\(M'\\) have the\nsame domain and agree on the non-logical terminology in \\(K'\\). A\nstraightforward induction establishes the following:\n\n\nTheorem 15. If \\(M'\\) is the restriction of \\(M\\) to\n\\(\\mathcal{L}1K'{=}\\), then for every sentence \\(\\theta\\) of\n\\(\\mathcal{L}1K'\\), \\(M\\vDash\\theta\\) if and only if \\(M'\\vDash\n\\theta\\).\n\nTheorem 16. If two interpretations \\(M_1\\) and\n\\(M_2\\) have the same domain and agree on all of the non-logical\nterminology of a sentence \\(\\theta\\), then \\(M_1\\vDash\\theta\\) if and\nonly if \\(M_2\\vDash \\theta\\).\n\n\nIn short, the satisfaction of a sentence \\(\\theta\\) only depends on\nthe domain of discourse and the interpretation of the non-logical\nterminology in \\(\\theta\\).\n\nWe say that an argument \\(\\langle \\Gamma,\\theta \\rangle\\) is\nsemantically valid, or just valid, written \\(\\Gamma\n\\vDash \\theta\\), if for every interpretation \\(M\\) of the language, if\n\\(M\\vDash\\psi\\), for every member \\(\\psi\\) of \\(\\Gamma\\), then\n\\(M\\vDash\\theta\\). If \\(\\Gamma \\vDash \\theta\\), we also say that\n\\(\\theta\\) is a logical consequence, or semantic\nconsequence, or model-theoretic consequence of\n\\(\\Gamma\\). The definition corresponds to the informal idea that an\nargument is valid if it is not possible for its premises to all be\ntrue and its conclusion false. Our definition of logical consequence\nalso sanctions the common thesis that a valid argument is\ntruth-preserving \u2013 to the extent that satisfaction represents\ntruth. Officially, an argument in \\(\\LKe\\) is valid if its conclusion\ncomes out true under every interpretation of the language in which the\npremises are true. Validity is the model-theoretic counterpart to\ndeducibility.\n\nA sentence \\(\\theta\\) is logically true, or valid,\nif \\(M\\vDash \\theta\\), for every interpretation \\(M\\). A sentence is\nlogically true if and only if it is a consequence of the empty set. If\n\\(\\theta\\) is logically true, then for any set \\(\\Gamma\\) of\nsentences, \\(\\Gamma \\vDash \\theta\\). Logical truth is the\nmodel-theoretic counterpart of theoremhood.\n\nA sentence \\(\\theta\\) is satisfiable if there is an\ninterpretation \\(M\\) such that \\(M\\vDash \\theta\\). That is, \\(\\theta\\)\nis satisfiable if there is an interpretation that satisfies it. A set\n\\(\\Gamma\\) of sentences is satisfiable if there is an interpretation\n\\(M\\) such that \\(M\\vDash\\theta\\), for every sentence \\(\\theta\\) in\n\\(\\Gamma\\). If \\(\\Gamma\\) is a set of sentences and if \\(M\\vDash\n\\theta\\) for each sentence \\(\\theta\\) in \\(\\Gamma\\), then we say that\n\\(M\\) is a model of \\(\\Gamma\\). So a set of sentences is\nsatisfiable if it has a model. Satisfiability is the model-theoretic\ncounterpart to consistency.\n\nNotice that \\(\\Gamma \\vDash \\theta\\) if and only if the set\n\\(\\Gamma,\\neg \\theta\\) is not satisfiable. It follows that if a set\n\\(\\Gamma\\) is not satisfiable, then if \\(\\theta\\) is any sentence,\n\\(\\Gamma \\vDash \\theta\\). This is a model-theoretic counterpart to\nex falso quodlibet (see Theorem 10). We have the following,\nas an analogue to Theorem 12:\n\n\nTheorem 17. Let \\(\\Gamma\\) be a set of sentences. The\nfollowing are equivalent: (a) \\(\\Gamma\\) is satisfiable; (b) there is\nno sentence \\(\\theta\\) such that both \\(\\Gamma \\vDash \\theta\\) and\n\\(\\Gamma \\vDash \\neg \\theta\\); (c) there is some sentence \\(\\psi\\)\nsuch that it is not the case that \\(\\Gamma \\vDash \\psi\\).\n\nProof: (a)\\(\\Rightarrow\\)(b): Suppose that \\(\\Gamma\\)\nis satisfiable and let \\(\\theta\\) be any sentence. There is an\ninterpretation \\(M\\) such that \\(M\\vDash \\psi\\) for every member\n\\(\\psi\\) of \\(\\Gamma\\). By the clause for negations, we cannot have\nboth \\(M\\vDash \\theta\\) and \\(M\\vDash \\neg \\theta\\). So either\n\\(\\langle \\Gamma,\\theta \\rangle\\) is not valid or else \\(\\langle\n\\Gamma,\\neg \\theta \\rangle\\) is not valid. (b)\\(\\Rightarrow\\)(c): This\nis immediate. (c)\\(\\Rightarrow\\)(a): Suppose that it is not the case\nthat \\(\\Gamma \\vDash \\psi\\). Then there is an interpretation \\(M\\)\nsuch that \\(M\\vDash \\theta\\), for every sentence \\(\\theta\\) in\n\\(\\Gamma\\) and it is not the case that \\(M\\vDash \\psi\\). A fortiori,\n\\(M\\) satisfies every member of \\(\\Gamma\\), and so \\(\\Gamma\\) is\nsatisfiable.\n\n5. Meta-theory\n\nWe now present some results that relate the deductive notions to their\nmodel-theoretic counterparts. The first one is probably the most\nstraightforward. We motivated both the various rules of the deductive\nsystem \\(D\\) and the various clauses in the definition of satisfaction\nin terms of the meaning of the English counterparts to the logical\nterminology (more or less, with the same simplifications in both\ncases). So one would expect that an argument is deducible, or\ndeductively valid, only if it is semantically valid.\n\n\nTheorem 18. Soundness. For any sentence \\(\\theta\\)\nand set \\(\\Gamma\\) of sentences, if \\(\\Gamma \\vdash_D \\theta\\), then\n\\(\\Gamma \\vDash \\theta\\).\n\nProof: We proceed by induction on the number of\nclauses used to establish \\(\\Gamma \\vdash \\theta\\). So let \\(n\\) be a\nnatural number, and assume that the theorem holds for any argument\nestablished as deductively valid with fewer than \\(n\\) steps. And\nsuppose that \\(\\Gamma \\vdash \\theta\\) was established using exactly\n\\(n\\) steps. If the last rule applied was \\((=\\)I) then \\(\\theta\\) is\na sentence in the form \\(t=t\\), and so \\(\\theta\\) is logically true. A\nfortiori, \\(\\Gamma \\vDash \\theta\\). If the last rule applied was (As),\nthen \\(\\theta\\) is a member of \\(\\Gamma\\), and so of course any\ninterpretation that satisfies every member of \\(\\Gamma\\) also\nsatisfies \\(\\theta\\). Suppose the last rule applied is (&I). So\n\\(\\theta\\) has the form \\((\\phi \\amp \\psi)\\), and we have \\(\\Gamma_1\n\\vdash \\phi\\) and \\(\\Gamma_2 \\vdash \\psi\\), with \\(\\Gamma = \\Gamma_1,\n\\Gamma_2\\). The induction hypothesis gives us \\(\\Gamma_1 \\vDash \\phi\\)\nand \\(\\Gamma_2 \\vDash \\psi\\). Suppose that \\(M\\) satisfies every\nmember of \\(\\Gamma\\). Then \\(M\\) satisfies every member of\n\\(\\Gamma_1\\), and so \\(M\\) satisfies \\(\\phi\\). Similarly, \\(M\\)\nsatisfies every member of \\(\\Gamma_2\\), and so \\(M\\) satisfies\n\\(\\psi\\). Thus, by the clause for \u201c\\(\\amp\\)\u201d in the\ndefinition of satisfaction, \\(M\\) satisfies \\(\\theta\\). So \\(\\Gamma\n\\vDash \\theta\\). \n\nSuppose the last clause applied was \\((\\exists\\mathrm{E})\\). So we\nhave \\(\\Gamma_1 \\vdash \\exists v\\phi\\) and \\(\\Gamma_2, \\phi(v|t)\n\\vdash \\theta\\), where \\(\\Gamma = \\Gamma_1, \\Gamma_2\\), and \\(t\\) does\nnot occur in \\(\\phi , \\theta \\), or in any member of \\(\\Gamma_2\\).\n\nWe need to show that \\(\\Gamma\\vDash\\theta\\). By the induction\nhypothesis, we have that \\(\\Gamma_1\\vDash\\exists v\\phi\\) and\n\\(\\Gamma_2, \\phi(v|t)\\vDash\\theta\\). Let \\(M\\) be an interpretation\nsuch that \\(M\\) makes every member of \\(\\Gamma\\) true. So, \\(M\\) makes\nevery member of \\(\\Gamma_1\\) and \\(\\Gamma_2\\) true. Then\n\\(M,s\\vDash\\exists v\\phi\\) for all variable assignments \\(s\\), so\nthere is an \\(s'\\) such that \\(M,s'\\vDash\\phi\\). Let \\(M'\\) differ\nfrom \\(M\\) only in that \\(I_{M'}(t)=s'(v)\\). Then,\n\\(M',s'\\vDash\\phi(v|t)\\) and \\(M',s'\\vDash\\Gamma_2\\) since \\(t\\) does\nnot occur in \\(\\phi\\) or \\(\\Gamma_2\\). So, \\(M',s'\\vDash\\theta\\).\nSince \\(t\\) does not occur in \\(\\theta\\) and \\(M'\\) differs from \\(M\\)\nonly with respect to \\(I_{M'}(t)\\), \\(M,s'\\vDash\\theta\\). Since\n\\(\\theta\\) is a sentence, \\(s'\\) doesn't matter, so \\(M\\vDash\\theta\\)\nas desired. Notice the role of the restrictions on \\((\\exists\\)E)\nhere. The other cases are about as straightforward. \n\nCorollary 19. Let \\(\\Gamma\\) be a set of sentences.\nIf \\(\\Gamma\\) is satisfiable, then \\(\\Gamma\\) is consistent. \n\nProof: Suppose that \\(\\Gamma\\) is satisfiable. So let\n\\(M\\) be an interpretation such that \\(M\\) satisfies every member of\n\\(\\Gamma\\). Assume that \\(\\Gamma\\) is inconsistent. Then there is a\nsentence \\(\\theta\\) such that \\(\\Gamma \\vdash \\theta\\) and \\(\\Gamma\n\\vdash \\neg \\theta\\). By soundness (Theorem 18), \\(\\Gamma \\vDash\n\\theta\\) and \\(\\Gamma \\vDash \\neg \\theta\\). So we have that \\(M\\vDash\n\\theta\\) and \\(M\\vDash \\neg \\theta\\). But this is impossible, given\nthe clause for negation in the definition of satisfaction.\n\n\nEven though the deductive system \\(D\\) and the model-theoretic\nsemantics were developed with the meanings of the logical terminology\nin mind, one should not automatically expect the converse to soundness\n(or Corollary 19) to hold. For all we know so far, we may not have\nincluded enough rules of inference to deduce every valid argument. The\nconverses to soundness and Corollary 19 are among the most important\nand influential results in mathematical logic. We begin with the\nlatter.\n\n\nTheorem 20. Completeness. G\u00f6del [1930]. Let\n\\(\\Gamma\\) be a set of sentences. If \\(\\Gamma\\) is consistent, then\n\\(\\Gamma\\) is satisfiable.\n\nProof: The proof of completeness is rather complex.\nWe only sketch it here. Let \\(\\Gamma\\) be a consistent set of\nsentences of \\(\\LKe\\). Again, we assume for simplicity that the set\n\\(K\\) of non-logical terminology is either finite or countably\ninfinite (although the theorem holds even if \\(K\\) is uncountable).\nThe task at hand is to find an interpretation \\(M\\) such that \\(M\\)\nsatisfies every member of \\(\\Gamma\\). Consider the language obtained\nfrom \\(\\LKe\\) by adding a denumerably infinite stock of new individual\nconstants \\(c_0, c_1,\\ldots\\) We stipulate that the constants, \\(c_0,\nc_1,\\ldots\\), are all different from each other and none of them occur\nin \\(K\\). One interesting feature of this construction, due to Leon\nHenkin, is that we build an interpretation of the language from the\nlanguage itself, using some of the constants as members of the domain\nof discourse. Let \\(\\theta_0 (x), \\theta_1 (x),\\ldots\\) be an\nenumeration of the formulas of the expanded language with at most one\nfree variable, so that each formula with at most one free variable\noccurs in the list eventually. Define a sequence \\(\\Gamma_0,\n\\Gamma_1,\\ldots\\) of sets of sentences (of the expanded language) by\nrecursion as follows: \\(\\Gamma_0 = \\Gamma\\); and \\(\\Gamma_{n+1} =\n\\Gamma_n,(\\exists x\\theta_n \\rightarrow \\theta_{n}(x|c_i))\\), where\n\\(c_i\\) is the first constant in the above list that does not occur in\n\\(\\theta_n\\) or in any member of \\(\\Gamma_n\\). The underlying idea\nhere is that if \\(\\exists x\\theta_n\\)is true, then \\(c_i\\) is to be\none such \\(x\\). Let \\(\\Gamma\\) be the union of the sets \\(\\Gamma_n\\).\n\n\nWe sketch a proof that \\(\\Gamma'\\) is consistent. Suppose that\n\\(\\Gamma'\\) is inconsistent. By Theorem 9, there is a finite subset of\n\\(\\Gamma\\) that is inconsistent, and so one of the sets \\(\\Gamma_m\\)\nis inconsistent. By hypothesis, \\(\\Gamma_0 = \\Gamma\\) is consistent.\nLet \\(n\\) be the smallest number such that \\(\\Gamma_n\\) is consistent,\nbut \\(\\Gamma_{n+1} = \\Gamma_n,(\\exists x\\theta_n \\rightarrow\n\\theta_{n}(x|c_i))\\) is inconsistent. By \\((\\neg\\)I), we have that\n\n\\[\\tag{1}\n\\Gamma_n \\vdash \\neg(\\exists x\\theta_n \\rightarrow \\theta_n(x|c_i)).\n\\]\n\n\nBy ex falso quodlibet (Theorem 10), \\(\\Gamma_n, \\neg \\exists\nx\\theta_n, \\exists x\\theta_n \\vdash \\theta_n (x|c_i)\\). So by\n\\((\\rightarrow\\)I), \\(\\Gamma_n, \\neg \\exists x\\theta_n \\vdash(\\exists\nx\\theta_n \\rightarrow \\theta_n (x|c_i))\\). From this and (1), we have\n\\(\\Gamma_n \\vdash \\neg \\neg \\exists x\\theta_n\\), by \\((\\neg\\)I), and\nby (DNE) we have \n\\[\\tag{2}\n\\Gamma_n \\vdash \\exists x\\theta_n .\n\\]\n\n\nBy (As), \\(\\Gamma_n, \\theta_n (x|c_i), \\exists x\\theta_n \\vdash\n\\theta_n (x|c_i)\\). So by \\((\\rightarrow\\)I), \\(\\Gamma_n, \\theta_n\n(x|c_i)\\vdash(\\exists x\\theta_{n} \\rightarrow \\theta_{n}(x|c_i))\\).\nFrom this and (1), we have \\(\\Gamma_n \\vdash \\neg \\theta_n (x|c_i)\\),\nby \\((\\neg\\)I). Let \\(t\\) be a term that does not occur in\n\\(\\theta_n\\) or in any member of \\(\\Gamma_n\\). By uniform substitution\nof \\(t\\) for \\(c_i\\), we can turn the derivation of \\(\\Gamma_n \\vdash\n\\neg \\theta_n (x|c_i)\\) into \\(\\Gamma_n \\vdash \\neg \\theta_n (x|t)\\).\nBy \\((\\forall\\)I), we have \n\\[\\tag{3}\n\\Gamma_n \\vdash \\forall v\\neg \\theta_n (x|v).\n\\]\n\n\nBy (As) we have \\(\\{\\forall v\\neg \\theta_n (x|v),\\theta_n\\}\\vdash\n\\theta_n\\) and by \\((\\forall\\)E) we have \\(\\{\\forall v\\neg \\theta_n\n(x|v), \\theta_n\\}\\vdash \\neg \\theta_n\\). So \\(\\{\\forall v\\neg \\theta_n\n(x|v), \\theta_n\\}\\) is inconsistent. Let \\(\\phi\\) be any sentence of\nthe language. By ex falso quodlibet (Theorem 10), we have\nthat \\(\\{\\forall v\\neg \\theta_n (x|v),\\theta_n\\}\\vdash \\phi\\) and\n\\(\\{\\forall v\\neg \\theta_n (x|v), \\theta_n\\}\\vdash \\neg \\phi\\). So\nwith (2), we have that \\(\\Gamma_n, \\forall v\\neg \\theta_n (x|v)\\vdash\n\\phi\\) and \\(\\Gamma_n, \\forall v\\neg \\theta_n (x|v)\\vdash \\neg \\phi\\),\nby \\((\\exists\\)E). By Cut (Theorem 11), \\(\\Gamma_n \\vdash \\phi\\) and\n\\(\\Gamma_n \\vdash \\neg \\phi\\). So \\(\\Gamma_n\\) is inconsistent,\ncontradicting the assumption. So \\(\\Gamma'\\) is consistent.\n\nApplying the Lindenbaum Lemma (Theorem 13), let \\(\\Gamma''\\) be a\nmaximally consistent set of sentences (of the expanded language) that\ncontains \\(\\Gamma'\\). So, of course, \\(\\Gamma''\\) contains \\(\\Gamma\\).\nWe can now define an interpretation \\(M\\) such that \\(M\\) satisfies\nevery member of \\(\\Gamma''\\).\n\nIf we did not have a sign for identity in the language, we would let\nthe domain of \\(M\\) be the collection of new constants \\(\\{c_0, c_1,\n\\ldots \\}\\). But as it is, there may be a sentence in the form\n\\(c_{i}=c_{j}\\), with \\(i\\ne j\\), in \\(\\Gamma''\\). If so, we cannot\nhave both \\(c_i\\) and \\(c_j\\) in the domain of the interpretation (as\nthey are distinct constants). So we define the domain \\(d\\) of \\(M\\)\nto be the set \\(\\{c_i\\) | there is no \\(j\\lt i\\) such that\n\\(c_{i}=c_{j}\\) is in \\(\\Gamma''\\}\\). In other words, a constant\n\\(c_i\\) is in the domain of \\(M\\) if \\(\\Gamma''\\) does not declare it\nto be identical to an earlier constant in the list. Notice that for\neach new constant \\(c_i\\), there is exactly one \\(j\\le i\\) such that\n\\(c_j\\) is in \\(d\\) and the sentence \\(c_{i}=c_{j}\\) is in\n\\(\\Gamma''\\).\n\nWe now define the interpretation function \\(I\\). Let \\(a\\) be any\nconstant in the expanded language. By \\((=\\)I) and \\((\\exists\\)I),\n\\(\\Gamma''\\vdash \\exists x x=a\\), and so \\(\\exists x x=a \\in\n\\Gamma''\\). By the construction of \\(\\Gamma'\\), there is a sentence in\nthe form \\((\\exists x x=a \\rightarrow c_i =a)\\) in \\(\\Gamma''\\). We\nhave that \\(c_i =a\\) is in \\(\\Gamma''\\). As above, there is exactly\none \\(c_j\\) in \\(d\\) such that \\(c_{i}=c_{j}\\) is in \\(\\Gamma''\\). Let\n\\(I(a)=c_j\\). Notice that if \\(c_i\\) is a constant in the domain\n\\(d\\), then \\(I\\)(c\\(_i)=c_i\\). That is each \\(c_i\\) in \\(d\\) denotes\nitself.\n\nLet \\(P\\) be a zero-place predicate letter in \\(K\\). Then \\(I(P)\\) is\ntruth if \\(P\\) is in \\(\\Gamma''\\) and \\(I(P)\\) is falsehood otherwise.\nLet \\(Q\\) be a one-place predicate letter in \\(K\\). Then \\(I(Q)\\) is\nthe set of constants \\(\\{\\)c\\(_i | c_i\\) is in \\(d\\) and the sentence\n\\(Qc\\) is in \\(\\Gamma''\\}\\). Let \\(R\\) be a binary predicate letter in\n\\(K\\). Then \\(I(R)\\) is the set of pairs of constants \\(\\{\\langle\nc_i,c_j\\rangle | c_i\\) is in \\(d, c_j\\) is in \\(d\\), and the sentence\n\\(Rc_{i}c_{j}\\) is in \\(\\Gamma''\\}\\). Three-place predicates, etc. are\ninterpreted similarly. In effect, \\(I\\) interprets the non-logical\nterminology as they are in \\(\\Gamma''\\).\n\nThe final item in this proof is a lemma that for every sentence\n\\(\\theta\\) in the expanded language, \\(M\\vDash \\theta\\) if and only if\n\\(\\theta\\) is in \\(\\Gamma''\\). This proceeds by induction on the\ncomplexity of \\(\\theta\\). The case where \\(\\theta\\) is atomic follows\nfrom the definitions of \\(M\\) (i.e., the domain \\(d\\) and the\ninterpretation function \\(I\\)). The other cases follow from the\nvarious clauses in the definition of satisfaction.\n\nSince \\(\\Gamma \\subseteq \\Gamma''\\), we have that \\(M\\) satisfies\nevery member of \\(\\Gamma\\). By Theorem 15, the restriction of \\(M\\) to\nthe original language \\(\\LKe\\) and \\(s\\) also satisfies every member\nof \\(\\Gamma\\). Thus \\(\\Gamma\\) is satisfiable. \n\n\nA converse to Soundness (Theorem 18) is a straightforward\ncorollary:\n\n\nTheorem 21. For any sentence \\(\\theta\\) and set\n\\(\\Gamma\\) of sentences, if \\(\\Gamma \\vDash \\theta\\), then \\(\\Gamma\n\\vdash_D \\theta\\).\n\nProof: Suppose that \\(\\Gamma \\vDash \\theta\\). Then\nthere is no interpretation \\(M\\) such that M satisfies every\nmember of \\(\\Gamma\\) but does not satisfy \\(\\theta\\). So the set\n\\(\\Gamma,\\neg \\theta\\) is not satisfiable. By Completeness (Theorem\n20), \\(\\Gamma,\\neg \\theta\\) is inconsistent. So there is a sentence\n\\(\\phi\\) such that \\(\\Gamma,\\neg \\theta \\vdash \\phi\\) and\n\\(\\Gamma,\\neg \\theta \\vdash \\neg \\phi\\). By \\((\\neg\\)I), \\(\\Gamma\n\\vdash \\neg \\neg \\theta\\), and by (DNE) \\(\\Gamma \\vdash \\theta\\).\n\n\nOur next item is a corollary of Theorem 9, Soundness (Theorem 18), and\nCompleteness:\n\n\nCorollary 22. Compactness. A set \\(\\Gamma\\) of\nsentences is satisfiable if and only if every finite subset of\n\\(\\Gamma\\) is satisfiable.\n\nProof: If \\(M\\) satisfies every member of \\(\\Gamma\\),\nthen \\(M\\) satisfies every member of each finite subset of \\(\\Gamma\\).\nFor the converse, suppose that \\(\\Gamma\\) is not satisfiable. Then we\nshow that some finite subset of \\(\\Gamma\\) is not satisfiable. By\nCompleteness (Theorem 20), \\(\\Gamma\\) is inconsistent. By Theorem 9\n(and Weakening), there is a finite subset \\(\\Gamma'\\subseteq \\Gamma\\)\nsuch that \\(\\Gamma'\\) is inconsistent. By Corollary \\(19, \\Gamma'\\) is\nnot satisfiable.\n\n\nSoundness and completeness together entail that an argument is\ndeducible if and only if it is valid, and a set of sentences is\nconsistent if and only if it is satisfiable. So we can go back and\nforth between model-theoretic and proof-theoretic notions,\ntransferring properties of one to the other. Compactness holds in the\nmodel theory because all derivations use only a finite number of\npremises.\n\nRecall that in the proof of Completeness (Theorem 20), we made the\nsimplifying assumption that the set \\(K\\) of non-logical constants is\neither finite or denumerably infinite. The interpretation we produced\nwas itself either finite or denumerably infinite. Thus, we have the\nfollowing:\n\nCorollary 23. L\u00f6wenheim-Skolem Theorem. Let\n\\(\\Gamma\\) be a satisfiable set of sentences of the language \\(\\LKe\\).\nIf \\(\\Gamma\\) is either finite or denumerably infinite, then\n\\(\\Gamma\\) has a model whose domain is either finite or denumerably\ninfinite. \n\nIn general, let \\(\\Gamma\\) be a satisfiable set of sentences of\n\\(\\LKe\\), and let \\(\\kappa\\) be the larger of the size of \\(\\Gamma\\)\nand denumerably infinite. Then \\(\\Gamma\\) has a model whose domain is\nat most size \\(\\kappa\\).\n\nThere is a stronger version of Corollary 23. Let \\(M_1 =\\langle\nd_1,I_1\\rangle\\) and \\(M_2 =\\langle d_2,I_2\\rangle\\) be\ninterpretations of the language \\(\\LKe\\). Define \\(M_1\\) to be a\nsubmodel of \\(M_2\\) if \\(d_1 \\subseteq d_2, I_1 (c) = I_2\n(c)\\) for each constant \\(c\\), and \\(I_1\\) is the restriction of\n\\(I_2\\) to \\(d_1\\). For example, if \\(R\\) is a binary relation letter\nin \\(K\\), then for all \\(a,b\\) in \\(d_1\\), the pair \\(\\langle\na,b\\rangle\\) is in \\(I_1 (R)\\) if and only if \\(\\langle a,b\\rangle\\)\nis in \\(I_2 (R)\\). If we had included function letters among the\nnon-logical terminology, we would also require that \\(d_1\\) be closed\nunder their interpretations in \\(M_2\\). Notice that if \\(M_1\\) is a\nsubmodel of \\(M_2\\), then any variable-assignment on \\(M_1\\) is also a\nvariable-assignment on \\(M_2\\).\n\nSay that two interpretations \\(M_1 =\\langle d_1,I_1\\rangle, M_2\n=\\langle d_2,I_2\\rangle\\) are equivalent if one of them is a\nsubmodel of the other, and for any formula of the language and any\nvariable-assignment \\(s\\) on the submodel, \\(M_1,s\\vDash \\theta\\) if\nand only if \\(M_2,s\\vDash \\theta\\). Notice that if two interpretations\nare equivalent, then they satisfy the same sentences.\n\n\nTheorem 25. Downward L\u00f6wenheim-Skolem Theorem.\nLet \\(M = \\langle d,I\\rangle\\) be an interpretation of the language\n\\(\\LKe\\). Let \\(d_1\\) be any subset of \\(d\\), and let \\(\\kappa\\) be\nthe maximum of the size of \\(K\\), the size of \\(d_1\\), and denumerably\ninfinite. Then there is a submodel \\(M' = \\langle d',I'\\rangle\\) of\n\\(M\\) such that (1) \\(d'\\) is not larger than \\(\\kappa\\), and (2)\n\\(M\\) and \\(M'\\) are equivalent. In particular, if the set \\(K\\) of\nnon-logical terminology is either finite or denumerably infinite, then\nany interpretation has an equivalent submodel whose domain is either\nfinite or denumerably infinite.\n\nProof: Like completeness, this proof is complex, and\nwe rest content with a sketch. The downward L\u00f6wenheim-Skolem\ntheorem invokes the axiom of choice, and indeed, is equivalent to the\naxiom of choice (see the entry on\n the axiom of choice).\n So let \\(C\\) be a choice function on the powerset of \\(d\\), so that\nfor each non-empty subset \\(e\\subseteq d, C(e)\\) is a member of \\(e\\).\nWe stipulate that if \\(e\\) is the empty set, then \\(C(e)\\) is\n\\(C(d)\\).\n\nLet \\(s\\) be a variable-assignment on \\(M\\), let \\(\\theta\\) be a\nformula of \\(\\LKe\\), and let \\(v\\) be a variable. Define the\n\\(v\\)-witness of \\(\\theta\\) over s, written \\(w_v\n(\\theta,s)\\), as follows: Let \\(q\\) be the set of all elements \\(c\\in\nd\\) such that there is a variable-assignment \\(s'\\) on \\(M\\) that\nagrees with \\(s\\) on every variable except possibly \\(v\\), such that\n\\(M,s'\\vDash \\theta\\), and \\(s'(v)=c\\). Then \\(w_v (\\theta,s) =\nC(q)\\). Notice that if \\(M,s\\vDash \\exists v\\theta\\), then \\(q\\) is\nthe set of elements of the domain that can go for \\(v\\) in \\(\\theta\\).\nIndeed, \\(M,s\\vDash \\exists v\\theta\\) if and only if \\(q\\) is\nnon-empty. So if \\(M,s\\vDash \\exists v\\theta\\), then \\(w_v\n(\\theta,s)\\) (i.e., \\(C(q))\\) is a chosen element of the domain that\ncan go for \\(v\\) in \\(\\theta\\). In a sense, it is a\n\u201cwitness\u201d that verifies \\(M,s\\vDash \\exists v\\theta\\).\n\nIf \\(e\\) is a non-empty subset of the domain \\(d\\), then define a\nvariable-assignment \\(s\\) to be an \\(e\\)-assignment if for\nall variables \\(u, s(u)\\) is in \\(e\\). That is, \\(s\\) is an\n\\(e\\)-assignment if \\(s\\) assigns an element of \\(e\\) to each\nvariable. Define \\(sk(e)\\), the Skolem-hull of \\(e\\), to be\nthe set: \n\\[\\begin{align*}\ne \\cup \\{w_v (\\theta,s)|& \\theta  \\text{ is a formula in } \\LKe, \\\\\n  & v \\text{ is a variable, and } \\\\\n  & s \\text{ is an } e\\text{-assignment} \\}.\n\\end{align*}\\]\n\n\nThat is, the Skolem-Hull of \\(e\\) is the set \\(e\\) together with every\n\\(v\\)-witness of every formula over every \\(e\\)-assignment. Roughly,\nthe idea is to start with \\(e\\) and then throw in enough elements to\nmake each existentially quantified formula true. But we cannot rest\ncontent with the Skolem-hull, however. Once we throw the\n\u201cwitnesses\u201d into the domain, we need to deal with\n\\(sk(e)\\) assignments. In effect, we need a set which is its own\nSkolem-hull, and also contains the given subset \\(d_1\\).\n\nWe define a sequence of non-empty sets \\(e_0, e_1,\\ldots\\) as follows:\nif the given subset \\(d_1\\) of \\(d\\) is empty and there are no\nconstants in \\(K\\), then let \\(e_0\\) be \\(C(d)\\), the choice function\napplied to the entire domain; otherwise let \\(e_0\\) be the union of\n\\(d_1\\) and the denotations under \\(I\\) of the constants in \\(K\\). For\neach natural number \\(n, e_{n+1}\\) is \\(sk(e_n)\\). Finally, let \\(d'\\)\nbe the union of the sets \\(e_n\\), and let \\(I'\\) be the restriction of\n\\(I\\) to \\(d'\\). Our interpretation is \\(M' = \\langle\nd',I'\\rangle\\).\n\nClearly, \\(d_1\\) is a subset of \\(d'\\), and so \\(M'\\) is a submodel of\n\\(M\\). Let \\(\\kappa\\) be the maximum of the size of \\(K\\), the size of\n\\(d_1\\), and denumerably infinite. A calculation reveals that the size\nof \\(d'\\) is at most \\(\\kappa\\), based on the fact that there are at\nmost \\(\\kappa\\)-many formulas, and thus, at most \\(\\kappa\\)-many\nwitnesses at each stage. Notice, incidentally, that this calculation\nrelies on the fact that a denumerable union of sets of size at most\n\\(\\kappa\\) is itself at most \\(\\kappa\\). This also relies on the axiom\nof choice.\n\nThe final item is to show that \\(M'\\) is equivalent to \\(M\\): For\nevery formula \\(\\theta\\) and every variable-assignment \\(s\\) on\n\\(M'\\), \n\\[\nM,s\\vDash \\theta \\text{ if and only if }\n  M',s\\vDash \\theta.\n\\]\n\n\nThe proof proceeds by induction on the complexity of \\(\\theta\\).\nUnfortunately, space constraints require that we leave this step as an\nexercise.\n\n\nAnother corollary to Compactness (Corollary 22) is the opposite of the\nL\u00f6wenheim-Skolem theorem:\n\n\nTheorem 26. Upward L\u00f6wenheim-Skolem Theorem. Let\n\\(\\Gamma\\) be any set of sentences of \\(\\LKe,\\) such that for each\nnatural number \\(n\\), there is an interpretation \\(M_n = \\langle\nd_n,I_n\\rangle\\), such that \\(d_n\\) has at least \\(n\\) elements, and\n\\(M_n\\) satisfies every member of \\(\\Gamma\\). In other words,\n\\(\\Gamma\\) is satisfiable and there is no finite upper bound to the\nsize of the interpretations that satisfy every member of \\(\\Gamma\\).\nThen for any infinite cardinal \\(\\kappa\\), there is an interpretation\n\\(M=\\langle d,I\\rangle\\), such that the size of \\(d\\) is at\nleast \\(\\kappa\\) and \\(M\\) satisfies every member of\n\\(\\Gamma\\).\n\nProof: Add a collection of new constants\n\\(\\{c_{\\alpha} | \\alpha \\lt \\kappa \\}\\), of size \\(\\kappa\\), to the\nlanguage, so that if \\(c\\) is a constant in \\(K\\), then \\(c_{\\alpha}\\)\nis different from \\(c\\), and if \\(\\alpha \\lt \\beta \\lt \\kappa\\), then\n\\(c_{\\alpha}\\) is a different constant than \\(c_{\\beta}\\). Consider\nthe set of sentences \\(\\Gamma'\\) consisting of \\(\\Gamma\\) together\nwith the set \\(\\{\\neg c_{\\alpha}=c_{\\beta} | \\alpha \\ne \\beta \\}\\).\nThat is, \\(\\Gamma'\\) consists of \\(\\Gamma\\) together with statements\nto the effect that any two different new constants denote different\nobjects. Let \\(\\Gamma''\\) be any finite subset of \\(\\Gamma'\\), and let\n\\(m\\) be the number of new constants that occur in \\(\\Gamma''\\). Then\nexpand the interpretation \\(M_m\\) to an interpretation \\(M_m'\\) of the\nnew language, by interpreting each of the new constants in\n\\(\\Gamma''\\) as a different member of the domain \\(d_m\\). By\nhypothesis, there are enough members of \\(d_m\\) to do this. One can\ninterpret the other new constants at will. So \\(M_m\\) is a restriction\nof \\(M_m'\\). By hypothesis (and Theorem 15), \\(M'_m\\) satisfies every\nmember of \\(\\Gamma\\). Also \\(M'_m\\) satisfies the members of \\(\\{\\neg\nc_{\\alpha}=c_{\\beta} | \\alpha \\ne \\beta \\}\\) that are in \\(\\Gamma''\\).\nSo \\(M'_m\\) satisfies every member of \\(\\Gamma''\\). By compactness,\nthere is an interpretation \\(M = \\langle d,I\\rangle\\) such that \\(M\\)\nsatisfies every member of \\(\\Gamma'\\). Since \\(\\Gamma'\\) contains\nevery member of \\(\\{\\neg c_{\\alpha}=c_{\\beta} | \\alpha \\ne \\beta \\}\\),\nthe domain \\(d\\) of \\(M\\) must be of size at least \\(\\kappa\\), since\neach of the new constants must have a different denotation. By Theorem\n15, the restriction of \\(M\\) to the original language \\(\\LKe\\)\nsatisfies every member of \\(\\Gamma\\).\n\n\nCombined, the proofs of the downward and upward L\u00f6wenheim-Skolem\ntheorems show that for any satisfiable set \\(\\Gamma\\) of sentences, if\nthere is no finite bound on the models of \\(\\Gamma\\), then for any\ninfinite cardinal \\(\\kappa\\), there is a model of \\(\\Gamma\\) whose\ndomain has size exactly \\(\\kappa\\). Moreover, if \\(M\\) is any\ninterpretation whose domain is infinite, then for any infinite\ncardinal \\(\\kappa\\), there is an interpretation \\(M'\\) whose domain\nhas size exactly \\(\\kappa\\) such that \\(M\\) and \\(M'\\) are\nequivalent.\n\nThese results indicate a weakness in the expressive resources of\nfirst-order languages like \\(\\LKe\\). No satisfiable set of sentences\ncan guarantee that its models are all denumerably infinite, nor can\nany satisfiable set of sentences guarantee that its models are\nuncountable. So in a sense, first-order languages cannot express the\nnotion of \u201cdenumerably infinite\u201d, at least not in the\nmodel theory. (See the entry on\n second-order and higher-order logic.)\n\nLet \\(A\\) be any set of sentences in a first-order language \\(\\LKe\\),\nwhere \\(K\\) includes terminology for arithmetic, and assume that every\nmember of \\(A\\) is true of the natural numbers. We can even let \\(A\\)\nbe the set of all sentences in \\(\\LKe\\) that are true of the natural\nnumbers. Then \\(A\\) has uncountable models, indeed models of any\ninfinite cardinality. Such interpretations are among those that are\nsometimes called unintended, or non-standard models\nof arithmetic. Let \\(B\\) be any set of first-order sentences that are\ntrue of the real numbers, and let \\(C\\) be any first-order\naxiomatization of set theory. Then if \\(B\\) and \\(C\\) are satisfiable\n(in infinite interpretations), then each of them has denumerably\ninfinite models. That is, any first-order, satisfiable set theory or\ntheory of the real numbers, has (unintended) models the size of the\nnatural numbers. This is despite the fact that a sentence (seemingly)\nstating that the universe is uncountable is provable in most\nset-theories. This situation, known as the Skolem paradox,\nhas generated much discussion, but we must refer the reader elsewhere\nfor a sample of it (see the entry on\n Skolem\u2019s paradox\n and Shapiro 1996).\n6. The One Right Logic?\n\nSurely, logic has something to do with correct reasoning, or at least\ncorrect deductive reasoning. The details of the connection are subtle,\nand controversial \u2013 see Harman [1984] for an influential study.\nIt is common to say that someone has reasoned poorly if they have not\nreasoned logically, or that a given (deductive) argument is bad, and\nmust be retracted, if it is shown to be invalid. \n\nSome philosophers and logicians have maintained that there is a single\nlogical system that is uniquely correct, in its role of characterizing\nvalidity. Among those, some, perhaps most, favor classical,\nfirst-order logic as uniquely correct, as the One True Logic. See, for\nexample, Quine [1986], Resnik [1996], Rumfitt [2015], Williamson\n[2017], and a host of others. \n\nThat classical, first-order logic should be given this role is perhaps\nnot surprising. It has rules which are more or less intuitive, and is\nsimple for how strong it is. As we have seen in section 5, classical,\nfirst-order logic has interesting and important meta-theoretic\nproperties, such as soundness and completeness, that have lead to many\nimportant mathematical and logical studies.\n\nHowever, as noted, the main meta-theoretic properties of classical,\nfirst-order logic lead to expressive limitations of the\nformal languages and model-theoretic semantics. Key notions, like\nfinitude, countability, minimal closure, natural number, and the like\ncannot be expressed. \n\nBarwise [1985, 5] once remarked:\n\nAs logicians, we do our subject a disservice by convincing others that\nlogic is first-order and then convincing them that almost none of the\nconcepts of modern mathematics can really be captured in first-order\nlogic.\n\n\nAnd Wang [1974, 154]:\n\nWhen we are interested in set theory or classical analysis, the\nL\u00f6wenheim-Skolem theorem is usually taken as a sort of defect...\nof the first-order logic... [W]hat is established [by these theorems]\nis not that first-order logic is the only possible logic but rather\nthat it is the only possible logic when we in a sense deny reality to\nthe concept of [the] uncountable...\n\n\nOther criticisms of classical, first-order logic have also been\nlodged. There are issues with its ability to deal with certain\nparadoxes (see, for example, the entry on\n Russel\u2019s paradox ),\n its apparent overgeneration of beliefs (see the entry on\n (the normative status of logic),\n and some argue that it has some arguments that do not match with the\nway we normally think we think (see for example, the entry on\n relevance logic).\n\nThere are two main options available to those who are critical of\nclassical, first-order logic, as the One True Logic. One is to propose\nsome other logic as the One True Logic. Priest [2006a] describes the\nmethodology one might use to settle in the One True Logic.\n\nThe other main option is to simply deny that there is a single logic\nthat qualifies as the One True Logic. One instance of this is a kind\nof logical nihilism, a thesis that there is no correct logic.\nAnother is a logical pluralism, the thesis that a variety of\ndifferent logical all qualify as correct, or best, or even the true\nlogic, at least in various contexts.\n\nOf course, this is not the place to pursue this matter in detail. See\nBeall and Restall [2006] and Shapiro [2014] for examples of pluralism,\nand the entry on\n logical pluralism\n for an overview of the terrain for both logical pluralism and logical\nnihilism.\n\nWe close with brief sketches of some of the main alternatives to\nclassical, first-order logic, providing references to other work and\nentries to this Encyclopedia. See also the second half of Shapiro and\nKouri Kissel [2022].\n6.1 Rivals to classical, first-order logic\nApproximations\n\nIn recent years, some work has been done to \"approximate\" classical\nlogic. The idea is to get as close to classical logic as possible, in\norder to preserve some of the benefits, while at the same time\nremoving some limitations of classical logic, like being closer to\nintuitive inference or applying to things like vagueness and\nparadoxes. \n\nFor example, Barrio, Pailos and Szmuc [2020] show that we can\napproximate classical logic in something called the ST-hierarchy (ST\nfor strict-tolerant, from Cobreros, Egre, Ripley and van Rooij\n[2012a,b]). This allows them to avoid certain classical problems at\neach level of the hierarchy, like some of the paradoxes, while at the\nsame time maintaining many of the benefits of the strength of\nclassical logic when considering the full hierarchy.\n\nDave Ripley [2013] provides a multi-sequent calculus version of\n\u201cclassical logic\u201d that he argues solves some of the\nparadoxes. Notably, he claims it solves at least the Sorites and Liar\nParadoxes (see the entries on the\n sorites paradox\n and\n liar Paradox).\n The system conservatively extends classical logic. Ripley claims that\nthis is what makes it classical. However, the system is not\ntransitive, and does not have a Cut rule.\n\nThere are, of course, some questions about whether these new logics\nare really classical, but it is informative work\nnonetheless.\nExpansions\n\nOne way to extend classical, first-order logic is to add additional\noperators to the underlying formal language. Modal logic adds\noperators which designate necessity and possibility. So, we can say\nthat a proposition is possibly true, or necessarily true, rather than\njust true.\n\nW. V. O Quine [1953] once argued that it is not coherent for\nquantifiers to bind variables inside modal operators, but opinion on\nthis matter has since changed considerably (see, for example, Barcan\n[1990]). There is now a thriving industry of developing modal logics\nto capture various kinds of modality and temporal operators. See the\nentry on\n modal logic.\n \n\nAll of the formal languages sketched above have only one sort of\nvariable. These are sometimes called first-order variables.\nEach interpretation of the language has a domain, which is the range\nof these first-order variables. It is what the language is about,\naccording to the given interpretation. Second-order variables\nrange over properties, sets, classes, relations, or functions of the\nitems in that domain. Third-order variables range over\nproperties, classes, relations of whatever is in the range of the\nsecond-order variables. And it goes on from there.\n\nA formal language is called second-order if it has\nsecond-order variables and first-order variables, and no others;\nThird-order if it has third-order, second-order, and\nfirst-order variables and no others, etc. A formal language is\nhigher-order if it is at least second-order.\n\nAs noted, it is not an exaggeration to say that classical, first-order\nlogic is the paradigm of contemporary logical theory. Most textbooks\ndo not mention higher-order languages at all, and most of the rest\ngive it scant treatment.\n\nA number of different deductive systems and model-theoretic semantics\nhave been proposed for second- and higher-order languages. For the\nsemantics, the main additional feature of the model-theory is to\nspecify a range of the higher-order variables.\n\nIn Henkin semantics, each interpretation specifies a specific\nrange of the higher-order variables. For monadic second-order\nvariables, each interpretation specifies a non-empty subset of the\npowerset of the domain, for two-place second-order variables, a\nnon-empty set of ordered pairs of members of the domain, etc. The\nsystem has all of the above limitative meta-theoretic results. There\nis a deductive system that is sound and complete for Henkin semantics;\nthe logic is compact; and the downward and upward\nL\u00f6wenheim-Skolem theorems all hold.\n\nIn so-called standard semantics, sometimes called full\nsemantics, monadic second-order variables range over the entire\npowerset of the domain; two-place second-order variables range over\nthe entire class of ordered pairs of members of the domain, etc. It\ncan be shown that second-order languages, with standard semantics, can\ncharacterize many mathematical notions and structures, up to\nisomorphism. Examples include the notions of finitude, countability,\nwell-foundedness, minimal closure, and structures like the natural\nnumbers, the real numbers, and the complex numbers. As a result, none\nof the limitative theorems of classical, first-order logic hold: there\nis no effective deductive system is both sound and complete, the logic\nis not compact, and both L\u00f6wenheim-Skolem theorems fail. Some,\nsuch as Quine [1986], argue that second-order logic, with standard\nsemantics is not really logic, but is a form of mathematics, set\ntheory in particular. For more on this, see Shapiro [1991] and the\nentry on\n higher-order logic,\n along with the many references cited there.\n\nOne might also consider generalized quantifiers as an expansion of\nclassical first-order logic (see the entry on\n generalized quantifiers).\n These quantifiers allow from an expansion between the classical\n\u201call\u201d and \u201csome\u201d , and can accommodate\nquantifiers like \u201cmost\u201d , \u201cless than half\u201d ,\n\u201cusually\u201d , etc. They are useful from both a logical and\nlinguistic perspective. For example, Kennedy and\nV\u00e4\u00e4n\u00e4nen [2021] use generalized quantifiers to argue\nthat \u201c uncountable\u201d is a logical notion.\n6.2 Sublogics of classical, first-order logic\n\nSome philosophers and logicians argue that classical, first-order\nlogic is too strong: it declares that some argument-forms are valid\nwhich are not. Here we sketch two kinds of proposals.\nIntuitionistic logic\n\nAdvocates of intuitionistic logic reject the validity of the\n(so-called) Law of Excluded Middle: \n\\[\n\\Phi \\vee \\neg \\Phi,\n\\]\n\n\nand other inferences related to this, such as Double Negation\nElimination (DNE): \n\\[\n{\\rm If}\\   \\Gamma \\vdash \\neg\\neg\\Phi \\ {\\rm then}\\  \\Gamma \\vdash \\Phi\n\\]\n\n\nRoughly speaking, there are two main motivations for these\nrestrictions. The traditional intuitionists L. E. J. Brouwer (e.g.,\n[1964a], [1964b]) and Arend Heyting (e.g. [1956]) held that the\nessence of mathematics is idealized mental construction. Consider, for\nexample, the proposition that for every natural number \\(n\\), there is\na prime number \\(m \\gt n\\) such that \\(m \\lt n!+2\\). For Brouwer, this\nproposition invokes a procedure that, given any natural\nnumber \\(n\\), produces a prime number \\(m\\) that is greater than \\(n\\)\nbut less than \\(n!+2\\). The proposition expresses the existence of\nsuch a procedure. Given this orientation, we have no reason to hold\nthat for any mathematical proposition \\(\\Phi\\), we can establish\neither the procedure associated with \\(\\Phi\\) or the procedure\nassociated with \\(\\neg \\Phi\\).\n\nMichael Dummett (e.g., [1978]) provides general arguments concerning\nhow language functions, as a vehicle of communication, to argue that\nintuitionistic logic is uniquely correct, the One True Logic, not just\nfor mathematics.\n\nFor an overview of intuitionistic logic, and its philosophical\nmotivation, see the entry on\n intuitionistic logic.\nRelevance and paraconsistency\n\nThis time the target inference, to be declared invalid, is the one we\nabove call ex falso quodlibet, abbreviated (EFQ):\n\n\\[\n{\\rm If} \\ \\Gamma_1 \\vdash \\Theta \\ {\\rm and} \\ \\Gamma_2 \\vdash \\neg\\Theta \\ {\\rm then} \\ \\Gamma_1, \\Gamma_2 \\vdash \\Psi\n\\]\n We can focus attention one kind of instance of this:\n\n\\[\n\\Phi, \\neg\\Phi \\vdash \\Psi,\n\\]\n sometimes colorfully called \u201cexplosion\u201d. It\nsays that anything at all follows from a contradiction.\n\nLogics that regard (EFQ) as invalid are called\nparaconsistent. Broadly speaking, there are two camps of\nlogicians advocating for paraconsistent systems, either as candidates\nfor the One True Logic or as instances of pluralism. One camp consists\nof logicians who insist that in a valid argument, the premises must be\nrelevant to the conclusion. Typically, relevance logicians\nalso demur from certain classical logical truths called paradoxes\nof material implication, such as \\((\\Phi \\rightarrow (\\Psi\n\\rightarrow \\Phi))\\) and \\((\\Phi \\rightarrow (\\Psi \\rightarrow\n\\Psi))\\). \n\nFor more, see the entry on\n relevance logic,\n or Kerr [2019]. Classic works include Anderson and Belnap [1975],\nAnderson Belnap and Dunn [1992], and Read [1988]. Neil Tennant\u2019s\n[2017] core logic is both relevant and intuitionistic.\n\nThe other main camp of logicians who prefer a paraconsistent logic (or\nparaconsistent logics) are advocates of dialetheism, the view\nthat some contradictions, some sentences in the form \n\\[\n(\\Phi \\wedge \\neg \\Phi),\n\\]\n are\ntrue. One supposed example is when \\(\\Phi\\) is a statement of a\nsemantic paradoxes, such as the Liar. Consider, for example, a\nsentence \\(\\Phi\\) that says that \\(\\Phi\\) is not true.\n\nIn a system in which (EFQ) holds, any true contradiction would entail\nevery sentence of the formal language, thus rendering the language and\ntheory trivial. So, clearly, any logic for dialetheism would have to\nbe paraconsistent. See the entry on\n dialetheism.\n The classic work here is Priest [2006a].\n\nOf course, the small sample presented here does not include every\nlogical system proposed as a rival to classical, first-order logic,\nagain either as a candidate for the One True Logic, or as a further\ninstance of logical pluralism. See, for example, the entries on\n substructural logics,\n fuzzy logic, and many others.\n",
    "bibliography": {
        "categories": [
            "Further Reading"
        ],
        "cat_ref_text": {
            "Further Reading": [
                "</h3>\n<p>\nThere are many fine textbooks on mathematical logic. A sample\nfollows.</p>\n<ul class=\"hanging\">",
                "Boolos, George S., John P. Burgess, and Richard C. Jeffrey, 2007,\n<em>Computability and logic</em>, fifth edition, Cambridge, England:\nCambridge University Press. Elementary and intermediate level.",
                "Bergmann, Merrie, James Moor, and Jack Nelson, 2013, <em>The logic\nbook</em>, sixth edition, New York: McGraw-Hill. Elementary and\nintermediate level.",
                "Church, Alonzo, 1956, <em>Introduction to mathematical logic</em>,\nPrinceton: Princeton University Press. Classic textbook.",
                "Enderton, Herbert, 1972, <em>A mathematical introduction to\nlogic</em>, New York: Academic Press. Textbook in mathematical logic,\naimed at a mathematical audience.",
                "Forbes, Graeme, 1994, <em>Modern Logic</em>, Oxford: Oxford\nUniversity Press. Elementary textbook.",
                "Mendelson, Elliott, 1987, <em>Introduction to mathematical\nlogic</em>, third edition, Princeton: van Nostrand. Intermediate.\n</ul>\n</div>"
            ]
        },
        "raw_text": "<div id=\"bibliography\">\n<h2 id=\"Bib\">Bibliography</h2>\n<ul class=\"hanging\">\n<li>Anderson, Alan and Nuel Belnap, 1975, <em>Entailment: The logic of\nrelevance and necessity I</em>, Princeton: Princeton University\nPress.</li>\n<li>Anderson, Alan, Nuel Belnap, and J. Michael Dunn, 1992,\n<em>Entailment: The logic of relevance and necessity II</em>,\nPrinceton: Princeton University Press.</li>\n<li>Barcan Marcus, Ruth. 1990, \u201cA Backwards Look at\nQuine\u2019s Animadversions on Modalities,\u201d in R. Bartrett and\nR. Gibson (eds.), <em>Perspectives on Quine</em>, Cambridge:\nBlackwell.</li>\n<li>Barrio, Eduardo Alejandro., Federico Pailos, and Damian Szmuc,\n2020, \u201cA Hierarchy of Classical and Paraconsistent\nLogics\u201d, <em>J Philos Logic</em>, 49: 93\u2013120.\ndoi:10.1007/s10992-019-09513-z</li>\n<li>Barwise, Jon, 1985, \u201cModel-theoretic logics: background and\naims\u201d, in <em>Model-theoretic logics</em>, edtied by Jon Barwise\nand Soloman Feferman (eds.), New York, Springer-Verlag, pp.\n3\u201323.</li>\n<li>Beall, Jc and Greg Restall, 2006, <em>Logical Pluralism</em>,\nOxford: Oxford University Press.</li>\n<li>Brouwer, L.E.J., 1949, \u201cConsciousness, Philosophy and\nMathematics\u201d, <em>Journal of Symbolic Logic</em>, 14(2):\n132\u2013133. </li>\n<li>\u2013\u2013\u2013, 1964b, \u201cIntuitionism and\nFormalism\u201d, in <em>Philosophy of mathematics: selected\nreadings</em> edited by P. Benacerraf and H. Putnam, Englewood Cliffs,\nNJ, Cambridge University Press, (eds.), 77\u201389.</li>\n<li>Cobreros, Pablo, Paul Egr\u00e9, David Ripley, and Robert van\nRooij, 2012, \u201cTolerance and Mixed Consequence in the\nS\u2019valuationist Setting\u201d, <em>Studia logica</em>, 100(4),\n855\u2013877.</li>\n<li>\u2013\u2013\u2013, 2012, \u201cTolerant, classical,\nstrict\u201d, <em>Journal of Philosophical Logic</em>, 41(2),\n347\u2013385.</li>\n<li>Cook, Roy, 2002, \u201cVagueness and mathematical\nprecision\u201d, <em>Mind</em>, 111: 227\u2013247.</li>\n<li>Corcoran, John, 1973, \u201cGaps between logical theory and\nmathematical practice\u201d, <em>The methodological unity of\nscience</em>, ed. by M. Bunge, Dordrecht: D. Reidel, 23\u201350.</li>\n<li>Davidson, Donald, 1984, <em>Inquiries into truth and\ninterpretation</em>, Oxford: Clarendon Press.</li>\n<li>Dummett, Michael, 2000, <em>Elements of intuitionism</em>, second\nedition, Oxford: Oxford University Press.</li>\n<li>\u2013\u2013\u2013, 1978, \u201cThe philosophical basis of\nintuitionistic logic\u201d, in <em>Truth and other enigmas</em>,\nCambridge, MA, Harvard University Press, 215\u2013247.</li>\n<li>G\u00f6del, Kurt, 1930, \u201cDie Vollst\u00e4ndigkeit der Axiome\ndes logischen Funktionenkalkuls\u201d, <em>Montatshefte f\u00fcr\nMathematik und Physik 37</em>, 349\u2013360; translated as \u201cThe\ncompleteness of the axioms of the functional calculus of logic\u201d,\nin van Heijenoort 1967, 582\u2013591.</li>\n<li>Harman, Gilbert, 1984, \u201cLogic and reasoning\u201d,\n<em>Synthese</em>, 60, 107\u2013127.</li>\n<li>Heyting, A., 1956, <em>Intuitionism</em>, Amsterdam: North-Holland\nPublishing.</li>\n<li>Kerr, Alison Duncan, 2019, \u201cA plea for KR\u201d,\n<em>Synthese</em>, 198(4): 3047\u20133071.</li>\n<li>Lycan, William, 1984, <em>Logical form in natural language</em>,\nCambridge, MA: The MIT Press.</li>\n<li>Montague, Richard, 1974, <em>Formal philosophy</em>, ed. by R.\nThomason, New Haven: Yale University Press.</li>\n<li>Kennedy, Juliette, and Jouko V\u00e4\u00e4n\u00e4nen, 2021,\nLogicality and modelclasses. <em>Bulletin of Symbolic Logic</em>,\n27(4): 385\u2013414.</li>\n<li>Priest, Graham, 2006a, <em>In contradiction, a study of the\ntransconsistent</em>, second, revised edition, Oxford: Clarendon\nPress.</li>\n<li>\u2013\u2013\u2013, 2006b, <em>Doubt truth to be a liar</em>,\nOxford: Clarendon Press.</li>\n<li>Quine, W. V. O., 1960, <em>Word and object</em>, Cambridge, MA:\nThe MIT Press.</li>\n<li>\u2013\u2013\u2013, 1953, \u201cThree grades of modal\ninvolvement\u201d, <em>Proceedings of the XI<sup>th</sup>\nInternational Congress of Philosophy</em>, 14, Amsterdam, North\nHolland Publishing Company, 65\u201381.</li>\n<li>\u2013\u2013\u2013, 1986, <em>Philosophy of logic</em>, second\nedition, Cambridge, Massachusetts: Harvard University Press.</li>\n<li>\u2013\u2013\u2013, 1986, <em>Philosophy of logic</em>, second\nedition, Englewood Cliffs: Prentice-Hall.</li>\n<li>Read, Stephen, 1988, <em>Relevant logic</em>, Oxford: Oxford\nUniversity Press. </li>\n<li>Resnik, Michael, 1996, \u201cOught there to be but one true\nlogic\u201d, in <em>Logic and Reality: Essays on the Legacy of Arthur\nPrior</em>, J. Copeland (ed.), Oxford: Oxford University Press,\n489\u2013517.</li>\n<li>Ripley, David, 2013, \u201cParadoxes and Failures of Cut\u201d,\n<em>Australasian Journal of Philosophy</em>, 91(1):\n139\u2013164.</li>\n<li>Rumfitt, Ian, 2015, <em>The Boundary Stones of Thought: An Essay\nin the Philosophy of Logic</em>, Oxford: Oxford University Press.</li>\n<li>Shapiro, Stewart, 1991, <em>Foundations without\nFoundationalism</em>, Oxford: Clarendon Press.</li>\n<li>\u2013\u2013\u2013, 1996, <em>The limits of logic: Second-order\nlogic and the Skolem paradox</em>, <em>The international research\nlibrary of philosophy</em>, Dartmouth Publishing Company, 1996. (An\nanthology containing many of the significant later papers on the\nSkolem paradox.)</li>\n<li>\u2013\u2013\u2013, 1998, \u201cLogical consequence: models\nand modality\u201d, in <em>The philosophy of mathematics today</em>,\nedited by M. Schirn, Oxford: Oxford University Press,\n131\u2013156.</li>\n<li>\u2013\u2013\u2013, 2014, <em>Varieties of Logic</em>, Oxford:\nOxford University Press.</li>\n<li>Shapiro, Stewart and Teresa Kouri Kissel, <em>Classical, first\norder logic, Cambridge Elements</em>, Cambridge, Cambridge University\nPress.</li>\n<li>Tennant, Neil, 1997, <em>The taming of the true</em>, Oxford:\nClarendon Press.</li>\n<li>Van Heijenoort, Jean, 1967, <em>From Frege to G\u00f6del</em>,\nCambridge, Massachusetts: Harvard University Press. An anthology\ncontaining many of the major historical papers on mathematical logic\nin the early decades of the twentieth century.</li>\n<li>Wang, Hao, 1974, <em>From Mathematics to Philosophy</em>, London,\nRoutledge and Kegan Paul.</li>\n<li>Williamson, Timothy, 2017, \u201cSemantic paradoxes and abductive\nmethodology\u201d, in <em>Reflections on the liar</em> edited by\nBradley Armour-Garb, Oxford, Oxford University Press,\n325\u2013346.</li>\n</ul>\n<h3 id=\"FurtRead\">Further Reading</h3>\n<p>\nThere are many fine textbooks on mathematical logic. A sample\nfollows.</p>\n<ul class=\"hanging\">\n<li>Boolos, George S., John P. Burgess, and Richard C. Jeffrey, 2007,\n<em>Computability and logic</em>, fifth edition, Cambridge, England:\nCambridge University Press. Elementary and intermediate level.</li>\n<li>Bergmann, Merrie, James Moor, and Jack Nelson, 2013, <em>The logic\nbook</em>, sixth edition, New York: McGraw-Hill. Elementary and\nintermediate level.</li>\n<li>Church, Alonzo, 1956, <em>Introduction to mathematical logic</em>,\nPrinceton: Princeton University Press. Classic textbook.</li>\n<li>Enderton, Herbert, 1972, <em>A mathematical introduction to\nlogic</em>, New York: Academic Press. Textbook in mathematical logic,\naimed at a mathematical audience.</li>\n<li>Forbes, Graeme, 1994, <em>Modern Logic</em>, Oxford: Oxford\nUniversity Press. Elementary textbook.</li>\n<li>Mendelson, Elliott, 1987, <em>Introduction to mathematical\nlogic</em>, third edition, Princeton: van Nostrand. Intermediate.</li>\n</ul>\n</div>"
    },
    "related_entries": {
        "entry_list": [
            "logic: free",
            "logic: infinitary",
            "logic: intuitionistic",
            "logic: linear",
            "logic: modal",
            "logic: paraconsistent",
            "logic: relevance",
            "logic: second-order and higher-order",
            "logic: substructural",
            "logic: temporal",
            "logical consequence",
            "logical form",
            "logical truth",
            "model theory",
            "model theory: first-order",
            "paradox: Skolem\u2019s",
            "proof theory: development of"
        ],
        "entry_link": [
            {
                "../logic-free/": "logic: free"
            },
            {
                "../logic-infinitary/": "logic: infinitary"
            },
            {
                "../logic-intuitionistic/": "logic: intuitionistic"
            },
            {
                "../logic-linear/": "logic: linear"
            },
            {
                "../logic-modal/": "logic: modal"
            },
            {
                "../logic-paraconsistent/": "logic: paraconsistent"
            },
            {
                "../logic-relevance/": "logic: relevance"
            },
            {
                "../logic-higher-order/": "logic: second-order and higher-order"
            },
            {
                "../logic-substructural/": "logic: substructural"
            },
            {
                "../logic-temporal/": "logic: temporal"
            },
            {
                "../logical-consequence/": "logical consequence"
            },
            {
                "../logical-form/": "logical form"
            },
            {
                "../logical-truth/": "logical truth"
            },
            {
                "../model-theory/": "model theory"
            },
            {
                "../modeltheory-fo/": "model theory: first-order"
            },
            {
                "../paradox-skolem/": "paradox: Skolem\u2019s"
            },
            {
                "../proof-theory-development/": "proof theory: development of"
            }
        ]
    },
    "academic_tools": {
        "listed_text": [
            "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=logic-classical\" target=\"other\">How to cite this entry</a>.",
            "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://leibniz.stanford.edu/friends/preview/logic-classical/\" target=\"other\">Preview the PDF version of this entry</a> at the\n <a href=\"https://leibniz.stanford.edu/friends/\" target=\"other\">Friends of the SEP Society</a>.",
            "<img alt=\"inpho icon\" src=\"../../symbols/inpho.png\"/>",
            "<a href=\"https://www.inphoproject.org/entity?sep=logic-classical&amp;redirect=True\" target=\"other\">Look up topics and thinkers related to this entry</a>\n at the Internet Philosophy Ontology Project (InPhO).",
            "<img alt=\"phil papers icon\" src=\"../../symbols/pp.gif\"/>",
            "<a href=\"https://philpapers.org/sep/logic-classical/\" target=\"other\">Enhanced bibliography for this entry</a>\nat <a href=\"https://philpapers.org/\" target=\"other\">PhilPapers</a>, with links to its database."
        ],
        "listed_links": [
            {
                "https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=logic-classical": "How to cite this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/preview/logic-classical/": "Preview the PDF version of this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/": "Friends of the SEP Society"
            },
            {
                "https://www.inphoproject.org/entity?sep=logic-classical&redirect=True": "Look up topics and thinkers related to this entry"
            },
            {
                "https://philpapers.org/sep/logic-classical/": "Enhanced bibliography for this entry"
            },
            {
                "https://philpapers.org/": "PhilPapers"
            }
        ]
    },
    "other_internet_resources": {
        "listed_text": [],
        "listed_links": []
    }
}