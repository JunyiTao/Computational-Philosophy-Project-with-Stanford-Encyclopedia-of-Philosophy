{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import json\n",
    "from urllib.request import urlopen, Request\n",
    "from lxml import html\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('src/')\n",
    "import preprocessing\n",
    "# import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def load_data(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_data(filepath, data):\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# find the index of https://philpapers.org/rec/SMUQAF\n",
    "last_idx = url_list.index(\"https://philpapers.org/rec/SMUQAF\")\n",
    "\n",
    "url_list = url_list[last_idx:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:  https://philpapers.org/rec/LACGKA 'NoneType' object has no attribute 'text'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def extract_categories(soup):\n",
    "    # Finding the <div> with the specific id and itemprop\n",
    "    categories_div = soup.find('div', {'id': 'record-categories', 'itemprop': 'keywords'})\n",
    "    if not categories_div:\n",
    "        return \"No categories found\"\n",
    "    \n",
    "    # Extracting category details from each <div> within the main <div>\n",
    "    categories = []\n",
    "    for div in categories_div.find_all('div'):\n",
    "        cat_name = div.find('a', class_='catName').text if div.find('a', class_='catName') else None\n",
    "        cat_area = div.find('a', class_='catArea').text if div.find('a', class_='catArea') else None\n",
    "        if cat_name and cat_area:\n",
    "            categories.append({'Category Name': cat_name, 'Category Area': cat_area})\n",
    "    return categories\n",
    "\n",
    "def extract_keywords(soup):\n",
    "    # First, find all <div> elements with itemprop=\"keywords\"\n",
    "    all_keywords_divs = soup.find_all('div', class_=\"\")\n",
    "\n",
    "    # We will filter these to exclude those that are part of the categories section\n",
    "    for div in all_keywords_divs:\n",
    "        # Check if this div is a direct child of the categories section (by checking parent properties)\n",
    "        if div.parent.get('itemprop') == 'keywords':\n",
    "            continue  # Skip this div, it's part of categories\n",
    "\n",
    "        # Assuming the correct keywords div has a class value that is empty\n",
    "        if div.find('div', itemprop='keywords'):\n",
    "            keywords = [keyword.text.strip() for keyword in div.find_all('a')]\n",
    "            return keywords\n",
    "\n",
    "    return \"No appropriate keywords section found\"\n",
    "\n",
    "def extract_meta(soup):\n",
    "    # title: <h1 class=\"recTitle\" itemprop=\"name\"\n",
    "    title = soup.find('h1', class_='recTitle').text\n",
    "    # auhor: <div class=\"recAuthors\"\n",
    "    author = soup.find('div', class_='recAuthors').text\n",
    "    # pubinfo: <div class=\"recPubInfo\"\n",
    "    pubinfo = soup.find('div', class_='recPubInfo').text\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"author\": author,\n",
    "        \"pubinfo\": pubinfo\n",
    "    }\n",
    "\n",
    "def get_categories_and_keywords(url, headers_list):\n",
    "    headers_index = 0\n",
    "    headers = headers_list[headers_index]\n",
    "    response = requests.get(url, headers=headers)\n",
    "    while response.status_code == 429:\n",
    "        retry_after = int(response.headers.get(\"Retry-After\", 35))\n",
    "        print(f\"Rate limit hit. Sleeping for {retry_after} seconds.\")\n",
    "        time.sleep(retry_after)\n",
    "        headers_index = (headers_index + 1) % len(headers_list)  # Rotate headers\n",
    "        headers = headers_list[headers_index]\n",
    "        response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # extracting categories and keywords\n",
    "    categories = extract_categories(soup)\n",
    "    keywords = extract_keywords(soup)\n",
    "\n",
    "    # extracting metadata\n",
    "    meta = extract_meta(soup)\n",
    "\n",
    "    # print(\"Categories:\", categories)\n",
    "    # print(\"Keywords:\", keywords)\n",
    "    item_dict = {\n",
    "        \"url\": url,\n",
    "        \"meta\": meta,\n",
    "        \"categories\": categories,\n",
    "        \"keywords\": keywords,\n",
    "        \"all_info\": soup.prettify()\n",
    "    }\n",
    "    return item_dict\n",
    "\n",
    "def process_urls():\n",
    "    enhanced_biblio_url_list = url_list\n",
    "    # problem_list = {}\n",
    "\n",
    "    headers_list = [\n",
    "        {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'},\n",
    "        {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:125.0) Gecko/20100101 Firefox/125.0'},\n",
    "        {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9'}\n",
    "    ]\n",
    "\n",
    "    for url in enhanced_biblio_url_list:\n",
    "        try:\n",
    "            item_dict = get_categories_and_keywords(url, headers_list)\n",
    "            file_name = url.split('/')[-1]\n",
    "            save_data(\"taxonomy_data/\" + file_name + \".json\", item_dict)\n",
    "            print(\"LOADED: \", url)\n",
    "        except Exception as e:\n",
    "            print(\"ERROR: \", url, str(e))\n",
    "            # problem_list[\n",
    "            # save\n",
    "            with open(\"taxonomy_data/to_check/problem_list.json\", 'a') as f:\n",
    "                json.dump({url: str(e)}, f, indent=4)\n",
    "            time.sleep(5)  # Simple retry mechanism\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    process_urls()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attatch keywords to entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22dc362db5447169549294c20722d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/69784 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69781\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m item_dict_list \u001b[38;5;241m=\u001b[39m get_all_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtaxonomy_data/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(item_dict_list))\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28mprint\u001b[39m(item_dict_list[:\u001b[38;5;241m10\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "# open all the json files in the directory taxonomy_data/\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_all_info(directory):\n",
    "    data = {}\n",
    "    for filename in tqdm(os.listdir(directory)):\n",
    "        if filename.endswith(\".json\"):\n",
    "            info_dict = load_data(os.path.join(directory, filename))\n",
    "            extracted_info_dict = {\n",
    "                # \"url\": info_dict[\"url\"],\n",
    "                # \"meta\": info_dict[\"meta\"],\n",
    "                \"categories\": info_dict[\"categories\"],\n",
    "                \"keywords\": info_dict[\"keywords\"]\n",
    "            }\n",
    "            data[info_dict[\"url\"]] = extracted_info_dict\n",
    "            # tqdm.write(f\"Loaded {len(data)} files\")\n",
    "\n",
    "    # save to database/sep_clean\n",
    "    save_data(\"taxonomy_data/sep_clean/philpaper_keywords.json\", data)\n",
    "    return data\n",
    "\n",
    "item_dict_list = get_all_info(\"taxonomy_data/\")\n",
    "print(len(item_dict_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e13be762ece409dbe85b50ce6a1f168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# info dict of entry and bibliography\n",
    "# load \"taxonomy_data/sep_clean/enhanced_bibliographies.json\"\n",
    "ent_bibli_dict = preprocessing.load_data(\"database/sep_clean/enhanced_bibliographies.json\")\n",
    "\n",
    "# load \"taxonomy_data/sep_clean/philpaper_keywords.json\"\n",
    "philpaper_keywords_dict = preprocessing.load_data(\"database/sep_clean/philpaper_keywords.json\")\n",
    "\n",
    "# merge the philpaper_keywords_dict into the items of ent_bibli_dict[url][\"philpaper_archived_url_list\"]\n",
    "for url, item in tqdm(ent_bibli_dict.items()):\n",
    "    item[\"philpaper_archived\"] = {}\n",
    "    for philpaper_url in item[\"philpaper_archived_url_list\"]:\n",
    "        if philpaper_url in philpaper_keywords_dict:\n",
    "            # only take \"Category Name\" from categories and \"keywords\" from keywords\n",
    "            info_dict = philpaper_keywords_dict[philpaper_url]\n",
    "            # categories = [] if \"No categories found\"\n",
    "            # keywords = [] if \"No appropriate keywords section found\"\n",
    "            cat = [i[\"Category Name\"] for i in info_dict[\"categories\"]] if info_dict[\"categories\"] != \"No categories found\" else []\n",
    "            kw = info_dict[\"keywords\"] if info_dict[\"keywords\"] != \"No appropriate keywords section found\" else []\n",
    "\n",
    "            extracted_info_dict = {\n",
    "                \"categories\": cat,\n",
    "                \"keywords\": kw\n",
    "            }\n",
    "            item[\"philpaper_archived\"][philpaper_url] = extracted_info_dict\n",
    "    # remove philpaper_archived_url_list\n",
    "    item.pop(\"philpaper_archived_url_list\")\n",
    "    # print(item)\n",
    "    # break\n",
    "\n",
    "# save to database/sep_clean\n",
    "preprocessing.save_data(\"database/sep_clean/entry_bibli_categories_keywords.json\", ent_bibli_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxonomy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "determine path_all_parents by \n",
    "\n",
    "first, locate the level of the current entry\n",
    "\n",
    "for example, \"forgiveness\" is 5: \"Subtopics\"\n",
    "\n",
    "so we start from level 5\n",
    "\n",
    "then gradually reduce the level as treversing through its parents\n",
    "\n",
    "so we get something like:\n",
    "\n",
    "{\"level1\":, # all parents (clusters) of level 2\n",
    "\"level2\": # all parents of level 3 parents\n",
    "\"level3\":((329,\"Mental States and Processes\",),(30,\"Normative Ethics\"),(6143,\"Practical Reason\")), # these are all parents of 29,\n",
    "\"level4\": (29, \"Moral Psychology\"), # this is the parent of 4712\n",
    "\"level5\": (4712,\"Moral States and Processes\")\n",
    "\n",
    "if the entry start from level 4, just leave level 5 as blank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated categories have been written to PhilPaper/philpaper_taxonomy_full_path.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def write_json_file(data, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def build_parent_map(categories):\n",
    "    parent_map = {1: {\"name\": \"Philosophy\", \"parent_ids\": [], \"primary_parent_id\": None}}  # Include root category\n",
    "    for category in categories:\n",
    "        parent_map[category[1]] = {\n",
    "            \"name\": category[0],\n",
    "            \"parent_ids\": list(map(int, category[2].split(','))),\n",
    "            \"primary_parent_id\": category[3]\n",
    "        }\n",
    "    return parent_map\n",
    "\n",
    "def determine_category_level(current_id, parent_map):\n",
    "    level = 1\n",
    "    while current_id != 1:\n",
    "        if parent_map[current_id][\"primary_parent_id\"] == 1:\n",
    "            break\n",
    "        current_id = parent_map[current_id][\"primary_parent_id\"]\n",
    "        level += 1\n",
    "    return level\n",
    "\n",
    "def level_to_name(level):\n",
    "    return {\n",
    "        0: \"Root\",\n",
    "        1: \"Clusters\",\n",
    "        2: \"Areas\",\n",
    "        3: \"Subareas\",\n",
    "        4: \"Topics\",\n",
    "        5: \"Subtopics\"\n",
    "    }.get(level, \"Unknown\")\n",
    "\n",
    "def update_categories_with_full_paths(categories):\n",
    "    parent_map = build_parent_map(categories)\n",
    "    updated_categories = {\n",
    "        \"1\": {\n",
    "        \"title\": \"Philosophy\",\n",
    "        \"level\": \"0 Root\",\n",
    "        \"primary_parent\": [],\n",
    "        \"path_primary_parents\": [],\n",
    "        \"path_all_parents\": []}\n",
    "    }\n",
    "    \n",
    "    for category in categories:\n",
    "        current_id = category[1]\n",
    "        primary_path = []\n",
    "        all_parents_path = []\n",
    "\n",
    "        level = determine_category_level(current_id, parent_map)\n",
    "        \n",
    "        # Traverse through primary parents to root\n",
    "        while current_id != 1:\n",
    "            category_info = parent_map.get(current_id, None)\n",
    "            if not category_info:\n",
    "                break\n",
    "            if category_info['primary_parent_id'] != 1:\n",
    "                # primary_path.append(str((category_info['primary_parent_id'], parent_map[category_info['primary_parent_id']]['name'])))\n",
    "                primary_path.append((category_info['primary_parent_id']))\n",
    "            current_id = category_info['primary_parent_id']\n",
    "\n",
    "        # traverse through all parents to root\n",
    "        current_id = category[1]\n",
    "        for lvl in range(1, level+1):\n",
    "            # all_parents_path.append([str((id, parent_map[id]['name'])) for id in map(int, parent_map[current_id]['parent_ids']) if id in parent_map])\n",
    "            all_parents_path.append([id for id in map(int, parent_map[current_id]['parent_ids']) if id in parent_map])\n",
    "            current_id = parent_map[current_id]['primary_parent_id']\n",
    "\n",
    "        # Reverse to maintain order from root to the category\n",
    "        primary_path.reverse()\n",
    "        all_parents_path.reverse()\n",
    "\n",
    "        # Create the detailed path for all parents\n",
    "        # for lvl in range(1, level+1):\n",
    "        #     all_parents_path[f\"level{lvl}\"] = [(id, parent_map[id]['name']) for id in map(int, categories[lvl-1][2].split(',')) if id in parent_map]\n",
    "\n",
    "        # get all parents found during traversal\n",
    "        # put them in the correct level\n",
    "\n",
    "        # Prepare the updated category entry as a dictionary\n",
    "        updated_categories[category[1]] = {\n",
    "            \"title\": category[0],\n",
    "            \"level\": f\"{level} {level_to_name(level)}\",\n",
    "            \"primary_parent\": str((category[3], parent_map[category[3]]['name'])),\n",
    "            \"path_primary_parents\": primary_path,\n",
    "            \"path_all_parents\": all_parents_path\n",
    "        }\n",
    "    \n",
    "    return updated_categories\n",
    "\n",
    "def main():\n",
    "    input_file = 'PhilPaper/philpaper_taxonomy.json'\n",
    "    output_file = 'PhilPaper/philpaper_taxonomy_full_path.json'\n",
    "    \n",
    "    categories = read_json_file(input_file)\n",
    "    updated_categories = update_categories_with_full_paths(categories)\n",
    "    write_json_file(updated_categories, output_file)\n",
    "    print(f'Updated categories have been written to {output_file}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correspond taxonomy to entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b61a5f3a2e4d928ba8f1a764a6a9a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://philpapers.org/sep/music/ 2\n",
      "https://philpapers.org/sep/wang-yangming/ 18\n",
      "https://philpapers.org/sep/nishida-kitaro/ 3\n",
      "https://philpapers.org/sep/kyoto-school/ 4\n",
      "https://philpapers.org/sep/school-names/ 37\n",
      "https://philpapers.org/sep/ibn-arabi/ 1\n",
      "https://philpapers.org/sep/ethics-ai/ 1\n",
      "https://philpapers.org/sep/mysticism/ 1\n",
      "https://philpapers.org/sep/daoism-religion/ 14\n",
      "https://philpapers.org/sep/chinese-social-political/ 19\n",
      "https://philpapers.org/sep/determinate-determinables/ 1\n",
      "https://philpapers.org/sep/kant-aesthetics/ 2\n",
      "https://philpapers.org/sep/chinese-metaphysics/ 56\n",
      "http://philpapers.org/sep/aesthetics-of-everyday/ 2\n",
      "https://philpapers.org/sep/knowledge-how/ 4\n",
      "https://philpapers.org/sep/chinese-epistemology/ 37\n",
      "http://philpapers.org/sep/contradiction/ 1\n",
      "https://philpapers.org/sep/confucian-gender/ 21\n",
      "https://philpapers.org/sep/confucianism-modern/ 13\n",
      "https://philpapers.org/sep/chinese-logic-language/ 45\n",
      "https://philpapers.org/sep/comparphil-chiwes/ 49\n",
      "https://philpapers.org/sep/buddhism-huayan/ 29\n",
      "http://philpapers.org/sep/vegetarianism/ 1\n",
      "https://philpapers.org/sep/korean-philosophy/ 5\n",
      "https://philpapers.org/sep/qing-philosophy/ 11\n",
      "https://philpapers.org/sep/action/ 1\n",
      "https://philpapers.org/sep/han-dynasty/ 7\n",
      "https://philpapers.org/sep/levinas/ 1\n",
      "https://philpapers.org/sep/implicit-bias/ 1\n",
      "http://philpapers.org/sep/loyalty/ 2\n",
      "https://philpapers.org/sep/concept-emotion-india/ 1\n",
      "https://philpapers.org/sep/chinese-legalism/ 18\n",
      "http://philpapers.org/sep/xunzi/ 17\n",
      "https://philpapers.org/sep/moral-relativism/ 2\n",
      "https://philpapers.org/sep/immunology/ 1\n",
      "https://philpapers.org/sep/emotions-chinese/ 20\n",
      "https://philpapers.org/sep/hobbes-moral/ 4\n",
      "https://philpapers.org/sep/madhyamaka/ 1\n",
      "https://philpapers.org/sep/daoism/ 46\n",
      "http://philpapers.org/sep/mereology/ 1\n",
      "https://philpapers.org/sep/buddhism-chan/ 2\n",
      "http://philpapers.org/sep/zhu-xi/ 43\n",
      "http://philpapers.org/sep/chinese-translate-interpret/ 35\n",
      "https://philpapers.org/sep/ethics-social-networking/ 2\n",
      "https://philpapers.org/sep/japanese-confucian/ 5\n",
      "https://philpapers.org/sep/justice-virtue/ 3\n",
      "https://philpapers.org/sep/song-ming-confucianism/ 36\n",
      "https://philpapers.org/sep/philosophy-religion/ 1\n",
      "https://philpapers.org/sep/mohism/ 44\n",
      "https://philpapers.org/sep/suhrawardi/ 1\n",
      "https://philpapers.org/sep/mencius/ 40\n",
      "http://philpapers.org/sep/chinese-phil-science/ 10\n",
      "https://philpapers.org/sep/findlay/ 1\n",
      "https://philpapers.org/sep/seneca/ 1\n",
      "http://philpapers.org/sep/mind-indian-buddhism/ 2\n",
      "https://philpapers.org/sep/god-ultimates/ 2\n",
      "https://philpapers.org/sep/religious-experience/ 2\n",
      "https://philpapers.org/sep/korean-confucianism/ 4\n",
      "https://philpapers.org/sep/erotic-art/ 1\n",
      "http://philpapers.org/sep/rule-of-law/ 1\n",
      "https://philpapers.org/sep/ethics-chinese/ 83\n",
      "https://philpapers.org/sep/chinese-mind/ 16\n",
      "https://philpapers.org/sep/feminism-aesthetics/ 1\n",
      "https://philpapers.org/sep/world-government/ 1\n",
      "https://philpapers.org/sep/meritocracy/ 4\n",
      "https://philpapers.org/sep/meister-eckhart/ 1\n",
      "https://philpapers.org/sep/japanese-zen/ 1\n",
      "https://philpapers.org/sep/respect/ 9\n",
      "https://philpapers.org/sep/ethics-virtue/ 10\n",
      "https://philpapers.org/sep/thoreau/ 1\n",
      "http://philpapers.org/sep/epistemology-india/ 1\n",
      "https://philpapers.org/sep/legitimacy/ 1\n",
      "https://philpapers.org/sep/aristotle-ethics/ 5\n",
      "https://philpapers.org/sep/ethics-environmental/ 1\n",
      "https://philpapers.org/sep/supererogation/ 1\n",
      "https://philpapers.org/sep/nothingness/ 3\n",
      "https://philpapers.org/sep/buddhism-tiantai/ 3\n",
      "https://philpapers.org/sep/neo-daoism/ 17\n",
      "https://philpapers.org/sep/evolution-before-darwin/ 1\n",
      "https://philpapers.org/sep/twotruths-tibet/ 1\n",
      "https://philpapers.org/sep/communitarianism/ 4\n",
      "http://philpapers.org/sep/laozi/ 30\n",
      "https://philpapers.org/sep/desert/ 1\n",
      "https://philpapers.org/sep/argument/ 3\n",
      "https://philpapers.org/sep/moral-character-empirical/ 2\n",
      "https://philpapers.org/sep/freewill/ 3\n",
      "https://philpapers.org/sep/dewey-aesthetics/ 4\n",
      "http://philpapers.org/sep/zhuangzi/ 34\n",
      "http://philpapers.org/sep/moral-psych-emp/ 1\n",
      "http://philpapers.org/sep/africana/ 2\n",
      "https://philpapers.org/sep/chinese-change/ 7\n",
      "http://philpapers.org/sep/mohist-canons/ 34\n",
      "http://philpapers.org/sep/chinese-phil-medicine/ 7\n",
      "https://philpapers.org/sep/happiness/ 2\n",
      "https://philpapers.org/sep/environmental-aesthetics/ 1\n",
      "https://philpapers.org/sep/confucius/ 9\n",
      "https://philpapers.org/sep/globalization/ 1\n",
      "https://philpapers.org/sep/wolff-christian/ 1\n"
     ]
    }
   ],
   "source": [
    "# load stanford_encyclopedia/PhilPaper/philpaper_taxonomy_full_path.json\n",
    "philpaper_taxonomy_full_path = load_data(\"PhilPaper/philpaper_taxonomy_full_path.json\")\n",
    "\n",
    "# load database/sep_clean/entry_bibli_categories_keywords.json\n",
    "entry_bibli_categories_keywords = load_data(\"database/sep_clean/entry_bibli_categories_keywords.json\")\n",
    "\n",
    "# for each philpaper_archived work in each entry\n",
    "# find \"title\" in dict in philpaper_taxonomy_full_path that matches \"categories\" in dict in entry_bibli_categories_keywords\n",
    "# add \"full_path\" to entry_bibli_categories_keywords\n",
    "\n",
    "\n",
    "# we need \n",
    "# 1/ label (identify categories) of cited archived works from philpaper assigned categories and keywords\n",
    "# 2/ label categories of the entry based on its cited works' labels\n",
    "\n",
    "\n",
    "for url, item in tqdm(entry_bibli_categories_keywords.items()): # for each entry\n",
    "    count_dict = {\n",
    "        62: 0\n",
    "        }\n",
    "    for philpaper_url, philpaper_item in item[\"philpaper_archived\"].items(): # for each philpaper archived work cited in the entry\n",
    "        philpaper_item[\"categories_with_path\"] = {}\n",
    "        for cat in philpaper_item[\"categories\"]:\n",
    "            # start search for path of this cat\n",
    "            for cat_id, cat_item in philpaper_taxonomy_full_path.items():\n",
    "                if cat == cat_item[\"title\"]:\n",
    "                    path_primary_parents = cat_item[\"path_primary_parents\"]\n",
    "                    path_all_parents = cat_item[\"path_all_parents\"]\n",
    "                    # philpaper_item[\"categories_with_path\"][cat_id] = {\n",
    "                    #     \"path_primary_parents\": cat_item[\"path_primary_parents\"],\n",
    "                    #     \"path_all_parents\": cat_item[\"path_all_parents\"]\n",
    "                    #     }\n",
    "                    break # end search\n",
    "            # identify categories\n",
    "            for k,v in count_dict.items():\n",
    "                if k in path_primary_parents: # only consider primary parents for now\n",
    "                    count_dict[k] += 1\n",
    "                    break\n",
    "    if count_dict[62] > 0:\n",
    "        print(url, count_dict[62])\n",
    "    # print(count_dict)\n",
    "             \n",
    "\n",
    "# save_data(\"database/sep_clean/entry_bibli_cat_with_path.json\", entry_bibli_categories_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75d3cd09312a5c5e4ea9ca503f0de7315f2a6856e8eb6e1d90df3dbfd3ff1a46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
