{
    "main_text": "The Bias-Variance Trade-off || Bounded Rationality and Bias-Variance Generalized\n4.2 Bounded Rationality and Bias-Variance Generalized\n\nIntuitively, the bias-variance decomposition brings to light a\ntrade-off between two extreme approaches to making a prediction. At\none extreme, you might adopt as an estimator a constant function which\nproduces the same answer no matter what data you see. Suppose 7 is\nyour lucky number and your estimator\u2019s prediction, \\(h(X) = 7\\).\nThen the variance of \\(h(\\cdot)\\) would be zero, since its prediction\nis always the same. The bias of your estimator, however, will be very\nlarge. In other words, your lucky number 7 model will massively\nunder fit your data.\n\nAt the other extreme, suppose you aim to make your bias error zero.\nThis occurs just when the predicted value of Y and the actual\nvalue of Y are identical, that is, \\(h(x_i) = y_i\\), for every\n\\((x_i, y_i)\\). Since you are presumed to not know the true function\n\\(r(X)\\) but instead only see a sample of data from the true model,\n\\(\\mathcal{D}\\), it is from this sample that you will aspire to\nconstruct an estimator that generalizes to accurately predict examples\noutside your training data \\(\\mathcal{D}\\). Yet if you were to fit\n\\(h_{\\mathcal{D}}(X)\\) perfectly to \\(\\mathcal{D}\\), then the variance\nof your estimator will be very high, since a different data set\n\\(\\mathcal{D}'\\) from the true model is not, by definition, identical\nto \\(\\mathcal{D}\\). How different is \\(\\mathcal{D}'\\) to\n\\(\\mathcal{D}\\)? The variation from one data set to another among all\nthe possible data sets is the variance or irreducible noise of the\ndata generated by the true model, which may be considerable.\nTherefore, in this zero-bias case your model will massively\noverfit your data.\n\nThe bias-variance trade-off therefore concerns the question of how\ncomplex a model ought to be to make reasonably accurate predictions on\nunseen or out-of-sample examples. The problem is to strike a balance\nbetween an under-fitting model, which erroneously ignores available\ninformation about the true function r, and an overfitting\nmodel, which erroneously includes information that is noise and\nthereby gives misleading information about the true function\nr.\n\nOne thing that human cognitive systems do very well is to generalize\nfrom a limited number of examples. The difference between humans and\nmachines is particularly striking when we compare how humans learn a\ncomplicated skill, such as driving a car, from how a machine learning\nsystem learns the same task. As harrowing an experience it is to teach\na teenager how to drive a car, they do not need to crash into a\nutility pole 10,000 times to learn that utility poles are not\ntraversable. What teenagers learn as children about the world through\nplay and observing other people drive lends to them an understanding\nthat utility poles are to be steered around, a piece of\ncommonsense that our current machine learning systems do not have but\nmust learn from scratch on a case-by-case basis. We, unlike our\nmachines, have a remarkable capacity to transfer what we learn from\none domain to another domain, a capacity fueled in part by our\ncuriosity (Kidd & Hayden 2015).\n\nViewed from the perspective of the bias-variance trade-off, the\nability to make accurate predictions from sparse data suggests that\nvariance is the dominant source of error but that our cognitive system\noften manages to keep these errors within reasonable limits\n(Gigerenzer & Brighton 2009). Indeed, Gigerenzer and Brighton make\na stronger argument, stating that \u201cthe bias-variance dilemma\nshows formally why a mind can be better off with an adaptive toolbox\nof biased, specialized heuristics\u201d (Gigerenzer & Brighton\n2009: 120); see also\n section 7.2.\n However, the bias-variance decomposition is a decomposition of\nsquared loss, which means that the decomposition above depends on how\ntotal error (loss) is measured. There are many loss functions,\nhowever, depending on the type of inference one is making along with\nthe stakes in making it. If one were to use a 0-1 loss function, for\nexample, where all non-zero errors are treated equally\u2014meaning\nthat \u201ca miss as good as a mile\u201d\u2014the decomposition\nabove breaks down. In fact, for 0-1 loss, bias and variance combine\nmultiplicatively (J. Friedman 1997)! A generalization of the\nbias-variance decomposition that applies to a variety of loss\nfunctions \\(\\mathrm{L}(\\cdot)\\), including 0-1 loss, has been offered\nby (Domingos 2000),  \n\n\\[\\mathrm{L}(h)\\ = \\ \\mathrm{B}(h)^2  \\ + \\ \\beta_1\\textrm{Var}(h) \\ + \\ \\beta_2\\mathrm{N}\\]\n\n\nwhere the original bias-variance decomposition,\n Equation 4,\n appears as a special case, namely when \\(\\mathrm{L}(h) =\n\\textrm{MSE}(h)\\) and \\(\\beta_1 = \\beta_2 = 1\\).\n",
    "section_title": "4.2 Bounded Rationality and Bias-Variance Generalized",
    "entry_title": "Bounded Rationality",
    "hierarchy_title": "Bounded Rationality || The Bias-Variance Trade-off || Bounded Rationality and Bias-Variance Generalized",
    "tokenized_text": [
        "biasvariance",
        "tradeoff",
        "bounded",
        "rationality",
        "biasvariance",
        "generalized",
        "bounded",
        "rationality",
        "biasvariance",
        "generalized",
        "intuitively",
        "biasvariance",
        "decomposition",
        "brings",
        "light",
        "tradeoff",
        "two",
        "extreme",
        "approach",
        "making",
        "prediction",
        "one",
        "extreme",
        "might",
        "adopt",
        "estimator",
        "constant",
        "function",
        "produce",
        "answer",
        "matter",
        "data",
        "see",
        "suppose",
        "lucky",
        "number",
        "estimator",
        "prediction",
        "h",
        "x",
        "variance",
        "h",
        "cdot",
        "would",
        "zero",
        "since",
        "prediction",
        "always",
        "bias",
        "estimator",
        "however",
        "large",
        "word",
        "lucky",
        "number",
        "model",
        "massively",
        "fit",
        "data",
        "extreme",
        "suppose",
        "aim",
        "make",
        "bias",
        "error",
        "zero",
        "occurs",
        "predicted",
        "value",
        "actual",
        "value",
        "identical",
        "h",
        "x_i",
        "y_i",
        "every",
        "x_i",
        "y_i",
        "since",
        "presumed",
        "know",
        "true",
        "function",
        "r",
        "x",
        "instead",
        "see",
        "sample",
        "data",
        "true",
        "model",
        "mathcal",
        "sample",
        "aspire",
        "construct",
        "estimator",
        "generalizes",
        "accurately",
        "predict",
        "example",
        "outside",
        "training",
        "data",
        "mathcal",
        "yet",
        "fit",
        "h_",
        "mathcal",
        "x",
        "perfectly",
        "mathcal",
        "variance",
        "estimator",
        "high",
        "since",
        "different",
        "data",
        "set",
        "mathcal",
        "true",
        "model",
        "definition",
        "identical",
        "mathcal",
        "different",
        "mathcal",
        "mathcal",
        "variation",
        "one",
        "data",
        "set",
        "another",
        "among",
        "possible",
        "data",
        "set",
        "variance",
        "irreducible",
        "noise",
        "data",
        "generated",
        "true",
        "model",
        "may",
        "considerable",
        "therefore",
        "zerobias",
        "case",
        "model",
        "massively",
        "overfit",
        "data",
        "biasvariance",
        "tradeoff",
        "therefore",
        "concern",
        "question",
        "complex",
        "model",
        "ought",
        "make",
        "reasonably",
        "accurate",
        "prediction",
        "unseen",
        "outofsample",
        "example",
        "problem",
        "strike",
        "balance",
        "underfitting",
        "model",
        "erroneously",
        "ignores",
        "available",
        "information",
        "true",
        "function",
        "r",
        "overfitting",
        "model",
        "erroneously",
        "includes",
        "information",
        "noise",
        "thereby",
        "give",
        "misleading",
        "information",
        "true",
        "function",
        "r",
        "one",
        "thing",
        "human",
        "cognitive",
        "system",
        "well",
        "generalize",
        "limited",
        "number",
        "example",
        "difference",
        "human",
        "machine",
        "particularly",
        "striking",
        "compare",
        "human",
        "learn",
        "complicated",
        "skill",
        "driving",
        "car",
        "machine",
        "learning",
        "system",
        "learns",
        "task",
        "harrowing",
        "experience",
        "teach",
        "teenager",
        "drive",
        "car",
        "need",
        "crash",
        "utility",
        "pole",
        "time",
        "learn",
        "utility",
        "pole",
        "traversable",
        "teenager",
        "learn",
        "child",
        "world",
        "play",
        "observing",
        "people",
        "drive",
        "lends",
        "understanding",
        "utility",
        "pole",
        "steered",
        "around",
        "piece",
        "commonsense",
        "current",
        "machine",
        "learning",
        "system",
        "must",
        "learn",
        "scratch",
        "casebycase",
        "basis",
        "unlike",
        "machine",
        "remarkable",
        "capacity",
        "transfer",
        "learn",
        "one",
        "domain",
        "another",
        "domain",
        "capacity",
        "fueled",
        "part",
        "curiosity",
        "kidd",
        "hayden",
        "viewed",
        "perspective",
        "biasvariance",
        "tradeoff",
        "ability",
        "make",
        "accurate",
        "prediction",
        "sparse",
        "data",
        "suggests",
        "variance",
        "dominant",
        "source",
        "error",
        "cognitive",
        "system",
        "often",
        "manages",
        "keep",
        "error",
        "within",
        "reasonable",
        "limit",
        "gigerenzer",
        "brighton",
        "indeed",
        "gigerenzer",
        "brighton",
        "make",
        "stronger",
        "argument",
        "stating",
        "biasvariance",
        "dilemma",
        "show",
        "formally",
        "mind",
        "better",
        "adaptive",
        "toolbox",
        "biased",
        "specialized",
        "heuristic",
        "gigerenzer",
        "brighton",
        "see",
        "also",
        "section",
        "however",
        "biasvariance",
        "decomposition",
        "decomposition",
        "squared",
        "loss",
        "mean",
        "decomposition",
        "depends",
        "total",
        "error",
        "loss",
        "measured",
        "many",
        "loss",
        "function",
        "however",
        "depending",
        "type",
        "inference",
        "one",
        "making",
        "along",
        "stake",
        "making",
        "one",
        "use",
        "loss",
        "function",
        "example",
        "nonzero",
        "error",
        "treated",
        "equallymeaning",
        "miss",
        "good",
        "mile",
        "the",
        "decomposition",
        "break",
        "fact",
        "loss",
        "bias",
        "variance",
        "combine",
        "multiplicatively",
        "j",
        "friedman",
        "generalization",
        "biasvariance",
        "decomposition",
        "applies",
        "variety",
        "loss",
        "function",
        "mathrm",
        "l",
        "cdot",
        "including",
        "loss",
        "offered",
        "domingo",
        "mathrm",
        "l",
        "h",
        "mathrm",
        "b",
        "h",
        "beta_textrm",
        "var",
        "h",
        "beta_mathrm",
        "n",
        "original",
        "biasvariance",
        "decomposition",
        "equation",
        "appears",
        "special",
        "case",
        "namely",
        "mathrm",
        "l",
        "h",
        "textrm",
        "mse",
        "h",
        "beta_",
        "beta_"
    ]
}