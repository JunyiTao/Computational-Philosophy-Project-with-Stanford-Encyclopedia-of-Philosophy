{
    "main_text": "Main Debates || Machine Ethics\n2.8 Machine Ethics\n\nMachine ethics is ethics for machines, for \u201cethical\nmachines\u201d, for machines as subjects, rather than for\nthe human use of machines as objects. It is often not very\nclear whether this is supposed to cover all of AI ethics or to be a\npart of it (Floridi and Saunders 2004; Moor 2006; Anderson and\nAnderson 2011; Wallach and Asaro 2017). Sometimes it looks as though\nthere is the (dubious) inference at play here that if machines act in\nethically relevant ways, then we need a machine ethics. Accordingly,\nsome use a broader notion:\n\n\nmachine ethics is concerned with ensuring that the behavior of\nmachines toward human users, and perhaps other machines as well, is\nethically acceptable. (Anderson and Anderson 2007: 15)\n\n\nThis might include mere matters of product safety, for example. Other\nauthors sound rather ambitious but use a narrower notion:\n\n\nAI reasoning should be able to take into account societal values,\nmoral and ethical considerations; weigh the respective priorities of\nvalues held by different stakeholders in various multicultural\ncontexts; explain its reasoning; and guarantee transparency. (Dignum\n2018: 1, 2)\n\n\nSome of the discussion in machine ethics makes the very substantial\nassumption that machines can, in some sense, be ethical agents\nresponsible for their actions, or \u201cautonomous moral\nagents\u201d (see van Wynsberghe and Robbins 2019). The basic idea of\nmachine ethics is now finding its way into actual robotics where the\nassumption that these machines are artificial moral agents in any\nsubstantial sense is usually not made (Winfield et al. 2019). It is\nsometimes observed that a robot that is programmed to follow ethical\nrules can very easily be modified to follow unethical rules\n(Vanderelst and Winfield 2018).\n\nThe idea that machine ethics might take the form of \u201claws\u201d\nhas famously been investigated by Isaac Asimov, who proposed\n\u201cthree laws of robotics\u201d (Asimov 1942):\n\n\nFirst Law\u2014A robot may not injure a human being or, through\ninaction, allow a human being to come to harm. Second Law\u2014A\nrobot must obey the orders given it by human beings except where such\norders would conflict with the First Law. Third Law\u2014A robot must\nprotect its own existence as long as such protection does not conflict\nwith the First or Second Laws.\n\n\nAsimov then showed in a number of stories how conflicts between these\nthree laws will make it problematic to use them despite their\nhierarchical organisation.\n\nIt is not clear that there is a consistent notion of \u201cmachine\nethics\u201d since weaker versions are in danger of reducing\n\u201chaving an ethics\u201d to notions that would not normally be\nconsidered sufficient (e.g., without \u201creflection\u201d or even\nwithout \u201caction\u201d); stronger notions that move towards\nartificial moral agents may describe a\u2014currently\u2014empty\nset.\n",
    "section_title": "2.8 Machine Ethics",
    "entry_title": "Ethics of Artificial Intelligence and Robotics",
    "hierarchy_title": "Ethics of Artificial Intelligence and Robotics || Main Debates || Machine Ethics",
    "tokenized_text": [
        "main",
        "debate",
        "machine",
        "ethic",
        "machine",
        "ethic",
        "machine",
        "ethic",
        "ethic",
        "machine",
        "ethical",
        "machine",
        "machine",
        "subject",
        "rather",
        "human",
        "use",
        "machine",
        "object",
        "often",
        "clear",
        "whether",
        "supposed",
        "cover",
        "ai",
        "ethic",
        "part",
        "floridi",
        "saunders",
        "moor",
        "anderson",
        "anderson",
        "wallach",
        "asaro",
        "sometimes",
        "look",
        "though",
        "dubious",
        "inference",
        "play",
        "machine",
        "act",
        "ethically",
        "relevant",
        "way",
        "need",
        "machine",
        "ethic",
        "accordingly",
        "use",
        "broader",
        "notion",
        "machine",
        "ethic",
        "concerned",
        "ensuring",
        "behavior",
        "machine",
        "toward",
        "human",
        "user",
        "perhaps",
        "machine",
        "well",
        "ethically",
        "acceptable",
        "anderson",
        "anderson",
        "might",
        "include",
        "mere",
        "matter",
        "product",
        "safety",
        "example",
        "author",
        "sound",
        "rather",
        "ambitious",
        "use",
        "narrower",
        "notion",
        "ai",
        "reasoning",
        "able",
        "take",
        "account",
        "societal",
        "value",
        "moral",
        "ethical",
        "consideration",
        "weigh",
        "respective",
        "priority",
        "value",
        "held",
        "different",
        "stakeholder",
        "various",
        "multicultural",
        "context",
        "explain",
        "reasoning",
        "guarantee",
        "transparency",
        "dignum",
        "discussion",
        "machine",
        "ethic",
        "make",
        "substantial",
        "assumption",
        "machine",
        "sense",
        "ethical",
        "agent",
        "responsible",
        "action",
        "autonomous",
        "moral",
        "agent",
        "see",
        "van",
        "wynsberghe",
        "robbins",
        "basic",
        "idea",
        "machine",
        "ethic",
        "finding",
        "way",
        "actual",
        "robotics",
        "assumption",
        "machine",
        "artificial",
        "moral",
        "agent",
        "substantial",
        "sense",
        "usually",
        "made",
        "winfield",
        "et",
        "al",
        "sometimes",
        "observed",
        "robot",
        "programmed",
        "follow",
        "ethical",
        "rule",
        "easily",
        "modified",
        "follow",
        "unethical",
        "rule",
        "vanderelst",
        "winfield",
        "idea",
        "machine",
        "ethic",
        "might",
        "take",
        "form",
        "law",
        "famously",
        "investigated",
        "isaac",
        "asimov",
        "proposed",
        "three",
        "law",
        "robotics",
        "asimov",
        "first",
        "lawa",
        "robot",
        "may",
        "injure",
        "human",
        "inaction",
        "allow",
        "human",
        "come",
        "harm",
        "second",
        "lawa",
        "robot",
        "must",
        "obey",
        "order",
        "given",
        "human",
        "being",
        "except",
        "order",
        "would",
        "conflict",
        "first",
        "law",
        "third",
        "lawa",
        "robot",
        "must",
        "protect",
        "existence",
        "long",
        "protection",
        "conflict",
        "first",
        "second",
        "law",
        "asimov",
        "showed",
        "number",
        "story",
        "conflict",
        "three",
        "law",
        "make",
        "problematic",
        "use",
        "despite",
        "hierarchical",
        "organisation",
        "clear",
        "consistent",
        "notion",
        "machine",
        "ethic",
        "since",
        "weaker",
        "version",
        "danger",
        "reducing",
        "ethic",
        "notion",
        "would",
        "normally",
        "considered",
        "sufficient",
        "eg",
        "without",
        "reflection",
        "even",
        "without",
        "action",
        "stronger",
        "notion",
        "move",
        "towards",
        "artificial",
        "moral",
        "agent",
        "may",
        "describe",
        "acurrentlyempty",
        "set"
    ]
}