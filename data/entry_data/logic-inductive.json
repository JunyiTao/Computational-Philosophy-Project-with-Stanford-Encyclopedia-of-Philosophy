{
    "url": "logic-inductive",
    "title": "Inductive Logic",
    "authorship": {
        "year": "Copyright \u00a9 2018",
        "author_text": "James Hawthorne\n<hawthorne@ou.edu>",
        "author_links": [
            {
                "http://james-hawthorne.oucreate.com/": "James Hawthorne"
            },
            {
                "mailto:hawthorne%40ou%2eedu": "hawthorne@ou.edu"
            }
        ],
        "raw_html": "<div id=\"article-copyright\">\n<p>\n<a href=\"../../info.html#c\">Copyright \u00a9 2018</a> by\n\n<br/>\n<a href=\"http://james-hawthorne.oucreate.com/\" target=\"other\">James Hawthorne</a>\n&lt;<a href=\"mailto:hawthorne%40ou%2eedu\"><em>hawthorne<abbr title=\" at \">@</abbr>ou<abbr title=\" dot \">.</abbr>edu</em></a>&gt;\n    </p>\n</div>"
    },
    "pubinfo": [
        "First published Mon Sep 6, 2004",
        "substantive revision Mon Mar 19, 2018"
    ],
    "preamble": "\n\nAn inductive logic is a logic of evidential support. In a deductive\nlogic, the premises of a valid deductive argument logically\nentail the conclusion, where logical entailment means\nthat every logically possible state of affairs that makes the premises\ntrue must make the conclusion true as well. Thus, the\npremises of a valid deductive argument provide total support\nfor the conclusion. An inductive logic extends this idea to weaker\narguments. In a good inductive argument, the truth of the premises\nprovides some degree of support for the truth of the\nconclusion, where this degree-of-support might be measured\nvia some numerical scale. By analogy with the notion of deductive\nentailment, the notion of inductive degree-of-support might mean\nsomething like this: among the logically possible states of affairs\nthat make the premises true, the conclusion must be true in (at least)\nproportion r of them\u2014where r is some numerical\nmeasure of the support strength.\n\nIf a logic of good inductive arguments is to be of any\nreal value, the measure of support it articulates should be up to the task. Presumably, the logic should at least satisfy the following condition: \n\n\nCriterion of Adequacy (CoA):\n\nThe logic should make it likely (as a matter of logic) that as evidence accumulates,\nthe total body of true evidence claims will eventually come to indicate, via the logic\u2019s measure of\nsupport, that false hypotheses are probably false and that true\nhypotheses are probably true.\n\n\nThe CoA stated here may strike some readers as surprisingly strong. Given a specific logic of evidential support, how might it be shown to satisfy such a condition?\nSection 4 will show precisely how this condition is satisfied by the logic of evidential support articulated in Sections 1 through 3 of this article.\n\nThis article will focus on the kind of the approach to inductive logic\nmost widely studied by epistemologists and logicians in recent years.\nThis approach employs conditional probability functions to represent\nmeasures of the degree to which evidence statements support\nhypotheses. Presumably, hypotheses should be empirically evaluated\nbased on what they say (or imply) about the likelihood that evidence claims will be true. A\nstraightforward theorem of probability theory, called Bayes\u2019\nTheorem, articulates the way in which what hypotheses say about the likelihoods of evidence claims influences the degree to which hypotheses are\nsupported by those evidence claims. Thus, this approach to the logic\nof evidential support is often called a Bayesian Inductive\nLogic or a Bayesian Confirmation Theory. This article will first provide a detailed explication of a Bayesian approach to inductive logic. It will\nthen examine the extent to which this logic may pass muster as\nan adequate logic of evidential support for hypotheses. In particular,\nwe will see how such a logic may be shown to satisfy the Criterion of\nAdequacy stated above.\n\nSections 1 through 3 present all of the main ideas underlying the\n(Bayesian) probabilistic logic of evidential support. These\nthree sections should suffice to provide an adequate understanding of\nthe subject. Section 5 extends this account to cases where the implications of\nhypotheses about evidence claims (called likelihoods)\nare vague or imprecise. After reading Sections 1 through 3, the reader may safely skip directly to Section 5, bypassing the rather technical account in Section 4 of how how the CoA is satisfied.\n\nSection 4\n is for the more advanced reader who wants an understanding of how\nthis logic may bring about convergence to the true hypothesis\nas evidence accumulates. This result shows that the Criterion of\nAdequacy is indeed satisfied\u2014that as evidence accumulates, false\nhypotheses will very probably come to have evidential support values\n(as measured by their posterior probabilities) that approach\n0; and as this happens, a true hypothesis may very probably acquire\nevidential support values (as measured by its posterior\nprobability) that approaches 1.\n",
    "toc": [
        {
            "#InduArgu": "1. Inductive Arguments"
        },
        {
            "#InduLogiInduProb": "2. Inductive Logic and Inductive Probabilities"
        },
        {
            "#HistOrigProbLogi": "2.1 The Historical Origins of Probabilistic Logic"
        },
        {
            "#ProbLogiAxioChar": "2.2 Probabilistic Logic: Axioms and Characteristics"
        },
        {
            "#TwoConcInduProb": "2.3 Two Conceptions of Inductive Probability"
        },
        {
            "#ApplInduProbEvalScieHypo": "3. The Application of Inductive Probabilities to the Evaluation of Scientific Hypotheses"
        },
        {
            "#Like": "3.1 Likelihoods"
        },
        {
            "#PostProbPrioProb": "3.2 Posterior Probabilities and Prior Probabilities"
        },
        {
            "#BayeTheo": "3.3 Bayes\u2019 Theorem"
        },
        {
            "#PrioProbReprVaguDivePlauAsse": "3.4 On Prior Probabilities and Representations of Vague and Diverse Plausibility Assessments"
        },
        {
            "#LikeRatiConvTheo": "4. The Likelihood Ratio Convergence Theorem"
        },
        {
            "#SpacPossOutcExpeObse": "4.1 The Space of Possible Outcomes of Experiments and Observations"
        },
        {
            "#ProbInde": "4.2 Probabilistic Independence"
        },
        {
            "#LikeRatiConvWhenFalsOutcPoss": "4.3 Likelihood Ratio Convergence when Falsifying Outcomes are Possible"
        },
        {
            "#LikeRatiConvWhenNoFalsOutcPoss": "4.4 Likelihood Ratio Convergence When No Falsifying Outcomes are Possible"
        },
        {
            "#WhenLikeVaguDive": "5. When the Likelihoods are Vague or Diverse"
        },
        {
            "#ListSupp": "List of Supplements"
        },
        {
            "#Bib": "Bibliography"
        },
        {
            "#Aca": "Academic Tools"
        },
        {
            "#Oth": "Other Internet Resources"
        },
        {
            "#Rel": "Related Entries"
        }
    ],
    "main_text": "\n1. Inductive Arguments\n\nLet us begin by considering some common kinds of examples of inductive arguments. Consider the following two arguments:\n\n\nExample 1. Every raven in a random sample of 3200\nravens is black. This strongly supports the following conclusion: All\nravens are black.\n\nExample 2. 62 percent of voters in a random sample of\n400 registered voters (polled on February 20, 2004) said that they\nfavor John Kerry over George W. Bush for President in the 2004\nPresidential election. This supports with a probability of at least\n.95 the following conclusion: Between 57 percent and 67 percent of all\nregistered voters favor Kerry over Bush for President (at or around\nthe time the poll was taken).\n\n\nThis kind of argument is often called an induction by\nenumeration. It is closely related to the technique of statistical\nestimation. We may represent the logical form of such arguments\nsemi-formally as follows:\n\n\nPremise: In random sample S consisting of n members of\npopulation B, the proportion of members that have attribute\nA is r.\n\nTherefore, with degree of support p,\n\nConclusion: The proportion of all members of B that have\nattribute A is between \\(r-q\\) and \\(r+q\\) (i.e., lies within\nmargin of error q of r).\n\n\nLet\u2019s lay out this argument more formally. The premise breaks\ndown into three separate\n statements:[1]\n\n\n\n\n\nSemi-formalization\nFormalization \n\nPremise 1\nThe frequency (or proportion) of members with attribute A\namong the members of S is r.\n\\(F[A,S] = r\\) \n\nPremise 2\nS is a random sample of B with respect to whether\nor not its members have A\n Rnd[\\(S,B,A\\)] \n\nPremise 3\nSample S has exactly n members\nSize[\\(S] = n\\) \n\nTherefore\nwith degree of support p\n\\(========\\{p\\}\\) \n\nConclusion\nThe proportion of members of B that have attribute\nA lies between \\(r-q\\) and \\(r+q\\) (i.e., lies within\nmargin of error q of r)\n\\(F[A,B] = r \\pm q\\) \n\n\n\nAny inductive logic that treats such arguments should address two\nchallenges. (1) It should tell us which enumerative inductive\narguments should count as good inductive arguments. In\nparticular, it should tell us how to determine the appropriate\ndegree p to which such premises inductively\nsupport the conclusion, for a given margin of error q. (2)\nIt should demonstrably satisfy the\n CoA.\n That is, it should be provable (as a metatheorem) that if a\nconclusion expressing the approximate proportion for an attribute in a\npopulation is true, then it is very likely that sufficiently\nnumerous random samples of the population will provide true premises\nfor good inductive arguments that confer degrees of\nsupport p approaching 1 for that true\nconclusion\u2014where, on pain of triviality, these sufficiently\nnumerous samples are only a tiny fraction of a large population.\nThe supplement on\n Enumerative Inductions: Bayesian Estimation and Convergence,\n shows precisely how a a Bayesian account of enumerative induction may\nmeet these two challenges.\n\nEnumerative induction is, however, rather limited in scope. This form\nof induction is only applicable to the support of claims involving\nsimple universal conditionals (i.e., claims of form \u2018All\nBs are As\u2019) and claims about the proportion of an\nattribute in a population (i.e., claims of form \u2018the frequency\nof As among the Bs is r\u2019). But, many\nimportant empirical hypotheses are not reducible to this simple form,\nand the evidence for these hypotheses is not composed of an\nenumeration of such instances. Consider, for example, the Newtonian\nTheory of Mechanics:\n\n\nAll objects remain at rest or in uniform motion unless acted upon by\nsome external force. An object\u2019s acceleration (i.e., the rate at\nwhich its motion changes from rest or from uniform motion) is in the\nsame direction as the force exerted on it; and the rate at which the\nobject accelerates due to a force is equal to the magnitude of the\nforce divided by the object\u2019s mass. If an object exerts a force\non another object, the second object exerts an equal amount of force\non the first object, but in the opposite direction to the force\nexerted by the first object.\n\n\nThe evidence for (and against) this theory is not gotten by examining\na randomly selected subset of objects and the forces acting upon them.\nRather, the theory is tested by calculating what this theory\nsays (or implies) about observable phenomena in a wide\nvariety of specific situations\u2014e.g., ranging from simple\ncollisions between small bodies to the trajectories of planets and\ncomets\u2014and then seeing whether those phenomena occur in the way\nthat the theory says they will. This approach to testing\nhypotheses and theories is ubiquitous, and should be captured by an adequate inductive logic.\n\nMore generally, for a wide range of cases where inductive\nreasoning is important, enumerative induction is inadequate. Rather,\nthe kind of evidential reasoning that judges the likely truth of hypotheses \non the basis of what\nthey say (or imply) about the evidence is more appropriate.\nConsider the kinds of inferences jury members are supposed to make,\nbased on the evidence presented at a murder trial. The inference to\nprobable guilt or innocence is based on a patchwork of evidence of\nvarious kinds. It almost never involves consideration of a randomly\nselected sequences of past situations when people like the accused\ncommitted similar murders. Or, consider how a doctor diagnoses her\npatient on the basis of his symptoms. Although the frequency of\noccurrence of various diseases when similar symptoms have been present may\nplay a role, this is clearly not the whole story. Diagnosticians\ncommonly employ a form of hypothesis evaluation\u2014e.g.,\nwould the hypothesis that the patient has a brain tumor account for his symptoms?; or are these symptoms more likely the result of\na minor stroke?; or may some other hypothesis better account for the\npatient\u2019s symptoms? Thus, a fully adequate account of inductive\nlogic should explicate the logic of hypothesis evaluation,\nthrough which a hypothesis or theory may be tested on the basis of\nwhat it says (or \"predicts\") about observable phenomena. In\n Section 3\n we will see how a kind of probabilistic inductive logic called \"Bayesian Inference\" or\n\"Bayesian Confirmation Theory\" captures such reasoning. The full logical\nstructure of such arguments will be spelled out in that section.\n2. Inductive Logic and Inductive Probabilities\n\nPerhaps the oldest and best understood way of representing partial\nbelief, uncertain inference, and inductive support is in terms\nof probability and the equivalent\nnotion odds. Mathematicians have studied probability for over\n350 years, but the concept is certainly much older. In recent times a\nnumber of other, related representations of partial belief and\nuncertain inference have emerged. Some of these approaches have found\nuseful application in computer based artificial intelligence systems\nthat perform inductive inferences in expert domains such as medical\ndiagnosis. Nevertheless, probabilistic representations have\npredominated in such application domains. So, in this article we will\nfocus exclusively on probabilistic representations of inductive\nsupport. A brief comparative description of some of the most prominent\nalternative representations of uncertainty and support-strength can be\nfound in the supplement\n Some Prominent Approaches to the Representation of Uncertain Inference.\n2.1 The Historical Origins of Probabilistic Logic\n\nThe mathematical study of probability originated with Blaise Pascal\nand Pierre de Fermat in the mid-17th century. From that\ntime through the early 19th century, as the mathematical\ntheory continued to develop, probability theory was primarily applied\nto the assessment of risk in games of chance and to drawing simple\nstatistical inferences about characteristics of large\npopulations\u2014e.g., to compute appropriate life insurance premiums\nbased on mortality rates. In the early 19th century Pierre\nde Laplace made further theoretical advances and showed how to apply\nprobabilistic reasoning to a much wider range of scientific and\npractical problems. Since that time probability has become an\nindispensable tool in the sciences, business, and many other areas of\nmodern life.\n\nThroughout the development of probability theory various researchers appear to have thought of it as a kind of logic. But the first extended treatment of\nprobability as an explicit part of logic was George Boole\u2019s\nThe Laws of Thought (1854). John Venn followed two decades\nlater with an alternative empirical frequentist account of probability\nin The Logic of Chance (1876). Not long after that the whole\ndiscipline of logic was transformed by new developments in deductive\nlogic.\n\nIn the late 19th and early 20th century Frege,\nfollowed by Russell and Whitehead, showed how deductive logic may be\nrepresented in the kind of rigorous formal system we now call\nquantified predicate logic. For the\nfirst time logicians had a fully formal deductive logic powerful\nenough to represent all valid deductive arguments that arise in\nmathematics and the sciences. In this logic the validity of deductive\narguments depends only on the logical structure of the sentences\ninvolved. This development in deductive logic spurred some logicians\nto attempt to apply a similar approach to inductive reasoning. The\nidea was to extend the deductive entailment relation to a notion of\nprobabilistic entailment for cases where premises provide\nless than conclusive support for conclusions. These partial\nentailments are expressed in terms of conditional\nprobabilities, probabilities of the form \\(P[C \\pmid B] = r\\)\n(read \u201cthe probability of C given B is\nr\u201d), where P is a probability function, C\nis a conclusion sentence, B is a conjunction of premise\nsentences, and r is the probabilistic degree of support that\npremises B provide for conclusion C. Attempts to develop\nsuch a logic vary somewhat with regard to the ways in which they attempt to\nemulate the paradigm of formal deductive logic.\n\nSome inductive logicians have tried to follow the deductive paradigm\nby attempting to specify inductive support probabilities solely in\nterms of the syntactic structures of premise and conclusion sentences.\nIn deductive logic the syntactic structure of the sentences involved\ncompletely determines whether premises logically entail a conclusion.\nSo these inductive logicians have attempted to follow suit. \nIn such a system each sentence confers a\nsyntactically specified degree of support on each of the other\nsentences of the language. Thus, the inductive probabilities in such a\nsystem are logical in the sense that they depend on syntactic\nstructure alone. This kind of conception was articulated to some\nextent by John Maynard Keynes in his Treatise on Probability\n(1921). Rudolf Carnap pursued this idea with greater rigor in his\nLogical Foundations of Probability (1950) and in several\nsubsequent works (e.g., Carnap 1952). (For details of Carnap\u2019s\napproach see the section on\n logical probability\n in the entry on\n interpretations of the probability calculus,\n in this Encyclopedia.) \n\nIn the inductive logics of Keynes and Carnap, Bayes\u2019 theorem, a\nstraightforward theorem of probability theory, plays a central role in\nexpressing how evidence comes to bear on hypotheses. Bayes\u2019\ntheorem expresses\nhow the probability of a hypothesis h on the evidence\ne, \\(P[h \\pmid e]\\), depends on the probability that e\nshould occur if h is true, \\(P[e \\pmid h]\\), and on the\nprobability of hypothesis h prior to taking the\nevidence into account, \\(P[h]\\) (called the prior probability\nof h). (Later we\u2019ll examine Bayes\u2019 theorem in detail.) So, such approaches might well be called Bayesian\nlogicist inductive logics. Other prominent Bayesian logicist\nattempts to develop a probabilistic inductive logic include the works\nof Jeffreys (1939), Jaynes (1968), and Rosenkrantz (1981).\n\nIt is now widely held that the core idea of this syntactic approach to\nBayesian logicism is fatally flawed\u2014that syntactic logical\nstructure cannot be the sole determiner of the degree to which\npremises inductively support conclusions. A crucial facet of the\nproblem faced by syntactic Bayesian logicism involves how the logic is\nsupposed to apply in scientific contexts where the conclusion sentence\nis some scientific hypothesis or theory, and the premises are evidence\nclaims. The difficulty is that in any probabilistic logic\nthat satisfies the usual axioms for probabilities, the inductive\nsupport for a hypothesis must depend in part on its prior\nprobability. This prior probability represents\n(arguably) how plausible the hypothesis is taken to be on the basis of\nconsiderations other than the observational and experimental evidence\n(e.g., perhaps due to various plausibility arguments). A syntactic\nBayesian logicist must tell us how to assign values to these\npre-evidential prior probabilities of hypotheses in a way\nthat relies only on the syntactic logical structure of the hypothesis,\nperhaps based on some measure of syntactic simplicity. There are\nsevere problems with getting this idea to work. Various\nkinds of examples seem to show that such an approach must assign\nintuitively quite unreasonable prior probabilities to hypotheses in\nspecific cases (see the footnote cited near the end of\n Section 3.2\n for details). Furthermore, for this idea to apply to the evidential\nsupport of real scientific theories, scientists would have to\nformalize theories in a way that makes their relevant syntactic\nstructures apparent, and then evaluate theories solely on that\nsyntactic basis (together with their syntactic relationships to\nevidence statements). Are we to evaluate alternative theories of\ngravitation, and alternative quantum theories, this way? This seems an\nextremely dubious approach to the evaluation of real scientific\nhypotheses and theories. Thus, it seems that logical structure alone\nmay not suffice for the inductive evaluation of scientific hypotheses.\n(This issue will be treated in more detail in\n Section 3,\n after we first see how probabilistic logics employ Bayes\u2019\ntheorem to represent the evidential support for hypotheses as a\nfunction of prior probabilities together with\nevidential likelihoods.)\n\nAt about the time that the syntactic Bayesian logicist idea was\ndeveloping, an alternative conception of probabilistic inductive\nreasoning was also emerging. This approach is now generally referred\nto as the Bayesian subjectivist or personalist\napproach to inductive reasoning (see, e.g., Ramsey 1926; De Finetti\n1937; Savage 1954; Edwards, Lindman, & Savage 1963; Jeffrey 1983,\n1992; Howson & Urbach 1993; Joyce 1999). This approach treats\ninductive probability as a measure of an agent\u2019s\ndegree-of-belief that a hypothesis is true, given the truth\nof the evidence. This approach was originally developed as part of a\nlarger normative theory of belief and action known as Bayesian\ndecision theory. The principal idea is that the strength of an\nagent\u2019s desires for various possible outcomes should combine\nwith her belief-strengths regarding claims about the world to produce\noptimally rational decisions. Bayesian subjectivists provide a logic\nof decision that captures this idea, and they attempt to justify this\nlogic by showing that in principle it leads to optimal decisions about\nwhich of various risky alternatives should be pursued. On the Bayesian\nsubjectivist or personalist account of inductive probability,\ninductive probability functions represent the subjective (or personal)\nbelief-strengths of ideally rational agents, the kind of belief\nstrengths that figure into rational decision making. (See the section\non\n subjective probability\n in the entry on\n interpretations of the probability calculus,\n in this Encyclopedia.)\n\nElements of a logicist conception of inductive logic live on today as\npart of the general approach called Bayesian inductive logic.\nHowever, among philosophers and statisticians the term\n\u2018Bayesian\u2019 is now most closely associated with the\nsubjectivist or personalist account of belief and decision. And the\nterm \u2018Bayesian inductive logic\u2019 has come to carry the\nconnotation of a logic that involves purely subjective probabilities.\nThis usage is misleading since, for inductive logics, the\nBayesian/non-Bayesian distinction should really turn on whether the\nlogic gives Bayes\u2019 theorem a prominent role, or the approach largely eschews the use of Bayes\u2019 theorem in inductive\ninferences, as do the classical approaches to statistical\ninference developed by R. A. Fisher (1922) and by Neyman & Pearson\n(1967)). Indeed, any inductive logic that employs the same probability\nfunctions to represent both the probabilities of evidence claims\ndue to hypotheses and the probabilities of hypotheses due to\nthose evidence claims must be a Bayesian inductive logic\nin this broader sense; because Bayes\u2019 theorem follows directly\nfrom the axioms that each probability function must satisfy, and\nBayes\u2019 theorem expresses a necessary connection between the\nprobabilities of evidence claims due to hypotheses and the\nprobabilities of hypotheses due to those evidence claims.\n\nIn this article the probabilistic inductive logic we will\nexamine is a Bayesian inductive logic in this broader sense.\nThis logic will not presuppose the subjectivist Bayesian\ntheory of belief and decision, and will avoid the objectionable\nfeatures of the syntactic version of Bayesian logicism. We will see\nthat there are good reasons to distinguish inductive\nprobabilities from degree-of-belief probabilities and\nfrom purely syntactic logical probabilities. So, the\nprobabilistic logic articulated in this article will be presented in a\nway that depends on neither of these conceptions of what the\nprobability functions are. However, this version of the logic\nwill be general enough that it may be fitted to a Bayesian\nsubjectivist or Bayesian syntactic-logicist program, if one desires to\ndo that.\n2.2 Probabilistic Logic: Axioms and Characteristics\n\nAll logics derive from the meanings of terms in sentences. What we now\nrecognize as formal deductive logic rests on the meanings\n(i.e., the truth-functional properties) of the standard logical terms.\nThese logical terms, and the symbols we will employ to represent them,\nare as follows: \n\n\u2018not\u2019, \u2018\\({\\nsim}\\)\u2019; \n\u2018and\u2019, \u2018\\(\\cdot\\)\u2019; \n\u2018inclusive or\u2019, \u2018\\(\\vee\\)\u2019;\ntruth-functional \u2018if-then\u2019, \u2018\\(\\supset\\)\u2019;\n\n\u2018if and only if\u2019, \u2018\\(\\equiv\\)\u2019; \nthe quantifiers\n\n\n\u2018all\u2019, \u2018\\(\\forall\\)\u2019, and \n\u2018some\u2019, \u2018\\(\\exists\\)\u2019; \n and \nthe identity relation, \u2018=\u2019.\n\n\nThe meanings of all other terms, the non-logical terms such as names\nand predicate and relational expressions, are permitted to\n\u201cfloat free\u201d. That is, the logical validity of deductive\narguments depends neither on the meanings of the name and predicate\nand relation terms, nor on the truth-values of sentences containing\nthem. It merely supposes that these non-logical terms are meaningful,\nand that sentences containing them have truth-values. Deductive logic\nthen tells us that the logical structures of some\nsentences\u2014i.e., the syntactic arrangements of their logical\nterms\u2014preclude them from being jointly true of any possible\nstate of affairs. This is the notion of logical\ninconsistency. The notion of logical entailment is\ninter-definable with it. A collection of premise sentences\nlogically entails a conclusion sentence just when the\nnegation of the conclusion is logically inconsistent with\nthose premises.\n\nAn inductive logic must, it seems, deviate from the paradigm provided\nby deductive logic in several significant ways. For one thing, logical\nentailment is an absolute, all-or-nothing relationship between\nsentences, whereas inductive support comes in degrees-of-strength. For\nanother, although the notion of inductive support is\nanalogous to the deductive notion of logical entailment, and\nis arguably an extension of it, there seems to be no inductive logic\nextension of the notion of logical inconsistency\u2014at\nleast none that is inter-definable with inductive support in\nthe way that logical inconsistency is inter-definable with\nlogical entailment. Indeed, it turns out that when the\nunconditional probability of \\((B\\cdot{\\nsim}A)\\) is very nearly 0\n(i.e., when \\((B\\cdot{\\nsim}A)\\) is \u201cnearly\ninconsistent\u201d), the degree to which B inductively\nsupports A, \\(P[A \\pmid B]\\), may range anywhere between 0\nand 1. \n\nAnother notable difference is that when B logically\nentails A, adding a premise C cannot undermine the\nlogical entailment\u2014i.e., \\((C\\cdot B)\\) must logically entail\nA as well. This property of logical entailment is\ncalled monotonicity. But inductive support is\nnonmonotonic. In general, depending on what \\(A, B\\), and\nC mean, adding a premise C to B may substantially\nraise the degree of support for A, or may substantially lower\nit, or may leave it completely unchanged\u2014i.e., \\(P[A \\pmid\n(C\\cdot B)]\\) may have a value much larger than \\(P[A \\pmid B]\\), or\nmay have a much smaller value, or it may have the same, or nearly the\nsame value as \\(P[A \\pmid B]\\).\n\nIn a formal treatment of probabilistic inductive logic, inductive\nsupport is represented by conditional probability functions defined on\nsentences of a formal language L. These conditional probability\nfunctions are constrained by certain rules or axioms that are\nsensitive to the meanings of the logical terms (i.e.,\n\u2018not\u2019, \u2018and\u2019, \u2018or\u2019, etc., the\nquantifiers \u2018all\u2019 and \u2018some\u2019, and the identity\nrelation). The axioms apply without regard for what the other terms of\nthe language may mean. In essence the axioms specify a family of\npossible support functions, \\(\\{P_{\\beta}, P_{\\gamma}, \\ldots\n,P_{\\delta}, \\ldots \\}\\) for a given language L. Although each\nsupport function satisfies these same axioms, the further issue of\nwhich among them provides an appropriate measure of inductive\nsupport is not settled by the axioms alone. That may depend on\nadditional factors, such as the meanings of the non-logical terms\n(i.e., the names and predicate expressions) of the language.\n\nA good way to specify the axioms of the logic of inductive support\nfunctions is as follows. These axioms are apparently weaker than the\nusual axioms for conditional probabilities. For instance, the usual\naxioms assume that conditional probability values are restricted to\nreal numbers between 0 and 1. The following axioms do not assume this,\nbut only that support functions assign some real numbers as values for\nsupport strengths. However, it turns out that the following axioms\nsuffice to derive all the usual axioms for conditional probabilities\n(including the usual restriction to values between 0 and 1). We draw\non these weaker axioms only to forestall some concerns about whether the support\nfunction axioms may assume too much, or may be overly restrictive.\n\n\nLet L be a language for predicate logic with identity, and let\n\u2018\\(\\vDash\\)\u2019 be the standard logical entailment\nrelation\u2014i.e., the expression \u2018\\(B\n\\vDash A\\)\u2019 says\n\u201cB logically entails A\u201d and the expression \u2018\\(\\vDash\nA\\)\u2019 says\n\u201cA is a tautology\u201d. A support function is a\nfunction \\(P_{\\alpha}\\) from pairs of sentences of L to real\nnumbers that satisfies the following axioms:\n\n(1)\\(P_{\\alpha}[E\n\\pmid F] \\ne P_{\\alpha}[G \\pmid H]\\) for at\nleast some sentences \\(E, F, G\\), and H. \n\n\nFor all sentence \\(A, B, C\\), and D:\n\n(2) If\n\\(B \\vDash A\\), then \\(P_{\\alpha}[A \\pmid B] \\ge P_{\\alpha}[C \\pmid\nD]\\); \n(3)\n\\(P_{\\alpha}[A \\pmid (B \\cdot C)] = P_{\\alpha}[A \\pmid (C \\cdot B)]\\);\n\n(4) If\n\\(C \\vDash{\\nsim}(B \\cdot A)\\), then either \n\n\\[P_{\\alpha}[(A \\vee B) \\pmid  C] = P_{\\alpha}[A \\pmid  C] + P_{\\alpha}[B \\pmid  C]\\]\n\n \n\n or else \\[P_{\\alpha}[E \\pmid  C] = P_{\\alpha}[C \\pmid  C]\\] for every sentence E; \n(5)\n\\(P_{\\alpha}[(A \\cdot B) \\pmid C] = P_{\\alpha}[A \\pmid (B \\cdot C)]\n\\times P_{\\alpha}[B \\pmid C]\\). \n\n\n\nThis axiomatization takes conditional probability as basic, as seems\nappropriate for evidential support functions. (These\nfunctions agree with the more usual unconditional probability\nfunctions when the latter are defined\u2014just let \\(P_{\\alpha}[A] =\nP_{\\alpha}[A \\pmid (D \\vee{\\nsim}D)]\\). However, these axioms permit\nconditional probabilities \\(P_{\\alpha}[A \\pmid C]\\) to remain defined\neven when condition statement C has probability 0\u2014i.e.,\neven when \\(P_{\\alpha}[C \\pmid (D\\vee{\\nsim}D)] = 0\\).)\n\nNotice that conditional probability functions apply only to pairs of\nsentences, a conclusion sentence and a premise sentence. So, in\nprobabilistic inductive logic we represent finite collections of\npremises by conjoining them into a single sentence. Rather than say,\n\n\n\nA is supported to degree r by the set of premises\n\\(\\{B_1\\), \\(B_2\\), \\(B_3\\),\u2026, \\(B_n\\}\\), \n\n\nwe instead say that \n\n\nA is supported to degree r by the conjunctive premise\n\\((((B_1\\cdot B_2)\\cdot B_3)\\cdot \\ldots \\cdot B_n)\\),\n\n\nand write this as  \n\n\\[P[A \\pmid  ( ((B_1\\cdot B_2)\\cdot B_3)\\cdot \\ldots \\cdot B_n)] = r.\\]\n\n\nThe above axioms are quite weak. For instance, they do not say that\nlogically equivalent sentences are supported by all other sentences to\nthe same degree; rather, that result is derivable from these axioms\n(see\n result 6\n below). Nor do these axioms say that logically equivalent sentences\nsupport all other sentences to the same degree; rather, that result is\nalso derivable (see\n result 8\n below). Indeed, from these axioms all of the usual theorems of\nprobability theory may be derived. The following results are\nparticularly useful in probabilistic logic. Their derivations from\nthese axioms are provided in note\n 2.[2]\n\n\nIf \\(B \\vDash A\\), then \\(P_{\\alpha}[A \\pmid B] =\n1\\). \nIf \\(C \\vDash{\\nsim}(B\\cdot A)\\), then either\n\n\\[P_{\\alpha}[(A \\vee B) \\pmid  C] = P_{\\alpha}[A \\pmid  C] + P_{\\alpha}[B \\pmid  C]\\] \n\n or else \\(P_{\\alpha}[E \\pmid C] = 1\\) for every sentence\nE. \n\\(P_{\\alpha}[{\\nsim}A \\pmid B] = 1 - P_{\\alpha}[A\n\\pmid B]\\) or else \\(P_{\\alpha}[C \\pmid B] = 1\\) for every sentence\nC.\n\\(1 \\ge P_{\\alpha}[A \\pmid B] \\ge 0\\).\nIf \\(B \\vDash A\\), then \\(P_{\\alpha}[A \\pmid C] \\ge\nP_{\\alpha}[B \\pmid C]\\).\nIf \\(B \\vDash A\\) and \\(A \\vDash B\\), then\n\\(P_{\\alpha}[A \\pmid C] = P_{\\alpha}[B \\pmid C]\\).\nIf \\(C \\vDash B\\), then \\(P_{\\alpha}[(A\\cdot B)\n\\pmid C] = P_{\\alpha}[(B\\cdot A) \\pmid C] = P_{\\alpha}[A \\pmid\nC]\\).\nIf \\(C \\vDash B\\) and \\(B \\vDash C\\), then\n\\(P_{\\alpha}[A \\pmid B] = P_{\\alpha}[A \\pmid C]\\).\n\\(P_{\\alpha}[B \\pmid C] \\gt 0\\), then \n\n\\[P_{\\alpha}[A \\pmid  (B\\cdot C)] = P_{\\alpha}[B \\pmid  (A\\cdot C)] \\times \\frac{P_{\\alpha}[A \\pmid  C]}{P_{\\alpha}[B \\pmid  C]}\\]\n\n(this is a simple form of Bayes\u2019 theorem).\n \\(P_{\\alpha}[(A\\vee B) \\pmid C] = P_{\\alpha}[A\n\\pmid C] + P_{\\alpha}[B \\pmid C] - P_{\\alpha}[(A\\cdot B) \\pmid C]\\).\n\nIf \\(\\{B_1 , \\ldots ,B_n\\}\\) is any finite set of\nsentences such that for each pair \\(B_i\\) and \\(B_j, C\n\\vDash{\\nsim}(B_{i}\\cdot B_{j})\\) (i.e., the members of the set are\nmutually exclusive, given C), then either \\(P_{\\alpha}[D\n\\pmid C] = 1\\) for\nevery sentence D, or \n\n\\[\nP_{\\alpha}[((B_1\\vee B_2)\\vee \\ldots \\vee B_n) \\pmid  C] = \\sum ^{n}_{i=1} P_{\\alpha}[B_i \\pmid  C].\n\\]\nIf \\(\\{B_1 , \\ldots ,B_n , \\ldots \\}\\) is any\ncountably infinite set of sentences such that for each pair \\(B_i\\)\nand \\(B_j, C \\vDash{\\nsim}(B_{i}\\cdot B_{j})\\), then either\n\\(P_{\\alpha}[D \\pmid C] = 1\\) for every sentence D,\n or[3]\n \n\n\\[\n\\lim_n P_{\\alpha}[((B_1\\vee B_2)\\vee \\ldots \\vee B_n) \\pmid  C] =\n\\sum^{\\infty}_{i=1}\nP_{\\alpha}[B_i \\pmid  C].\n\\]\n\n \n\n\nLet us now briefly consider each axiom to see how plausible it is as a\nconstraint on a quantitative measure of inductive support, and how it\nextends the notion of deductive entailment. First notice that each\ndegree-of-support function \\(P_{\\alpha}\\) on L\nmeasures support strength with some real number values, but\nthe axioms don\u2019t explicitly restrict these values to lie between\n0 and 1. It turns out that the all support values must lie between 0\nand 1, but this follows from the axioms, rather than being assumed by\nthem. The scaling of inductive support via the real numbers is surely\na reasonable way to go.\n\nAxiom 1\n is a non-triviality requirement. It says that the support values\ncannot be the same for all sentence pairs. This axiom merely rules out\nthe trivial support function that assigns the same amount of support\nto each sentence by every sentence. One might replace this axiom with\nthe following rule:  \n\n\\[P_{\\alpha}[(A\\vee{\\nsim}A) \\pmid  (A\\vee{\\nsim}A)] \\ne P_{\\alpha}[(A\\cdot{\\nsim}A) \\pmid  (A\\vee{\\nsim}A)].\\]\n\n\nBut this alternative rule turns out to be derivable from axiom 1\ntogether with the other axioms.\n\nAxiom 2\n asserts that when B logically entail A, the\nsupport of A by B is as strong as support can possibly\nbe. This comports with the idea that an inductive support function is\na generalization of the deductive entailment relation, where the\npremises of deductive entailments provide the strongest possible\nsupport for their conclusions.\n\nAxiom 3\n merely says that \\((B \\cdot C)\\) supports sentences to precisely the\nsame degree that \\((C \\cdot B)\\) supports them. This is an especially\nweak axiom. But taken together with the other axioms, it suffices to\nentail that logically equivalent sentences support all sentences to\nprecisely the same degree.\n\nAxiom 4\n says that inductive support adds up in a plausible way. When\nC logically entails the incompatibility of A and\nB, i.e., when no possible state of affairs can make both\nA and B true together, the degrees of support that\nC provides to each of them individually must sum to the support\nit provides to their disjunction. The only exception is in those cases\nwhere C acts like a logical contradiction and supports all\nsentences to the maximum possible degree (in deductive logic a logical\ncontradiction logically entails every sentence).\n\nTo understand what\n axiom 5\n says, think of a support function \\(P_{\\alpha}\\) as describing a\nmeasure on possible states of affairs. Read each degree-of-support\nexpression of form \u2018\\(P_{\\alpha}[D \\pmid E] = r\\)\u2019 to say\nthat the proportion of states of affairs in which D is true\namong those states of affairs where E is true is r. Read\nthis way, axiom 5 then says the following. Suppose B is true in\nproportion q of all the states of affairs where C is\ntrue, and suppose A is true in fraction r of those\nstates where B and C are true together. Then A\nand B should be true together in what proportion of all the\nstates where C is true? In fraction r (the \\((A\\cdot\nB)\\) part) of proportion q (the B portion) of all those\nstates where C is true.\n\nThe degree to which a sentence B supports a sentence A\nmay well depend on what these sentences mean. In particular it will\nusually depend on the meanings we associate with the non-logical terms\n(those terms other than the logical terms not, and,\nor, etc., the quantifiers, and identity), that is, on the\nmeanings of the names, and the predicate and relation terms of the\nlanguage. For example, we should want  \n\n\\[P_{\\alpha}[\\textrm{George is not married} \\pmid  \\textrm{George is a bachelor}] = 1,\\]\n\n\ngiven the usual meanings of \u2018bachelor\u2019 and\n\u2018married\u2019, since \u201call bachelors are unmarried\u201d\nis analytically true\u2014i.e. no empirical evidence is required to\nestablish this connection. (In the formal language for predicate\nlogic, if we associate the meaning \u201cis married\u201d with\npredicate term \u2018M\u2019, the meaning \u201cis a\nbachelor\u201d with the predicate term \u2018B\u2019, and\ntake the name term \u2018g\u2019 to refer to George, then we\nshould want \\(P_{\\alpha}[{\\nsim}Mg \\pmid Bg] = 1\\), since \\(\\forall x\n(Bx \\supset{\\nsim}Mx)\\) is analytically true on this meaning\nassignment to the non-logical terms.) So, let\u2019s associate with\neach individual support function \\(P_{\\alpha}\\) a specific assignment\nof meanings (primary intensions) to all the non-logical terms\nof the language. (However, evidential support functions should not\npresuppose meaning assignments in the sense of so-called secondary\nintensions\u2014e.g., those associated with rigid designators across possible states of affairs. For, we should not want a confirmation function\n\\(P_{\\alpha}\\) to make  \n\n\\[P_{\\alpha}[\\textrm{This glass is full of H\\(_2\\)O} \\pmid  \\textrm{This glass is full of water}] = 1,\\]\n\n\nsince we presumably want the inductive logic to draw on explicit\nempirical evidence to support the claim that water is made of\nH2O. Thus, the meanings of terms we associate with a\nsupport function should only be their primary intensions, not their\nsecondary intensions.)\nIn the context of inductive logic it\nmakes good sense to supplement the above axioms with two additional\naxioms. Here is the first of them:\n\n(6) If\nA is an axiom of set theory or any other piece of pure\nmathematics employed by the sciences, or if A is\nanalytically true (i.e., if the truth of A depends\nonly on the meanings of the words it contains, where the specific\nmeanings for names and predicates are those associated with the\nparticular support function \\(P_{\\alpha})\\), then, for all sentences\nC, \\(P_{\\alpha}[A \\pmid C] = P_{\\alpha}[C \\pmid C]\\) (i.e.,\n\\(P_{\\alpha}[A \\pmid C] = 1)\\).\n\n\nHere is how axiom 6 applies to the above example, yielding\n\\(P_{\\alpha}[{\\nsim}Mg \\pmid Bg] = 1\\) when the meaning assignments to\nnon-logical terms associated with support function \\(P_{\\alpha}\\)\nmakes \\(\\forall x(Bx \\supset{\\nsim}Mx)\\) analytically true. From\naxiom 6 (followed by results 7, 5, and 4) we have  \n\n\\[\n\\begin{align}\n1 & = P_{\\alpha}[\\forall x(Bx \\supset{\\nsim}Mx) \\pmid  Bg] \\\\\n& = P_{\\alpha}[(Bg \\cdot \\forall x(Bx \\supset{\\nsim}Mx)) \\pmid  Bg]\\\\\n& \\le P_{\\alpha}[{\\nsim}Mg \\pmid  Bg] \\\\\n& \\le 1;\n\\end{align}\n\\]\n\n\nthus, \\(P_{\\alpha}[{\\nsim}Mg \\pmid Bg] = 1\\). The idea behind axiom 6\nis that inductive logic is about evidential support for contingent\nclaims. Nothing can count as empirical evidence for or against\nnon-contingent truths. In particular, analytic truths should be\nmaximally supported by all premises C. \n\nOne important respect in which inductive logic should follow\nthe deductive paradigm is that the logic should not presuppose the truth of\ncontingent statements. If a statement C is contingent, then some other statements should be able to count as evidence against C. Otherwise, a support function \\(P_{\\alpha}\\) will take C and all of its logical consequences to be supported to degree 1 by all possible evidence claims.\nThis is no way for an inductive logic to behave. The whole idea of inductive logic is\nto provide a measure of the extent to which premise statements indicate\nthe likely truth-values of contingent conclusion statements. This idea\nwon\u2019t work properly if the truth-values of some contingent\nstatements are presupposed by assigning them support value 1 on every possible premise. Such probability assignments would make the inductive logic enthymematic\nby hiding significant premises in inductive support relationships.\nIt would be analogous to permitting deductive arguments to count as valid \nin cases where the explicitly stated premises are insufficient to logically entail the conclusion, but where the validity of the argument is permitted to depend on additional unstated premises. This is not how a\nrigorous approach to deductive logic should work, and it should not be a common\npractice in a rigorous approach to inductive logic.\n\nNevertheless, it is common practice for probabilistic logicians to\nsweep provisionally accepted contingent claims under the rug by\nassigning them probability 1 (regardless of the fact that no explicit\nevidence for them is provided). This practice saves\nthe trouble of repeatedly writing a given contingent sentence B\nas a premise, since \\(P_{\\gamma}[A \\pmid B\\cdot C]\\) will equal\n\\(P_{\\gamma}[A \\pmid C]\\) whenever \\(P_{\\gamma}[B \\pmid C] = 1\\).\nAlthough this convention is useful, such probability functions should\nbe considered mere abbreviations for proper, logically explicit,\nnon-enthymematic, inductive support relations. Thus, properly\nspeaking, an inductive support function \\(P_{\\alpha}\\) should not\nassign probability 1 to a sentence on every possible premise unless\nthat sentence is either (i) logically true, or (ii) an axiom of set\ntheory or some other piece of pure mathematics employed by the\nsciences, or (iii) unless according to the interpretation of the\nlanguage that \\(P_{\\alpha}\\) presupposes, the sentence is\nanalytic (and so outside the realm of evidential support).\nThus, we adopt the following version of the so-called \u201caxiom of\nregularity\u201d.\n\n(7) If,\nfor all C, \\(P_{\\alpha}[A \\pmid C] = P_{\\alpha}[C \\pmid C]\\)\n(i.e., \\(P_{\\alpha}[A \\pmid C] = 1\\)), then A must be a logical\ntruth or an axiom of set theory or some other piece of pure\nmathematics employed by the sciences, or A must be\nanalytically true (according to the meanings of the terms of\nL associated with support function \\(P_{\\alpha})\\).\n\n\nAxioms 6 and 7 taken together say that a support function\n\\(P_{\\alpha}\\) counts as non-contingently true, and so not subject to\nempirical support, just those sentences that are assigned probability\n1 by every premise.\n\nSome Bayesian logicists have proposed that an inductive logic might be\nmade to depend solely on the logical form of sentences, as is the case\nfor deductive logic. The idea is, effectively, to supplement axioms\n1\u20137 with additional axioms that depend only on the logical\nstructures of sentences, and to introduce enough such axioms to reduce\nthe number of possible support functions to a single uniquely best\nsupport function. It is now widely agreed that this project cannot be\ncarried out in a plausible way. Perhaps support functions should obey\nsome rules in addition to axioms 1\u20137. But it is doubtful that\nany plausible collection of additional rules can suffice to determine\na single, uniquely qualified support function. Later, in\n Section 3, we will briefly return to this issue,\n after we develop a more detailed account of how inductive probabilities\ncapture the relationship between hypotheses and evidence.\n2.3 Two Conceptions of Inductive Probability\n\nAxioms 1\u20137 for conditional probability functions merely place\nformal constraints on what may properly count as a degree of\nsupport function. Each function \\(P_{\\alpha}\\) that satisfies\nthese axioms may be viewed as a possible way of applying the notion of\ninductive support to a language L that respects the\nmeanings of the logical terms, much as each possible truth-value\nassignment for a language represents a possible way of assigning\ntruth-values to its sentences in a way that respects the meanings of the logical terms. The issue of which\nof the possible truth-value assignments to a language\nrepresents the actual truth or falsehood of its sentences\ndepends on more than this. It depends on the meanings of the\nnon-logical terms and on the state of the actual world. Similarly, the\ndegree to which some sentences actually support others in a\nfully meaningful language must rely on something more than the mere\nsatisfaction of the axioms for support functions. It must, at least, rely\non what the sentences of the language mean, and perhaps on much more\nbesides. But, what more? Perhaps a better understanding of what inductive probability is may provide some help by filling out our conception of what\ninductive support is about. Let\u2019s pause to\ndiscuss two prominent views\u2014two interpretations of the notion of inductive probability.\n\nOne kind of non-syntactic logicist reading of inductive probability takes each support\nfunction \\(P_{\\alpha}\\) to be a measure on possible states of affairs. The idea is that,\ngiven a fully meaningful language (associated with support function \\(P_{\\alpha}\\))\n\u2018\\(P_{\\alpha}[A \\pmid B] = r\\)\u2019 says that among those\nstates of affairs in which B is true, A is true in\nproportion r of them. There will not generally be a single\nprivileged way to define such a measure on possible states of affairs.\nRather, each of a number of functions \\(P_{\\alpha}\\), \\(P_{\\beta}\\),\n\\(P_{\\gamma}\\),\u2026, etc., that satisfy the constraints imposed by\naxioms 1\u20137 may represent a viable measure of the inferential\nimport of the propositions expressed by sentences of the\nlanguage. This idea needs more fleshing out, of course. The next\nsection will provide some indication of how that might\ngo.\n\nSubjectivist Bayesians offer an alternative reading of the\nsupport functions. First, they usually take unconditional probability\nas basic, and take conditional probabilities as defined in terms of\nunconditional probabilities: the conditional probability\n\u2018\\(P_{\\alpha}[A \\pmid B]\\)\u2019 is defined as a ratio of\nunconditional probabilities:  \n\n\\[P_{\\alpha}[A \\pmid  B] =  \\frac{P_{\\alpha}[A\\cdot B]}{P_{\\alpha}[B]}.\\]\n\n\nSubjectivist Bayesians take each unconditional probability\nfunction \\(P_{\\alpha}\\) to represent the belief-strengths or\nconfidence-strengths of an ideally rational agent, \\(\\alpha\\). On this\nunderstanding \u2018\\(P_{\\alpha}[A] =r\\)\u2019 says, \u201cthe\nstrength of \\(\\alpha\\)\u2019s belief (or confidence) that A is\ntruth is r\u201d. Subjectivist Bayesians usually tie such\nbelief strengths to how much money (or how many units of\nutility) the agent would be willing to bet on A turning\nout to be true. Roughly, the idea is this. Suppose that an ideally\nrational agent \\(\\alpha\\) would be willing to accept a wager that\nwould yield (no less than) $u if A turns out to be true\nand would lose him $1 if A turns out to be false. Then, under\nreasonable assumptions about the agent\u2019s desire money, it can be\nshown that the agent\u2019s belief strength that A is true\nshould be  \n\n\\[P_{\\alpha}[A] = \\frac{1}{(u+1)}. \\]\n\n\nAnd it can further be shown that any function \\(P_{\\alpha}\\) that\nexpresses such betting-related belief-strengths on all statements in\nagent \\(\\alpha\\)\u2019s language must satisfy axioms for\nunconditional probabilities analogous to axioms\n 1\u20135.[4]\n Moreover, it can be shown that any function \\(P_{\\beta}\\) that\nsatisfies these axioms is a possible rational belief function for some\nideally rational agent \\(\\beta\\). These relationships between\nbelief-strengths and the desirability of outcomes (e.g., gaining money\nor goods on bets) are at the core of subjectivist Bayesian\ndecision theory. Subjectivist Bayesians usually take\ninductive probability to just be this notion of\nprobabilistic belief-strength.\n\nUndoubtedly real agents do believe some claims more strongly than\nothers. And, arguably, the belief strengths of real agents can be\nmeasured on a probabilistic scale between 0 and 1, at least\napproximately. And clearly the inductive support of a hypothesis by\nevidence should influence the strength of an agent\u2019s belief in\nthe truth of that hypothesis\u2014that\u2019s the point of engaging\nin inductive reasoning, isn\u2019t it? However, there is good reason\nfor caution about viewing inductive support functions as\nBayesian belief-strength functions, as we\u2019ll see a bit later.\nSo, perhaps an agent\u2019s support function is not simply\nidentical to his belief function, and perhaps the\nrelationship between inductive support and\nbelief-strength is somewhat more complicated.\n\nIn any case, some account of what support functions are supposed to\nrepresent is clearly needed. The belief function account and the\nlogicist account (in terms of measures on possible states of affairs)\nare two attempts to provide this account. But let us put this interpretative\nissue aside for now. One may be able to get a better handle on what\ninductive support functions really are after one sees how the\ninductive logic that draws on them is supposed to work.\n3. The Application of Inductive Probabilities to the Evaluation of Scientific Hypotheses\n\nOne of the most important applications of an inductive logic is its treatment of\nthe evidential evaluation of scientific hypotheses.\nThe logic should capture the structure of evidential support for all\nsorts of scientific hypotheses, ranging from simple diagnostic claims (e.g.,\n\u201cthe patient is infected by the HIV\u201d) to complex scientific theories about the fundamental nature of the world, such as quantum\nmechanics or the theory of relativity. This section will show how\nevidential support functions (a.k.a. Bayesian confirmation functions)\nrepresent the evidential evaluation of scientific hypotheses and theories. This logic is essentially comparative. The evaluation of a hypothesis depends on how strongly evidence supports it over alternative hypotheses.\n\nConsider some collection of mutually incompatible, alternative hypotheses (or theories)\nabout a common subject matter, \\(\\{h_1, h_2 , \\ldots \\}\\). The collection of\nalternatives may be very simple, e.g., {\u201cthe patient has\nHIV\u201d, \u201cthe patient is free of HIV\u201d}. Or, when the\nphysician is trying to determine which among a range of diseases is\ncausing the patient\u2019s symptoms, the collection of alternatives may\nconsist of a long list of possible disease hypotheses. For the cosmologist, the collection of alternatives may consist of several distinct gravitational\ntheories, or several empirically distinct variants of the \u201csame\u201d theory. Whenever two variants of a hypothesis (or theory) differ in empirical import, they count as distinct hypotheses. (This should not be confused with the converse positivistic assertion that theories with the same empirical content are really the same theory. Inductive logic\ndoesn\u2019t necessarily endorse that view.)\n\nThe collection of competing hypotheses (or theories) to be evaluated by the logic may be finite in number, or may be countably infinite. No realistic language contains more than a countable number of expressions; so it suffices for a logic to apply to countably infinite number of sentences. From a purely logical perspective the collection of competing alternatives may consist of every rival hypothesis (or theory) about a given subject matter that can be expressed within a given language \u2014 e.g., all possible theories of the origin and evolution of the universe expressible in English and contemporary mathematics. In practice, alternative hypotheses (or theories) will often be constructed and evidentially evaluated over a long period of time. The logic of evidential support works in much the same way regardless of whether all alternative hypotheses are considered together, or only a few alternative hypotheses are available at a time.\n\nEvidence for scientific hypotheses consists of the results of specific\nexperiments or observations. For a given experiment or observation,\nlet \u2018\\(c\\)\u2019 represent a description of the relevant conditions under which it is performed, and let\n\u2018\\(e\\)\u2019 represent a description of the result of the experiment or observation, the evidential outcome of \nconditions \\(c\\).\n\nThe logical connection between scientific hypotheses and the evidence often requires the mediation of background information and auxiliary hypotheses. Let \u2018\\(b\\)\u2019 represent whatever background and auxiliary hypotheses are required to connect each hypothesis \\(h_i\\) among the competing hypotheses \\(\\{h_1, h_2 , \\ldots \\}\\) to the evidence. Although the claims expressed by the auxiliary hypotheses within \\(b\\) may themselves be subject to empirical evaluation, they should be the kinds of claims that\nare not at issue in the evaluation of the alternative hypothesis in the collection\n\\(\\{h_1, h_2 , \\ldots \\}\\). Rather, each of the alternative hypotheses under consideration draws on the same background and auxiliaries to\nlogically connect to the evidential events. (If competing hypotheses \\(h_i\\) and\n\\(h_j\\) draw on distinct auxiliary hypotheses \\(a_i\\) and \\(a_j\\),\nrespectively, in making logical contact with evidential claims, then\nthe following treatment should be applied to the respective\nconjunctive hypotheses, \\((h_{i}\\cdot a_{i})\\) and \\((h_{j}\\cdot\na_{j})\\), since these alternative conjunctive hypotheses will\nconstitute the empirically distinct alternatives at issue.) \n\nIn cases where a hypothesis is deductively related to an\noutcome \\(e\\) of an observational or experimental condition\n\\(c\\) (via background and auxiliaries \\(b\\)), we will have\neither \\(h_i\\cdot b\\cdot c \\vDash\ne\\) or \\(h_i\\cdot b\\cdot c\n\\vDash{\\nsim}e\\). For example, \\(h_i\\) might be the Newtonian\nTheory of Gravitation. A test of the theory might involve a condition\nstatement \\(c\\) that describes the results of some earlier measurements\nof Jupiter\u2019s position, and that describes the means by which the\nnext position measurement will be made; the outcome description\n\\(e\\) states the result of this additional position measurement;\nand the background information (and auxiliary hypotheses) \\(b\\)\nmight state some already well confirmed theory about the workings and\naccuracy of the devices used to make the position measurements. Then,\nfrom \\(h_i\\cdot b\\cdot c\\) we may calculate the specific outcome\n\\(e\\) we expect to find; thus, the following logical entailment\nholds: \\(h_i\\cdot b\\cdot c \\vDash\ne\\). Then, provided that the experimental and observational\nconditions stated by \\(c\\) are in fact true, if the evidential\noutcome described by \\(e\\) actually occurs, the resulting conjoint\nevidential claim \\((c\\cdot e)\\) may be considered good evidence for\n\\(h_i\\), given \\(b\\). (This method of theory evaluation is called the\nhypothetical-deductive approach to evidential support.) On\nthe other hand, when from \\(h_i\\cdot b\\cdot c\\) we calculate some\noutcome incompatible with the observed evidential outcome \\(e\\),\nthen the following logical entailment holds: \\(h_i\\cdot\nb\\cdot c \\vDash{\\nsim}e\\). In that case, from deductive logic alone we\nmust also have that \\(b\\cdot c\\cdot e\n\\vDash{\\nsim}h_i\\); thus, \\(h_i\\) is said to be\nfalsified by \\(b\\cdot c\\cdot e\\). The Bayesian account of\nevidential support we will be describing below extends this\ndeductivist approach to include cases where the hypothesis \\(h_i\\)\n(and its alternatives) may not be deductive related to the evidence,\nbut may instead imply that the evidential outcome is likely or unlikely\nto some specific degree r. That is, the Bayesian approach applies to cases where we may have neither \\(h_i\\cdot b\\cdot c\n\\vDash e\\) nor \\(h_i\\cdot\nb\\cdot c \\vDash{\\nsim}e\\), but may instead only have \\(P[e\n\\pmid h_i\\cdot b\\cdot c] = r\\), where r is some\n\u201centailment strength\u201d between 0 and 1. \n\nBefore going on to describing the logic of evidential support in more\ndetail, perhaps a few more words are in order about the background knowledge\nand auxiliary hypotheses, represented here by \u2018\\(b\\)\u2019.\nDuhem (1906) and Quine (1953) are generally credited with alerting\ninductive logicians to the importance of auxiliary hypotheses in\nconnecting scientific hypotheses and theories to empirical evidence.\n(See the entry on\n Pierre Duhem.)\n They point out that scientific hypotheses often make little contact\nwith evidence claims on their own. Rather, in most cases scientific hypotheses\nmake testable predictions only relative to background information and\nauxiliary hypotheses that tie them to the evidence. (Some specific examples of such auxiliary hypotheses will be provided in the next subsection.) Typically\nauxiliaries are highly confirmed hypotheses from other scientific\ndomains. They often describe the operating characteristics of various\ndevices (e.g., measuring instruments) used to make observations or\nconduct experiments. Their credibility is usually not at issue in the testing of hypothesis \\(h_i\\) against its competitors, because \\(h_i\\) and its alternatives\nusually rely on the same auxiliary hypotheses to tie them to the\nevidence. But even when an auxiliary hypothesis is already\nwell-confirmed, we cannot simply assume that it is unproblematic, or\njust known to be true. Rather, the evidential support or\nrefutation of a hypothesis \\(h_i\\) is relative to whatever\nauxiliaries and background information (in \\(b\\)) is being\nsupposed in the confirmational context. In other contexts the auxiliary hypotheses used to test \\(h_i\\) may themselves be among a collection of alternative hypotheses\nthat are subject to evidential support or refutation. Furthermore, to\nthe extent that competing hypotheses employ different auxiliary\nhypotheses in accounting for evidence, the evidence only tests each\nsuch hypothesis in conjunction with its distinct auxiliaries against\nalternative hypotheses packaged with their distinct auxiliaries, as\ndescribed earlier. Thus, what counts as a hypothesis to be\ntested, \\(h_i\\), and what counts as auxiliary hypotheses and\nbackground information, \\(b\\), may depend on the epistemic context\u2014on what class of alternative hypotheses are being tested by a collection of experiments or observations, and on what claims are presupposed in that context.\nNo statement is intrinsically a test hypothesis, or\nintrinsically an auxiliary hypothesis or background condition. Rather, these categories are roles statements may play in a particular epistemic context.\n\nIn a probabilistic inductive logic the degree to which the evidence\n\\((c\\cdot e)\\) supports a hypothesis \\(h_i\\) relative to background and auxiliaries\n\\(b\\) is represented by the posterior probability of\n\\(h_i\\), \\(P_{\\alpha}[h_i \\pmid b\\cdot c\\cdot e]\\), according to an evidential\nsupport function \\(P_{\\alpha}\\). It turns out that the posterior\nprobability of a hypothesis depends on just two kinds of factors:\n(1) its prior probability, \\(P_{\\alpha}[h_i \\pmid b]\\),\ntogether with the prior probabilities of its competitors,\n\\(P_{\\alpha}[h_j \\pmid b]\\), \\(P_{\\alpha}[h_k \\pmid b]\\), etc.; and (2) the likelihood of evidential outcomes \\(e\\) according to \\(h_i\\) in conjunction with with \\(b\\) and \\(c\\), \\(P[e \\pmid h_i\\cdot b\\cdot c]\\), together with\nthe likelihoods of these same evidential outcomes according to competing hypotheses, \\(P[e\n\\pmid h_j\\cdot b\\cdot c]\\), \\(P[e \\pmid h_k\\cdot b\\cdot c]\\), etc. We will now examine each of these factors in some detail. Following that we will see precisely how the values of posterior probabilities depend on the values of likelihoods\nand prior probabilities.\n3.1 Likelihoods\n\nIn probabilistic inductive logic the likelihoods carry the\nempirical import of hypotheses. A likelihood is a support\nfunction probability of form \\(P[e \\pmid h_i\\cdot b\\cdot c]\\). It\nexpresses how likely it is that outcome \\(e\\) will occur according\nto hypothesis \\(h_i\\) together with the background and auxiliaries \\(b\\) and the experimental (or observational) conditions \\(c\\).[5]\n If a hypothesis together with auxiliaries and experimental/observation conditions\ndeductively entails an evidence claim, the axioms of probability make\nthe corresponding likelihood objective in the sense that every support\nfunction must agree on its values: \\(P[e \\pmid h_i\\cdot b\\cdot c] =\n1\\) if \\(h_i\\cdot b\\cdot c \\vDash e\\); \\(P[e \\pmid h_i\\cdot b\\cdot c]\n= 0\\) if \\(h_i\\cdot b\\cdot c \\vDash{\\nsim}e\\). However, in many cases\na hypothesis \\(h_i\\) will not be deductively related to the evidence,\nbut will only imply it probabilistically. There are several ways this\nmight happen: (1) hypothesis \\(h_i\\) may itself be an explicitly\nprobabilistic or statistical hypothesis; (2) an auxiliary statistical\nhypothesis, as part of the background b, may connect hypothesis\n\\(h_i\\) to the evidence; (3) the connection between the hypothesis and\nthe evidence may be somewhat loose or imprecise, not mediated by\nexplicit statistical claims, but nevertheless objective enough for the\npurposes of evidential evaluation. Let\u2019s briefly consider\nexamples of the first two kinds. We\u2019ll treat case (3) in\n Section 5,\n which addresses the the issue of vague and imprecise likelihoods.\n\nThe hypotheses being tested may themselves be statistical in nature.\nOne of the simplest examples of statistical hypotheses and their role\nin likelihoods are hypotheses about the chance characteristic of\ncoin-tossing. Let \\(h_{[r]}\\)\nbe a hypothesis that says a specific coin has a propensity (or\nobjective chance) r for coming up heads on normal tosses, let \\(b\\) say that such tosses are probabilistically independent of one another. Let \\(c\\)\nstate that the coin is tossed n times in the normal way; and\nlet \\(e\\) say that on these tosses the coin comes up heads m\ntimes. In cases like this the value of the likelihood of the outcome\n\\(e\\) on hypothesis \\(h_{[r]}\\)\nfor condition \\(c\\) is given by the well-known binomial formula:  \n\n\\[\nP[e \\pmid  h_{[r]}\\cdot b\\cdot c] =\n\\frac{n!}{m! \\times(n-m)!}\n\\times r^m (1-r)^{n-m}.\n\\]\n\n\nThere are, of course, more complex cases of likelihoods involving\nstatistical hypotheses. Consider, for example, the hypothesis that\nplutonium 233 nuclei have a half-life of 20 minutes\u2014i.e., that\nthe propensity (or objective chance) for a Pu-233 nucleus to\ndecay within a 20 minute period is 1/2. The full statistical model for\nthe lifetime of such a system says that the propensity (or\nobjective chance) for that system to remain intact (i.e., to\nnot decay) within any time period x is governed by the\nformula \\(1/2^{x/\\tau}\\), where \\(\\tau\\) is the half-life of such a\nsystem. Let \\(h\\) be a hypothesis that says that this statistical\nmodel applies to Pu-233 nuclei with \\(\\tau = 20\\) minutes; let\n\\(c\\) say that some specific Pu-233 nucleus is intact within a decay detector (of some specific kind) at an initial time \\(t_0\\); let \\(e\\) say that no decay of this same Pu-233 nucleus is detected by the later time \\(t\\); and let \\(b\\) say that the detector is completely accurate (it always registers a real decay, and it never registers false-positive detections).  Then, the associated likelihood of\n\\(e\\) given \\(h\\) and \\(c\\) is this: \\(P[e \\pmid h\\cdot b\\cdot c] =\n1/2^{(t - t_0)/\\tau}\\), where the value of \\(\\tau\\) is 20 minutes.\n\nAn auxiliary statistical hypothesis, as part of the background\n\\(b\\), may be required to connect hypothesis \\(h_i\\) to the evidence. For example,\na blood test for HIV has a known false-positive rate and a known\ntrue-positive rate. Suppose the false-positive rate is .05\u2014i.e.,\nthe test tends to incorrectly show the blood sample to be positive for\nHIV in 5% of all cases where HIV is not present. And suppose that the\ntrue-positive rate is .99\u2014i.e., the test tends to correctly show\nthe blood sample to be positive for HIV in 99% of all cases where HIV\nreally is present. When a particular patient\u2019s blood is tested, the hypotheses under consideration are this patient is infected with HIV, \\(h\\), and this patient is not infected with HIV, \\({\\nsim}h\\). In this context the known test characteristics function as background information, b. The\nexperimental condition \\(c\\) merely states that this particular\npatient was subjected to this specific kind of blood test for HIV,\nwhich was processed by the lab using proper procedures. Let us suppose\nthat the outcome \\(e\\) states that the result is a positive test\nresult for HIV. The relevant likelihoods then, are \\(P[e \\pmid h\\cdot\nb\\cdot c] = .99\\) and \\(P[e \\pmid {\\nsim}h\\cdot b\\cdot c]\\) = .05.\n\n\nIn this example the values of the likelihoods are entirely due to the\nstatistical characteristics of the accuracy of the test, which is\ncarried by the background/auxiliary information \\(b\\). The hypothesis\n\\(h\\) being tested by the evidence is not itself statistical.\n\nThis kind of situation may, of course, arise for much more complex\nhypotheses. The alternative hypotheses of interest may be deterministic\nphysical theories, say Newtonian Gravitation Theory and some specific alternatives. \nSome of the experiments that test this theory relay on somewhat imprecise\nmeasurements that have known statistical error characteristics, which\nare expressed as part of the background or auxiliary hypotheses,\n\\(b\\). For example, the auxiliary \\(b\\) may describe the error\ncharacteristics of a device that measures the torque imparted to a\nquartz fiber, where the measured torque is used to assess the strength\nof the gravitational force between test masses. In that case \\(b\\)\nmay say that for this kind of device the measurement errors are\nnormally distributed about whatever value a given gravitational theory\npredicts, with some specified standard deviation that is\ncharacteristic of the device. This results in specific values \\(r_i\\)\nfor the likelihoods, \\(P[e \\pmid h_i\\cdot b\\cdot c] = r_i\\), for each\nof the various gravitational theories, \\(h_i\\), being\ntested.\n\nLikelihoods that arise from explicit statistical claims\u2014either\nwithin the hypotheses being tested, or from explicit statistical\nbackground claims that tie the hypotheses to the evidence\u2014are\noften called direct inference likelihoods. Such likelihoods\nshould be completely objective. So, all evidential support functions should agree on their values, just as all support functions agree on likelihoods when evidence is logically\nentailed. Direct inference likelihoods are logical in an\nextended, non-deductive sense. Indeed, some logicians have attempted\nto spell out the logic of direct inferences in terms of the\nlogical form of the sentences\n involved.[6]\n But regardless of whether that project succeeds, it seems reasonable\nto take likelihoods of this sort to have highly objective or\nintersubjectively agreed values.\n\nNot all likelihoods of interest in confirmational contexts are\nwarranted deductively or by explicitly stated statistical claims. In\nsuch cases the likelihoods may have vague, imprecise values, but\nvalues that are determinate enough to still underwrite an objective\nevaluation of hypotheses on the evidence. In\n Section 5\n we\u2019ll consider such cases, where no underlying statistical\ntheory is involved, but where likelihoods are determinate enough to\nplay their standard role in the evidential evaluation of scientific\nhypotheses. However, the proper treatment of such cases will be more\neasily understood after we have first seen how the logic works when\nlikelihoods are precisely known (such as cases where the likelihood\nvalues are endorsed by explicit statistical hypotheses and/or explicit\nstatistical auxiliaries). In any case, the likelihoods that relate\nhypotheses to evidence claims in many scientific contexts will have\nsuch objective values. So, although a variety of different support\nfunctions \\(P_{\\alpha}\\), \\(P_{\\beta}\\),\u2026, \\(P_{\\gamma}\\),\netc., may be needed to represent the differing \u201cinductive\nproclivities\u201d of the various members of a scientific community,\nfor now we will consider cases where all evidential support functions\nagree on the values of the likelihoods. For,\nthe likelihoods represent the empirical content of a scientific hypothesis, what\nthe hypothesis (together with experimental conditions, \\(c\\), and background and auxiliaries \\(b\\))\nsays or probabilistically implies about the\nevidence. Thus, the empirical objectivity of a science relies on a\nhigh degree of objectivity or intersubjective agreement among\nscientists on the numerical values of likelihoods.\n\nTo see the point more vividly, imagine what a science would be like if\nscientists disagreed widely about the values of likelihoods. Each\npractitioner interprets a theory to say quite different\nthings about how likely it is that various possible evidence\nstatements will turn out to be true. Whereas scientist \\(\\alpha\\)\ntakes theory \\(h_1\\) to probabilistically imply that event \\(e\\) is\nhighly likely, his colleague \\(\\beta\\) understands the empirical\nimport of \\(h_1\\) to say that \\(e\\) is very unlikely. And,\nconversely, \\(\\alpha\\) takes competing theory \\(h_2\\) to\nprobabilistically imply that \\(e\\) is very unlikely, whereas\n\\(\\beta\\) reads \\(h_2\\) to say that \\(e\\) is extremely likely. So,\nfor \\(\\alpha\\) the evidential outcome \\(e\\) supplies strong support\nfor \\(h_1\\) over \\(h_2\\), because  \n\n\\[P_{\\alpha}[e \\pmid  h_1\\cdot b\\cdot c]  \\gg P_{\\alpha}[e \\pmid  h_2\\cdot b\\cdot c].\\]\n\n\nBut his colleague \\(\\beta\\) takes outcome \\(e\\) to show just the\nopposite, that \\(h_2\\) is strongly supported over \\(h_1\\), because\n\n \\[P_{\\beta}[e \\pmid  h_2\\cdot b\\cdot c] \\gg  P_{\\beta}[e \\pmid  h_1\\cdot b\\cdot c].\\]\n\n\nIf this kind of situation were to occur often, or for significant evidence\nclaims in a scientific domain, it would make a shambles of the\nempirical objectivity of that science. It would completely undermine\nthe empirical testability of such hypotheses and theories within that\nscientific domain. Under these circumstances, although each scientist\nemploys the same sentences to express a given theory\n\\(h_i\\), each understands the empirical import of these\nsentences so differently that \\(h_i\\) as understood by\n\\(\\alpha\\) is an empirically different theory than \\(h_i\\) as\nunderstood by \\(\\beta\\). (Indeed, arguably, \\(\\alpha\\) must take\nat least one of the two sentences, \\(h_1\\) or \\(h_2\\), to express a different proposition than does \\(\\beta\\).) Thus, the empirical\nobjectivity of the sciences requires that experts should be in close\nagreement about the values of the likelihoods.[7]\n\n\nFor now we will suppose that the likelihoods have objective or\nintersubjectively agreed values, common to all agents in a scientific\ncommunity. We mark this agreement by dropping the subscript\n\u2018\\(\\alpha\\)\u2019, \u2018\\(\\beta\\)\u2019, etc., from\nexpressions that represent likelihoods, since all support functions\nunder consideration are supposed to agree on the values for\nlikelihoods. One might worry that this supposition is overly strong.\nThere are legitimate scientific contexts where, although scientists\nshould have enough of a common understanding of the empirical import\nof hypotheses to assign quite similar values to likelihoods, precise\nagreement on their numerical values may be unrealistic. This point is\nright in some important kinds of cases. So later, in Section 5, we will see how to relax the supposition that precise\nlikelihood values are available, and see how the logic works in such\ncases. But for now the main ideas underlying probabilistic inductive\nlogic will be more easily explained if we focus on those contexts were\nobjective or intersubjectively agreed likelihoods are available. Later\nwe will see that much the same logic continues to apply in contexts\nwhere the values of likelihoods may be somewhat vague, or where\nmembers of the scientific community disagree to some extent about\ntheir values.\n\nAn adequate treatment of the likelihoods calls for the introduction of\none additional notational device. Scientific hypotheses are generally\ntested by a sequence of experiments or observations conducted over a\nperiod of time. To explicitly represent the accumulation of evidence,\nlet the series of sentences \\(c_1\\), \\(c_2\\), \u2026, \\(c_n\\),\ndescribe the conditions under which a sequence of experiments or\nobservations are conducted. And let the corresponding outcomes of\nthese observations be represented by sentences \\(e_1\\), \\(e_2\\),\n\u2026, \\(e_n\\). We will abbreviate the conjunction of the first\nn descriptions of experimental or observational conditions by\n\u2018\\(c^n\\)\u2019, and abbreviate the conjunction of descriptions\nof their outcomes by \u2018\\(e^n\\)\u2019. Then, for a stream of\nn observations or experiments and their outcomes, the\nlikelihoods take form \\(P[e^n \\pmid h_{i}\\cdot b\\cdot c^{n}] = r\\),\nfor appropriate values of \\(r\\). In many cases the likelihood\nof the evidence stream will be equal to the product of the likelihoods\nof the individual outcomes: \n\n\\[\nP[e^n \\pmid  h_{i}\\cdot b\\cdot c^{n}] = P[e_1 \\pmid  h_i\\cdot b\\cdot c_1] \\times \\cdots \\times P[e_n \\pmid  h_{i}\\cdot b\\cdot c_{n}].\n\\]\n\n\nWhen this equality holds, the individual bits of evidence are said to\nbe probabilistically independent on the hypothesis (together with\nauxiliaries). In the following account of the logic of evidential\nsupport, such probabilistic independence will not be assumed,\nexcept in those places where it is explicitly invoked.\n3.2 Posterior Probabilities and Prior Probabilities\n\nThe probabilistic logic of evidential support represents the net\nsupport of a hypothesis by the posterior probability of the\nhypothesis, \n\n\\(P_{\\alpha}[h_i \\pmid  b\\cdot c^{n}\\cdot e^{n}]\\).\n\nThe posterior probability represents the net support for the\nhypothesis that results from the evidence, \\(c^n \\cdot e^n\\), together\nwith whatever plausibility considerations are taken to be\nrelevant to the assessment of \\(h_i\\). Whereas the likelihoods are the\nmeans through which evidence contributes to the posterior probability\nof a hypothesis, all other relevant plausibility consideration are\nrepresented by a separate factor, called the prior probability of\nthe hypothesis: \\(P_{\\alpha}[h_i \\pmid b]\\). The prior\nprobability represents the weight of any important considerations\nnot captured by the evidential likelihoods. Any relevant\nconsiderations that go beyond the evidence itself may be explicitly\nstated within expression \\(b\\) (in addition to whatever auxiliary hypotheses\n\\(b\\) may contain in support of the likelihoods). Thus, the prior probability of \\(h_i\\)\nmay depend explicitly on the content of \\(b\\). It turns out that posterior\nprobabilities depend only on the values of evidential\nlikelihoods together with the values of prior probabilities.\n\nAs an illustration of the role of prior probabilities, consider the\nHIV test example described in the previous section. What the\nphysician and the patient want to know is the value of the posterior\nprobability, \\(P_{\\alpha}[h \\pmid b\\cdot c\\cdot e]\\), that the patient\nhas HIV, \\(h\\), given the evidence of the positive test, \\(c\\cdot\ne\\), and given the error rates of the test, described within \\(b\\).\nThe value of this posterior probability depends on the likelihood (due\nto the error rates) of this patient obtaining a true-positive result,\n\\(P[e \\pmid h\\cdot b\\cdot c] = .99\\), and of obtaining a\nfalse-positive result, \\(P[e \\pmid {\\nsim}h\\cdot b\\cdot c] = .05\\). In\naddition, the value of the of the posterior probability depends on how\nplausible it is that the patient has HIV prior to taking the test\nresults into account, \\(P_{\\alpha}[h \\pmid b]\\). In the context of\nmedical diagnosis, this prior probability is usually assessed on the\nbasis of the base rate for HIV in the patient\u2019s risk\ngroup (i.e., whether the patient is an IV drug user, has unprotected sex with\nmultiple partners, etc.). On a rigorous approach to the logic, such\ninformation and its risk-relevance should be explicitly stated within the\nbackground information \\(b\\). To see the importance of this\ninformation, consider the following numerical results (which may be\ncalculated using the formula called Bayes\u2019 Theorem, presented in\nthe next section). If the base rate for the patient\u2019s risk group\nis relatively high, say \\(P_{\\alpha}[h \\pmid b] = .10\\), then the\npositive test result yields a posterior probability value for his\nhaving HIV of \\(P_{\\alpha}[h \\pmid b\\cdot c\\cdot e] = .69\\). However,\nif the patient is in a very low risk group, say \\(P_{\\alpha}[h \\pmid\nb] = .001\\), then a positive test result only raises the posterior\nprobability of his having an HIV infection to \\(P_{\\alpha}[h \\pmid\nb\\cdot c\\cdot e] = .02\\). This posterior probability is much higher\nthan the prior probability of .001, but should not worry the patient\ntoo much. This positive test result may well be due to the comparatively high\nfalse-positive rate for the test, rather than to the presence of HIV.\nThis sort of test, with a false-positive rate as large as .05, is\nbest used as a screening test; a positive result warrants conducting a\nsecond, more rigorous, less error-prone test.\n\nMore generally, in the evidential evaluation of scientific hypotheses and theories, prior\nprobabilities represent assessments of non-evidential plausibility weightings among hypotheses. However, because the strengths of such plausibility assessments may\nvary among members of a scientific community, critics often brand such assessments as merely subjective, and take their role in Bayesian inference to be highly problematic. Bayesian inductivists counter that plausibility\nassessments play an important, legitimate role in the sciences, especially\nwhen evidence cannot suffice to distinguish among some alternative hypotheses. And, they argue, the epithet \u201cmerely subjective\u201d is unwarranted. Such plausibility assessments are\noften backed by extensive arguments that may draw on forceful\nconceptual considerations.\n\nScientists often bring plausibility arguments to bear\nin assessing competing views. Although such arguments are seldom\ndecisive, they may bring the scientific community into widely shared\nagreement, especially with regard to the implausibility of some\nlogically possible alternatives. This seems to be the primary\nepistemic role of thought experiments.\nConsider, for example, the kinds of plausibility arguments that have\nbeen brought to bear on the various interpretations of quantum theory\n(e.g., those related to the measurement problem). These arguments go\nto the heart of conceptual issues that were central to the original\ndevelopment of the theory. Many of these issues were first raised by\nthose scientists who made the greatest contributions to the development of quantum theory, in their attempts to get a conceptual hold on the theory and its implications.\n\nGiven any body of evidence, it is fairly easy to cook up\na host of logically possible alternative hypotheses that make the evidence as probable as desired. In particular, it is easy to cook up hypotheses that logically entail any given body evidence, providing likelihood values equal to 1 for all the available evidence. Although most of these cooked up hypotheses will be laughably implausible, evidential likelihoods cannot rule them out. But, the only factors other than likelihoods that figure into the values of posterior probabilities for hypotheses are the values of their prior probabilities; so only prior probability assessments provide a place for the Bayesian logic to bring important plausibility considerations to bear. Thus, the Bayesian logic can only give implausible hypotheses their due via prior probability assessments.\n\nIt turns out that the mathematical structure of Bayesian inference makes prior probabilities especially well-suited to represent plausibility assessments among competing hypotheses. For, in the fully fleshed out account of evidential support for hypotheses (spelled out below), it will turn out that only ratios of prior probabilities for competing hypotheses, \\(P_{\\alpha}[h_j \\pmid b] / P_{\\alpha}[h_i \\pmid b]\\), together with ratios of likelihoods, \\(P_{\\alpha}[e \\pmid  h_j\\cdot b\\cdot c] / P_{\\alpha}[e \\pmid  h_2\\cdot b\\cdot c]\\), play essential roles. The ratio of prior probabilities is well-suited to represent how much more (or less) plausible hypothesis \\(h_j\\) is than competing hypothesis \\(h_i\\). Furthermore, the plausibility arguments on which such this comparative assessment is based may be explicitly stated within \\(b\\). So, given that an inductive logic needs to incorporate well-considered plausibility assessments (e.g. in order to lay low wildly implausible alternative hypotheses), the comparative assessment of Bayesian prior probabilities seems well-suited to do the job.\n\nThus, although prior probabilities may be subjective in the sense that\nagents may disagree on the relative strengths of plausibility\narguments, the priors used in scientific contexts need not\nrepresent mere subjective whims. Rather, the comparative strengths of the priors for hypotheses should be supported by arguments about\nhow much more plausible one hypothesis is than another. The important\nrole of plausibility assessments is captured by such received bits of\nscientific wisdom as the well-known scientific aphorism, extraordinary claims require\nextraordinary evidence. That is, it takes especially strong\nevidence, in the form of extremely high values for (ratios of)\nlikelihoods, to overcome the extremely low pre-evidential plausibility values\npossessed by some hypotheses. In the next section we\u2019ll see precisely how this idea works, and we\u2019ll return to it again in\n Section 3.4.\n\nWhen sufficiently strong evidence becomes available, it turns out that the contributions of prior plausibility assessments to the values of posterior probabilities may be substantially \u201cwashed\nout\u201d, overridden by the evidence. That is, provided the prior probability of a true hypothesis isn\u2019t assessed to be too\nclose to zero, the influence of the values of\nthe prior probabilities will very probably fade away as evidence accumulates. In Section 4 we\u2019ll see precisely how this kind of Bayesian convergence to the true hypothesis works.\nThus, it turns out that prior plausibility assessments play their most important role\nwhen the distinguishing evidence represented by the likelihoods remains weak.\n\nOne more point before moving on to the logic of Bayes\u2019 Theorem. Some Bayesian logicists have maintained that posterior\nprobabilities of hypotheses should be determined by syntactic logical\nform alone. The idea is that the likelihoods might reasonably be\nspecified in terms of syntactic logical form; so if syntactic form\nmight be made to determine the values of prior probabilities as well,\nthen inductive logic would be fully \u201cformal\u201d in the same\nway that deductive logic is \u201cformal\u201d. Keynes and Carnap\ntried to implement this idea through syntactic versions of the\nprinciple of indifference\u2014the idea that syntactically similar\nhypotheses should be assigned the same prior probability values.\nCarnap showed how to carry out this project in detail, but only for\nextremely simple formal languages. Most logicians now take the project\nto have failed because of a fatal flaw with the whole idea that\nreasonable prior probabilities can be made to depend on logical form\nalone. Semantic content should matter. Goodmanian grue-predicates\nprovide one way to illustrate this\n point.[8]\n Furthermore, as suggested earlier, for this idea to apply to the\nevidential support of real scientific theories, scientists would have\nto assess the prior probabilities of each alternative theory based\nonly on its syntactic structure. That seems an unreasonable way to\nproceed. Are we to evaluate the prior probabilities of alternative\ntheories of gravitation, or for alternative quantum theories, by\nexploring only their syntactic structures, with absolutely no regard\nfor their content\u2014with no regard for what they\nsay about the world? This seems an extremely dubious approach\nto the evaluation of real scientific theories. Logical structure alone\ncannot, and should not suffice for determining reasonable prior\nprobability values for real scientific theories. Moreover, real\nscientific hypotheses and theories are inevitably subject to\nplausibility considerations based on what they say about the\nworld. Prior probabilities are well-suited to represent the comparative weight of plausibility considerations for alternative hypotheses. But no reasonable assessment of comparative plausibility can derive solely from the logical form of hypotheses.\n\nWe will return to a discussion of prior probabilities a bit later. Let\u2019s now see how Bayesian logic combines likelihoods with prior probabilities\nto yield posterior probabilities for hypotheses.\n3.3 Bayes\u2019 Theorem\n\nAny probabilistic inductive logic that draws on the usual\nrules of probability theory to represent how evidence supports\nhypotheses must be a Bayesian inductive logic in the broad\nsense. For, Bayes\u2019 Theorem follows directly from the usual axioms of probability theory. Its importance derives from the relationship it expresses\nbetween hypotheses and evidence. It\nshows how evidence, via the likelihoods, combines with prior\nprobabilities to produce posterior probabilities for hypotheses.\nWe now examine several forms of Bayes\u2019 Theorem, each derivable from axioms 1\u20135.\nThe simplest version of Bayes\u2019 Theorem as it applies to evidence for a hypothesis goes like this:\n\n\nBayes\u2019 Theorem: Simple Form \n\n\\[\\begin{align*}\nP_{\\alpha}[h_i \\pmid e] &= \\frac{P_{\\alpha}[e \\pmid h_i]\\times P_{\\alpha}[h_i]}{P_{\\alpha}[e]}\n\\end{align*}\\]\n\n\nThis equation expresses the posterior probability of hypothesis\n\\(h_i\\) due to evidence \\(e\\), \\(P_{\\alpha}[h_i \\pmid e]\\), in terms of the likelihood of\nthe evidence on that hypothesis, \\(P_{\\alpha}[e \\pmid h_i]\\), the prior probability of the hypothesis, \\(P_{\\alpha}[h_i]\\), and the simple probability of the evidence, \\(P_{\\alpha}[e]\\). The factor \\(P_{\\alpha}[e]\\) is often called the expectedness of the evidence. Written this way, the theorem suppresses the experimental (or observational) conditions, \\(c\\), and all background information and auxiliary hypotheses, \\(b\\). As discussed earlier, both of these terms play an important role in logically connecting the hypothesis at issue, \\(h_i\\), to the evidence \\(e\\). In scientific contexts the objectivity of the likelihoods, \\(P_{\\alpha}[e \\pmid  h_i\\cdot b \\cdot c]\\), almost always depends on such terms. So, although the suppression of experimental (or observational) conditions and auxiliary hypotheses is a common practice in accounts of Bayesian inference, the treatment below, and throughout the remainder of this article will make the role of these terms explicit.\n\nThe subscript \\(\\alpha\\) on the evidential support function \\(P_{\\alpha}\\) is there to remind us that more than one such function exists. A host of distinct probability functions satisfy axioms 1\u20135, so each of them satisfies Bayes\u2019 Theorem. Some of these probability functions may provide a better fit with our intuitive conception of how the evidential support for hypotheses should work. Nevertheless, there are bound to be reasonable differences among Bayesian agents regarding to the initial plausibility of a hypothesis \\(h_i\\). This diversity in initial plausibility assessments is represented by diverse values for prior probabilities for the hypothesis: \\(P_{\\alpha}[h_i]\\), \\(P_{\\beta}[h_i]\\), \\(P_{\\gamma}[h_i]\\), etc. This usually results in diverse values for posterior probabilities for hypotheses: \\(P_{\\alpha}[h_i \\pmid e]\\), \\(P_{\\beta}[h_i \\pmid e]\\), \\(P_{\\gamma}[h_i \\pmid e]\\), etc. So it is important to keep the diversity among evidential support functions in mind.\n\nHere is how the Simple Form of Bayes\u2019 Theorem looks\nwhen terms for the experimental (or observational) conditions, \\(c\\), and the\nbackground information and auxiliary hypotheses \\(b\\) are made explicit:\n\n\nBayes\u2019 Theorem: Simple Form with explicit Experimental Conditions, Background Information and Auxiliary Hypotheses \n\n\\[\\tag{8}\n\\begin{align}\n P_{\\alpha}[h_i \\pmid  b\\cdot c\\cdot e]\n&=\n\\frac{P[e \\pmid  h_i\\cdot b \\cdot c] \\times P_{\\alpha}[h_i \\pmid  b]}\n{P_{\\alpha}[e \\pmid  b \\cdot c]}\n\n\\\\ &\\qquad\n\\times\n\n\\frac{P_{\\alpha}[c \\pmid  h_i\\cdot b]}\n{P_{\\alpha}[c \\pmid  b]}\\\\[3ex]\n& =\n\\frac{P[e \\pmid  h_i\\cdot b\\cdot c] \\times P_{\\alpha}[h_i \\pmid  b]}\n{P_{\\alpha}[e \\pmid  b\\cdot c]}\\\\[2ex]\n& \\textrm{when  }\nP_{\\alpha}[c \\pmid  h_j\\cdot b] =\nP_{\\alpha}[c \\pmid b].\n\n\\end{align}\n\\]\n\n\nThis version of the theorem determines the posterior probability of the hypothesis,\n\\(P_{\\alpha}[h_i \\pmid b\\cdot c\\cdot e]\\), from the value of the\nlikelihood of the evidence according to that hypothesis (taken together with\nbackground and auxiliaries and the experimental conditions), \\(P[e \\pmid h_i\\cdot b\\cdot c]\\), the value of the prior probability of the hypothesis (on background and auxiliaries), \\(P_{\\alpha}[h_i \\pmid b]\\), and the value of the expectedness of the evidence (on background and auxiliaries and the experimental conditions), \\(P_{\\alpha}[e \\pmid b\\cdot c]\\). Notice that in the factor for the likelihood, \\(P[e \\pmid h_i\\cdot b\\cdot c]\\), the subscript \\(\\alpha\\) has been dropped. This marks the fact that in scientific contexts the likelihood of an evidential outcome \\(e\\) on the hypothesis together with explicit background and auxiliary hypotheses and the description of the experimental conditions, \\(h_i\\cdot b\\cdot c\\), is usually objectively determinate. This factor represents what the hypothesis (in conjunction with background and auxiliaries) objectively says about the likelihood of possible evidential outcomes of the experimental conditions. So, all reasonable support functions should agree on the values for likelihoods. (Section 5 will treat cases where the likelihoods may lack this kind of objectivity.)\n\nThis version of Bayes\u2019 Theorem includes a term that represents the ratio of the likelihood of the experimental conditions on the hypothesis and background information (and auxiliaries) to the\n\u201clikelihood\u201d of the experimental conditions on\nthe background (and auxiliaries) alone:\n\n\\(P_{\\alpha}[c \\pmid  h_i\\cdot b]/ P_{\\alpha}[c \\pmid  b]\\).\n\nArguably the value of this term should be 1, or very nearly 1, since the\ntruth of the hypothesis at issue should not significantly affect how\nlikely it is that the experimental conditions are satisfied. If\nvarious alternative hypotheses assign significantly different\nlikelihoods to the experimental conditions themselves, then such\nconditions should more properly be included as part of the evidential\noutcome \\(e\\).\n\nBoth the prior probability of the hypothesis and the\nexpectedness tend to be somewhat subjective factors in that\nvarious agents from the same scientific community may legitimately\ndisagree on what values these factors should take. Bayesian logicians\nusually accept the apparent subjectivity of the prior probabilities of\nhypotheses, but find the subjectivity of the expectedness to\nbe more troubling. This is due at least in part to the fact that in a\nBayesian logic of evidential support the value of the expectedness\ncannot be determined independently of likelihoods and prior\nprobabilities of hypotheses. That is, when, for each member of a collection\nof alternative hypotheses, the likelihood \\(P[e \\pmid h_j\\cdot b\\cdot\nc]\\) has an objective (or intersubjectively agreed) value, the\nexpectedness is constrained by the following equation (where\nthe sum ranges over a mutually exclusive and exhaustive collection of\nalternative hypotheses \\(\\{h_1, h_2 , \\ldots ,h_m , \\ldots \\}\\), which\nmay be finite or countably infinite):\n\n\n\\[\n\\begin{align}\nP_{\\alpha}[e \\pmid  b\\cdot c] &= \\sum_j P[e \\pmid  h_j\\cdot b\\cdot c] \\times P_{\\alpha}[h_j \\pmid  b \\cdot c].\n\\end{align}\n\\]\n\n\nThis equation shows that the values for the prior probabilities\ntogether with the values of the likelihoods uniquely determine the\nvalue for the expectedness of the evidence. Furthermore, it\nimplies that the value of the expectedness must lie between\nthe largest and smallest of the various likelihood values implied by\nthe alternative hypotheses. However, the precise value of the\nexpectedness can only be calculated this way when every\nalternative to hypothesis \\(h_j\\) is specified. In cases where some\nalternative hypotheses remain unspecified (or undiscovered), the value\nof the expectedness is constrained in principle by the\ntotality of possible alternative hypotheses, but there is no way to\nfigure out precisely what its value should be.\n\nTroubles with determining a numerical value for the expectedness of the evidence\nmay be circumvented by appealing to another form of Bayes\u2019\nTheorem, a ratio form that compares hypotheses one pair at a time:\n\n\nBayes\u2019 Theorem: Ratio Form \n\n\\[\\tag{9}\n\\begin{align}\n\\frac{P_{\\alpha}[h_j \\pmid  b\\cdot c\\cdot e]}\n{P_{\\alpha}[h_i \\pmid  b\\cdot c\\cdot e]}\n& =\n\\frac{P[e \\pmid  h_j\\cdot b\\cdot c]}\n{P[e \\pmid  h_i\\cdot b\\cdot c]}\n\\times\n\\frac{P_{\\alpha}[h_j \\pmid  b]}\n{P_{\\alpha}[h_i \\pmid  b]}\\\\\n&\\qquad\\times\n\\frac{P_{\\alpha}[c \\pmid  h_j\\cdot b]}\n{P_{\\alpha}[c \\pmid  h_i\\cdot b]}\\\\[2ex]\n& =\n\\frac{P[e \\pmid  h_j\\cdot b\\cdot c]}\n{P[e \\pmid  h_i\\cdot b\\cdot c]}\n \\times\n\\frac{P_{\\alpha}[h_j \\pmid  b]}\n{P_{\\alpha}[h_i \\pmid  b]}\n\\\\[2ex]\n& \\textrm{when  }\nP_{\\alpha}[c \\pmid  h_j\\cdot b] =\nP_{\\alpha}[c \\pmid  h_i\\cdot b].\n\n\\end{align}\n\\]\n\n\n\nThe clause \n\n\\(P_{\\alpha}[c \\pmid  h_j\\cdot b] = P_{\\alpha}[c \\pmid  h_i\\cdot b]\\)\n\nsays that the experimental (or observation) condition described by \\(c\\) is as likely on \\((h_i\\cdot b)\\) as on \\((h_j\\cdot b)\\) \u2014 i.e., the experimental or observation conditions are no more likely according to one hypothesis than according to the other.[9]\n\n\nThis Ratio Form of Bayes\u2019 Theorem expresses how much more\nplausible, on the evidence, one hypothesis is than another. Notice\nthat the likelihood ratios carry the full import of the\nevidence. The evidence influences the evaluation of hypotheses in no\nother way. The only other factor that influences the value of the\nratio of posterior probabilities is the ratio of the prior\nprobabilities. When the likelihoods are fully objective, any\nsubjectivity that affects the ratio of posteriors can only arise via\nsubjectivity in the ratio of the priors. \n\nThis version of Bayes\u2019s Theorem shows that in order to evaluate\nthe posterior probability ratios for pairs of hypotheses, the\nprior probabilities of hypotheses need not be evaluated absolutely;\nonly their ratios are needed. That is, with regard to the priors, the\nBayesian evaluation of hypotheses only relies on how much more\nplausible one hypothesis is than another (due to considerations\nexpressed within b). This kind of Bayesian evaluation of\nhypotheses is essentially comparative in that only ratios of\nlikelihoods and ratios of prior probabilities are ever\nreally needed for the assessment of scientific hypotheses.\nFurthermore, we will soon see that the absolute values of the\nposterior probabilities of hypotheses entirely derive from the\nposterior probability ratios provided by the Ratio Form of\nBayes\u2019 Theorem. \n\nWhen the evidence consists of a collection of n distinct\nexperiments or observations, we may explicitly represent this fact by\nreplacing the term \u2018\\(c\\)\u2019 by the conjunction of experimental or observational conditions, \\((c_1\\cdot\nc_2\\cdot \\ldots \\cdot c_n)\\), and replacing the term\n\u2018\\(e\\)\u2019 by the conjunction of their respective outcomes, \\((e_1\\cdot e_2\\cdot \\ldots \\cdot e_n)\\). For notational convenience, let\u2019s use the term\n\u2018\\(c^n\\)\u2019 to abbreviate the conjunction of n the experimental conditions, and we use the term \u2018\\(e^n\\)\u2019 to abbreviate the corresponding conjunction of n their respective outcomes. Relative to any given hypothesis \\(h\\), the evidential\noutcomes of distinct experiments or observations will usually be\nprobabilistically independent of one another, and also independent of the\nexperimental conditions for one another. In that case we have: \n\n\\[\nP[e^n \\pmid  h\\cdot b\\cdot c^n] =  P[e_1 \\pmid  h\\cdot b\\cdot c_1] \\times \\cdots \\times  P[e_n \\pmid  h\\cdot b\\cdot c_n].\n\\]\n\n\nWhen the Ratio Form of Bayes\u2019 Theorem is extended to explicitly represent the evidence as consisting of a collection of n of distinct experiments (or observations) and their respective outcomes, it takes the following form. \n\n\nBayes\u2019 Theorem: Ratio Form for a Collection of n\nDistinct Evidence Claims \n\\[\\tag{9*}\n\\begin{align}\n\n\\frac{P_{\\alpha}[h_j \\pmid  b\\cdot  c^n   \\cdot  e^n ] }\n{P_{\\alpha}[h_i \\pmid  b\\cdot  c^n   \\cdot  e^n ]}\n&  =\n\\frac{P [ e^n   \\pmid   h_j\\cdot b\\cdot  c^n ]}\n{P [ e^n   \\pmid   h_i\\cdot b\\cdot  c^n ]}\n\\times\n\\frac{P_{\\alpha}[h_j \\pmid  b]}\n{P_{\\alpha}[h_i \\pmid  b]}\\\\[2ex]\n&\\qquad \\times\n\\frac{P_{\\alpha} [ c^n   \\pmid   h_j\\cdot b]}\n{P_{\\alpha} [ c^n   \\pmid   h_i\\cdot b]}\\\\[2ex]\n& =\n\\frac{P [ e^n   \\pmid   h_j\\cdot b\\cdot  c^n ]}\n{P [ e^n   \\pmid   h_i\\cdot b\\cdot  c^n ]}\n\\times\n\\frac{P_{\\alpha}[h_j \\pmid  b]}\n{P_{\\alpha}[h_i \\pmid  b]}\n\\\\[2ex]\n&\\textrm{when }\nP_{\\alpha} [ c ^n   \\pmid   h _j\\cdot b ] =\nP_{\\alpha} [ c ^n   \\pmid   h _i\\cdot b ].\n\\end{align}\n\\]\n\nFurthermore, when evidence claims are probabilistically independent of one another, we have\n\n\n\\[\\tag{9**}\n\\begin{align}\n\n\\frac{P_{\\alpha}[h_j \\pmid  b\\cdot  c^n   \\cdot  e^n  ] }\n{P_{\\alpha}[h_i \\pmid  b\\cdot  c^n   \\cdot  e^n  ]}\n&  =\n\\frac{P[e_1 \\pmid  h_j\\cdot b\\cdot c_1]}\n{P[e_1 \\pmid  h_i\\cdot b\\cdot c_1]}\n\\times \\cdots \\\\[2ex]\n&\\qquad \\times\n\\frac{P[e_n \\pmid  h_{j }\\cdot b\\cdot c_{ n}]}\n{P[e_n \\pmid  h_{i }\\cdot b\\cdot c_{ n}]}\n\\times\n\\frac{P_{\\alpha}[h_j \\pmid  b]}\n{P_{\\alpha}[h_i \\pmid  b]}.\n\n\\end{align}\n\\]\n\n\nLet\u2019s consider a simple example of how the Ratio Form of\nBayes\u2019 Theorem applies to a collection of independent evidential events. Suppose we possess a warped coin\nand want to determine its propensity for heads when tossed in\nthe usual way. Consider two hypotheses, \\(h_{[p]}\\) and\n\\(h_{[q]}\\), which say that the propensities for the coin to come up\nheads on the usual kinds of tosses are \\(p\\) and \\(q\\),\nrespectively. Let \\(c^n\\) report that the coin is tossed n\ntimes in the normal way, and let \\(e^n\\) report that precisely\nm occurrences of heads has resulted. Supposing that\nthe outcomes of such tosses are probabilistically independent (asserted by \\(b\\)),\nthe respective likelihoods take the binomial form  \n\n\\[\nP[e^n \\pmid  h_{[r]}\\cdot b\\cdot c^n] =\n\\frac{n!}{m! \\times(n-m)!}\n\\times r^m (1-r)^{n-m},\n\\]\n \nwith \\(r\\) standing in for \\(p\\) and for \\(q\\), respectively. Then, Equation 9**\n yields the following formula, where the likelihood ratio is the\nratio of the respective binomial terms: \n\n\\[\n\\frac{P_{\\alpha}[h_{[p]} \\pmid  b\\cdot c^{n }\\cdot e^{ n}]}\n{P_{\\alpha}[h_{[q]} \\pmid  b\\cdot c^{n }\\cdot e^{ n}]}\n=\n\\frac{p^m (1-p)^{n-m}}\n{q^m (1-q)^{n-m}}\n\\times\n\\frac{P_{\\alpha}[h_{[p]} \\pmid  b]}\n{P_{\\alpha}[h_{[q]} \\pmid  b]}\n\\]\n\n\nWhen, for instance, the coin is tossed \\(n = 100\\) times and comes up\nheads \\(m = 72\\) times, the evidence for hypothesis\n\\(h_{[1/2]}\\) as compared to \\(h_{[3/4]}\\) is given by the likelihood\nratio \n\n\\[\\frac{P [ e^n   \\pmid   h_{[1/2]}\\cdot b\\cdot  c^n ]}\n{P [ e^n   \\pmid   h_{[3/4]}\\cdot b\\cdot  c^n ]} = \\frac{[(1/2)^{72}(1/2)^{28}]}{[(3/4)^{72}(1/4)^{28}]} = .000056269. \\]\n\n\nIn that case, even if the prior plausibility considerations\n(expressed within \\(b\\)) make it 100 times more plausible that the\ncoin is fair than that it is warped towards heads with\npropensity 3/4 \u2014 i.e., even if \\(P_{\\alpha}[h_{[1/2]} \\pmid b] / P_{\\alpha}[h_{[3/4]} \\pmid b] = 100\\) \u2014 the evidence provided by these tosses makes the posterior plausibility that the coin is fair\nonly about 6/1000ths as plausible as the hypothesis that it\nis warped towards heads with propensity 3/4:\n\n\\[\n\\frac{P_{\\alpha}[h_{[1/2]} \\pmid  b\\cdot c^{n}\\cdot e^{n}]}{P_{\\alpha}[h_{[3/4]} \\pmid  b\\cdot c^{n}\\cdot e^{n}]} = .0056269.\n\\]\n\n\nThus, such evidence strongly refutes the \u201cfairness\nhypothesis\u201d relative to the \u201c3/4-heads\nhypothesis\u201d, provided the assessment of prior\nprior plausibilities doesn\u2019t make the latter hypothesis too\nextremely implausible to begin with. Notice, however, that\nstrong refutation is not absolute refutation.\nAdditional evidence could reverse this trend towards the\nrefutation of the fairness hypothesis.\n\nThis example employs repetitions of the same kind of\nexperiment\u2014repeated tosses of a coin. But the point holds more\ngenerally. If, as the evidence increases, the likelihood\nratios  \n\n\\[\\frac{P[e^n \\pmid  h_{j}\\cdot b\\cdot c^{n}]}{P[e^n \\pmid  h_{i}\\cdot b\\cdot c^{n}]}\\]\n\n\napproach 0, then the Ratio Forms of Bayes\u2019 Theorem, Equations \\(9*)\\) and \\(9**)\\), \n show that the posterior probability of \\(h_j\\) must approach 0 as\nwell, since  \n\n\\[P_{\\alpha}[h_j \\pmid  b\\cdot c^{n}\\cdot e^{n}] \\le \\frac{P_{\\alpha}[h_j \\pmid  b\\cdot c^{n}\\cdot e^{n}]}{P_{\\alpha}[h_i \\pmid  b\\cdot c^{n}\\cdot e^{n}]}.\\]\n\n\nSuch evidence comes to strongly refute \\(h_j\\), with little regard for\nits prior plausibility value. Indeed, Bayesian induction turns out to\nbe a version of eliminative induction, and Equation \\(9*\\) and \\(9**\\) begin\nto illustrate this. For, suppose that \\(h_i\\) is the true hypothesis,\nand consider what happens to each of its false competitors,\n\\(h_j\\). If enough evidence becomes available to drive each of the\nlikelihood ratios  \n\n\\[\\frac{P[e^n \\pmid  h_{j}\\cdot b\\cdot c^{n}]}{P[e^n \\pmid  h_{i}\\cdot b\\cdot c^{n}]}\\] \n\n\ntoward 0 (as n increases), then Equation \\(9*\\) says that each false\n\\(h_j\\) will become effectively refuted \u2014 each of their posterior\nprobabilities will approaches 0 (as n increases). As a result, the posterior probability of \\(h_i\\) must approach 1. The next two equations show precisely how\nthis works.\n\nIf we sum the ratio versions of Bayes\u2019 Theorem in Equation\n\\(9*\\) over all alternatives to hypothesis \\(h_i\\) (including the\ncatch-all alternative \\(h_K\\), if appropriate), we get the Odds Form\nof Bayes\u2019 Theorem. By definition, the odds against a statement \\(A\\) given \\(B\\) is related to the probability of \\(A\\) given \\(B\\) as follows: \n\n \\[\\Omega_{\\alpha}[{\\nsim}A \\pmid  B] = \\frac{P_{\\alpha}[{\\nsim}A \\pmid  B]}{P_{\\alpha}[A \\pmid  B]} = \\frac{1 - P_{\\alpha}[A \\pmid  B]}{P_{\\alpha}[A \\pmid  B]}.\\]\n\n\nThis notion of odds gives rise to the following version of Bayes\u2019 Theorem:\n\n\nBayes\u2019 Theorem: Odds Form \n\n\\[\\tag{10}\n\\begin{align}\n   \\Omega_{\\alpha}[{\\nsim}h_i \\pmid  b\\cdot c^{n }\\cdot e^{ n}]\n& =  \\sum_{j\\ne i}\n\\frac{P_{\\alpha}[h_j \\pmid  b\\cdot c^{n }\\cdot e^{ n}]}\n{P_{\\alpha}[h_i \\pmid  b\\cdot c^{n }\\cdot e^{ n}]}\n\\\\ &\\qquad\n+\n\\frac{P_{\\alpha}[h_K \\pmid  b\\cdot c^{n }\\cdot e^{ n}]}\n{P_{\\alpha}[h_i \\pmid  b\\cdot c^{n }\\cdot e^{ n}]}\\\\[2ex]\n& =\n\\sum_{ j\\ne i}\n\\frac{P[e^n \\pmid  h_{j }\\cdot b\\cdot c^{ n}]}\n{P[e^n \\pmid  h_{i }\\cdot b\\cdot c^{ n}]}\n\\times\n\\frac{P_{\\alpha}[h_j \\pmid  b]}\n{P_{\\alpha}[h_i \\pmid  b]}\\\\[2ex]\n&\\qquad +\n\\frac{P_{\\alpha}[e^n \\pmid  h_{K }\\cdot b\\cdot c^{ n}]}\n{P[e^n \\pmid  h_{i }\\cdot b\\cdot c^{ n}]}\n\\times\n\\frac{P_{\\alpha}[h_K \\pmid  b]}\n{P_{\\alpha}[h_i \\pmid  b]}\n\\end{align}\n\\]\n\n\nwhere the factor following the \u2018+\u2019 sign is only\nrequired in cases where a catch-all alternative hypothesis, \\(h_K\\),\nis needed. \n\n\nRecall that when we have a finite collection of concrete alternative\nhypotheses available, \\(\\{h_1, h_2 , \\ldots ,h_m\\}\\), but where this\nset of alternatives is not exhaustive (where additional,\nunarticulated, undiscovered alternative hypotheses may exist), the\ncatch-all alternative hypothesis \\(h_K\\) is just the denial of each of\nthe concrete alternatives, \\(({\\nsim}h_1\\cdot{\\nsim}h_2\\cdot \\ldots\n\\cdot{\\nsim}h_m)\\). Generally, the likelihood of evidence claims relative to\na catch-all hypothesis will not enjoy the same kind of objectivity possessed by\nthe likelihoods for concrete alternative hypotheses. So, we leave the\nsubscript \\(\\alpha\\) attached to the likelihood for the catch-all hypothesis\nto indicate this lack of objectivity.\n\nAlthough the catch-all hypothesis may lack objective likelihoods, the\ninfluence of the catch-all term in Bayes\u2019 Theorem diminishes as\nadditional concrete hypotheses are articulated. That is, as new\nhypotheses are discovered they are \u201cpeeled off\u201d of the\ncatch-all. So, when a new hypothesis \\(h_{m+1}\\) is formulated and\nmade explicit, the old catch-all hypothesis \\(h_K\\) is replaced by a\nnew catch-all, \\(h_{K*}\\), of form \\(({\\nsim}h_1\\cdot\n\\cdot{\\nsim}h_2\\cdot \\ldots \\cdot{\\nsim}h_{m}\\cdot{\\nsim}h_{m+1})\\);\nand the prior probability for the new catch-all hypothesis is gotten\nby diminishing the prior of the old catch-all: \\(P_{\\alpha}[h_{K*}\n\\pmid b] = P_{\\alpha}[h_K \\pmid b] - P_{\\alpha}[h_{m+1} \\pmid b]\\).\nThus, the influence of the catch-all term should diminish towards 0 as\nnew alternative hypotheses are made\n explicit.[10]\n\n\nIf increasing evidence drives towards 0 the likelihood ratios\ncomparing each competitor \\(h_j\\) with hypothesis \\(h_i\\), then the\nodds against \\(h_i\\), \\(\\Omega_{\\alpha}[{\\nsim}h_i \\pmid b\\cdot\nc^{n}\\cdot e^{n}]\\), will approach 0 (provided that priors of\ncatch-all terms, if needed, approach 0 as well, as new alternative\nhypotheses are made explicit and peeled off). And, as\n\\(\\Omega_{\\alpha}[{\\nsim}h_i \\pmid b\\cdot c^{n}\\cdot e^{n}]\\)\napproaches 0, the posterior probability of \\(h_i\\) goes to 1. This derives from the fact that the odds against \\(h_i\\) is related to and its posterior probability by the following formula:\n\n\nBayes\u2019 Theorem: General Probabilistic Form\n\n\\[\\tag{11}\n P_{\\alpha}[h_i \\pmid  b\\cdot c^{n}\\cdot e^n] = \\frac{1}{1 + \\Omega_{\\alpha}[{\\nsim}h_i \\pmid  b\\cdot c^n\\cdot e^{n}]}.\n\\]\n\n\n\nThe odds against a hypothesis depends only on the values of ratios\nof posterior probabilities, which entirely derive from the Ratio\nForm of Bayes\u2019 Theorem. Thus, we see that the individual value\nof the posterior probability of a hypothesis depends only on the\nratios of posterior probabilities, which come from the Ratio\nForm of Bayes\u2019 Theorem. Thus, the Ratio Form of Bayes\u2019\nTheorem captures all the essential features of the Bayesian\nevaluation of hypothesis. It shows how the impact of evidence (in the\nform of likelihood ratios) combines with comparative plausibility\nassessments of hypotheses (in the form of ratios of prior\nprobabilities) to provide a net assessment of the extent to which\nhypotheses are refuted or supported via contests with their rivals.\n\n\nThere is a result, a kind of Bayesian Convergence Theorem,\nthat shows that if \\(h_i\\) (together with \\(b\\cdot c^n)\\) is true,\nthen the likelihood ratios \n\n\\[\\frac{P[e^n \\pmid  h_{j}\\cdot b\\cdot c^{n}]}{P[e^n \\pmid  h_{i}\\cdot b\\cdot c^{n}]}\\]\n\n\ncomparing evidentially distinguishable alternative hypothesis \\(h_j\\)\nto \\(h_i\\) will very probably approach 0 as evidence\naccumulates (i.e., as n increases). Let\u2019s call this\nresult the Likelihood Ratio Convergence Theorem. When this\ntheorem applies,\n Equation \\(9^*\\)\n shows that the posterior probability of a false competitor \\(h_j\\)\nwill very probably approach 0 as evidence accumulates, regardless of\nthe value of its prior probability \\(P_{\\alpha}[h_j \\pmid b]\\). As\nthis happens to each of \\(h_i\\)\u2019s false competitors,\n Equations 10\n and\n 11\n say that the posterior probability of the true hypothesis, \\(h_i\\),\nwill approach 1 as evidence\n increases.[11]\n Thus, Bayesian induction is at bottom a version of induction by\nelimination, where the elimination of alternatives comes by way\nof likelihood ratios approaching 0 as evidence accumulates. Thus, when\nthe Likelihood Ratio Convergence Theorem applies, the\nCriterion of Adequacy for an Inductive Logic described at the\nbeginning of this article will be satisfied: As evidence accumulates,\nthe degree to which the collection of true evidence\nstatements comes to support a hypothesis, as measured by the\nlogic, should very probably come to indicate that false hypotheses are\nprobably false and that true hypotheses are probably true. We will\nexamine this Likelihood Ratio Convergence Theorem in\n Section 4.[12]\n\n\nA view called Likelihoodism relies on likelihood ratios in\nmuch the same way as the Bayesian logic articulated above. However,\nLikelihoodism attempts to avoid the use of prior\nprobabilities. For an account of this alternative view, see\nthe supplement\n Likelihood Ratios, Likelihoodism, and the Law of Likelihood.\nFor more discussion of\n Bayes\u2019 Theorem and its application, see the entries on\n Bayes\u2019 Theorem\n and on\n Bayesian Epistemology\n in this Encyclopedia.\n3.4 On Prior Probabilities and Representations of Vague and Diverse Plausibility Assessments\n\nGiven that a scientific community should largely agree on the values\nof the likelihoods, any significant disagreement among them with\nregard to the values of posterior probabilities of hypotheses should\nderive from disagreements over their assessments of values for the\nprior probabilities of those hypotheses. We saw in\n Section 3.3\n that the Bayesian logic of evidential support need only rely on\nassessments of ratios of prior probabilities\u2014on how\nmuch more plausible one hypothesis is than another. Thus, the logic of\nevidential support only requires that scientists can assess the\ncomparative plausibilities of various hypotheses. Presumably, in\nscientific contexts the comparative plausibility values for hypotheses\nshould depend on explicit plausibility arguments, not merely on\nprivately held opinions. (Formally, the logic may represent\ncomparative plausibility arguments by explicit statements expressed\nwithin \\(b\\).) It would be highly unscientific for a\nmember of the scientific community to disregard or dismiss a\nhypothesis that other members take to be a reasonable proposal with\nonly the comment, \u201cdon\u2019t ask me to give my reasons,\nit\u2019s just my opinion\u201d. Even so, agents may be unable to\nspecify precisely how much more strongly the available\nplausibility arguments support a hypothesis over an alternative; so\nprior probability ratios for hypotheses may be vague. Furthermore,\nagents in a scientific community may disagree about how strongly the\navailable plausibility arguments support a hypothesis over a rival\nhypothesis; so prior probability ratios may be somewhat diverse as\nwell. \n\nBoth the vagueness of comparative plausibilities assessments for\nindividual agents and the diversity of such assessments among the\ncommunity of agents can be represented formally by sets of support\nfunctions, \\(\\{P_{\\alpha}, P_{\\beta}, \\ldots \\}\\), that agree on the\nvalues for the likelihoods but encompass a range of values for the\n(ratios of) prior probabilities of hypotheses. Vagueness and\ndiversity are somewhat different issues, but they may be\nrepresented in much the same way. Let\u2019s briefly consider each in\nturn.\n\nAssessments of the prior plausibilities of hypotheses will often be\nvague\u2014not subject to the kind of precise quantitative treatment\nthat a Bayesian version of probabilistic inductive logic may seem to\nrequire for prior probabilities. So, it may seem that the kind of\nassessment of prior probabilities required to get the Bayesian\nalgorithm going cannot be accomplished in practice. To see how\nBayesian inductivists address this worry, first recall the Ratio Form\nof Bayes\u2019 Theorem, Equation \\(9^*\\). \n\n\\[\n\\frac{P_{\\alpha}[h_j \\pmid  b\\cdot c^{n }\\cdot e^{ n}]}\n{P_{\\alpha}[h_i \\pmid  b\\cdot c^{n }\\cdot e^{ n}]}\n=\n\\frac{P[e^n \\pmid  h_{j }\\cdot b\\cdot c^{ n}]}\n{P[e^n \\pmid  h_{i }\\cdot b\\cdot c^{ n}]}\n\\times\n\\frac{P_{\\alpha}[h_j \\pmid  b]}\n{P_{\\alpha}[h_i \\pmid  b]}\n\\]\n\n\nRecall that this Ratio Form of the theorem captures the essential\nfeatures of the logic of evidential support, even though it only\nprovides a value for the ratio of the posterior probabilities. Notice\nthat the ratio form of the theorem easily accommodates situations\nwhere we don\u2019t have precise numerical values for prior\nprobabilities. It only depends on our ability to assess how much\nmore or less plausible alternative hypothesis \\(h_j\\) is than\nhypothesis \\(h_i\\)\u2014only the value of the ratio \\(P_{\\alpha}[h_j\n\\pmid b] / P_{\\alpha}[h_i \\pmid b]\\) need be assessed; the values of\nthe individual prior probabilities are not needed. Such comparative\nplausibilities are much easier to assess than specific numerical\nvalues for the prior probabilities of individual hypotheses. When\ncombined with the ratio of likelihoods, this ratio of\npriors suffices to yield an assessment of the ratio of\nposterior plausibilities, \n\n \\[\\frac{P_{\\alpha}[h_j \\pmid  b\\cdot c^{n}\\cdot e^{n}]}{P_{\\alpha}[h_i \\pmid  b\\cdot c^{n}\\cdot e^{n}]}.\\]\n\n\nAlthough such posterior ratios don\u2019t supply values for the\nposterior probabilities of individual hypotheses, they place a crucial\nconstraint on the posterior support of hypothesis \\(h_j\\), since\n\n\\[\n\\begin{align}\nP_{\\alpha}[h_j \\pmid  b\\cdot c^{n }\\cdot e^{ n}]  \n& \\lt \n\\frac{P_{\\alpha}[h_j \\pmid  b\\cdot c^{n }\\cdot e^{ n}]}\n{P_{\\alpha}[h_i \\pmid  b\\cdot c^{n }\\cdot e^{ n}]}\\\\\n& =\n\\frac{P[e^n \\pmid  h_{j }\\cdot b\\cdot c^{ n}]}\n{P[e^n \\pmid  h_{i }\\cdot b\\cdot c^{ n}]}\n\\times\n\\frac{P_{\\alpha}[h_j \\pmid  b]}\n{P_{\\alpha}[h_i \\pmid  b]}\n\\end{align}\n\\]\n\n\nThis Ratio Form of Bayes\u2019 Theorem tolerates a good deal of\nvagueness or imprecision in assessments of the ratios of prior\nprobabilities. In practice one need only assess bounds for these prior\nplausibility ratios to achieve meaningful results. Given a prior ratio\nin a specific interval,  \n\n\\[\nq \\le \\frac{P_{\\alpha}[h_j \\pmid  b]}{P_{\\alpha}[h_i \\pmid  b]} \\le r\n\\]\n\n\na likelihood ratio \n\n \\[\\frac{P[e^n \\pmid  h_{j}\\cdot b\\cdot c^{n}]}{P[e^n \\pmid  h_{i}\\cdot b\\cdot c^{n}]} = \\LR^n\\]\n\n\nresults in a posterior support ratio in the interval \n\n\\[\n(\\LR^n\\times q) \\le \\frac{P_{\\alpha}[h_j \\pmid  b\\cdot c^{n}\\cdot e^{n}]}{P_{\\alpha}[h_i \\pmid  b\\cdot c^{n}\\cdot e^{n}]} \\le (\\LR^n \\times r).\n\\]\n\n\n\n(Technically each probabilistic support function assigns a specific\nnumerical value to each pair of sentences; so when we write an\ninequality like  \n\n  \\[q \\le \\frac{P_{\\alpha}[h_j \\pmid  b]}{P_{\\alpha}[h_i \\pmid  b]} \\le r\\]\n\n\nwe are really referring to a set of probability functions\n\\(P_{\\alpha}\\), a vagueness set, for which the inequality\nholds. Thus, technically, the Bayesian logic employs sets of\nprobabilistic support functions to represent the vagueness in\ncomparative plausibility values for hypotheses.)\n\n\nObserve that if the likelihood ratio values \\(\\LR^n\\) approach 0 as\nthe amount of evidence \\(e^n\\) increases, the interval of values for\nthe posterior probability ratio must become tighter as the upper bound\n(\\(\\LR^n\\times r)\\) approaches 0. Furthermore, the absolute degree of\nsupport for \\(h_j\\), \\(P_{\\alpha}[h_j \\pmid b\\cdot c^{n}\\cdot\ne^{n}]\\), must also approach 0.\n\nThis observation is really useful. For, it can be shown that when\n\\(h_{i}\\cdot b\\cdot c^{n}\\) is true and \\(h_j\\) is empirically\ndistinct from \\(h_i\\), the continual pursuit of evidence is very\nlikely to result in evidential outcomes \\(e^n\\) that (as\nn increases) yield values of likelihood ratios \\(P[e^n \\pmid\nh_{j}\\cdot b\\cdot c^{n}] / P[e^n \\pmid h_{i}\\cdot b\\cdot c^{n}]\\) that\napproach 0 as the amount of evidence increases. This result, called\nthe Likelihood Ratio Convergence Theorem, will be\ninvestigated in more detail in\n Section 4.\n When that kind of convergence towards 0 for likelihood ratios occurs,\nthe upper bound on the posterior probability ratio also approaches 0,\ndriving the posterior probability of \\(h_j\\) to approach 0 as well,\neffectively refuting hypothesis \\(h_j\\). Thus, false competitors of a\ntrue hypothesis will effectively be eliminated by increasing evidence.\nAs this happens, Equations\n 9*\n through\n 11\n show that the posterior probability \\(P_{\\alpha}[h_i \\pmid b\\cdot\nc^{n}\\cdot e^{n}]\\) of the true hypothesis \\(h_i\\) approaches 1. \n\nThus, Bayesian logic of inductive support for hypotheses is a form of\neliminative induction, where the evidence effectively refutes false\nalternatives to the true hypothesis. Because of its eliminative\nnature, the Bayesian logic of evidential support doesn\u2019t require\nprecise values for prior probabilities. It only needs to draw on\nbounds on the values of comparative plausibility ratios, and these\nbounds only play a significant role while evidence remains fairly\nweak. If the true hypothesis is assessed to be comparatively plausible\n(due to plausibility arguments contained in b), then\nplausibility assessments give it a leg-up over alternatives. If the\ntrue hypothesis is assessed to be comparatively implausible, the\nplausibility assessments merely slow down the rate at which it comes\nto dominate its rivals, reflecting the idea that extraordinary\nhypotheses require extraordinary evidence (or an extraordinary\naccumulation of evidence) to overcome their initial implausibilities.\nThus, as evidence accumulates, the agent\u2019s vague initial\nplausibility assessments transform into quite sharp posterior\nprobabilities that indicate their strong refutation or support by the\nevidence.\n\nWhen the various agents in a community may widely disagree over the\nnon-evidential plausibilities of hypotheses, the Bayesian logic of\nevidential support may represent this kind of diversity\nacross the community of agents as a collection of the agents\u2019\nvagueness sets of support functions. Let\u2019s call such a\ncollection of support functions a diversity set. That is, a\ndiversity set is just a set of support functions\n\\(P_{\\alpha}\\) that cover the ranges of values for comparative\nplausibility assessments for pairs of competing hypotheses\n\n\\[\nq \\le \\frac{P_{\\alpha}[h_j \\pmid  b]}{P_{\\alpha}[h_i \\pmid  b]} \\le r\n\\]\n\n\nas assessed by the scientific community. But, once again, if\naccumulating evidence drives the likelihood ratios comparing various\nalternative hypotheses to the true hypothesis towards 0, the range of\nsupport functions in a diversity set will come to near\nagreement, near 0, on the values for posterior probabilities of false\ncompetitors of the true hypothesis. So, not only does such evidence\nfirm up each agent\u2019s vague initial plausibility\nassessment, it also brings the whole community into agreement on the\nnear refutation of empirically distinct competitors of a true\nhypothesis. As this happens, the posterior probability of the true\nhypothesis may approach 1. The Likelihood Ratio Convergence\nTheorem implies that this kind of convergence to the truth should\nvery probably happen, provided that the true hypothesis is\nempirically distinct enough from its rivals.\n\nOne more point about prior probabilities and Bayesian convergence\nshould be mentioned before proceeding to\n Section 4.\n Some subjectivist versions of Bayesian induction seem to suggest that\nan agent\u2019s prior plausibility assessments for hypotheses should\nstay fixed once-and-for-all, and that all plausibility updating should\nbe brought about via the likelihoods in accord with Bayes\u2019\nTheorem. Critics argue that this is unreasonable. The members of a\nscientific community may quite legitimately revise their (comparative)\nprior plausibility assessments for hypotheses from time to time as\nthey rethink plausibility arguments and bring new considerations to\nbear. This seems a natural part of the conceptual development of a\nscience. It turns out that such reassessments of the comparative\nplausibilities of hypotheses poses no difficulty for the probabilistic\ninductive logic discussed here. Such reassessments may be represented\nby the addition or modification of explicit statements that modify the\nbackground information b. Such reassessments may result in\n(non-Bayesian) transitions to new vagueness sets for\nindividual agents and new diversity sets for the community.\nThe logic of Bayesian induction (as described here) has\nnothing to say about what values the prior plausibility assessments\nfor hypotheses should have; and it places no restrictions on how they\nmight change over time. Provided that the series of reassessments of\n(comparative) prior plausibilities doesn\u2019t happen to diminish\nthe (comparative) prior plausibility value of the true hypothesis\ntowards zero (or, at least, doesn\u2019t do so too quickly), the\nLikelihood Ratio Convergence Theorem implies that the\nevidence will very probably bring the posterior probabilities of\nempirically distinct rivals of the true hypothesis to approach 0 via\ndecreasing likelihood ratios; and as this happens, the posterior\nprobability of the true hypothesis will head towards 1.\n\n(Those interested in a Bayesian account of Enumerative Induction and\nthe estimation of values for relative frequencies of attributes in\npopulations should see the supplement,\n Enumerative Inductions: Bayesian Estimation and Convergence.)\n4. The Likelihood Ratio Convergence Theorem\n\nIn this section we will investigate the Likelihood Ratio\nConvergence Theorem. This theorem shows that under certain\nreasonable conditions, when hypothesis \\(h_i\\) (in conjunction with\nauxiliaries in b) is true and an alternative hypothesis \\(h_j\\)\nis empirically distinct from \\(h_i\\) on some possible outcomes of\nexperiments or observations described by conditions \\(c_k\\), then it\nis very likely that a long enough sequence of such\nexperiments and observations c\\(^n\\) will produce a sequence\nof outcomes \\(e^n\\) that yields likelihood ratios \\(P[e^n \\pmid\nh_{j}\\cdot b\\cdot c^{n}] / P[e^n \\pmid h_{i}\\cdot b\\cdot c^{n}]\\) that\napproach 0, favoring \\(h_i\\) over \\(h_j\\), as evidence accumulates\n(i.e., as n increases). This theorem places an explicit lower\nbound on the \u201crate of probable convergence\u201d of these\nlikelihood ratios towards 0. That is, it puts a lower bound on how\nlikely it is, if \\(h_i\\) is true, that a stream of outcomes will occur\nthat yields likelihood ratio values against \\(h_j\\) as compared to\n\\(h_i\\) that lie within any specified small distance above 0. \n\nThe theorem itself does not require the full apparatus of Bayesian\nprobability functions. It draws only on likelihoods. Neither the\nstatement of the theorem nor its proof employ prior probabilities of\nany kind. So even likelihoodists, who eschew the use of\nBayesian prior probabilities, may embrace this result. Given the forms\nof Bayes\u2019 Theorem, 9*-11 from the previous section, the\nLikelihood Ratio Convergence Theorem further implies the\nlikely convergence to 0 of the posterior probabilities of false\ncompetitors of a true hypothesis. That is, when the ratios \\(P[e^n\n\\pmid h_{j}\\cdot b\\cdot c^{n}] / P[e^n \\pmid h_{i}\\cdot b\\cdot\nc^{n}]\\) approach 0 for increasing n, the Ratio Form of\nBayes\u2019 Theorem,\n Equation 9*,\n says that the posterior probability of \\(h_j\\) must also approach 0\nas evidence accumulates, regardless of the value of its prior\nprobability. So, support functions in collections representing vague\nprior plausibilities for an individual agent (i.e., a\nvagueness set) and representing the diverse range of priors\nfor a community of agents (i.e., a diversity set) will come\nto agree on the near 0 posterior probability of empirically distinct\nfalse rivals of a true hypothesis. And as the posterior probabilities\nof false competitors fall, the posterior probability of the true\nhypothesis heads towards 1. Thus, the theorem establishes that the\ninductive logic of probabilistic support functions satisfies the\n Criterion of Adequacy (CoA)\n suggested at the beginning of this article.\n\nThe Likelihood Ratio Convergence Theorem merely provides some\nsufficient conditions for probable convergence. But likelihood ratios\nmay well converge towards 0 (in the way described by the theorem) even\nwhen the antecedent conditions of the theorem are not satisfied. This\ntheorem overcomes many of the objections raised by critics of Bayesian\nconvergence results. First, this theorem does not employ\nsecond-order probabilities; it says noting about the\nprobability of a probability. It only concerns the probability of a\nparticular disjunctive sentence that expresses a disjunction of\nvarious possible sequences of experimental or observational outcomes.\nThe theorem does not require evidence to consist of sequences of\nevents that, according to the hypothesis, are identically distributed\n(like repeated tosses of a die). The result is most easily expressed\nin cases where the individual outcomes of a sequence of experiments or\nobservations are probabilistically independent, given each hypothesis.\nSo that is the version that will be presented in this section.\nHowever, a version of the theorem also holds when the individual\noutcomes of the evidence stream are not probabilistically independent,\ngiven the hypotheses. (This more general version of the theorem will\nbe presented in a supplement on the\n Probabilistic Refutation Theorem,\n below, where the proof of both versions is provided.) In addition,\nthis result does not rely on supposing that the probability functions\ninvolved are countably additive. Furthermore, the explicit\nlower bounds on the rate of convergence provided by this result means\nthat there is no need to wait for the infinitely long run before\nconvergence occurs (as some critics seem to think).\n\nIt is sometimes claimed that Bayesian convergence results only work\nwhen an agent locks in values for the prior probabilities of\nhypotheses once-and-for-all, and then updates posterior probabilities\nfrom there only by conditioning on evidence via Bayes Theorem. The\nLikelihood Ratio Convergence Theorem, however, applies even\nif agents revise their prior probability assessments over time. Such\nnon-Bayesian shifts from one support function (or vagueness\nset) to another may arise from new plausibility arguments or from\nreassessments of the strengths of old ones. The Likelihood Ratio\nConvergence Theorem itself only involves the values of\nlikelihoods. So, provided such reassessments don\u2019t push the\nprior probability of the true hypothesis towards 0 too\nrapidly, the theorem implies that the posterior probabilities of\neach empirically distinct false competitor will very probably\napproach 0 as evidence\n increases.[13]\n\n4.1 The Space of Possible Outcomes of Experiments and Observations\n\nTo specify the details of the Likelihood Ratio Convergence\nTheorem we\u2019ll need a few additional notational conventions\nand definitions. Here they are.\n\nFor a given sequence of n experiments or observations \\(c^n\\),\nconsider the set of those possible sequences of outcomes that would\nresult in likelihood ratios for \\(h_j\\) over \\(h_i\\) that are less\nthan some chosen small number \\(\\varepsilon \\gt 0\\). This set is\nrepresented by the expression, \n\n\\[\n\\left\\{e^n : \\frac{P[e^n \\pmid  h_{j }\\cdot b\\cdot c^{ n}]}{P[e^n \\pmid  h_{i }\\cdot b\\cdot c^{ n}]} \\lt \\varepsilon \\right\\}.\n\\]\n\n\nPlacing the disjunction symbol \u2018\\(\\vee\\)\u2019 in front of this\nexpression yields an expression,  \n\n\\[\n\\vee \\left\\{  e^n : \\frac{P[e^n \\pmid  h_{j }\\cdot b\\cdot c^{ n}]}{P[e^n \\pmid  h_{i }\\cdot b\\cdot c^{ n}]} \\lt \\varepsilon \\right\\} ,\n\\]\n\n\nthat we\u2019ll use to represent the disjunction of all outcome\nsequences \\(e^n\\) in this set. So, \n\n\\[\n\\vee \\left\\{   e^n : \\frac{P[e^n \\pmid  h_{j }\\cdot b\\cdot c^{ n}]}{P[e^n \\pmid  h_{i }\\cdot b\\cdot c^{ n}]} \\lt \\varepsilon \\right\\} \n\\]\n\n\nis just a particular sentence that says, in effect, \u201cone of the\nsequences of outcomes of the first n experiments or\nobservations will occur that makes the likelihood ratio for \\(h_j\\)\nover \\(h_i\\) less than \\(\\varepsilon\\)\u201d.\n\nThe Likelihood Ratio Convergence Theorem says that under\ncertain conditions (covered in detail below), the likelihood of a\ndisjunctive sentence of this sort, given that \u2018\\(h_{i}\\cdot\nb\\cdot c^{n}\\)\u2019 is true, \n\n\\[\n P \\left[\\vee \\left\\{   e^n : \\frac{P[e^n \\pmid  h_{j }\\cdot b\\cdot c^n]}{P[e^n \\pmid  h_i\\cdot b\\cdot c^{ n}]} \\lt \\varepsilon \\right\\}  \\pmid    h_{i }\\cdot b\\cdot c^{ n}\\right] ,\n\\]\n\n\nmust be at least \\(1-(\\psi /n)\\), for some explicitly calculable term\n\\(\\psi\\). Thus, the true hypothesis \\(h_i\\) probabilistically implies\nthat as the amount of evidence, n, increases, it becomes highly\nlikely (as close to 1 as you please) that one of the outcome sequences\n\\(e^n\\) will occur that yields a likelihood ratio \\(P[e^n \\pmid\nh_{j}\\cdot b\\cdot c^{n}] / P[e^n \\pmid h_{i}\\cdot b\\cdot c^{n}]\\) less\nthan \\(\\varepsilon\\); and this holds for any specific value of\n\\(\\varepsilon\\) you may choose. As this happens, the posterior\nprobability of \\(h_i\\)\u2019s false competitor, \\(h_j\\), must\napproach 0, as required by the Ratio Form of Bayes\u2019 Theorem,\n Equation 9*.\n\nThe term \\(\\psi\\) in the lower bound of this probability depends on a\nmeasure of the empirical distinctness of the two hypotheses \\(h_j\\)\nand \\(h_i\\) for the proposed sequence of experiments and observations\n\\(c^n\\). To specify this measure we need to contemplate the collection\nof possible outcomes of each experiment or observation. So, consider\nsome sequence of experimental or observational conditions described by\nsentences \\(c_1,c_2 ,\\ldots ,c_n\\). Corresponding to each condition\n\\(c_k\\) there will be some range of possible alternative outcomes. Let\n\\(O_{k} = \\{o_{k1},o_{k2},\\ldots ,o_{kw}\\}\\) be a set of statements\ndescribing the alternative possible outcomes for condition \\(c_k\\).\n(The number of alternative outcomes will usually differ for distinct\nexperiments among those in the sequence \\(c_1 ,\\ldots ,c_n\\); so, the\nvalue of w may depend on \\(c_k\\).) For each hypothesis \\(h_j\\),\nthe alternative outcomes of \\(c_k\\) in \\(O_k\\) are mutually exclusive\nand exhaustive, so we have: \n\n\\[\n    P[o_{ku }\\cdot o_{kv} \\pmid  h_j\\cdot b\\cdot c_{ k}] = 0  \n\\textrm{  and }\n\\sum^{w}_{u = 1}    P[o_{ku} \\pmid  h_{j }\\cdot b\\cdot c_{ k}] =1 .  \n\\]\n\n\nWe now let expressions of form \u2018\\(e_k\\)\u2019 act as variables\nthat range over the possible outcomes of condition \\(c_k\\)\u2014i.e.,\n\\(e_k\\) ranges over the members of \\(O_k\\). As before,\n\u2018\\(c^n\\)\u2019 denotes the conjunction of the first n\ntest conditions, \\((c_1\\cdot c_2\\cdot \\ldots \\cdot c_n)\\), and\n\u2018\\(e^n\\)\u2019 represents possible sequences of corresponding\noutcomes, \\((e_1\\cdot e_2\\cdot \\ldots \\cdot e_n)\\). Let\u2019s use\nthe expression \u2018E\\(^n\\)\u2019 to represent the set of\nall possible outcome sequences that may result from the sequence of\nconditions c\\(^n\\). So, for each hypothesis \\(h_j\\)\n(including \\(h_i)\\), \\(\\sum_{e^n\\in E^n} P[e^n \\pmid h_{j}\\cdot b\\cdot\nc^{n}] = 1\\). \n\nEverything introduced in this subsection is mere notational\nconvention. No substantive suppositions (other than the axioms of\nprobability theory) have yet been introduced. The version of the\nLikelihood Ratio Convergence Theorem I\u2019ll present below\ndoes, however, draw on one substantive supposition, although a rather\nweak one. The next subsection will discuss that supposition in\ndetail.\n4.2 Probabilistic Independence\n\nIn most scientific contexts the outcomes in a stream of experiments or\nobservations are probabilistically independent of one another\nrelative to each hypothesis under consideration, or can at least be\ndivided up into probabilistically independent parts. For our purposes\nprobabilistic independence of evidential outcomes on a\nhypothesis divides neatly into two types.\n\n\nDefinition: Independent Evidence Conditions:\n\n A sequence of outcomes \\(e^k\\) is\ncondition-independent of a condition for an\nadditional experiment or observation \\(c_{k+1}\\), given \\(h\\cdot b\\)\ntogether with its own conditions \\(c^k\\), if and only if\n\n\\[\nP[e^k \\pmid  h\\cdot b\\cdot c^{k }\\cdot c_{ k+1}]  = P[e^k \\pmid  h\\cdot b\\cdot c^k] .\n\\]\n\n \n An individual outcome \\(e_k\\) is \nresult-independent of a sequence of other observations and\ntheir outcomes \\((c^{k-1}\\cdot e^{k-1})\\), given \\(h\\cdot b\\) and its\nown condition \\(c_k\\), if and only if \n\n\\[\nP[e_k \\pmid  h\\cdot b\\cdot c_k\\cdot(c^{k-1 }\\cdot e^{ k-1})]  = P[e_k \\pmid  h\\cdot b\\cdot c_k] .\n  \\]\n\n \n\n\n\nWhen these two conditions hold, the likelihood for an evidence\nsequence may be decomposed into the product of the likelihoods for\nindividual experiments or observations. To see how the two\nindependence conditions affect the decomposition, first\nconsider the following formula, which holds even when neither\nindependence condition is satisfied: \n\n\\[\\tag{12}\n    P[e^n \\pmid  h_{j }\\cdot b\\cdot c^{ n}] =  \n\\prod^{n}_{k = 1}\n    P[e_k \\pmid  h_{j }\\cdot b\\cdot c^n\\cdot e^{ k-1}] .  \n\\]\n\n\nWhen condition-independence holds, the likelihood of the\nwhole evidence stream parses into a product of likelihoods that\nprobabilistically depend on only past observation conditions\nand their outcomes. They do not depend on the conditions for other\nexperiments whose outcomes are not yet specified. Here is the\nformula: \n\n\\[\\tag{13}\n    P[e^n \\pmid  h_{j }\\cdot b\\cdot c^{ n}] =  \n \\prod^{n}_{k = 1}\n    P[e_k \\pmid  h_{j }\\cdot b\\cdot c_k\\cdot (c^{k-1}\\cdot e^{ k-1})] .  \n\\]\n\n\nFinally, whenever both independence conditions are satisfied\nwe have the following relationship between the likelihood of the\nevidence stream and the likelihoods of individual experiments or\nobservations: \n\n\\[\\tag{14}\n    P[e^n \\pmid  h_{j }\\cdot b\\cdot c^{ n}] =  \n\\prod^{n}_{k = 1}\n    P[e_k \\pmid  h_{j }\\cdot b\\cdot c_{ k}] .  \n\\]\n\n\n(For proofs of Equations 12\u201314 see the supplement\n Immediate Consequences of Independent Evidence Conditions.)\n\nIn scientific contexts the evidence can almost always be divided into\nparts that satisfy both clauses of the Independent Evidence\nCondition with respect to each alternative hypothesis. To see\nwhy, let us consider each independence condition more carefully.\n\nCondition-independence says that the mere addition of a new\nobservation condition \\(c_{k+1}\\), without specifying one of its\noutcomes, does not alter the likelihood of the outcomes \\(e^k\\)\nof other experiments \\(c^k\\). To appreciate the significance of this\ncondition, imagine what it would be like if it were violated. Suppose\nhypothesis \\(h_j\\) is some statistical theory, say, for example, a\nquantum theory of superconductivity. The conditions expressed in\n\\(c^k\\) describe a number of experimental setups, perhaps conducted in\nnumerous labs throughout the world, that test a variety of aspects of\nthe theory (e.g., experiments that test electrical conductivity in\ndifferent materials at a range of temperatures). An outcome sequence\n\\(e^k\\) describes the results of these experiments. The violation of\ncondition-independence would mean that merely adding to\n\\(h_{j}\\cdot b\\cdot c^{k}\\) a statement \\(c_{k+1}\\) describing how an\nadditional experiment has been set up, but with no mention of its\noutcome, changes how likely the evidence sequence \\(e^k\\) is taken to\nbe. What \\((h_j\\cdot b)\\) says via likelihoods about the\noutcomes \\(e^k\\) of experiments \\(c^k\\) differs as a result of merely\nsupplying a description of another experimental arrangement,\n\\(c_{k+1}\\). Condition-independence, when it holds, rules out\nsuch strange effects.\n\nResult-independence says that the description of previous\ntest conditions together with their outcomes is irrelevant to\nthe likelihoods of outcomes for additional experiments. If this\ncondition were widely violated, then in order to specify the most\ninformed likelihoods for a given hypothesis one would need to include\ninformation about volumes of past observations and their outcomes.\nWhat a hypothesis says about future cases would depend on how past\ncases have gone. Such dependence had better not happen on a\nlarge scale. Otherwise, the hypothesis would be fairly useless, since\nits empirical import in each specific case would depend on taking into\naccount volumes of past observational and experimental results.\nHowever, even if such dependencies occur, provided they are not too\npervasive, result-independence can be accommodated rather\neasily by packaging each collection of result-dependent data\ntogether, treating it like a single extended experiment or\nobservation. The result-independence condition will then be\nsatisfied by letting each term \u2018\\(c_k\\)\u2019 in the statement\nof the independence condition represent a conjunction of test\nconditions for a collection of result-dependent tests, and by\nletting each term \u2018\\(e_k\\)\u2019 (and each term\n\u2018\\(o_{ku}\\)\u2019) stand for a conjunction of the corresponding\nresult-dependent outcomes. Thus, by packaging\nresult-dependent data together in this way, the\nresult-independence condition is satisfied by those\n(conjunctive) statements that describe the separate,\nresult-independent\n chunks.[14]\n\n\nThe version of the Likelihood Ratio Convergence Theorem we\nwill examine depends only on the Independent Evidence\nConditions (together with the axioms of probability theory). It\ndraws on no other assumptions. Indeed, an even more general version of\nthe theorem can be established, a version that draws on neither of the\nIndependent Evidence Conditions. However, the Independent\nEvidence Conditions will be satisfied in almost all scientific\ncontexts, so little will be lost by assuming them. (And the\npresentation will run more smoothly if we side-step the added\ncomplications needed to explain the more general result.)\n\nFrom this point on, let us assume that the following versions of the\nIndependent Evidence Conditions hold. \n\n\nAssumption: Independent Evidence Assumptions. For\neach hypothesis h and background b under consideration,\nwe assume that the experiments and observations can be packaged into\ncondition statements, \\(c_1 ,\\ldots ,c_k, c_{k+1},\\ldots\\), and\npossible outcomes in a way that satisfies the following\nconditions:\n\n Each sequence of possible outcomes \\(e^k\\) of a sequence of\nconditions \\(c^k\\) is condition-independent of\nadditional conditions \\(c_{k+1}\\)\u2014i.e., \n\n\\[P[e^k \\pmid  h\\cdot b\\cdot c^{k}\\cdot c_{k+1}] = P[e^k \\pmid  h\\cdot b\\cdot c^k].\\]\n Each possible outcome \\(e_k\\) of condition \\(c_k\\) is\nresult-independent of sequences of other observations\nand possible outcomes \\((c^{k-1}\\cdot e^{k-1})\\)\u2014i.e.,\n\n\\[P[e_k \\pmid  h\\cdot b\\cdot c_k\\cdot(c^{k-1}\\cdot e^{k-1})] = P[e_k \\pmid  h\\cdot b\\cdot c_k].\\]\n\n\n\nWe now have all that is needed to begin to state the Likelihood\nRatio Convergence Theorem.\n4.3 Likelihood Ratio Convergence when Falsifying Outcomes are Possible\n\nThe Likelihood Ratio Convergence Theorem comes in two parts.\nThe first part applies only to those experiments or observations\n\\(c_k\\) within the total evidence stream \\(c^n\\) for which some of the\npossible outcomes have 0 likelihood of occurring according to\nhypothesis \\(h_j\\) but have non-0 likelihood of occurring according to\n\\(h_i\\). Such outcomes are highly desirable. If they occur, the\nlikelihood ratio comparing \\(h_j\\) to \\(h_i\\) will become 0, and\n\\(h_j\\) will be falsified. So-called crucial\nexperiments are a special case of this, where for at least one\npossible outcome \\(o_{ku}\\), \\(P[o_{ku} \\pmid h_{i}\\cdot b\\cdot c_{k}]\n= 1\\) and \\(P[o_{ku} \\pmid h_{j}\\cdot b\\cdot c_{k}] = 0\\). In the more\ngeneral case \\(h_i\\) together with b says that one of the\noutcomes of \\(c_k\\) is at least minimally probable, whereas \\(h_j\\)\nsays that this outcome is impossible\u2014i.e., \\(P[o_{ku} \\pmid\nh_{i}\\cdot b\\cdot c_{k}] \\gt 0\\) and \\(P[o_{ku} \\pmid h_{j}\\cdot\nb\\cdot c_{k}] = 0\\). It will be convenient to define a term for this\nsituation.\n\n\nDefinition: Full Outcome Compatibility. Let\u2019s\ncall \\(h_j\\) fully outcome-compatible with \\(h_i\\) on\nexperiment or observation \\(c_k\\) just when, for each of its\npossible outcomes \\(e_k\\), if \\(P[e_k \\pmid h_{i}\\cdot b\\cdot c_{k}]\n\\gt 0\\), then \\(P[e_k \\pmid h_{j}\\cdot b\\cdot c_{k}] \\gt 0\\).\nEquivalently, \\(h_j\\) is fails to be fully outcome-compatible\nwith \\(h_i\\) on experiment or observation \\(c_k\\) just when,\nfor at least one of its possible outcomes \\(e_k\\), \\(P[e_k \\pmid\nh_{i}\\cdot b\\cdot c_{k}] \\gt 0\\) but \\(P[e_k \\pmid h_{j}\\cdot b\\cdot\nc_{k}] = 0\\). \n\n\nThe first part of the Likelihood Ratio Convergence Theorem\napplies to that part of the total stream of evidence (i.e., that\nsubsequence of the total evidence stream) on which hypothesis \\(h_j\\)\nfails to be fully outcome-compatible with hypothesis \\(h_i\\);\nthe second part of the theorem applies to the remaining part of the\ntotal stream of evidence, that subsequence of the total evidence\nstream on which \\(h_j\\) is fully outcome-compatible with\n\\(h_i\\). It turns out that these two kinds of cases must be treated\ndifferently. (This is due to the way in which the expected\ninformation content for empirically distinguishing between the\ntwo hypotheses will be measured for experiments and observations that\nare fully outcome compatible; this measure of information\ncontent blows up (becomes infinite) for experiments and observations\nthat fail to be fully outcome compatible). Thus, the\nfollowing part of the convergence theorem applies to just that part of\nthe total stream of evidence that consists of experiments and\nobservations that fail to be fully outcome compatible for the\npair of hypotheses involved. Here, then, is the first part of the\nconvergence theorem.\n\n\nLikelihood Ratio Convergence Theorem 1\u2014The Falsification\nTheorem:\n\nSuppose that the total stream of evidence \\(c^n\\) contains precisely\nm experiments or observations on which \\(h_j\\) fails to be\nfully outcome-compatible with \\(h_i\\). And suppose that the\nIndependent Evidence Conditions hold for evidence stream\n\\(c^n\\) with respect to each of these two hypotheses. Furthermore,\nsuppose there is a lower bound \\(\\delta \\gt 0\\) such that for each\n\\(c_k\\) on which \\(h_j\\) fails to be fully outcome-compatible\nwith \\(h_i\\),  \n\n\\[P[\\vee \\{ o_{ku} : P[o_{ku} \\pmid  h_{j}\\cdot b\\cdot c_{k}] = 0\\} \\pmid  h_{i}\\cdot b\\cdot c_{k}]  \\ge \\delta\\]\n\n\n\u2014i.e., \\(h_i\\) together with \\(b\\cdot c_k\\) says, with\nlikelihood at least as large as \\(\\delta\\), that one of the outcomes\nwill occur that \\(h_j\\) says cannot occur. Then, \n\n\\[\\begin{align}\nP \\left[\\vee \\left\\{ e^n : \\frac{P[e^n \\pmid  h_{j}\\cdot b\\cdot c^n]}{P[e^n \\pmid  h_i\\cdot b\\cdot c^{n}]} = 0\\right\\} \\pmid   h_{i}\\cdot b\\cdot c^{n}\\right]\\\\[2ex]\n = P\\left[\\vee \\left\\{ e^n : P[e^n \\pmid  h_{j}\\cdot b\\cdot c^{n}] = 0\\right\\} \\pmid   h_{i}\\cdot b\\cdot c^{n}\\right]\\\\\n \\ge 1 - (1-\\delta)^m,\n\\end{align}\n\\]\n\n\nwhich approaches 1 for large m. (For proof see\n Proof of the Falsification Theorem.)\n \n\n\nIn other words, we only suppose that for each of m\nobservations, \\(c_k, h_i\\) says observation \\(c_k\\) has at\nleast a small likelihood \\(\\delta\\) of producing one of the outcomes\n\\(o_{ku}\\) that \\(h_j\\) says is impossible. If the number\nm of such experiments or observations is large enough (or if\nthe lower bound \\(\\delta\\) on the likelihoods of getting such outcomes\nis large enough), and if \\(h_i\\) (together with \\(b\\cdot c^n)\\) is\ntrue, then it is highly likely that one of the outcomes held to be\nimpossible by \\(h_j\\) will actually occur. If one of these outcomes\ndoes occur, then the likelihood ratio for \\(h_j\\) as compared to over\n\\(h_i\\) will become 0. According to Bayes\u2019 Theorem, when this\nhappen, \\(h_j\\) is absolutely refuted by the evidence\u2014its\nposterior probability becomes 0.\n\nThe Falsification Theorem is quite commonsensical. First, notice that\nif there is a crucial experiment in the evidence stream, the\ntheorem is completely obvious. That is, suppose for the specific\nexperiment \\(c_k\\) (in evidence stream \\(c^n)\\) there are two\nincompatible possible outcomes \\(o_{kv}\\) and \\(o_{ku}\\) such that\n\\(P[o_{kv} \\pmid h_{j}\\cdot b\\cdot c_{k}] = 1\\) and \\(P[o_{ku} \\pmid\nh_{i}\\cdot b\\cdot c_{k}] = 1\\). Then, clearly, \\(P[\\vee \\{ o_{ku}:\nP[o_{ku} \\pmid h_{j}\\cdot b\\cdot c_{k}] = 0\\} \\pmid h_{i}\\cdot b\\cdot\nc_{k}] = 1\\), since \\(o_{ku}\\) is one of the \\(o_{ku}\\) such that\n\\(P[o_{ku} \\pmid h_{j}\\cdot b\\cdot c_{k}] = 0\\). So, where a crucial\nexperiment is available, the theorem applies with \\(m = 1\\) and\n\\(\\delta = 1\\).\n\nThe theorem is equally commonsensical for cases where no crucial\nexperiment is available. To see what it says in such cases, consider\nan example. Let \\(h_i\\) be some theory that implies a specific rate of\nproton decay, but a rate so low that there is only a very small\nprobability that any particular proton will decay in a given year.\nConsider an alternative theory \\(h_j\\) that implies that protons\nnever decay. If \\(h_i\\) is true, then for a persistent enough\nsequence of observations (i.e., if proper detectors can keep trillions\nof protons under observation for long enough), eventually a proton\ndecay will almost surely be detected. When this happens, the\nlikelihood ratio becomes 0. Thus, the posterior probability of \\(h_j\\)\nbecomes 0.\n\nIt is instructive to plug some specific values into the formula given\nby the Falsification Theorem, to see what the convergence rate might\nlook like. For example, the theorem tells us that if we compare any\npair of hypotheses \\(h_i\\) and \\(h_j\\) on an evidence stream \\(c^n\\)\nthat contains at least \\(m = 19\\) observations or experiments, where\neach has a likelihood \\(\\delta \\ge .10\\) of yielding a falsifying\noutcome, then the likelihood (on \\(h_{i}\\cdot b\\cdot c^{n})\\) of\nobtaining an outcome sequence \\(e^n\\) that yields likelihood-ratio\n\n \\[\\frac{P[e^n \\pmid  h_{j}\\cdot b\\cdot c^{n}] }{P[e^n \\pmid  h_{i}\\cdot b\\cdot c^{n}]} = 0,\\]\n\n\nwill be at least as large as \\((1 - (1-.1)^{19}) = .865\\). (The reader\nis invited to try other values of \\(\\delta\\) and m.)\n\nA comment about the need for and usefulness of such\nconvergence theorems is in order, now that we\u2019ve seen one. Given\nsome specific pair of scientific hypotheses \\(h_i\\) and \\(h_j\\) one\nmay directly compute the likelihood, given \\((h_{i}\\cdot b\\cdot\nc^{n})\\), that a proposed sequence of experiments or observations\n\\(c^n\\) will result in one of the sequences of outcomes that would\nyield low likelihood ratios. So, given a specific pair of hypotheses\nand a proposed sequence of experiments, we don\u2019t need a general\nConvergence Theorem to tell us the likelihood of obtaining\nrefuting evidence. The specific hypotheses \\(h_i\\) and \\(h_j\\) tell us\nthis themselves. They tell us the likelihood of obtaining\neach specific outcome stream, including those that either refute the\ncompetitor or produce a very small likelihood ratio for it.\nFurthermore, after we\u2019ve actually performed an experiment and\nrecorded its outcome, all that matters is the actual ratio of\nlikelihoods for that outcome. Convergence theorems become moot.\n\nThe point of the Likelihood Ratio Convergence Theorem (both the\nFalsification Theorem and the part of the theorem still to come) is to\nassure us in advance of considering any specific pair of\nhypotheses that if the possible evidence streams that test\nhypotheses have certain characteristics which reflect the empirical\ndistinctness of the two hypotheses, then it is highly likely that one\nof the sequences of outcomes will occur that yields a very small\nlikelihood ratio. These theorems provide finite lower bounds on how\nquickly such convergence is likely to be. Thus, they show that the\n CoA\n is satisfied in advance of our using the logic to test specific pairs\nof hypotheses against one another.\n4.4 Likelihood Ratio Convergence When No Falsifying Outcomes are Possible\n\nThe Falsification Theorem applies whenever the evidence stream\nincludes possible outcomes that may falsify the alternative\nhypothesis. However, it completely ignores the influence of any\nexperiments or observations in the evidence stream on which hypothesis\n\\(h_j\\) is fully outcome-compatible with hypothesis \\(h_i\\).\nWe now turn to a theorem that applies to those evidence streams (or to\nparts of evidence streams) consisting only of experiments and\nobservations on which hypothesis \\(h_j\\) is fully\noutcome-compatible with hypothesis \\(h_i\\). Evidence streams of\nthis kind contain no possibly falsifying outcomes. In such\ncases the only outcomes of an experiment or observation \\(c_k\\) for\nwhich hypothesis \\(h_j\\) may specify 0 likelihoods are those for which\nhypothesis \\(h_i\\) specifies 0 likelihoods as well. \n\nHypotheses whose connection with the evidence is entirely statistical\nin nature will usually be fully outcome-compatible on the\nentire evidence stream. So, evidence streams of this kind are\nundoubtedly much more common in practice than those containing\npossibly falsifying outcomes. Furthermore, whenever an entire stream\nof evidence contains some mixture of experiments and observations on\nwhich the hypotheses are not fully outcome compatible along\nwith others on which they are fully outcome compatible, we\nmay treat the experiments and observations for which full outcome\ncompatibility holds as a separate subsequence of the entire\nevidence stream, to see the likely impact of that part of the evidence\nin producing values for likelihood ratios. \n\nTo cover evidence streams (or subsequences of evidence streams)\nconsisting entirely of experiments or observations on which \\(h_j\\) is\nfully outcome-compatible with hypothesis \\(h_i\\) we will\nfirst need to identify a useful way to measure the degree to which\nhypotheses are empirically distinct from one another on such evidence.\nConsider some particular sequence of outcomes \\(e^n\\) that results\nfrom observations \\(c^n\\). The likelihood ratio \\(P[e^n \\pmid\nh_{j}\\cdot b\\cdot c^{n}] / P[e^n \\pmid h_{i}\\cdot b\\cdot c^{n}]\\)\nitself measures the extent to which the outcome sequence distinguishes\nbetween \\(h_i\\) and \\(h_j\\). But as a measure of the power of evidence\nto distinguish among hypotheses, raw likelihood ratios provide a\nrather lopsided scale, a scale that ranges from 0 to infinity with the\nmidpoint, where \\(e^n\\) doesn\u2019t distinguish at all between\n\\(h_i\\) and \\(h_j\\), at 1. So, rather than using raw likelihood ratios\nto measure the ability of \\(e^n\\) to distinguish between hypotheses,\nit proves more useful to employ a symmetric measure. The logarithm of\nthe likelihood ratio provides such a measure.\n\n\nDefinition: QI\u2014the Quality of the Information.\n\nFor each experiment or observation \\(c_k\\), define the quality of\nthe information provided by possible outcome \\(o_{ku}\\) for\ndistinguishing \\(h_j\\) from \\(h_i\\), given b, as follows (where\nhenceforth we take \u201clogs\u201d to be base-2): \n\n\\[\n\\QI[o_{ku} \\pmid  h_i /h_j \\pmid  b\\cdot c_k] = \\log\\left[\\frac{P[o_{ku} \\pmid  h_{i}\\cdot b\\cdot c_k]}{P[o_{ku} \\pmid  h_j\\cdot b\\cdot c_{k}]}\\right].\n\\]\n\n\nSimilarly, for the sequence of experiments or observations \\(c^n\\),\ndefine the quality of the information provided by possible\noutcome \\(e^n\\) for distinguishing \\(h_j\\) from \\(h_i\\), given\nb, as follows: \n\n\\[\n\\QI[e^n \\pmid  h_i /h_j \\pmid  b\\cdot c^n] = \\log\\left[\\frac{P[e^n \\pmid  h_{i}\\cdot b\\cdot c^n]}{P[e^n \\pmid  h_j\\cdot b\\cdot c^{n}]}\\right].\n\\]\n\n\nThat is, QI is the base-2 logarithm of the likelihood ratio for\n\\(h_i\\) over that for \\(h_j\\).\n\n\nSo, we\u2019ll measure the Quality of the Information an\noutcome would yield in distinguishing between two hypotheses as the\nbase-2 logarithm of the likelihood ratio. This is clearly a symmetric\nmeasure of the outcome\u2019s evidential strength at distinguishing\nbetween the two hypotheses. On this measure hypotheses \\(h_i\\) and\n\\(h_j\\) assign the same likelihood value to a given outcome \\(o_{ku}\\)\njust when \\(\\QI[o_{ku} \\pmid h_i /h_j \\pmid b\\cdot c_k] =\n0\\). Thus, QI measures information on a logarithmic scale that is\nsymmetric about the natural no-information midpoint, 0. This measure\nis set up so that positive information favors \\(h_i\\) over\n\\(h_j\\), and negative information favors \\(h_j\\) over\n\\(h_i\\).\n\nGiven the Independent Evidence Assumptions with respect to\neach hypothesis, it\u2019s easy to show that the QI for a sequence of\noutcomes is just the sum of the QIs of the individual outcomes in the\nsequence: \n\n\\[\\tag{15}\n   \\QI[e^n \\pmid  h_i /h_j \\pmid  b\\cdot c^n] =\n\\sum^{n}_{k = 1}\n   \\QI[e_k \\pmid  h_i /h_j \\pmid  b\\cdot c_k].  \n\\]\n\n\nProbability theorists measure the expected value of a\nquantity by first multiplying each of its possible values by\ntheir probabilities of occurring, and then summing these products.\nThus, the expected value of QI is given by the following\nformula:\n\n\nDefinition: EQI\u2014the Expected Quality of the\nInformation.\n\nWe adopt the convention that if \\(P[o_{ku} \\pmid h_{i}\\cdot b\\cdot\nc_{k}] = 0\\), then the term \\(\\QI[o_{ku} \\pmid h_i /h_j \\pmid b\\cdot\nc_k] \\times P[o_{ku} \\pmid h_{i}\\cdot b\\cdot c_{k}] = 0\\). This\nconvention will make good sense in the context of the following\ndefinition because, whenever the outcome \\(o_{ku}\\) has 0 probability\nof occurring according to \\(h_i\\) (together with \\(b\\cdot c_k)\\), it\nmakes good sense to give it 0 impact on the ability of the evidence to\ndistinguish between \\(h_j\\) and \\(h_i\\) when \\(h_i\\) (together with\n\\(b\\cdot c_k)\\) is true. Also notice that the full\noutcome-compatibility of \\(h_j\\) with \\(h_i\\) on \\(c_k\\) means\nthat whenever \\(P[e_k \\pmid h_{j}\\cdot b\\cdot c_{k}] = 0\\), we must\nhave \\(P[e_k \\pmid h_{i}\\cdot b\\cdot c_{k}] = 0\\) as well; so whenever\nthe denominator would be 0 in the term  \n\n\\[\\QI[o_{ku} \\pmid  h_i /h_j \\pmid  b\\cdot c_k] = \\log\\left[\\frac{P[o_{ku} \\pmid  h_{i}\\cdot b\\cdot c_k]}{P[o_{ku} \\pmid  h_j\\cdot b\\cdot c_{k}]}\\right],\\]\n\n\nthe convention just described makes the term  \n\n\\[\\QI[o_{ku} \\pmid  h_i /h_j \\pmid  b\\cdot c_k] \\times P[o_{ku} \\pmid  h_{i}\\cdot b\\cdot c_{k}] = 0.\\]\n\n\nThus the following notion is well-defined: \n\nFor \\(h_j\\) fully outcome-compatible with \\(h_i\\) on\nexperiment or observation \\(c_k\\), define \n\n\\[\n\\EQI[c_k \\pmid  h_i /h_j \\pmid  b] = \\sum_u \\QI[o_{ku} \\pmid  h_i /h_j \\pmid  b\\cdot c_k] \\times P[o_{ku} \\pmid  h_{i}\\cdot b\\cdot c_{k}].\n\\]\n\n\nAlso, for \\(h_j\\) fully outcome-compatible with \\(h_i\\) on\neach experiment and observation in the sequence \\(c^n\\), define\n\n   \\[\n\\EQI[c^n \\pmid  h_i /h_j \\pmid  b]\n  =\n  \\sum_{e^n\\in E^n}\n   \\QI[e^n \\pmid  h_i /h_j \\pmid  b\\cdot c^n] \\times P[e^n \\pmid  h_{i}\\cdot b\\cdot c^{n}].  \n\\]\n\n\n\nThe EQI of an experiment or observation is the Expected Quality of\nits Information for distinguishing \\(h_i\\) from \\(h_j\\) when\n\\(h_i\\) is true. It is a measure of the expected evidential strength\nof the possible outcomes of an experiment or observation at\ndistinguishing between the hypotheses when \\(h_i\\) (together with\n\\(b\\cdot c)\\) is true. Whereas QI measures the ability of each\nparticular outcome or sequence of outcomes to empirically distinguish\nhypotheses, EQI measures the tendency of experiments or observations\nto produce distinguishing outcomes. It can be shown that EQI tracks\nempirical distinctness in a very precise way. We return to this in a\nmoment.\n\nIt is easily seen that the EQI for a sequence of observations \\(c^n\\)\nis just the sum of the EQIs of the individual observations \\(c_k\\) in\nthe sequence: \n\n\\[\\tag{16}\n   \\EQI[c^n \\pmid  h_i /h_j \\pmid  b]\n  =\n   \\sum^{n}_{k=1}\n   \\EQI[c_k \\pmid  h_i /h_j \\pmid  b_{}].  \n\\]\n\n\n(For proof see the supplement\n Proof that the EQI for \\(c^n\\) is the sum of the EQI for the individual \\(c_k\\).)\n\nThis suggests that it may be useful to average the values of the\n\\(\\EQI[c_k \\pmid h_i /h_j \\pmid b]\\) over the number of observations\nn to obtain a measure of the average expected quality of\nthe information among the experiments and observations that make\nup the evidence stream \\(c^n\\).\n\n\nDefinition: The Average Expected Quality of\nInformation\n\nFor \\(h_j\\) fully outcome-compatible with \\(h_i\\) on each\nexperiment and observation in the evidence stream \\(c^n\\), define the\naverage expected quality of information, \\(\\bEQI\\), from \\(c^n\\) for\ndistinguishing \\(h_j\\) from \\(h_i\\), given \\(h_i\\cdot b\\), as\nfollows: \n\n\\[\n\\begin{align}\n   \\bEQI[c^n \\pmid  h_i /h_j \\pmid  b] \n&   =\n   \\frac{\\EQI[c^n \\pmid  h_i /h_j \\pmid  b]}{n}\\\\\n&   =\n  (1/n) \\times\n   \\sum^{n}_{k=1}\n   \\EQI[c_k \\pmid  h_i /h_j \\pmid  b_{}].\n\\end{align}\n  \\]\n\n\n\nIt turns out that the value of \\(\\EQI[c_k \\pmid h_i /h_j \\pmid b_{}]\\)\ncannot be less than 0; and it must be greater than 0 just in case\n\\(h_i\\) is empirically distinct from \\(h_j\\) on at least one\noutcome \\(o_{ku}\\)\u2014i.e., just in case it is empirically\ndistinct in the sense that \\(P[o_{ku} \\pmid h_{i}\\cdot b\\cdot\nc_{k}] \\ne P[o_{ku} \\pmid h_{j}\\cdot b\\cdot c_{k}]\\), for at least one\noutcome \\(o_{ku}\\). The same goes for the average, \\(\\bEQI[c^n \\pmid\nh_i /h_j \\pmid b]\\).\n\n\nTheorem: Nonnegativity of EQI.\n\n\\(\\EQI[c_k \\pmid h_i /h_j \\pmid b_{}] \\ge 0\\); and \\(\\EQI[c_k \\pmid\nh_i /h_j \\pmid b_{}] \\gt 0\\) if and only if for at least one\nof its possible outcomes \\(o_{ku}\\),  \n\n\\[P[o_{ku} \\pmid  h_{i}\\cdot b\\cdot c_{k}] \\ne P[o_{ku} \\pmid  h_{j}\\cdot b\\cdot c_{k}].\\]\n\n\nAs a result, \\(\\bEQI[c^n \\pmid h_i /h_j \\pmid b] \\ge 0\\); and\n\\(\\bEQI[c^n \\pmid h_i /h_j \\pmid b] \\gt 0\\) if and only if at\nleast one experiment or observation \\(c_k\\) has at least one possible\noutcome \\(o_{ku}\\) such that  \n\n\\[P[o_{ku} \\pmid  h_{i}\\cdot b\\cdot c_{k}] \\ne P[o_{ku} \\pmid  h_{j}\\cdot b\\cdot c_{k}].\\]\n\n\n(For proof, see the supplement\n The Effect on EQI of Partitioning the Outcome Space More Finely\u2014Including Proof of the Nonnegativity of EQI.)\n\n\nIn fact, the more finely one partitions the outcome space \\(O_{k} =\n\\{o_{k1},\\ldots ,o_{kv},\\ldots ,o_{kw}\\}\\) into distinct outcomes that\ndiffer on likelihood ratio values, the larger EQI\n becomes.[15]\n This shows that EQI tracks empirical distinctness in a precise way.\nThe importance of the Non-negativity of EQI result for the\nLikelihood Ratio Convergence Theorem will become clear in a\nmoment.\n\nWe are now in a position to state the second part of the\nLikelihood Ratio Convergence Theorem. It applies to all\nevidence streams not containing possibly falsifying outcomes\nfor \\(h_j\\) when \\(h_i\\) holds\u2014i.e., it applies to all evidence\nstreams for which \\(h_j\\) is fully outcome-compatible with\n\\(h_i\\) on each \\(c_k\\) in the stream.\n\n\nLikelihood Ratio Convergence Theorem 2\u2014The Probabilistic\nRefutation Theorem.\n\nSuppose the evidence stream \\(c^n\\) contains only experiments or\nobservations on which \\(h_j\\) is fully outcome-compatible\nwith \\(h_i\\)\u2014i.e., suppose that for each condition \\(c_k\\) in\nsequence \\(c^n\\), for each of its possible outcomes possible outcomes\n\\(o_{ku}\\), either \\(P[o_{ku} \\pmid h_{i}\\cdot b\\cdot c_{k}] = 0\\) or\n\\(P[o_{ku} \\pmid h_{j}\\cdot b\\cdot c_{k}] \\gt 0\\). In addition (as a\nslight strengthening of the previous supposition), for some \\(\\gamma\n\\gt 0\\) a number smaller than \\(1/e^2\\) (\\(\\approx .135\\); where\ne\u2019 is the base of the natural logarithm), suppose that\nfor each possible outcome \\(o_{ku}\\) of each observation condition\n\\(c_k\\) in \\(c^n\\), either \\(P[o_{ku} \\pmid h_{i}\\cdot b\\cdot c_{k}] =\n0\\) or  \n\n\\[\\frac{P[o_{ku} \\pmid  h_{j}\\cdot b\\cdot c_{k}]}{P[o_{ku} \\pmid  h_{i}\\cdot b\\cdot c_{k}]} \\ge \\gamma.\\]\n\n\nAnd suppose that the Independent Evidence Conditions hold for\nevidence stream \\(c^n\\) with respect to each of these hypotheses. Now,\nchoose any positive \\(\\varepsilon \\lt 1\\), as small as you like, but\nlarge enough (for the number of observations n being\ncontemplated) that the value of  \n\n  \\[\\bEQI[c^n \\pmid  h_i /h_j \\pmid  b]  \\gt -\\frac{(\\log \\varepsilon)}{n}.\\]\n\n\nThen: \n\n\\[\n\\begin{multline}\nP\\left[\\vee \\left\\{ e^n : \\frac{P[e^n \\pmid  h_{j}\\cdot b\\cdot c^{n}]}{P[e^n \\pmid  h_{i}\\cdot b\\cdot c^{n}]} \\lt \\varepsilon \\right\\} \\pmid   h_{i}\\cdot b\\cdot c^{n}\\right]\\\\[2ex]\n\\gt 1 - \\frac{1}{n} \\times  \n\\frac{(\\log \\gamma)^2}\n{(\\bEQI[c^n \\pmid  h_i /h_j \\pmid  b] + (\\log \\varepsilon)/n)^2}\n\\end{multline}\n\\]\n\n\nFor \\(\\varepsilon = 1/2^m\\) and \\(\\gamma = 1/2^q\\), this formula\nbecomes, \n\n\\[\n\\begin{multline}\nP\\left[\\vee \\left\\{ e^n : \\frac{P[e^n \\pmid  h_{j}\\cdot b\\cdot c^{n}]}{P[e^n \\pmid  h_{i}\\cdot b\\cdot c^{n}]} \\lt 1/2^m\\right\\} \\pmid   h_{i}\\cdot b\\cdot c^{n}\\right]\\\\\n  \\gt 1  - \\frac{1}{n}\n  \\times\n\\frac{q^2}\n{(\\bEQI[c^n \\pmid  h_i /h_j \\pmid  b] - (m/n) )^2}\n  \\end{multline}\n\\]\n\n\n(For proof see the supplement\n Proof of the Probabilistic Refutation Theorem.)\n \n\n\nThis theorem provides sufficient conditions for the likely\nrefutation of false alternatives via exceeding small likelihood\nratios. The conditions under which this happens characterize the\ndegree to which the hypotheses involved are empirically distinct from\none another. The theorem says that when these conditions are met,\naccording to hypothesis \\(h_i\\) (taken together with \\(b\\cdot c^n)\\),\nthe likelihood is near 1 that that one of the outcome sequence \\(e^n\\)\nwill occur for which the likelihood ratio is smaller than\n\\(\\varepsilon\\) (for any value of \\(\\varepsilon\\) you may choose). The\nlikelihood of getting such an evidential outcome \\(e^n\\) is quite\nclose to 1\u2014i.e., no more than the amount  \n\n\\[\\frac{1}{n} \\times \\frac{(\\log \\gamma)^2}{\\left(\\bEQI[c^n \\pmid  h_i /h_j \\pmid  b] + \\frac{(\\log \\varepsilon)}{n}\\right)^2}\\] \n\n\nbelow 1. (Notice that this amount below 1 goes to 0 as n\nincreases.) \n\nIt turns out that in almost every case (for almost any pair of\nhypotheses) the actual likelihood of obtaining such evidence (i.e.,\nevidence that has a likelihood ratio value less than \\(\\varepsilon)\\)\nwill be much closer to 1 than this factor\n indicates.[16]\n Thus, the theorem provides an overly cautious lower bound on the\nlikelihood of obtaining small likelihood ratios. It shows that the\nlarger the value of \\(\\bEQI\\) for an evidence stream, the more likely\nthat stream is to produce a sequence of outcomes that yield a very\nsmall likelihood ratio value. But even if \\(\\bEQI\\) remains quite\nsmall, a long enough evidence stream, n, of such low-grade\nevidence will, nevertheless, almost surely produce an outcome sequence\nhaving a very small likelihood ratio\n value.[17]\n\n\nNotice that the antecedent condition of the theorem, that\n\u201ceither \n\n\\[P[o_{ku} \\pmid  h_{i}\\cdot b\\cdot c_{k}] = 0\\] \n\n\nor \n\n\\[\\frac{P[o_{ku} \\pmid  h_{j}\\cdot b\\cdot c_{k}]}{P[o_{ku} \\pmid  h_{i}\\cdot b\\cdot c_{k}]}  \\ge \\gamma,\\]\n\n\nfor some \\(\\gamma \\gt 0\\) but less than \\(1/e^2\\) (\\(\\approx\n.135\\))\u201d, does not favor hypothesis \\(h_i\\) over \\(h_j\\) in any\nway. The condition only rules out the possibility that some outcomes\nmight furnish extremely strong evidence against\n\\(h_j\\) relative to \\(h_i\\)\u2014by making \\(P[o_{ku} \\pmid\nh_{i}\\cdot b\\cdot c_{k}] = 0\\) or by making  \n\n\\[\\frac{P[o_{ku} \\pmid  h_{j}\\cdot b\\cdot c_{k}] }{P[o_{ku} \\pmid  h_{i}\\cdot b\\cdot c_{k}]}\\] \n\n\nless than some quite small \\(\\gamma\\). This condition is only needed\nbecause our measure of evidential distinguishability, QI, blows up\nwhen the ratio  \n\n\\[\\frac{P[o_{ku} \\pmid  h_{j}\\cdot b\\cdot c_{k}]}{P[o_{ku} \\pmid  h_{i}\\cdot b\\cdot c_{k}]}\\]\n\n\nis extremely small. Furthermore, this condition is really no\nrestriction at all on possible experiments or observations. If \\(c_k\\)\nhas some possible outcome sentence \\(o_{ku}\\) that would make \n\n\\[\\frac{P[o_{ku} \\pmid  h_{j}\\cdot b\\cdot c_{k}]}{P[o_{ku} \\pmid  h_{i}\\cdot b\\cdot c_{k}]}  \\lt \\gamma\\] \n\n\n(for a given small \\(\\gamma\\) of interest), one may disjunctively lump\n\\(o_{ku}\\) together with some other outcome sentence \\(o_{kv}\\) for\n\\(c_k\\). Then, the antecedent condition of the theorem will be\nsatisfied, but with the sentence \u2018\\((o_{ku} \\vee\no_{kv})\\)\u2019 treated as a single outcome. It can be proved that\nthe only effect of such \u201cdisjunctive lumping\u201d is to make\n\\(\\bEQI\\) smaller than it would otherwise be (whereas larger values of\n\\(\\bEQI\\) are more desirable). If the too strongly refuting\ndisjunct \\(o_{ku}\\) actually occurs when the experiment or observation\n\\(c_k\\) is conducted, all the better, since this results in a\nlikelihood ratio  \n\n\\[\\frac{P[o_{ku} \\pmid  h_{j}\\cdot b\\cdot c_{k}]}{P[o_{ku} \\pmid  h_{i}\\cdot b\\cdot c_{k}]}\\]\n\n\nsmaller than \\(\\gamma\\) on that particular evidential outcome. We\nmerely failed to take this more strongly refuting possibility\ninto account when computing our lower bound on the likelihood that\nrefutation via likelihood ratios would occur.\n\nThe point of the two Convergence Theorems explored in this\nsection is to assure us, in advance of the consideration of any\nspecific pair of hypotheses, that if the possible evidence streams\nthat test them have certain characteristics which reflect their\nevidential distinguishability, it is highly likely that outcomes\nyielding small likelihood ratios will result. These theorems provide\nfinite lower bounds on how quickly convergence is likely to occur.\nThus, there is no need to wait through some infinitely long run for\nconvergence to occur. Indeed, for any evidence sequence on which the\nprobability distributions are at all well behaved, the actual\nlikelihood of obtaining outcomes that yield small likelihood\nratio values will inevitably be much higher than the lower\nbounds given by Theorems 1 and 2.\n\nIn sum, according to Theorems 1 and 2, each hypothesis \\(h_i\\)\nsays, via likelihoods, that given enough observations,\nit is very likely to dominate its empirically distinct rivals\nin a contest of likelihood ratios. The true hypothesis speaks\ntruthfully about this, and its competitors lie. Even a sequence of\nobservations with an extremely low average expected quality of\ninformation is very likely to do the job if that evidential\nsequence is long enough. Thus (by\n Equation 9*),\n as evidence accumulates, the degree of support for false\nhypotheses will very probably approach 0, indicating that they are\nprobably false; and as this happens, (by Equations 10 and 11) the\ndegree of support for the true hypothesis will approach 1, indicating\nits probable truth. Thus, the Criterion of Adequacy\n(CoA) is satisfied.\n5. When the Likelihoods are Vague or Diverse\n\nUp to this point we have been supposing that likelihoods possess\nobjective or agreed numerical values. Although this supposition is\noften satisfied in scientific contexts, there are important settings\nwhere it is unrealistic, where hypotheses only support vague\nlikelihood values, and where there is enough ambiguity in what\nhypotheses say about evidential claims that the scientific\ncommunity cannot agree on precise values for the likelihoods of\nevidential\n claims.[18]\n Let us now see how the supposition of precise, agreed likelihood\nvalues may be relaxed in a reasonable way.\n\nRecall why agreement, or near agreement, on precise values for\nlikelihoods is so important to the scientific enterprise. To the\nextent that members of a scientific community disagree on the\nlikelihoods, they disagree about the empirical content of their\nhypotheses, about what each hypothesis says about how the\nworld is likely to be. This can lead to disagreement about which\nhypotheses are refuted or supported by a given body of evidence.\nSimilarly, to the extent that the values of likelihoods are only\nvaguely implied by hypotheses as understood by an individual agent,\nthat agent may be unable to determine which of several hypotheses is\nrefuted or supported by a given body of evidence.\n\nWe have seen, however, that the individual values of likelihoods are\nnot really crucial to the way evidence impacts hypotheses. Rather, as\nEquations 9\u201311 show, it is ratios of likelihoods that\ndo the heavy lifting. So, even if two support functions \\(P_{\\alpha}\\)\nand \\(P_{\\beta}\\) disagree on the values of individual likelihoods,\nthey may, nevertheless, largely agree on the refutation or support\nthat accrues to various rival hypotheses, provided that the following\ncondition is satisfied: \n\nDirectional Agreement Condition:\n\nThe likelihood ratios due to each of a pair of support functions\n\\(P_{\\alpha}\\) and \\(P_{\\beta}\\) are said to agree in\ndirection (with respect to the possible outcomes of experiments\nor observations relevant to a pair of hypotheses) just in\ncase\n\nwhenever possible outcome sequence \\(e^n\\) makes \n\n\\[\\frac{P_{\\alpha}[e^n \\pmid  h_{j}\\cdot b\\cdot c^{n}]}{P_{\\alpha}[e^n \\pmid  h_{i}\\cdot b\\cdot c^{n}]}  \\lt 1,\\]\n\n it\nalso makes \n\n\\[\\frac{P_{\\beta}[e^n \\pmid  h_{j}\\cdot b\\cdot c^{n}]}{P_{\\beta}[e^n \\pmid  h_{i}\\cdot b\\cdot c^{n}]}  \\lt 1;\\] \nwhenever possible outcome sequence \\(e^n\\) makes \n\n\\[\\frac{P_{\\alpha}[e^n \\pmid  h_{j}\\cdot b\\cdot c^{n}]}{P_{\\alpha}[e^n \\pmid  h_{i}\\cdot b\\cdot c^{n}]}  \\gt 1,\\]\n\n it\nalso makes \n\n\\[\\frac{P_{\\beta}[e^n \\pmid  h_{j}\\cdot b\\cdot c^{n}]}{P_{\\beta}[e^n \\pmid  h_{i}\\cdot b\\cdot c^{n}]}  \\gt 1;\\] \neach of these likelihood ratios is either close to 1 for both of\nthese support functions, or is quite far from 1 for both of\n them.[19]\n\n\n\n\nWhen this condition holds, the evidence will support \\(h_i\\) over\n\\(h_j\\) according to \\(P_{\\alpha}\\) just in case it does so for\n\\(P_{\\beta}\\) as well, although the strength of support may differ.\nFurthermore, although the rate at which the likelihood ratios\nincrease or decrease on a stream of evidence may differ for the two\nsupport functions, the impact of the cumulative evidence should\nultimately affect their refutation or support in much the same way.\n\n\nWhen likelihoods are vague or diverse, we may take an approach similar\nto that we employed for vague and diverse prior\nplausibility assessments. We may extend the vagueness sets\nfor individual agents to include a collection of inductive support\nfunctions that cover the range of values for likelihood ratios of\nevidence claims (as well as cover the ranges of comparative support\nstrengths for hypotheses due to plausibility arguments within\nb, as represented by ratios of prior probabilities). Similarly,\nwe may extend the diversity sets for communities of agents to\ninclude support functions that cover the ranges of likelihood ratio\nvalues that arise within the vagueness sets of members of the\nscientific community.\n\nThis broadening of vagueness and diversity sets to\naccommodate vague and diverse likelihood values makes no trouble for\nthe convergence to truth results for hypotheses. For,\nprovided that the Directional Agreement Condition is\nsatisfied by all support functions in an extended vagueness\nor diversity set under consideration, the Likelihood\nRatio Convergence Theorem applies to each individual support\nfunction in that set. For, the the proof of that convergence theorem\ndoesn\u2019t depend on the supposition that likelihoods are objective\nor have intersubjectively agreed values. Rather, it applies to each\nindividual support function \\(P_{\\alpha}\\). The only possible problem\nwith applying this result across a range of support functions is that\nwhen their values for likelihoods differ, function \\(P_{\\alpha}\\) may\ndisagree with \\(P_{\\beta}\\) on which of the hypotheses is favored by a\ngiven sequence of evidence. That can happen because different support\nfunctions may represent the evidential import of hypotheses\ndifferently, by specifying different likelihood values for the very\nsame evidence claims. So, an evidence stream that favors \\(h_i\\)\naccording to \\(P_{\\alpha}\\) may instead favor \\(h_j\\) according to\n\\(P_{\\beta}\\). However, when the Directional Agreement\nCondition holds for a given collection of support functions, this\nproblem cannot arise. Directional Agreement means that the\nevidential import of hypotheses is similar enough for \\(P_{\\alpha}\\)\nand \\(P_{\\beta}\\) that a sequence of outcomes may favor a hypothesis\naccording to \\(P_{\\alpha}\\) only if it does so for \\(P_{\\beta}\\) as\nwell.\n\nThus, when the Directional Agreement Condition holds for all\nsupport functions in a vagueness or diversity set\nthat is extended to include vague or diverse likelihoods, and provided\nthat enough evidentially distinguishing experiments or observations\ncan be performed, all support functions in the extended\nvagueness or diversity set will very probably come\nto agree that the likelihood ratios for empirically distinct false\ncompetitors of a true hypothesis are extremely small. As that happens,\nthe community comes to agree on the refutation of these competitors,\nand the true hypothesis rises to the top of the\n heap.[20]\n\n\nWhat if the true hypothesis has evidentially equivalent rivals? Their\nposterior probabilities must rise as well. In that case we are only\nassured that the disjunction of the true hypothesis with its\nevidentially equivalent rivals will be driven to 1 as evidence lays\nlow its evidentially distinct rivals. The true hypothesis will itself\napproach 1 only if either it has no evidentially equivalent rivals, or\nwhatever equivalent rivals it does have can be laid low by\nplausibility arguments of a kind that don\u2019t depend on the\nevidential likelihoods, but only show up via the comparative\nplausibility assessments represented by ratios of prior\nprobabilities.\n",
    "bibliography": {
        "categories": [],
        "cat_ref_text": {
            "ref_list": [
                "Boole, George, 1854, <em>The Laws of Thought</em>, London:\nMacMillan. Republished in 1958 by Dover: New York.",
                "Bovens, Luc and Stephan Hartmann, 2003, <em>Bayesian\nEpistemology</em>, Oxford: Oxford University Press.\ndoi:10.1093/0199269750.001.0001",
                "Carnap, Rudolf, 1950, <em>Logical Foundations of Probability</em>,\nChicago: University of Chicago Press.",
                "\u2013\u2013\u2013, 1952, <em>The Continuum of Inductive\nMethods</em>, Chicago: University of Chicago Press.",
                "\u2013\u2013\u2013, 1963, \u201cReplies and Systematic\nExpositions\u201d, in <em>The Philosophy of Rudolf Carnap</em>, Paul\nArthur Schilpp (ed.),La Salle, IL: Open Court.",
                "Chihara, Charles S., 1987, \u201cSome Problems for Bayesian\nConfirmation Theory\u201d, <em>British Journal for the Philosophy of\nScience</em>, 38(4): 551\u2013560. doi:10.1093/bjps/38.4.551",
                "Christensen, David, 1999, \u201cMeasuring Confirmation\u201d,\n<em>Journal of Philosophy</em>, 96(9): 437\u201361.\ndoi:10.2307/2564707",
                "\u2013\u2013\u2013, 2004, <em>Putting Logic in its Place:\nFormal Constraints on Rational Belief</em>, Oxford: Oxford University\nPress. doi:10.1093/0199263256.001.0001",
                "De Finetti, Bruno, 1937, \u201cLa Pr\u00e9vision: Ses Lois\nLogiques, Ses Sources Subjectives\u201d, <em>Annales de\nl\u2019Institut Henri Poincar\u00e9</em>, 7: 1\u201368; translated\nby Henry E. Kyburg, Jr. as \u201cForesight. Its Logical Laws, Its\nSubjective Sources\u201d, in <em>Studies in Subjective\nProbability</em>, Henry E. Kyburg, Jr. and H.E. Smokler (eds.), Robert\nE. Krieger Publishing Company, 1980.",
                "Dowe, David L., Steve Gardner, and Graham Oppy, 2007,\n\u201cBayes, Not Bust! Why Simplicity is No Problem for\nBayesians\u201d, <em>British Journal for the Philosophy of\nScience</em>, 58(4): 709\u2013754. doi:10.1093/bjps/axm033",
                "Dubois, Didier J. and Henri Prade, 1980, <em>Fuzzy Sets and\nSystems</em>, (Mathematics in Science and Engineering, 144), New York:\nAcademic Press.",
                "\u2013\u2013\u2013, 1990, \u201cAn Introduction to\nPossibilistic and Fuzzy Logics\u201d, in Glenn Shafer and Judea Pearl\n(eds.), <em>Readings in Uncertain Reasoning</em>, San Mateo, CA:\nMorgan Kaufmann, 742\u2013761..",
                "Duhem, P., 1906, <em>La theorie physique. Son objet et sa\nstructure</em>, Paris: Chevalier et Riviere; translated by P.P.\nWiener, <em>The Aim and Structure of Physical Theory</em>, Princeton,\nNJ: Princeton University Press, 1954.",
                "Earman, John, 1992, <em>Bayes or Bust? A Critical Examination of\nBayesian Confirmation Theory</em>, Cambridge, MA: MIT Press.",
                "Edwards, A.W.F., 1972, <em>Likelihood: an account of the\nstatistical concept of likelihood and its application to scientific\ninference</em>, Cambridge: Cambridge University Press.",
                "Edwards, Ward, Harold Lindman, and Leonard J. Savage, 1963,\n\u201cBayesian Statistical Inference for Psychological\nResearch\u201d, <em>Psychological Review</em>, 70(3): 193\u2013242.\ndoi:10.1037/h0044139",
                "Eells, Ellery, 1985, \u201cProblems of Old Evidence\u201d,\n<em>Pacific Philosophical Quarterly</em>, 66(3\u20134):\n283\u2013302. doi:10.1111/j.1468-0114.1985.tb00254.x",
                "\u2013\u2013\u2013, 2006, \u201cConfirmation Theory\u201d,\nSarkar and Pfeifer 2006..",
                "Eells, Ellery and Branden Fitelson, 2000, \u201cMeasuring\nConfirmation and Evidence\u201d, <em>Journal of Philosophy</em>,\n97(12): 663\u2013672. doi:10.2307/2678462",
                "Field, Hartry H., 1977, \u201cLogic, Meaning, and Conceptual\nRole\u201d, <em>Journal of Philosophy</em>, 74(7): 379\u2013409.\ndoi:10.2307/2025580",
                "Fisher, R.A., 1922, \u201cOn the Mathematical Foundations of\nTheoretical Statistics\u201d, <em>Philosophical Transactions of the\nRoyal Society, series A </em>, 222(594\u2013604): 309\u2013368.\ndoi:10.1098/rsta.1922.0009 ",
                "Fitelson, Branden, 1999, \u201cThe Plurality of Bayesian Measures\nof Confirmation and the Problem of Measure Sensitivity\u201d,\n<em>Philosophy of Science</em>, 66: S362\u2013S378.\ndoi:10.1086/392738",
                "\u2013\u2013\u2013, 2001, \u201cA Bayesian Account of\nIndependent Evidence with Applications\u201d, <em>Philosophy of\nScience</em>, 68(S3): S123\u2013S140. doi:10.1086/392903",
                "\u2013\u2013\u2013, 2002, \u201cPutting the Irrelevance Back\nInto the Problem of Irrelevant Conjunction\u201d, <em>Philosophy of\nScience</em>, 69(4): 611\u2013622. doi:10.1086/344624",
                "\u2013\u2013\u2013, 2006, \u201cInductive Logic\u201d, Sarkar\nand Pfeifer 2006..",
                "\u2013\u2013\u2013, 2006, \u201cLogical Foundations of\nEvidential Support\u201d, <em>Philosophy of Science</em>, 73(5):\n500\u2013512. doi:10.1086/518320",
                "\u2013\u2013\u2013, 2007, \u201cLikelihoodism, Bayesianism,\nand Relational Confirmation\u201d, <em>Synthese</em>, 156(3):\n473\u2013489. doi:10.1007/s11229-006-9134-9",
                "Fitelson, Branden and James Hawthorne, 2010, \u201cHow Bayesian\nConfirmation Theory Handles the Paradox of the Ravens\u201d, in Eells\nand Fetzer (eds.), <em>The Place of Probability in Science</em>, Open\nCourt.\n [<a href=\"http://fitelson.org/ravens.pdf\" target=\"other\">Fitelson &amp; Hawthorne 2010 preprint available from the author (PDF)</a>]",
                "Forster, Malcolm and Elliott Sober, 2004, \u201cWhy\nLikelihood\u201d, in Mark L. Taper and Subhash R. Lele (eds.),\n<em>The Nature of Scientific Evidence</em>, Chicago: University of\nChicago Press.",
                "Friedman, Nir and Joseph Y. Halpern, 1995, \u201cPlausibility\nMeasures: A User\u2019s Guide\u201d, in <em>UAI 95: Proceedings of\nthe Eleventh Conference on Uncertainty in Artificial\nIntelligence</em>, 175\u2013184.",
                "Gaifman, Haim and Marc Snir, 1982, \u201cProbabilities Over Rich\nLanguages, Testing and Randomness\u201d, <em>Journal of Symbolic\nLogic</em>, 47(3): 495\u2013548. doi:10.2307/2273587 ",
                "Gillies, Donald, 2000, <em>Philosophical Theories of\nProbability</em>, London: Routledge.",
                "Glymour, Clark N., 1980, <em>Theory and Evidence</em>, Princeton,\nNJ: Princeton University Press.",
                "Goodman, Nelson, 1983, <em>Fact, Fiction, and Forecast</em>,\n4<sup>th</sup> edition, Cambridge, MA: Harvard University Press.",
                "Hacking, Ian, 1965, <em>Logic of Statistical Inference</em>,\nCambridge: Cambridge University Press.",
                "\u2013\u2013\u2013, 1975, <em>The Emergence of Probability: a\nPhilosophical Study of Early Ideas about Probability, Induction and\nStatistical Inference</em>, Cambridge: Cambridge University Press.\ndoi:10.1017/CBO9780511817557",
                "\u2013\u2013\u2013, 2001, <em>An Introduction to Probability\nand Inductive Logic</em>, Cambridge: Cambridge University Press.\ndoi:10.1017/CBO9780511801297",
                "H\u00e1jek, Alan, 2003a, \u201cWhat Conditional Probability\nCould Not Be\u201d, <em>Synthese</em>, 137(3):, 273\u2013323.\ndoi:10.1023/B:SYNT.0000004904.91112.16",
                "\u2013\u2013\u2013, 2003b, \u201cInterpretations of the\nProbability Calculus\u201d, in the <em>Stanford Encyclopedia of\nPhilosophy</em>, (Summer 2003 Edition), Edward N. Zalta (ed.), URL =\n &lt;<a href=\"https://plato.stanford.edu/archives/sum2003/entries/probability-interpret/\" target=\"other\">https://plato.stanford.edu/archives/sum2003/entries/probability-interpret/</a>&gt;",
                "\u2013\u2013\u2013, 2005, \u201cScotching Dutch Books?\u201d\n<em>Philosophical Perspectives</em>, 19 (Epistemology): 139\u2013151.\ndoi:10.1111/j.1520-8583.2005.00057.x",
                "\u2013\u2013\u2013, 2007, \u201cThe Reference Class Problem is\nYour Problem Too\u201d, <em>Synthese</em>, 156(3): 563\u2013585.\ndoi:10.1007/s11229-006-9138-5",
                "Halpern, Joseph Y., 2003, <em>Reasoning About Uncertainty</em>,\nCambridge, MA: MIT Press.",
                "Harper, William L., 1976, \u201cRational Belief Change, Popper\nFunctions and Counterfactuals\u201d, in Harper and Hooker 1976:\n73\u2013115. doi:10.1007/978-94-010-1853-1_5",
                "Harper, William L. and Clifford Alan Hooker (eds.), 1976,\n<em>Foundations of Probability Theory, Statistical Inference, and\nStatistical Theories of Science, volume I Foundations and Philosophy\nof Epistemic Applications of Probability Theory</em>, (The Western\nOntario Series in Philosophy of Science, 6a), Dordrecht: Reidel.\ndoi:10.1007/978-94-010-1853-1",
                "Hawthorne, James, 1993, \u201cBayesian Induction <em>is</em>\nEliminative Induction\u201d, <em>Philosophical Topics</em>, 21(1):\n99\u2013138. doi:10.5840/philtopics19932117",
                "\u2013\u2013\u2013, 1994,\u201cOn the Nature of Bayesian\nConvergence\u201d, <em>PSA: Proceedings of the Biennial Meeting of\nthe Philosophy of Science Association 1994</em>, 1: 241\u2013249.\ndoi:10.1086/psaprocbienmeetp.1994.1.193029",
                "\u2013\u2013\u2013, 2005, \u201c<em>Degree-of-Belief</em> and\n<em>Degree-of-Support</em>: Why Bayesians Need Both Notions\u201d,\n<em>Mind</em>, 114(454): 277\u2013320. doi:10.1093/mind/fzi277",
                "\u2013\u2013\u2013, 2009, \u201cThe Lockean Thesis and the\nLogic of Belief\u201d, in Franz Huber and Christoph Schmidt-Petri\n(eds.), <em>Degrees of Belief</em>, (Synthese Library, 342),\nDordrecht: Springer, pp. 49\u201374.\ndoi:10.1007/978-1-4020-9198-8_3",
                "Hawthorne, James and Luc Bovens, 1999, \u201cThe Preface, the\nLottery, and the Logic of Belief\u201d, <em>Mind</em>, 108(430):\n241\u2013264. doi:10.1093/mind/108.430.241",
                "Hawthorne, James and Branden Fitelson, 2004, \u201cDiscussion:\nRe-solving Irrelevant Conjunction With Probabilistic\nIndependence\u201d, <em>Philosophy of Science</em>, 71(4):\n505\u2013514. doi:10.1086/423626",
                "Hellman, Geoffrey, 1997, \u201cBayes and Beyond\u201d,\n<em>Philosophy of Science</em>, 64(2): 191\u2013221.\ndoi:10.1086/392548",
                "Hempel, Carl G., 1945, \u201cStudies in the Logic of\nConfirmation\u201d, <em>Mind</em>, 54(213): 1\u201326,\n54(214):97\u2013121. doi:10.1093/mind/LIV.213.1\ndoi:10.1093/mind/LIV.214.97",
                "Horwich, Paul, 1982, <em>Probability and Evidence</em>, Cambridge:\nCambridge University Press. doi:10.1017/CBO9781316494219",
                "Howson, Colin, 1997, \u201cA Logic of Induction\u201d,\n<em>Philosophy of Science</em>, 64(2): 268\u2013290.\ndoi:10.1086/392551",
                "\u2013\u2013\u2013, 2000, <em>Hume\u2019s Problem: Induction\nand the Justification of Belief</em>, Oxford: Oxford University Press.\ndoi:10.1093/0198250371.001.0001",
                "\u2013\u2013\u2013, 2002, \u201cBayesianism in\nStatistics\u201c, in Swinburne 2002: 39\u201371.\ndoi:10.5871/bacad/9780197263419.003.0003",
                "\u2013\u2013\u2013, 2007, \u201cLogic With Numbers\u201d,\n<em>Synthese</em>, 156(3): 491\u2013512.\ndoi:10.1007/s11229-006-9135-8",
                "Howson, Colin and Peter Urbach, 1993, <em>Scientific Reasoning:\nThe Bayesian Approach</em>, La Salle, IL: Open\nCourt. [3rd edition, 2005.]",
                "Huber, Franz, 2005a, \u201cSubjective Probabilities as Basis for\nScientific Reasoning?\u201d <em>British Journal for the Philosophy of\nScience</em>, 56(1): 101\u2013116. doi:10.1093/phisci/axi105",
                "\u2013\u2013\u2013, 2005b, \u201cWhat Is the Point of\nConfirmation?\u201d <em>Philosophy of Science</em>, 72(5):\n1146\u20131159. doi:10.1086/508961",
                "Jaynes, Edwin T., 1968, \u201cPrior Probabilities\u201d,\n<em>IEEE Transactions on Systems Science and Cybernetics</em>,\nSSC\u20134(3): 227\u2013241. doi:10.1109/TSSC.1968.300117",
                "Jeffrey, Richard C., 1983, <em>The Logic of Decision</em>, 2nd\nedition, Chicago: University of Chicago Press.",
                "\u2013\u2013\u2013, 1987, \u201cAlias Smith and Jones: The\nTestimony of the Senses\u201d, <em>Erkenntnis</em>, 26(3):\n391\u2013399. doi:10.1007/BF00167725",
                "\u2013\u2013\u2013, 1992, <em>Probability and the Art of\nJudgment</em>, New York: Cambridge University Press.\ndoi:10.1017/CBO9781139172394",
                "\u2013\u2013\u2013, 2004, <em>Subjective Probability: The Real\nThing</em>, Cambridge: Cambridge University Press.\ndoi:10.1017/CBO9780511816161",
                "Jeffreys, Harold, 1939, <em>Theory of Probability</em>, Oxford:\nOxford University Press. ",
                "Joyce, James M., 1998, \u201cA Nonpragmatic Vindication of\nProbabilism\u201d, <em>Philosophy of Science</em>, 65(4):\n\n575\u2013603. doi:10.1086/392661",
                "\u2013\u2013\u2013, 1999, <em>The Foundations of Causal\nDecision Theory</em>, New York: Cambridge University Press.\ndoi:10.1017/CBO9780511498497",
                "\u2013\u2013\u2013, 2003, \u201cBayes\u2019 Theorem\u201d,\nin the <em>Stanford Encyclopedia of Philosophy</em>, (Summer 2003\nEdition), Edward N. Zalta (ed.), URL =\n &lt;<a href=\"https://plato.stanford.edu/archives/win2003/entries/bayes-theorem/\" target=\"other\">https://plato.stanford.edu/archives/win2003/entries/bayes-theorem/</a>&gt;",
                "\u2013\u2013\u2013, 2004, \u201cBayesianism\u201d, in Alfred\nR. Mele and Piers Rawling (eds.), <em>The Oxford Handbook of\nRationality</em>, Oxford: Oxford University Press, pp. 132\u2013153.\ndoi:10.1093/0195145399.003.0008",
                "\u2013\u2013\u2013, 2005, \u201cHow Probabilities Reflect\nEvidence\u201d, <em>Philosophical Perspectives</em>, 19:\n153\u2013179. doi:10.1111/j.1520-8583.2005.00058.x",
                "Kaplan, Mark, 1996, <em>Decision Theory as Philosophy</em>,\nCambridge: Cambridge University Press.",
                "Kelly, Kevin T., Oliver Schulte, and Cory Juhl, 1997,\n\u201cLearning Theory and the Philosophy of Science\u201d,\n<em>Philosophy of Science</em>, 64(2): 245\u2013267.\ndoi:10.1086/392550",
                "Keynes, John Maynard, 1921, <em>A Treatise on Probability</em>,\nLondon: Macmillan and Co.",
                "Kolmogorov, A.N., 1956, <em>Foundations of the Theory of\nProbability</em> (<em>Grundbegriffe der\nWahrscheinlichkeitsrechnung</em>, 2<sup>nd</sup> edition, New York:\nChelsea Publishing Company.",
                "Koopman, B.O., 1940, \u201cThe Bases of Probability\u201d,\n<em>Bulletin of the American Mathematical Society</em>, 46(10):\n763\u2013774. Reprinted in H. Kyburg and H. Smokler (eds.), 1980,\n<em>Studies in Subjective Probability</em>, 2nd edition, Huntington,\nNY: Krieger Publ. Co.\n [<a href=\"https://projecteuclid.org/euclid.bams/1183503229\" target=\"other\">Koopman 1940 available online</a>]",
                "Kyburg, Henry E., Jr., 1974, <em>The Logical Foundations of\nStatistical Inference</em>, Dordrecht: Reidel.\ndoi:10.1007/978-94-010-2175-3",
                "\u2013\u2013\u2013, 1977, \u201cRandomness and the Right\nReference Class\u201d, <em>Journal of Philosophy</em>, 74(9):\n501\u2013520. doi:10.2307/2025794",
                "\u2013\u2013\u2013, 1978, \u201cAn Interpolation Theorem for\nInductive Relations\u201d, <em>Journal of Philosophy</em>,\n75:93\u201398.",
                "\u2013\u2013\u2013, 2006, \u201cBelief, Evidence, and\nConditioning\u201d, <em>Philosophy of Science</em>, 73(1):\n42\u201365. doi:10.1086/510174",
                "Lange, Marc, 1999, \u201cCalibration and the Epistemological Role\nof Bayesian Conditionalization\u201d, <em>Journal of Philosophy</em>,\n96(6): 294\u2013324. doi:10.2307/2564680",
                "\u2013\u2013\u2013, 2002, \u201cOkasha on Inductive\nScepticism\u201d, <em>The Philosophical Quarterly</em>, 52(207):\n226\u2013232. doi:10.1111/1467-9213.00264",
                "Laudan, Larry, 1997, \u201cHow About Bust? Factoring Explanatory\nPower Back into Theory Evaluation\u201d, <em>Philosophy of\nScience</em>, 64(2): 206\u2013216. doi:10.1086/392553",
                "Lenhard Johannes, 2006, \u201cModels and Statistical Inference:\nThe Controversy Between Fisher and Neyman-Pearson\u201d, <em>British\nJournal for the Philosophy of Science</em>, 57(1): 69\u201391.\ndoi:10.1093/bjps/axi152",
                "Levi, Isaac, 1967, <em>Gambling with Truth: An Essay on Induction\nand the Aims of Science</em>, New York: Knopf. ",
                "\u2013\u2013\u2013, 1977, \u201cDirect Inference\u201d,\n<em>Journal of Philosophy</em>, 74(1): 5\u201329.\ndoi:10.2307/2025732",
                "\u2013\u2013\u2013, 1978, \u201cConfirmational\nConditionalization\u201d, <em>Journal of Philosophy</em>, 75(12):\n730\u2013737. doi:10.2307/2025516",
                "\u2013\u2013\u2013, 1980, <em>The Enterprise of Knowledge: An\nEssay on Knowledge, Credal Probability, and Chance</em>, Cambridge,\nMA: MIT Press.",
                "Lewis, David, 1980, \u201cA Subjectivist\u2019s Guide to\nObjective Chance\u201d, in Richard C. Jeffrey, (ed.), <em>Studies in\nInductive Logic and Probability</em>, vol. 2, Berkeley: University of\nCalifornia Press, 263\u2013293.",
                "Maher, Patrick, 1993, <em>Betting on Theories</em>, Cambridge:\nCambridge University Press.",
                "\u2013\u2013\u2013, 1996, \u201cSubjective and Objective\nConfirmation\u201d, <em>Philosophy of Science</em>, 63(2):\n149\u2013174. doi:10.1086/289906",
                "\u2013\u2013\u2013, 1997, \u201cDepragmatized Dutch Book\nArguments\u201d, <em>Philosophy of Science</em>, 64(2):\n291\u2013305. doi:10.1086/392552",
                "\u2013\u2013\u2013, 1999, \u201cInductive Logic and the Ravens\nParadox\u201d, <em>Philosophy of Science</em>, 66(1): 50\u201370.\ndoi:10.1086/392676",
                "\u2013\u2013\u2013, 2004, \u201cProbability Captures the Logic\nof Scientific Confirmation\u201d, in Christopher Hitchcock (ed.),\n<em>Contemporary Debates in Philosophy of Science</em>, Oxford:\nBlackwell, 69\u201393.",
                "\u2013\u2013\u2013, 2005, \u201cConfirmation Theory\u201d,\n<em>The Encyclopedia of Philosophy</em>, 2nd edition, Donald M.\nBorchert (ed.), Detroit: Macmillan.",
                "\u2013\u2013\u2013, 2006a, \u201cThe Concept of Inductive\nProbability\u201d, <em>Erkenntnis</em>, 65(2): 185\u2013206.\ndoi:10.1007/s10670-005-5087-5",
                "\u2013\u2013\u2013, 2006b, \u201cA Conception of Inductive\nLogic\u201d, <em>Philosophy of Science</em>, 73(5): 513\u2013523.\ndoi:10.1086/518321",
                "\u2013\u2013\u2013, 2010, \u201cBayesian Probability\u201d,\n<em>Synthese</em>, 172(1): 119\u2013127.\ndoi:10.1007/s11229-009-9471-6",
                "Mayo, Deborah G., 1996, <em>Error and the Growth of Experimental\nKnowledge</em>, Chicago: University of Chicago Press.",
                "\u2013\u2013\u2013, 1997, \u201cDuhem\u2019s Problem, the\nBayesian Way, and Error Statistics, or \u2018What\u2019s Belief Got\nto do with It?\u2019\u201d, <em>Philosophy of Science</em>, 64(2):\n222\u2013244. doi:10.1086/392549",
                "Mayo Deborah and Aris Spanos, 2006, \u201cSevere Testing as a\nBasic Concept in a Neyman-Pearson Philosophy of Induction\u201c,\n<em>British Journal for the Philosophy of Science</em>, 57(2):\n323\u2013357. doi:10.1093/bjps/axl003",
                "McGee, Vann, 1994, \u201cLearning the Impossible\u201d, in E.\nEells and B. Skyrms (eds.), <em>Probability and Conditionals: Belief\nRevision and Rational Decision</em>, New York: Cambridge University\nPress, 179\u2013200.",
                "McGrew, Timothy J., 2003, \u201cConfirmation, Heuristics, and\nExplanatory Reasoning\u201d, <em>British Journal for the Philosophy\nof Science</em>, 54: 553\u2013567.",
                "McGrew, Lydia and Timothy McGrew, 2008, \u201cFoundationalism,\nProbability, and Mutual Support\u201d, <em>Erkenntnis</em>, 68(1):\n55\u201377. doi:10.1007/s10670-007-9062-1",
                "Neyman, Jerzy and E.S. Pearson, 1967, <em>Joint Statistical\nPapers</em>, Cambridge: Cambridge University Press.",
                "Norton, John D., 2003, \u201cA Material Theory of\nInduction\u201d, <em>Philosophy of Science</em>, 70(4):\n647\u2013670. doi:10.1086/378858",
                "\u2013\u2013\u2013, 2007, \u201cProbability\nDisassembled\u201d, <em>British Journal for the Philosophy of\nScience</em>, 58(2): 141\u2013171. doi:10.1093/bjps/axm009",
                "Okasha, Samir, 2001, \u201cWhat Did Hume Really Show About\nInduction?\u201d, <em>The Philosophical Quarterly</em>, 51(204):\n307\u2013327. doi:10.1111/1467-9213.00231",
                "Popper, Karl, 1968, <em>The Logic of Scientific Discovery</em>,\n3<sup>rd</sup> edition, London: Hutchinson.",
                "Quine, W.V., 1953, \u201cTwo Dogmas of Empiricism\u201d, in\n<em>From a Logical Point of View</em>, New York: Harper Torchbooks.\nRoutledge Encyclopedia of Philosophy, Version 1.0, London:\nRoutledge",
                "Ramsey, F.P., 1926, \u201cTruth and Probability\u201d, in\n<em>Foundations of Mathematics and other Essays</em>, R.B. Braithwaite\n(ed.), Routledge &amp; P. Kegan,1931, 156\u2013198. Reprinted in\n<em>Studies in Subjective Probability</em>, H. Kyburg and H. Smokler\n(eds.), 2<sup>nd</sup> ed., R.E. Krieger Publishing Company, 1980,\n23\u201352. Reprinted in <em>Philosophical Papers</em>, D.H. Mellor\n(ed.), Cambridge: University Press, Cambridge, 1990,",
                "Reichenbach, Hans, 1938, <em>Experience and Prediction: An\nAnalysis of the Foundations and the Structure of Knowledge</em>,\nChicago: University of Chicago Press.",
                "R\u00e9nyi, Alfred, 1970, <em>Foundations of Probability</em>, San\nFrancisco, CA: Holden-Day.",
                "Rosenkrantz, R.D., 1981, <em>Foundations and Applications of\nInductive Probability</em>, Atascadero, CA: Ridgeview Publishing.",
                "Roush, Sherrilyn , 2004, \u201cDiscussion Note: Positive\nRelevance Defended\u201d, <em>Philosophy of Science</em>, 71(1):\n110\u2013116. doi:10.1086/381416",
                "\u2013\u2013\u2013, 2006, \u201cInduction, Problem of\u201d,\nSarkar and Pfeifer 2006..",
                "\u2013\u2013\u2013, 2006, <em>Tracking Truth: Knowledge,\nEvidence, and Science</em>, Oxford: Oxford University Press.",
                "Royall, Richard M., 1997, <em>Statistical Evidence: A Likelihood\nParadigm</em>, New York: Chapman &amp; Hall/CRC.",
                "Salmon, Wesley C., 1966, <em>The Foundations of Scientific\nInference</em>, Pittsburgh, PA: University of Pittsburgh Press.",
                "\u2013\u2013\u2013, 1975, \u201cConfirmation and\nRelevance\u201d, in H. Feigl and G. Maxwell (eds.), <em>Induction,\nProbability, and Confirmation</em>, (Minnesota Studies in the\nPhilosophy of Science, 6), Minneapolis: University of Minnesota Press,\n3\u201336.",
                "Sarkar, Sahotra and Jessica Pfeifer (eds.), 2006, <em>The\nPhilosophy of Science: An Encyclopedia</em>, 2 volumes, New York:\nRoutledge.",
                "Savage, Leonard J., 1954, <em>The Foundations of Statistics</em>,\nJohn Wiley (2nd ed., New York: Dover 1972).",
                "Savage, Leonard J., et al., 1962, <em>The Foundations of\nStatistical Inference</em>, London: Methuen.",
                "Schlesinger, George N., 1991, <em>The Sweep of Probability</em>,\nNotre Dame, IN: Notre Dame University Press.",
                "Seidenfeld, Teddy, 1978, \u201cDirect Inference and Inverse\nInference\u201d, <em>Journal of Philosophy</em>, 75(12):\n709\u2013730. doi:10.2307/2025515",
                "\u2013\u2013\u2013, 1992, \u201cR.A. Fisher\u2019s Fiducial\nArgument and Bayes\u2019 Theorem\u201d, <em>Statistical\nScience</em>, 7(3): 358\u2013368. doi:10.1214/ss/1177011232",
                "Shafer, Glenn, 1976, <em>A Mathematical Theory of Evidence</em>,\nPrinceton, NJ: Princeton University Press.",
                "\u2013\u2013\u2013, 1990, \u201cPerspectives on the Theory and\nPractice of Belief Functions\u201d, <em>International Journal of\nApproximate Reasoning</em>, 4(5\u20136): 323\u2013362.\ndoi:10.1016/0888-613X(90)90012-Q",
                "Skyrms, Brian, 1984, <em>Pragmatics and Empiricism</em>, New\nHaven, CT: Yale University Press.",
                "\u2013\u2013\u2013, 1990, <em>The Dynamics of Rational\nDeliberation</em>, Cambridge, MA: Harvard University Press.",
                "\u2013\u2013\u2013, 2000, <em>Choice and Chance: An\nIntroduction to Inductive Logic</em>, 4<sup>th</sup> edition, Belmont,\nCA: Wadsworth, Inc.",
                "Sober, Elliott, 2002, \u201cBayesianism\u2014Its Scope and\nLimits\u201d, in Swinburne 2002: 21\u201338.\ndoi:10.5871/bacad/9780197263419.003.0002",
                "Spohn, Wolfgang, 1988, \u201cOrdinal Conditional Functions: A\nDynamic Theory of Epistemic States\u201d, in William L. Harper and\nBrian Skyrms (eds.), <em>Causation in Decision, Belief Change, and\nStatistics</em>, vol. 2, Dordrecht: Reidel, 105\u2013134.\ndoi:10.1007/978-94-009-2865-7_6",
                "Strevens, Michael, 2004, \u201cBayesian Confirmation Theory:\nInductive Logic, or Mere Inductive Framework?\u201d\n<em>Synthese</em>, 141(3): 365\u2013379.\ndoi:10.1023/B:SYNT.0000044991.73791.f7",
                "Suppes, Patrick, 2007, \u201cWhere do Bayesian Priors Come\nFrom?\u201d <em>Synthese</em>, 156(3): 441\u2013471.\ndoi:10.1007/s11229-006-9133-x",
                "Swinburne, Richard, 2002, <em>Bayes\u2019 Theorem</em>, Oxford:\nOxford University Press. doi:10.5871/bacad/9780197263419.001.0001",
                "Talbot, W., 2001, \u201cBayesian Epistemology\u201d, in the\n<em>Stanford Encyclopedia of Philosophy</em>, (Fall 2001 Edition),\nEdward N. Zalta (ed.), URL =\n &lt;<a href=\"https://plato.stanford.edu/archives/fall2001/entries/epistemology-bayesian/\" target=\"other\">https://plato.stanford.edu/archives/fall2001/entries/epistemology-bayesian/</a>&gt;",
                "Teller, Paul, 1976, \u201cConditionalization, Observation, and\nChange of Preference\u201d, in Harper and Hooker 1976: 205\u2013259.\ndoi:10.1007/978-94-010-1853-1_9",
                "Van Fraassen, Bas C., 1983, \u201cCalibration: A Frequency\nJustification for Personal Probability \u201d, in R.S. Cohen and L.\nLaudan (eds.), <em>Physics, Philosophy, and Psychoanalysis: Essays in\nHonor of Adolf Grunbaum</em>, Dordrecht: Reidel.\ndoi:10.1007/978-94-009-7055-7_15",
                "Venn, John, 1876, <em>The Logic of Chance</em>, 2<sup>nd</sup>\ned., Macmillan and co; reprinted, New York, 1962.",
                "Vineberg, Susan, 2006, \u201cDutch Book Argument\u201d, Sarkar\nand Pfeifer 2006..",
                "Vranas, Peter B.M., 2004, \u201cHempel\u2019s Raven Paradox: A\nLacuna in the Standard Bayesian Solution\u201d, <em>British Journal\nfor the Philosophy of Science</em>, 55(3): 545\u2013560.\ndoi:10.1093/bjps/55.3.545",
                "Weatherson, Brian, 1999, \u201cBegging the Question and\nBayesianism\u201d, <em>Studies in History and Philosophy of Science\n[Part A]</em>, 30(4): 687\u2013697.\ndoi:10.1016/S0039-3681(99)00020-5",
                "Williamson, Jon, 2007, \u201cInductive Influence\u201d,\n<em>British Journal for Philosophy of Science</em>, 58(4):\n689\u2013708. doi:10.1093/bjps/axm032",
                "Zadeh, Lotfi A., 1965, \u201cFuzzy Sets\u201d, <em>Information\nand Control</em>, 8(3): 338\u2013353.\ndoi:10.1016/S0019-9958(65)90241-X",
                "\u2013\u2013\u2013, 1978, \u201cFuzzy Sets as a Basis for a\nTheory of Possibility\u201d, <em>Fuzzy Sets and Systems</em>, vol. 1,\n3\u201328."
            ]
        },
        "raw_text": "<div id=\"bibliography\">\n<h2 id=\"Bib\">Bibliography</h2>\n<ul class=\"hanging\">\n<li>Boole, George, 1854, <em>The Laws of Thought</em>, London:\nMacMillan. Republished in 1958 by Dover: New York.</li>\n<li>Bovens, Luc and Stephan Hartmann, 2003, <em>Bayesian\nEpistemology</em>, Oxford: Oxford University Press.\ndoi:10.1093/0199269750.001.0001</li>\n<li>Carnap, Rudolf, 1950, <em>Logical Foundations of Probability</em>,\nChicago: University of Chicago Press.</li>\n<li>\u2013\u2013\u2013, 1952, <em>The Continuum of Inductive\nMethods</em>, Chicago: University of Chicago Press.</li>\n<li>\u2013\u2013\u2013, 1963, \u201cReplies and Systematic\nExpositions\u201d, in <em>The Philosophy of Rudolf Carnap</em>, Paul\nArthur Schilpp (ed.),La Salle, IL: Open Court.</li>\n<li>Chihara, Charles S., 1987, \u201cSome Problems for Bayesian\nConfirmation Theory\u201d, <em>British Journal for the Philosophy of\nScience</em>, 38(4): 551\u2013560. doi:10.1093/bjps/38.4.551</li>\n<li>Christensen, David, 1999, \u201cMeasuring Confirmation\u201d,\n<em>Journal of Philosophy</em>, 96(9): 437\u201361.\ndoi:10.2307/2564707</li>\n<li>\u2013\u2013\u2013, 2004, <em>Putting Logic in its Place:\nFormal Constraints on Rational Belief</em>, Oxford: Oxford University\nPress. doi:10.1093/0199263256.001.0001</li>\n<li>De Finetti, Bruno, 1937, \u201cLa Pr\u00e9vision: Ses Lois\nLogiques, Ses Sources Subjectives\u201d, <em>Annales de\nl\u2019Institut Henri Poincar\u00e9</em>, 7: 1\u201368; translated\nby Henry E. Kyburg, Jr. as \u201cForesight. Its Logical Laws, Its\nSubjective Sources\u201d, in <em>Studies in Subjective\nProbability</em>, Henry E. Kyburg, Jr. and H.E. Smokler (eds.), Robert\nE. Krieger Publishing Company, 1980.</li>\n<li>Dowe, David L., Steve Gardner, and Graham Oppy, 2007,\n\u201cBayes, Not Bust! Why Simplicity is No Problem for\nBayesians\u201d, <em>British Journal for the Philosophy of\nScience</em>, 58(4): 709\u2013754. doi:10.1093/bjps/axm033</li>\n<li>Dubois, Didier J. and Henri Prade, 1980, <em>Fuzzy Sets and\nSystems</em>, (Mathematics in Science and Engineering, 144), New York:\nAcademic Press.</li>\n<li>\u2013\u2013\u2013, 1990, \u201cAn Introduction to\nPossibilistic and Fuzzy Logics\u201d, in Glenn Shafer and Judea Pearl\n(eds.), <em>Readings in Uncertain Reasoning</em>, San Mateo, CA:\nMorgan Kaufmann, 742\u2013761..</li>\n<li>Duhem, P., 1906, <em>La theorie physique. Son objet et sa\nstructure</em>, Paris: Chevalier et Riviere; translated by P.P.\nWiener, <em>The Aim and Structure of Physical Theory</em>, Princeton,\nNJ: Princeton University Press, 1954.</li>\n<li>Earman, John, 1992, <em>Bayes or Bust? A Critical Examination of\nBayesian Confirmation Theory</em>, Cambridge, MA: MIT Press.</li>\n<li>Edwards, A.W.F., 1972, <em>Likelihood: an account of the\nstatistical concept of likelihood and its application to scientific\ninference</em>, Cambridge: Cambridge University Press.</li>\n<li>Edwards, Ward, Harold Lindman, and Leonard J. Savage, 1963,\n\u201cBayesian Statistical Inference for Psychological\nResearch\u201d, <em>Psychological Review</em>, 70(3): 193\u2013242.\ndoi:10.1037/h0044139</li>\n<li>Eells, Ellery, 1985, \u201cProblems of Old Evidence\u201d,\n<em>Pacific Philosophical Quarterly</em>, 66(3\u20134):\n283\u2013302. doi:10.1111/j.1468-0114.1985.tb00254.x</li>\n<li>\u2013\u2013\u2013, 2006, \u201cConfirmation Theory\u201d,\nSarkar and Pfeifer 2006..</li>\n<li>Eells, Ellery and Branden Fitelson, 2000, \u201cMeasuring\nConfirmation and Evidence\u201d, <em>Journal of Philosophy</em>,\n97(12): 663\u2013672. doi:10.2307/2678462</li>\n<li>Field, Hartry H., 1977, \u201cLogic, Meaning, and Conceptual\nRole\u201d, <em>Journal of Philosophy</em>, 74(7): 379\u2013409.\ndoi:10.2307/2025580</li>\n<li>Fisher, R.A., 1922, \u201cOn the Mathematical Foundations of\nTheoretical Statistics\u201d, <em>Philosophical Transactions of the\nRoyal Society, series A </em>, 222(594\u2013604): 309\u2013368.\ndoi:10.1098/rsta.1922.0009 </li>\n<li>Fitelson, Branden, 1999, \u201cThe Plurality of Bayesian Measures\nof Confirmation and the Problem of Measure Sensitivity\u201d,\n<em>Philosophy of Science</em>, 66: S362\u2013S378.\ndoi:10.1086/392738</li>\n<li>\u2013\u2013\u2013, 2001, \u201cA Bayesian Account of\nIndependent Evidence with Applications\u201d, <em>Philosophy of\nScience</em>, 68(S3): S123\u2013S140. doi:10.1086/392903</li>\n<li>\u2013\u2013\u2013, 2002, \u201cPutting the Irrelevance Back\nInto the Problem of Irrelevant Conjunction\u201d, <em>Philosophy of\nScience</em>, 69(4): 611\u2013622. doi:10.1086/344624</li>\n<li>\u2013\u2013\u2013, 2006, \u201cInductive Logic\u201d, Sarkar\nand Pfeifer 2006..</li>\n<li>\u2013\u2013\u2013, 2006, \u201cLogical Foundations of\nEvidential Support\u201d, <em>Philosophy of Science</em>, 73(5):\n500\u2013512. doi:10.1086/518320</li>\n<li>\u2013\u2013\u2013, 2007, \u201cLikelihoodism, Bayesianism,\nand Relational Confirmation\u201d, <em>Synthese</em>, 156(3):\n473\u2013489. doi:10.1007/s11229-006-9134-9</li>\n<li>Fitelson, Branden and James Hawthorne, 2010, \u201cHow Bayesian\nConfirmation Theory Handles the Paradox of the Ravens\u201d, in Eells\nand Fetzer (eds.), <em>The Place of Probability in Science</em>, Open\nCourt.\n [<a href=\"http://fitelson.org/ravens.pdf\" target=\"other\">Fitelson &amp; Hawthorne 2010 preprint available from the author (PDF)</a>]</li>\n<li>Forster, Malcolm and Elliott Sober, 2004, \u201cWhy\nLikelihood\u201d, in Mark L. Taper and Subhash R. Lele (eds.),\n<em>The Nature of Scientific Evidence</em>, Chicago: University of\nChicago Press.</li>\n<li>Friedman, Nir and Joseph Y. Halpern, 1995, \u201cPlausibility\nMeasures: A User\u2019s Guide\u201d, in <em>UAI 95: Proceedings of\nthe Eleventh Conference on Uncertainty in Artificial\nIntelligence</em>, 175\u2013184.</li>\n<li>Gaifman, Haim and Marc Snir, 1982, \u201cProbabilities Over Rich\nLanguages, Testing and Randomness\u201d, <em>Journal of Symbolic\nLogic</em>, 47(3): 495\u2013548. doi:10.2307/2273587 </li>\n<li>Gillies, Donald, 2000, <em>Philosophical Theories of\nProbability</em>, London: Routledge.</li>\n<li>Glymour, Clark N., 1980, <em>Theory and Evidence</em>, Princeton,\nNJ: Princeton University Press.</li>\n<li>Goodman, Nelson, 1983, <em>Fact, Fiction, and Forecast</em>,\n4<sup>th</sup> edition, Cambridge, MA: Harvard University Press.</li>\n<li>Hacking, Ian, 1965, <em>Logic of Statistical Inference</em>,\nCambridge: Cambridge University Press.</li>\n<li>\u2013\u2013\u2013, 1975, <em>The Emergence of Probability: a\nPhilosophical Study of Early Ideas about Probability, Induction and\nStatistical Inference</em>, Cambridge: Cambridge University Press.\ndoi:10.1017/CBO9780511817557</li>\n<li>\u2013\u2013\u2013, 2001, <em>An Introduction to Probability\nand Inductive Logic</em>, Cambridge: Cambridge University Press.\ndoi:10.1017/CBO9780511801297</li>\n<li>H\u00e1jek, Alan, 2003a, \u201cWhat Conditional Probability\nCould Not Be\u201d, <em>Synthese</em>, 137(3):, 273\u2013323.\ndoi:10.1023/B:SYNT.0000004904.91112.16</li>\n<li>\u2013\u2013\u2013, 2003b, \u201cInterpretations of the\nProbability Calculus\u201d, in the <em>Stanford Encyclopedia of\nPhilosophy</em>, (Summer 2003 Edition), Edward N. Zalta (ed.), URL =\n &lt;<a href=\"https://plato.stanford.edu/archives/sum2003/entries/probability-interpret/\" target=\"other\">https://plato.stanford.edu/archives/sum2003/entries/probability-interpret/</a>&gt;</li>\n<li>\u2013\u2013\u2013, 2005, \u201cScotching Dutch Books?\u201d\n<em>Philosophical Perspectives</em>, 19 (Epistemology): 139\u2013151.\ndoi:10.1111/j.1520-8583.2005.00057.x</li>\n<li>\u2013\u2013\u2013, 2007, \u201cThe Reference Class Problem is\nYour Problem Too\u201d, <em>Synthese</em>, 156(3): 563\u2013585.\ndoi:10.1007/s11229-006-9138-5</li>\n<li>Halpern, Joseph Y., 2003, <em>Reasoning About Uncertainty</em>,\nCambridge, MA: MIT Press.</li>\n<li>Harper, William L., 1976, \u201cRational Belief Change, Popper\nFunctions and Counterfactuals\u201d, in Harper and Hooker 1976:\n73\u2013115. doi:10.1007/978-94-010-1853-1_5</li>\n<li>Harper, William L. and Clifford Alan Hooker (eds.), 1976,\n<em>Foundations of Probability Theory, Statistical Inference, and\nStatistical Theories of Science, volume I Foundations and Philosophy\nof Epistemic Applications of Probability Theory</em>, (The Western\nOntario Series in Philosophy of Science, 6a), Dordrecht: Reidel.\ndoi:10.1007/978-94-010-1853-1</li>\n<li>Hawthorne, James, 1993, \u201cBayesian Induction <em>is</em>\nEliminative Induction\u201d, <em>Philosophical Topics</em>, 21(1):\n99\u2013138. doi:10.5840/philtopics19932117</li>\n<li>\u2013\u2013\u2013, 1994,\u201cOn the Nature of Bayesian\nConvergence\u201d, <em>PSA: Proceedings of the Biennial Meeting of\nthe Philosophy of Science Association 1994</em>, 1: 241\u2013249.\ndoi:10.1086/psaprocbienmeetp.1994.1.193029</li>\n<li>\u2013\u2013\u2013, 2005, \u201c<em>Degree-of-Belief</em> and\n<em>Degree-of-Support</em>: Why Bayesians Need Both Notions\u201d,\n<em>Mind</em>, 114(454): 277\u2013320. doi:10.1093/mind/fzi277</li>\n<li>\u2013\u2013\u2013, 2009, \u201cThe Lockean Thesis and the\nLogic of Belief\u201d, in Franz Huber and Christoph Schmidt-Petri\n(eds.), <em>Degrees of Belief</em>, (Synthese Library, 342),\nDordrecht: Springer, pp. 49\u201374.\ndoi:10.1007/978-1-4020-9198-8_3</li>\n<li>Hawthorne, James and Luc Bovens, 1999, \u201cThe Preface, the\nLottery, and the Logic of Belief\u201d, <em>Mind</em>, 108(430):\n241\u2013264. doi:10.1093/mind/108.430.241</li>\n<li>Hawthorne, James and Branden Fitelson, 2004, \u201cDiscussion:\nRe-solving Irrelevant Conjunction With Probabilistic\nIndependence\u201d, <em>Philosophy of Science</em>, 71(4):\n505\u2013514. doi:10.1086/423626</li>\n<li>Hellman, Geoffrey, 1997, \u201cBayes and Beyond\u201d,\n<em>Philosophy of Science</em>, 64(2): 191\u2013221.\ndoi:10.1086/392548</li>\n<li>Hempel, Carl G., 1945, \u201cStudies in the Logic of\nConfirmation\u201d, <em>Mind</em>, 54(213): 1\u201326,\n54(214):97\u2013121. doi:10.1093/mind/LIV.213.1\ndoi:10.1093/mind/LIV.214.97</li>\n<li>Horwich, Paul, 1982, <em>Probability and Evidence</em>, Cambridge:\nCambridge University Press. doi:10.1017/CBO9781316494219</li>\n<li>Howson, Colin, 1997, \u201cA Logic of Induction\u201d,\n<em>Philosophy of Science</em>, 64(2): 268\u2013290.\ndoi:10.1086/392551</li>\n<li>\u2013\u2013\u2013, 2000, <em>Hume\u2019s Problem: Induction\nand the Justification of Belief</em>, Oxford: Oxford University Press.\ndoi:10.1093/0198250371.001.0001</li>\n<li>\u2013\u2013\u2013, 2002, \u201cBayesianism in\nStatistics\u201c, in Swinburne 2002: 39\u201371.\ndoi:10.5871/bacad/9780197263419.003.0003</li>\n<li>\u2013\u2013\u2013, 2007, \u201cLogic With Numbers\u201d,\n<em>Synthese</em>, 156(3): 491\u2013512.\ndoi:10.1007/s11229-006-9135-8</li>\n<li>Howson, Colin and Peter Urbach, 1993, <em>Scientific Reasoning:\nThe Bayesian Approach</em>, La Salle, IL: Open\nCourt. [3rd edition, 2005.]</li>\n<li>Huber, Franz, 2005a, \u201cSubjective Probabilities as Basis for\nScientific Reasoning?\u201d <em>British Journal for the Philosophy of\nScience</em>, 56(1): 101\u2013116. doi:10.1093/phisci/axi105</li>\n<li>\u2013\u2013\u2013, 2005b, \u201cWhat Is the Point of\nConfirmation?\u201d <em>Philosophy of Science</em>, 72(5):\n1146\u20131159. doi:10.1086/508961</li>\n<li>Jaynes, Edwin T., 1968, \u201cPrior Probabilities\u201d,\n<em>IEEE Transactions on Systems Science and Cybernetics</em>,\nSSC\u20134(3): 227\u2013241. doi:10.1109/TSSC.1968.300117</li>\n<li>Jeffrey, Richard C., 1983, <em>The Logic of Decision</em>, 2nd\nedition, Chicago: University of Chicago Press.</li>\n<li>\u2013\u2013\u2013, 1987, \u201cAlias Smith and Jones: The\nTestimony of the Senses\u201d, <em>Erkenntnis</em>, 26(3):\n391\u2013399. doi:10.1007/BF00167725</li>\n<li>\u2013\u2013\u2013, 1992, <em>Probability and the Art of\nJudgment</em>, New York: Cambridge University Press.\ndoi:10.1017/CBO9781139172394</li>\n<li>\u2013\u2013\u2013, 2004, <em>Subjective Probability: The Real\nThing</em>, Cambridge: Cambridge University Press.\ndoi:10.1017/CBO9780511816161</li>\n<li>Jeffreys, Harold, 1939, <em>Theory of Probability</em>, Oxford:\nOxford University Press. </li>\n<li>Joyce, James M., 1998, \u201cA Nonpragmatic Vindication of\nProbabilism\u201d, <em>Philosophy of Science</em>, 65(4):\n\n575\u2013603. doi:10.1086/392661</li>\n<li>\u2013\u2013\u2013, 1999, <em>The Foundations of Causal\nDecision Theory</em>, New York: Cambridge University Press.\ndoi:10.1017/CBO9780511498497</li>\n<li>\u2013\u2013\u2013, 2003, \u201cBayes\u2019 Theorem\u201d,\nin the <em>Stanford Encyclopedia of Philosophy</em>, (Summer 2003\nEdition), Edward N. Zalta (ed.), URL =\n &lt;<a href=\"https://plato.stanford.edu/archives/win2003/entries/bayes-theorem/\" target=\"other\">https://plato.stanford.edu/archives/win2003/entries/bayes-theorem/</a>&gt;</li>\n<li>\u2013\u2013\u2013, 2004, \u201cBayesianism\u201d, in Alfred\nR. Mele and Piers Rawling (eds.), <em>The Oxford Handbook of\nRationality</em>, Oxford: Oxford University Press, pp. 132\u2013153.\ndoi:10.1093/0195145399.003.0008</li>\n<li>\u2013\u2013\u2013, 2005, \u201cHow Probabilities Reflect\nEvidence\u201d, <em>Philosophical Perspectives</em>, 19:\n153\u2013179. doi:10.1111/j.1520-8583.2005.00058.x</li>\n<li>Kaplan, Mark, 1996, <em>Decision Theory as Philosophy</em>,\nCambridge: Cambridge University Press.</li>\n<li>Kelly, Kevin T., Oliver Schulte, and Cory Juhl, 1997,\n\u201cLearning Theory and the Philosophy of Science\u201d,\n<em>Philosophy of Science</em>, 64(2): 245\u2013267.\ndoi:10.1086/392550</li>\n<li>Keynes, John Maynard, 1921, <em>A Treatise on Probability</em>,\nLondon: Macmillan and Co.</li>\n<li>Kolmogorov, A.N., 1956, <em>Foundations of the Theory of\nProbability</em> (<em>Grundbegriffe der\nWahrscheinlichkeitsrechnung</em>, 2<sup>nd</sup> edition, New York:\nChelsea Publishing Company.</li>\n<li>Koopman, B.O., 1940, \u201cThe Bases of Probability\u201d,\n<em>Bulletin of the American Mathematical Society</em>, 46(10):\n763\u2013774. Reprinted in H. Kyburg and H. Smokler (eds.), 1980,\n<em>Studies in Subjective Probability</em>, 2nd edition, Huntington,\nNY: Krieger Publ. Co.\n [<a href=\"https://projecteuclid.org/euclid.bams/1183503229\" target=\"other\">Koopman 1940 available online</a>]</li>\n<li>Kyburg, Henry E., Jr., 1974, <em>The Logical Foundations of\nStatistical Inference</em>, Dordrecht: Reidel.\ndoi:10.1007/978-94-010-2175-3</li>\n<li>\u2013\u2013\u2013, 1977, \u201cRandomness and the Right\nReference Class\u201d, <em>Journal of Philosophy</em>, 74(9):\n501\u2013520. doi:10.2307/2025794</li>\n<li>\u2013\u2013\u2013, 1978, \u201cAn Interpolation Theorem for\nInductive Relations\u201d, <em>Journal of Philosophy</em>,\n75:93\u201398.</li>\n<li>\u2013\u2013\u2013, 2006, \u201cBelief, Evidence, and\nConditioning\u201d, <em>Philosophy of Science</em>, 73(1):\n42\u201365. doi:10.1086/510174</li>\n<li>Lange, Marc, 1999, \u201cCalibration and the Epistemological Role\nof Bayesian Conditionalization\u201d, <em>Journal of Philosophy</em>,\n96(6): 294\u2013324. doi:10.2307/2564680</li>\n<li>\u2013\u2013\u2013, 2002, \u201cOkasha on Inductive\nScepticism\u201d, <em>The Philosophical Quarterly</em>, 52(207):\n226\u2013232. doi:10.1111/1467-9213.00264</li>\n<li>Laudan, Larry, 1997, \u201cHow About Bust? Factoring Explanatory\nPower Back into Theory Evaluation\u201d, <em>Philosophy of\nScience</em>, 64(2): 206\u2013216. doi:10.1086/392553</li>\n<li>Lenhard Johannes, 2006, \u201cModels and Statistical Inference:\nThe Controversy Between Fisher and Neyman-Pearson\u201d, <em>British\nJournal for the Philosophy of Science</em>, 57(1): 69\u201391.\ndoi:10.1093/bjps/axi152</li>\n<li>Levi, Isaac, 1967, <em>Gambling with Truth: An Essay on Induction\nand the Aims of Science</em>, New York: Knopf. </li>\n<li>\u2013\u2013\u2013, 1977, \u201cDirect Inference\u201d,\n<em>Journal of Philosophy</em>, 74(1): 5\u201329.\ndoi:10.2307/2025732</li>\n<li>\u2013\u2013\u2013, 1978, \u201cConfirmational\nConditionalization\u201d, <em>Journal of Philosophy</em>, 75(12):\n730\u2013737. doi:10.2307/2025516</li>\n<li>\u2013\u2013\u2013, 1980, <em>The Enterprise of Knowledge: An\nEssay on Knowledge, Credal Probability, and Chance</em>, Cambridge,\nMA: MIT Press.</li>\n<li>Lewis, David, 1980, \u201cA Subjectivist\u2019s Guide to\nObjective Chance\u201d, in Richard C. Jeffrey, (ed.), <em>Studies in\nInductive Logic and Probability</em>, vol. 2, Berkeley: University of\nCalifornia Press, 263\u2013293.</li>\n<li>Maher, Patrick, 1993, <em>Betting on Theories</em>, Cambridge:\nCambridge University Press.</li>\n<li>\u2013\u2013\u2013, 1996, \u201cSubjective and Objective\nConfirmation\u201d, <em>Philosophy of Science</em>, 63(2):\n149\u2013174. doi:10.1086/289906</li>\n<li>\u2013\u2013\u2013, 1997, \u201cDepragmatized Dutch Book\nArguments\u201d, <em>Philosophy of Science</em>, 64(2):\n291\u2013305. doi:10.1086/392552</li>\n<li>\u2013\u2013\u2013, 1999, \u201cInductive Logic and the Ravens\nParadox\u201d, <em>Philosophy of Science</em>, 66(1): 50\u201370.\ndoi:10.1086/392676</li>\n<li>\u2013\u2013\u2013, 2004, \u201cProbability Captures the Logic\nof Scientific Confirmation\u201d, in Christopher Hitchcock (ed.),\n<em>Contemporary Debates in Philosophy of Science</em>, Oxford:\nBlackwell, 69\u201393.</li>\n<li>\u2013\u2013\u2013, 2005, \u201cConfirmation Theory\u201d,\n<em>The Encyclopedia of Philosophy</em>, 2nd edition, Donald M.\nBorchert (ed.), Detroit: Macmillan.</li>\n<li>\u2013\u2013\u2013, 2006a, \u201cThe Concept of Inductive\nProbability\u201d, <em>Erkenntnis</em>, 65(2): 185\u2013206.\ndoi:10.1007/s10670-005-5087-5</li>\n<li>\u2013\u2013\u2013, 2006b, \u201cA Conception of Inductive\nLogic\u201d, <em>Philosophy of Science</em>, 73(5): 513\u2013523.\ndoi:10.1086/518321</li>\n<li>\u2013\u2013\u2013, 2010, \u201cBayesian Probability\u201d,\n<em>Synthese</em>, 172(1): 119\u2013127.\ndoi:10.1007/s11229-009-9471-6</li>\n<li>Mayo, Deborah G., 1996, <em>Error and the Growth of Experimental\nKnowledge</em>, Chicago: University of Chicago Press.</li>\n<li>\u2013\u2013\u2013, 1997, \u201cDuhem\u2019s Problem, the\nBayesian Way, and Error Statistics, or \u2018What\u2019s Belief Got\nto do with It?\u2019\u201d, <em>Philosophy of Science</em>, 64(2):\n222\u2013244. doi:10.1086/392549</li>\n<li>Mayo Deborah and Aris Spanos, 2006, \u201cSevere Testing as a\nBasic Concept in a Neyman-Pearson Philosophy of Induction\u201c,\n<em>British Journal for the Philosophy of Science</em>, 57(2):\n323\u2013357. doi:10.1093/bjps/axl003</li>\n<li>McGee, Vann, 1994, \u201cLearning the Impossible\u201d, in E.\nEells and B. Skyrms (eds.), <em>Probability and Conditionals: Belief\nRevision and Rational Decision</em>, New York: Cambridge University\nPress, 179\u2013200.</li>\n<li>McGrew, Timothy J., 2003, \u201cConfirmation, Heuristics, and\nExplanatory Reasoning\u201d, <em>British Journal for the Philosophy\nof Science</em>, 54: 553\u2013567.</li>\n<li>McGrew, Lydia and Timothy McGrew, 2008, \u201cFoundationalism,\nProbability, and Mutual Support\u201d, <em>Erkenntnis</em>, 68(1):\n55\u201377. doi:10.1007/s10670-007-9062-1</li>\n<li>Neyman, Jerzy and E.S. Pearson, 1967, <em>Joint Statistical\nPapers</em>, Cambridge: Cambridge University Press.</li>\n<li>Norton, John D., 2003, \u201cA Material Theory of\nInduction\u201d, <em>Philosophy of Science</em>, 70(4):\n647\u2013670. doi:10.1086/378858</li>\n<li>\u2013\u2013\u2013, 2007, \u201cProbability\nDisassembled\u201d, <em>British Journal for the Philosophy of\nScience</em>, 58(2): 141\u2013171. doi:10.1093/bjps/axm009</li>\n<li>Okasha, Samir, 2001, \u201cWhat Did Hume Really Show About\nInduction?\u201d, <em>The Philosophical Quarterly</em>, 51(204):\n307\u2013327. doi:10.1111/1467-9213.00231</li>\n<li>Popper, Karl, 1968, <em>The Logic of Scientific Discovery</em>,\n3<sup>rd</sup> edition, London: Hutchinson.</li>\n<li>Quine, W.V., 1953, \u201cTwo Dogmas of Empiricism\u201d, in\n<em>From a Logical Point of View</em>, New York: Harper Torchbooks.\nRoutledge Encyclopedia of Philosophy, Version 1.0, London:\nRoutledge</li>\n<li>Ramsey, F.P., 1926, \u201cTruth and Probability\u201d, in\n<em>Foundations of Mathematics and other Essays</em>, R.B. Braithwaite\n(ed.), Routledge &amp; P. Kegan,1931, 156\u2013198. Reprinted in\n<em>Studies in Subjective Probability</em>, H. Kyburg and H. Smokler\n(eds.), 2<sup>nd</sup> ed., R.E. Krieger Publishing Company, 1980,\n23\u201352. Reprinted in <em>Philosophical Papers</em>, D.H. Mellor\n(ed.), Cambridge: University Press, Cambridge, 1990,</li>\n<li>Reichenbach, Hans, 1938, <em>Experience and Prediction: An\nAnalysis of the Foundations and the Structure of Knowledge</em>,\nChicago: University of Chicago Press.</li>\n<li>R\u00e9nyi, Alfred, 1970, <em>Foundations of Probability</em>, San\nFrancisco, CA: Holden-Day.</li>\n<li>Rosenkrantz, R.D., 1981, <em>Foundations and Applications of\nInductive Probability</em>, Atascadero, CA: Ridgeview Publishing.</li>\n<li>Roush, Sherrilyn , 2004, \u201cDiscussion Note: Positive\nRelevance Defended\u201d, <em>Philosophy of Science</em>, 71(1):\n110\u2013116. doi:10.1086/381416</li>\n<li>\u2013\u2013\u2013, 2006, \u201cInduction, Problem of\u201d,\nSarkar and Pfeifer 2006..</li>\n<li>\u2013\u2013\u2013, 2006, <em>Tracking Truth: Knowledge,\nEvidence, and Science</em>, Oxford: Oxford University Press.</li>\n<li>Royall, Richard M., 1997, <em>Statistical Evidence: A Likelihood\nParadigm</em>, New York: Chapman &amp; Hall/CRC.</li>\n<li>Salmon, Wesley C., 1966, <em>The Foundations of Scientific\nInference</em>, Pittsburgh, PA: University of Pittsburgh Press.</li>\n<li>\u2013\u2013\u2013, 1975, \u201cConfirmation and\nRelevance\u201d, in H. Feigl and G. Maxwell (eds.), <em>Induction,\nProbability, and Confirmation</em>, (Minnesota Studies in the\nPhilosophy of Science, 6), Minneapolis: University of Minnesota Press,\n3\u201336.</li>\n<li>Sarkar, Sahotra and Jessica Pfeifer (eds.), 2006, <em>The\nPhilosophy of Science: An Encyclopedia</em>, 2 volumes, New York:\nRoutledge.</li>\n<li>Savage, Leonard J., 1954, <em>The Foundations of Statistics</em>,\nJohn Wiley (2nd ed., New York: Dover 1972).</li>\n<li>Savage, Leonard J., et al., 1962, <em>The Foundations of\nStatistical Inference</em>, London: Methuen.</li>\n<li>Schlesinger, George N., 1991, <em>The Sweep of Probability</em>,\nNotre Dame, IN: Notre Dame University Press.</li>\n<li>Seidenfeld, Teddy, 1978, \u201cDirect Inference and Inverse\nInference\u201d, <em>Journal of Philosophy</em>, 75(12):\n709\u2013730. doi:10.2307/2025515</li>\n<li>\u2013\u2013\u2013, 1992, \u201cR.A. Fisher\u2019s Fiducial\nArgument and Bayes\u2019 Theorem\u201d, <em>Statistical\nScience</em>, 7(3): 358\u2013368. doi:10.1214/ss/1177011232</li>\n<li>Shafer, Glenn, 1976, <em>A Mathematical Theory of Evidence</em>,\nPrinceton, NJ: Princeton University Press.</li>\n<li>\u2013\u2013\u2013, 1990, \u201cPerspectives on the Theory and\nPractice of Belief Functions\u201d, <em>International Journal of\nApproximate Reasoning</em>, 4(5\u20136): 323\u2013362.\ndoi:10.1016/0888-613X(90)90012-Q</li>\n<li>Skyrms, Brian, 1984, <em>Pragmatics and Empiricism</em>, New\nHaven, CT: Yale University Press.</li>\n<li>\u2013\u2013\u2013, 1990, <em>The Dynamics of Rational\nDeliberation</em>, Cambridge, MA: Harvard University Press.</li>\n<li>\u2013\u2013\u2013, 2000, <em>Choice and Chance: An\nIntroduction to Inductive Logic</em>, 4<sup>th</sup> edition, Belmont,\nCA: Wadsworth, Inc.</li>\n<li>Sober, Elliott, 2002, \u201cBayesianism\u2014Its Scope and\nLimits\u201d, in Swinburne 2002: 21\u201338.\ndoi:10.5871/bacad/9780197263419.003.0002</li>\n<li>Spohn, Wolfgang, 1988, \u201cOrdinal Conditional Functions: A\nDynamic Theory of Epistemic States\u201d, in William L. Harper and\nBrian Skyrms (eds.), <em>Causation in Decision, Belief Change, and\nStatistics</em>, vol. 2, Dordrecht: Reidel, 105\u2013134.\ndoi:10.1007/978-94-009-2865-7_6</li>\n<li>Strevens, Michael, 2004, \u201cBayesian Confirmation Theory:\nInductive Logic, or Mere Inductive Framework?\u201d\n<em>Synthese</em>, 141(3): 365\u2013379.\ndoi:10.1023/B:SYNT.0000044991.73791.f7</li>\n<li>Suppes, Patrick, 2007, \u201cWhere do Bayesian Priors Come\nFrom?\u201d <em>Synthese</em>, 156(3): 441\u2013471.\ndoi:10.1007/s11229-006-9133-x</li>\n<li>Swinburne, Richard, 2002, <em>Bayes\u2019 Theorem</em>, Oxford:\nOxford University Press. doi:10.5871/bacad/9780197263419.001.0001</li>\n<li>Talbot, W., 2001, \u201cBayesian Epistemology\u201d, in the\n<em>Stanford Encyclopedia of Philosophy</em>, (Fall 2001 Edition),\nEdward N. Zalta (ed.), URL =\n &lt;<a href=\"https://plato.stanford.edu/archives/fall2001/entries/epistemology-bayesian/\" target=\"other\">https://plato.stanford.edu/archives/fall2001/entries/epistemology-bayesian/</a>&gt;</li>\n<li>Teller, Paul, 1976, \u201cConditionalization, Observation, and\nChange of Preference\u201d, in Harper and Hooker 1976: 205\u2013259.\ndoi:10.1007/978-94-010-1853-1_9</li>\n<li>Van Fraassen, Bas C., 1983, \u201cCalibration: A Frequency\nJustification for Personal Probability \u201d, in R.S. Cohen and L.\nLaudan (eds.), <em>Physics, Philosophy, and Psychoanalysis: Essays in\nHonor of Adolf Grunbaum</em>, Dordrecht: Reidel.\ndoi:10.1007/978-94-009-7055-7_15</li>\n<li>Venn, John, 1876, <em>The Logic of Chance</em>, 2<sup>nd</sup>\ned., Macmillan and co; reprinted, New York, 1962.</li>\n<li>Vineberg, Susan, 2006, \u201cDutch Book Argument\u201d, Sarkar\nand Pfeifer 2006..</li>\n<li>Vranas, Peter B.M., 2004, \u201cHempel\u2019s Raven Paradox: A\nLacuna in the Standard Bayesian Solution\u201d, <em>British Journal\nfor the Philosophy of Science</em>, 55(3): 545\u2013560.\ndoi:10.1093/bjps/55.3.545</li>\n<li>Weatherson, Brian, 1999, \u201cBegging the Question and\nBayesianism\u201d, <em>Studies in History and Philosophy of Science\n[Part A]</em>, 30(4): 687\u2013697.\ndoi:10.1016/S0039-3681(99)00020-5</li>\n<li>Williamson, Jon, 2007, \u201cInductive Influence\u201d,\n<em>British Journal for Philosophy of Science</em>, 58(4):\n689\u2013708. doi:10.1093/bjps/axm032</li>\n<li>Zadeh, Lotfi A., 1965, \u201cFuzzy Sets\u201d, <em>Information\nand Control</em>, 8(3): 338\u2013353.\ndoi:10.1016/S0019-9958(65)90241-X</li>\n<li>\u2013\u2013\u2013, 1978, \u201cFuzzy Sets as a Basis for a\nTheory of Possibility\u201d, <em>Fuzzy Sets and Systems</em>, vol. 1,\n3\u201328.</li>\n</ul>\n</div>"
    },
    "related_entries": {
        "entry_list": [
            "Bayes\u2019 Theorem",
            "epistemology: Bayesian",
            "probability, interpretations of"
        ],
        "entry_link": [
            {
                "../bayes-theorem/": "Bayes\u2019 Theorem"
            },
            {
                "../epistemology-bayesian/": "epistemology: Bayesian"
            },
            {
                "../probability-interpret/": "probability, interpretations of"
            }
        ]
    },
    "academic_tools": {
        "listed_text": [
            "\n<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>\n",
            "<a href=\"https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=logic-inductive\" target=\"other\">How to cite this entry</a>.",
            "\n<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>\n",
            "<a href=\"https://leibniz.stanford.edu/friends/preview/logic-inductive/\" target=\"other\">Preview the PDF version of this entry</a>\n at the\n <a href=\"https://leibniz.stanford.edu/friends/\" target=\"other\">Friends of the SEP Society</a>.",
            "\n<img alt=\"inpho icon\" src=\"../../symbols/inpho.png\"/>\n",
            "<a href=\"https://www.inphoproject.org/entity?sep=logic-inductive&amp;redirect=True\" target=\"other\">Look up topics and thinkers related to this entry</a>\n at the Internet Philosophy Ontology Project (InPhO).",
            "\n<img alt=\"phil papers icon\" src=\"../../symbols/pp.gif\"/>\n",
            "<a href=\"http://philpapers.org/sep/logic-inductive/\" target=\"other\">Enhanced bibliography for this entry</a>\n at\n <a href=\"http://philpapers.org/\" target=\"other\">PhilPapers</a>,\n with links to its database."
        ],
        "listed_links": [
            {
                "https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=logic-inductive": "How to cite this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/preview/logic-inductive/": "Preview the PDF version of this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/": "Friends of the SEP Society"
            },
            {
                "https://www.inphoproject.org/entity?sep=logic-inductive&redirect=True": "Look up topics and thinkers related to this entry"
            },
            {
                "http://philpapers.org/sep/logic-inductive/": "Enhanced bibliography for this entry"
            },
            {
                "http://philpapers.org/": "PhilPapers"
            }
        ]
    },
    "other_internet_resources": {
        "listed_text": [
            "<a href=\"http://www.iep.utm.edu/c/conf-ind.htm\" target=\"other\">Confirmation and Induction</a>.\n Really nice overview by Franz Huber in the <em>Internet Encyclopedia\nof Philosophy</em>.",
            "<a href=\"http://fitelson.org/il.pdf\" target=\"other\">Inductive Logic</a>,\n (in PDF), by Branden Fitelson, <em>Philosophy of Science: An\nEncyclopedia</em>, (J. Pfeifer and S. Sarkar, eds.), Routledge. An\nextensive encyclopedia article on inductive logic.",
            "<a href=\"http://www.ditext.com/clay/armendt2.html\" target=\"other\">Teaching Theory of Knowledge: Probability and Induction</a>.\n A very extensive outline of issues in Probability and Induction, each\ntopic accompanied by a list of relevant books and articles (without\nlinks), compiled by Brad Armendt and Martin Curd.",
            "<a href=\"http://www.cs.ubc.ca/~murphyk/Bayes/Charniak_91.pdf\" target=\"other\">Bayesian Networks Without Tears</a>,\n (in PDF), by Eugene Charniak (Computer Science and Cognitive Science,\nBrown University). An introductory article on Bayesian inference.",
            "<a href=\"http://www.princeton.edu/~bayesway/\" target=\"other\">Miscellany of Works on Probabilistic Thinking</a>.\n A collection of on-line articles on Subjective Probability and\nprobabilistic reasoning by Richard Jeffrey and by several other\nphilosophers writing on related issues.",
            "<a href=\"http://fitelson.org/confirmation/\" target=\"other\">Fitelson\u2019s course on Confirmation Theory</a>.\n Main page of Branden Fitelson\u2019s course on Confirmation Theory.\nThe\n <a href=\"http://fitelson.org/confirmation/syllabus.html\" target=\"other\">Syllabus</a>\n provides an extensive list of links to readings. The\n <a href=\"http://fitelson.org/confirmation/notes.html\" target=\"other\">Notes, Handouts, &amp; Links</a>\n page has Fitelson\u2019s weekly course notes and some links to\nuseful internet resources on confirmation theory.",
            "<a href=\"http://fitelson.org/probability/\" target=\"other\">Fitelson\u2019s course on Probability and Induction</a>.\n Main page of Branden Fitelson\u2019s course on Probability and\nInduction. The\n <a href=\"http://fitelson.org/probability/syllabus.html\" target=\"other\">Syllabus</a>\n provides an extensive list of links to readings on the subject. The\n <a href=\"http://fitelson.org/probability/notes.html\" target=\"other\">Notes &amp; Handouts</a>\n page has Fitelson\u2019s powerpoint slides for each of his lectures\nand some links to handouts for the course. The\n <a href=\"http://fitelson.org/probability/links.html\" target=\"other\">Links</a>\n page contains links to some useful internet resources."
        ],
        "listed_links": [
            {
                "http://www.iep.utm.edu/c/conf-ind.htm": "Confirmation and Induction"
            },
            {
                "http://fitelson.org/il.pdf": "Inductive Logic"
            },
            {
                "http://www.ditext.com/clay/armendt2.html": "Teaching Theory of Knowledge: Probability and Induction"
            },
            {
                "http://www.cs.ubc.ca/~murphyk/Bayes/Charniak_91.pdf": "Bayesian Networks Without Tears"
            },
            {
                "http://www.princeton.edu/~bayesway/": "Miscellany of Works on Probabilistic Thinking"
            },
            {
                "http://fitelson.org/confirmation/": "Fitelson\u2019s course on Confirmation Theory"
            },
            {
                "http://fitelson.org/confirmation/syllabus.html": "Syllabus"
            },
            {
                "http://fitelson.org/confirmation/notes.html": "Notes, Handouts, & Links"
            },
            {
                "http://fitelson.org/probability/": "Fitelson\u2019s course on Probability and Induction"
            },
            {
                "http://fitelson.org/probability/syllabus.html": "Syllabus"
            },
            {
                "http://fitelson.org/probability/notes.html": "Notes & Handouts"
            },
            {
                "http://fitelson.org/probability/links.html": "Links"
            }
        ]
    }
}