{
    "url": "risk",
    "title": "Risk",
    "authorship": {
        "year": "Copyright \u00a9 2022",
        "author_text": "Sven Ove Hansson\n<soh@kth.se>",
        "author_links": [
            {
                "http://people.kth.se/~soh/": "Sven Ove Hansson"
            },
            {
                "mailto:soh%40kth%2ese": "soh@kth.se"
            }
        ],
        "raw_html": "<div id=\"article-copyright\">\n<p>\n<a href=\"../../info.html#c\">Copyright \u00a9 2022</a> by\n\n<br/>\n<a href=\"http://people.kth.se/~soh/\" target=\"other\">Sven Ove Hansson</a>\n&lt;<a href=\"mailto:soh%40kth%2ese\"><em>soh<abbr title=\" at \">@</abbr>kth<abbr title=\" dot \">.</abbr>se</em></a>&gt;\n    </p>\n</div>"
    },
    "pubinfo": [
        "First published Tue Mar 13, 2007",
        "substantive revision Thu Dec 8, 2022"
    ],
    "preamble": "\n\nSince the 1970s, studies of risk have grown into a major\ninterdisciplinary field of research. Although relatively few\nphilosophers have focused their work on risk, there are important\nconnections between risk studies and several philosophical\nsubdisciplines. This entry summarizes the most well-developed of these\nconnections and introduces some of the major topics in the philosophy\nof risk. It consists of seven sections dealing with the definition of\nrisk and with treatments of risk related to epistemology, the\nphilosophy of science, the philosophy of technology, ethics, decision\ntheory, and the philosophy of economics.\n",
    "toc": [
        {
            "#DefiRisk": "1. Defining risk"
        },
        {
            "#Epis": "2. Epistemology"
        },
        {
            "#PhilScie": "3. Philosophy of science"
        },
        {
            "#PhilTech": "4. Philosophy of technology"
        },
        {
            "#Ethi": "5. Ethics"
        },
        {
            "#DiffForMoraTheo": "5.1 A difficulty for moral theories"
        },
        {
            "#Util": "5.2 Utilitarianism"
        },
        {
            "#RighBaseMoraTheo": "5.3 Rights-based moral theories"
        },
        {
            "#DeonMoraTheo": "5.4 Deontological moral theories"
        },
        {
            "#ContTheo": "5.5 Contract theories"
        },
        {
            "#SummOutl": "5.6 Summary and outlook"
        },
        {
            "#DeciTheo": "6. Decision theory"
        },
        {
            "#DeciWeig": "6.1 Decision weights"
        },
        {
            "#PessCautPrecPrin": "6.2 Pessimism, cautiousness and the precautionary principle"
        },
        {
            "#RiskEconAnal": "7. Risk in economic analysis"
        },
        {
            "#MeasEconRisk": "7.1 Measures of economic risks"
        },
        {
            "#MeasAttiRisk": "7.2 Measures of attitudes to risks"
        },
        {
            "#ExpeEcon": "7.3 Experimental economics"
        },
        {
            "#RiskBeneAnal": "7.4 Risk-benefit analysis"
        },
        {
            "#Bib": "Bibliography"
        },
        {
            "#Aca": "Academic Tools"
        },
        {
            "#Oth": "Other Internet Resources"
        },
        {
            "#Rel": "Related Entries"
        }
    ],
    "main_text": "\n1. Defining risk\n\nIn non-technical contexts, the word \u201crisk\u201d refers, often\nrather vaguely, to situations in which it is possible but not certain\nthat some undesirable event will occur. In technical contexts, the\nword has several more specialized uses and meanings. Five of these are\nparticularly important since they are widely used across\ndisciplines:\n\nrisk = an unwanted event that may or may not occur.\n\n\nAn example of this usage is: \u201cLung cancer is one of the major\nrisks that affect smokers.\u201d \n\nrisk = the cause of an unwanted event that may or may not\noccur.\n\n\nAn example of this usage is: \u201cSmoking is by far the most\nimportant health risk in industrialized countries.\u201d (The\nunwanted event implicitly referred to here is a disease caused by\nsmoking.) Both (1) and (2) are qualitative senses of risk. The word\nalso has quantitative senses, of which the following is the oldest\none:\n\nrisk = the probability of an unwanted event that may or\nmay not occur.\n\n\nThis usage is exemplified by the following statement: \u201cThe risk\nthat a smoker\u2019s life is shortened by a smoking-related disease\nis about 50%.\u201d \n\nrisk = the statistical expectation value of an unwanted\nevent that may or may not occur.\n\n\nThe expectation value of a possible negative event is the product of\nits probability and some measure of its severity. It is common to use\nthe number of killed persons as a measure of the severity of an\naccident. With this measure of severity, the \u201crisk\u201d (in\nsense 4) associated with a potential accident is equal to the\nstatistically expected number of deaths. Other measures of severity\ngive rise to other measures of risk.\n\nAlthough expectation values have been calculated since the\n17th century, the use of the term \u201crisk\u201d in\nthis sense is relatively new. It was introduced into risk analysis in\nthe influential Reactor Safety Study, WASH-1400 (Rasmussen et al.,\n1975, Rechard 1999). Today it is the standard technical meaning of the\nterm \u201crisk\u201d in many disciplines. It is regarded by some\nrisk analysts as the only correct usage of the term.\n\nrisk = the fact that a decision is made under conditions of\nknown probabilities (\u201cdecision under risk\u201d as\nopposed to \u201cdecision under uncertainty\u201d)\n\n\nIn addition to these five common meanings of \u201crisk\u201d there\nare several other more technical meanings, which are well-established\nin specialized fields of inquiry. Some of the major definitions of\nrisk that are used in economic analysis will be introduced below in\nsection 7.1.\n\nAlthough most of the above-mentioned meanings of \u201crisk\u201d\nhave been referred to by philosophers, a large part of the\nphilosophical literature on risk refers to risk in the more informal\nsense that was mentioned at the beginning of this section, namely as a\nstate of affairs in which an undesirable event may or may not occur.\nSeveral philosophers have criticized the technical definitions of risk\nfor being too limited and not covering all aspects that should be\nincluded in risk assessments (Buchak 2014; Pritchard 2015;\nShrader-Frechette 1991). Linguistic evidence indicates that technical\ndefinitions of risk have had virtually no impact on the non-technical\nusage of the word (Boholm et al. 2016).\n\nTerminological note: Some philosophers distinguish between\n\u201csubjective\u201d and \u201cobjective\u201d probabilities.\nOthers reserve the term \u201cprobability\u201d for the subjective\nnotion. Here, the former terminology is used, i.e.\n\u201cprobability\u201d can refer either to subjective probability\nor to objective chances.\n2. Epistemology\n\nWhen there is a risk, there must be something that is unknown or has\nan unknown outcome. Therefore, knowledge about risk is knowledge about\nlack of knowledge. This combination of knowledge and lack thereof\ncontributes to making issues of risk complicated from an\nepistemological point of view.\n\nIn non-regimented usage, \u201crisk\u201d and\n\u201cuncertainty\u201d differ along the subjective\u2014objective\ndimension. Whereas \u201cuncertainty\u201d seems to belong to the\nsubjective realm, \u201crisk\u201d has a strong objective component.\nIf a person does not know whether or not the grass snake is poisonous,\nthen she is in a state of uncertainty with respect to its ability to\npoison her. However, since this species has no poison there is no risk\nto be poisoned by it. The relationship between the two concepts\n\u201crisk\u201d and \u201cuncertainty\u201d seems to be in part\nanalogous to that between \u201ctruth\u201d and\n\u201cbelief\u201d.\n\nRegimented decision-theoretical usage differs from this. In decision\ntheory, a decision is said to be made \u201cunder risk\u201d if the\nrelevant probabilities are available and \u201cunder\nuncertainty\u201d if they are unavailable or only partially\navailable. Partially determined probabilities are sometimes expressed\nwith probability intervals, e.g., \u201cthe probability of rain\ntomorrow is between 0.1 and 0.4\u201d. (The term \u201cdecision\nunder ignorance\u201d is sometimes used about the case when no\nprobabilistic information at all is available.) \n\nAlthough this distinction between risk and uncertainty is\ndecision-theoretically useful, from an epistemological point of view\nit is in need of clarification. Only very rarely are probabilities\nknown with certainty. Strictly speaking, the only clear-cut cases of\n\u201crisk\u201d (known probabilities) seem to be idealized textbook\ncases that refer to devices such as dice or coins that are supposed to\nbe known with certainty to be fair. In real-life situations, even if\nwe act upon a determinate probability estimate, we are not fully\ncertain that this estimate is exactly correct, hence there is\nuncertainty. It follows that almost all decisions are made\n\u201cunder uncertainty\u201d. If a decision problem is treated as a\ndecision \u201cunder risk\u201d, then this does not mean that the\ndecision in question is made under conditions of completely known\nprobabilities. Rather, it means that a choice has been made to\nsimplify the description of this decision problem by treating it as a\ncase of known probabilities. This is often a highly useful\nidealization in decision theory. However, in practical applications it\nis important to distinguish between those probabilities that can be\ntreated as known and those that are uncertain and therefore much more\nin need of continuous updating. Typical examples of the former are the\nfailure frequencies of a technical component that are inferred from\nextensive and well-documented experience of its use. The latter case\nis exemplified by experts\u2019 estimates of the expected failure\nfrequencies of a new type of component.\n\nA major problem in the epistemology of risk is how to deal with the\nlimitations that characterize our knowledge of the behaviour of unique\ncomplex systems that are essential for estimates of risk, such as the\nclimate system, ecosystems, the world economy, etc. Each of these\nsystems contains so many components and potential interactions that\nimportant aspects of it are unpredictable. However, in spite of this\nuncertainty, reasonably reliable statements about many aspects of\nthese systems can be made. Anthropogenic climate change is an example\nof this. Although many details are unknown, the general picture is\nclear, and there is no reasonable doubt about the existence of\nanthropogenic climate change, about its major causes and mechanisms,\nor the overall nature of the risks that it creates for our societies\n(Mayer et al. 2017; Lewandowsky et al. 2018). The epistemological\nstatus of such partial knowledge about complex systems, and the nature\nof the uncertainty involved, are still in need of further\nclarification (McKinney 1996, Shrader-Frechette 1997).\n\nIn the risk sciences, it is common to distinguish between\n\u201cobjective risk\u201d and \u201csubjective risk\u201d. The\nformer concept is in principle fairly unproblematic since it refers to\na frequentist interpretation of probability. The latter concept is\nmore ambiguous. In the psychometric literature on risk from the 1970s,\nsubjective risk was often conceived as a subjective estimate of\nobjective risk. In more recent literature, a more complex picture has\nemerged. Subjective appraisals of (the severity of) risk depend to a\nlarge extent on factors that are not covered in traditional measures\nof objective risk, such as control and tampering with nature. If the\nterms are taken in this sense, subjective risk is influenced by the\nsubjective estimate of objective risk, but cannot be identified with\nit. In the psychological literature, subjective risk is often\nconceived as the individual\u2019s overall assessment of the\nseriousness of a danger or alleged danger. Such individual assessments\nare commonly called \u201crisk perception\u201d, but strictly\nspeaking the term is misleading. This is not a matter of perception,\nbut rather a matter of attitudes and expectations. Subjective risk can\nbe studied with methods of attitude measurement and psychological\nscaling (Sj\u00f6berg 2004).\n\nThe probabilistic approach to risk dominates in philosophy as well as\nin other disciplines, but some philosophers have investigated an\nalternative, modal account of risk. According to this account,\n\u201c[t]o say that a target event is risky is to say that (keeping\nrelevant initial conditions for that event fixed) it obtains in close\npossible worlds\u201d (Pritchard 2016, 562). The risk is smaller, the\nmore distant the nearest possible world is in which the target event\ntakes place. This approach has interesting connections with\npsychological accounts of risk, but it is far from clear how distance\nbetween possible worlds should be defined and determined.\n3. Philosophy of science\n\nThe role of values in science has been particularly controversial in\nissues of risk. Risk assessments have frequently been criticized for\ncontaining \u201chidden\u201d values that induce a too high\nacceptance of risk (Cranor 2016; 2017; Intemann 2016; Heinzerling\n2000). There is also a discussion on the need to strengthen the impact\nof certain values in risk assessment, such as considerations of\njustice (Shrader-Frechette 2005a), human rights (Shrader-Frechette\n2005b), and the rights and welfare of future people (Caney 2009; Ng\n2005). Issues of risk have also given rise to heated debates on what\nlevels of scientific evidence are needed for policy decisions. The\nproof standards of science are apt to cause difficulties whenever\nscience is applied to practical problems that require standards of\nproof or evidence different from those of science. \n\nA decision to accept or reject a scientific statement (for instance an\nhypothesis) is in practice always subject to the possibility of error.\nThe chance of such an error is often called the inductive risk (Hempel\n1965, 92). There are two major types of errors. The first of these\nconsists in concluding that there is a phenomenon or an effect when in\nfact there is none. This is called an error of type I (false\npositive). The second consists in missing an existing phenomenon or\neffect. This is called an error of type II (false negative). In the\ninternal dealings of science, errors of type I are in general regarded\nas more problematic than those of type II. The common scientific\nstandards of statistical significance substantially reduce the risk of\ntype I errors but do not protect against type II errors\n(Shrader-Frechette 2008; John 2017).\n\nMany controversies on risk assessment concern the balance between\nrisks of type I and type II errors. Whereas science gives higher\npriority to avoiding type I errors than to avoiding type II errors,\nthe balance can shift when errors have practical consequences. This\ncan be seen from a case in which it is uncertain whether there is a\nserious defect in an airplane engine. A type II error, i.e., acting as\nif there were no such a defect when there is one, would in this case\nbe counted as more serious than a type I error, i.e., acting as if\nthere were such a defect when there is none. (The distinction between\ntype I and type II errors depends on the delimitation of the effect\nunder study. In discussions of risk, this delimitation is mostly\nuncontroversial. Lemons et al. 1997; van den Belt and Gremmen\n2002.)\n\nIn this particular case it is fairly uncontroversial that avoidance of\ntype II error should be given priority over avoidance of type I error.\nIn other words, it is better to delay the flight and then find out\nthat the engine was in good shape than to fly with an engine that\nturns out to malfunction. However, in other cases the balance between\nthe two error types is more controversial. Controversies are common,\nfor instance, over what degree of evidence should be required for\nactions against possible negative effects of chemical substances on\nhuman health and the environment.\n\n\n\nFigure 1. The use of scientific data for policy purposes.\n\n\nSuch controversies can be clarified with the help of a simple but\nillustrative model of how scientific data influence both scientific\njudgments and practical decisions (Hansson 2008). Scientific knowledge\nbegins with data that originate in experiments and other observations.\n(See Figure 1.) Through a process of critical assessment, these data\ngive rise to the scientific corpus (arrow 1). Roughly speaking, the\ncorpus consists of those statements that could, for the time being,\nlegitimately be made without reservation in a (sufficiently detailed)\ntextbook. When determining whether or not a scientific hypothesis\nshould be accepted, for the time being, as part of the corpus, the\nonus of proof falls on its adherents. Similarly, those who claim the\nexistence of an as yet unproven phenomenon have the burden of proof.\nThese proof standards are essential for the integrity of science. \n\nThe most obvious way to use scientific information for policy-making\nis to employ information from the corpus (arrow 2). For many purposes,\nthis is the only sensible thing to do. However, in risk management\ndecisions, exclusive reliance on the corpus may have unwanted\nconsequences. Suppose that toxicological investigations are performed\non a substance that has not previously been studied from a\ntoxicological point of view. These investigations turn out to be\ninconclusive. They give rise to science-based suspicions that the\nsubstance is dangerous to human health, but they do not amount to full\nscientific proof in the matter. Since the evidence is not sufficient\nto warrant an addition to the scientific corpus, this information\ncannot influence policies in the standard way (via arrows 1 and 2).\nThere is a strict requirement to avoid type I errors in the process\nrepresented by arrow 1, and this process filters out information that\nmight in this case have been practically relevant and justified\ncertain protective measures.\n\nIn cases like this, a direct road from data to policies is often taken\n(arrow 3). This means that a balance between type I and type II errors\nis determined in the particular case, based on practical\nconsiderations, rather than relying on the standard scientific\nprocedure with its strong emphasis on the avoidance of type I\nerrors.\n\nIt is essential to distinguish here between two kinds of risk-related\ndecision processes. One consists in determining which statements about\nrisks should be included in the scientific corpus. The other consists\nin determining how risk-related information should influence practical\nmeasures to protect health and the environment. It would be a strange\ncoincidence if the criteria of evidence in these two types of\ndecisions were always the same. Strong reasons can be given for strict\nstandards of proof in science, i.e. high entry requirements for the\ncorpus. At the same time, there can be valid policy reasons to allow\nrisk management decisions to be influenced by scientifically plausible\nindications of danger that are not yet sufficiently well-confirmed to\nqualify for inclusion into the scientific corpus. \n\nThe term inductive risk is usually reserved for the (type I\nand type II) risks that follow directly from the acceptance or\nrejection of an hypothesis. The term epistemic risk is used\nfor a wider category of risks in belief formation, such as risks taken\nwhen choosing a methodology, accepting a background assumption, or\ndeciding how to interpret data (Biddle 2016).\n\nPolicy issues concerning risk have often been the targets of extensive\ndisinformation campaigns characterized by science denial and other\nforms of pseudoscience (Oreskes 2010). Several philosophers have been\nactive in the repudiation of invalid claims and the defence of science\nin risk-related issues (Cranor 2005; 2016; 2017; Goodwin 2009;\nProthero 2013; Shrader-Frechette 2014; Hansson 2017).\n4. Philosophy of technology\n\nSafety and the avoidance of risk are major concerns in practical\nengineering. Safety engineering has also increasingly become the\nsubject of academic investigations. However, these discussions are\nlargely fragmented between different areas of technology. The same\nbasic ideas or \u201csafety philosophies\u201d are discussed under\ndifferent names for instance in chemical, nuclear, and electrical\nengineering. Nevertheless, much of the basic thinking seems to be the\nsame in the different areas of safety engineering (M\u00f6ller and\nHansson 2008). \n\nSimple safety principles, often expressible as rules of thumb, have a\ncentral role in safety engineering. Three of the most important of\nthese are inherent safety, safety factors, and multiple barriers.\n\nInherent safety, also called primary prevention, consists in the\nelimination of a hazard. It is contrasted with secondary prevention\nthat consists in reducing the risk associated with a hazard. For a\nsimple example, consider a process in which inflammable materials are\nused. Inherent safety would consist in replacing them by\nnon-inflammable materials. Secondary prevention would consist in\nremoving or isolating sources of ignition and/or installing\nfire-extinguishing equipment. As this example shows, secondary\nprevention usually involves added-on safety equipment. The major\nreason to prefer inherent safety to secondary prevention is that as\nlong as the hazard still exists, it can be realized by some\nunanticipated triggering event. Even with the best of control\nmeasures, if inflammable materials are present, some unforeseen chain\nof events can start a fire. \n\nSafety factors are numerical factors employed in the design process\nfor our houses, bridges, vehicles, tools, etc., in order to ensure\nthat our constructions are stronger than the bare minimum expected to\nbe required for their functions. Elaborate systems of safety factors\nhave been specified in norms and standards. A safety factor most\ncommonly refers to the ratio between a measure of the maximal load not\nleading to a specified type of failure and a corresponding measure of\nthe maximal expected load. It is common to make bridges and other\nconstructions strong enough to withstand twice or three times the\npredicted maximal load. This means that a safety factor of two or\nthree is employed.\n\nSafety factors are also used in regulatory toxicology and\necotoxicology. For instance, in food toxicology, the highest dose\nallowed for human exposure has traditionally been calculated as one\nhundredth of the highest dose (per kilogram body weight) that gave no\nobservable negative effect in experimental animals. Today, higher\nsafety factors than 100 have become common (Dourson and Stara 1983;\nPressman et al. 2017).\n\nSafety barriers are often arranged in chains. Ideally, each barrier is\nindependent of its predecessors so that if the first fails, then the\nsecond is still intact, etc. For example, in an ancient fortress, if\nthe enemy managed to pass the first wall, then additional layers would\nprotect the defending forces. Some engineering safety barriers follow\nthe same principle of concentric physical barriers. Others are\narranged serially in a temporal or functional rather than a spatial\nsense. One of the lessons that engineers learned from the\nTitanic disaster is that improved construction of early\nbarriers is not of much help if it leads to neglect of the later\nbarriers (in that case lifeboats).\n\nThe major problem in the construction of safety barriers is how to\nmake them as independent of each other as possible. If two or more\nbarriers are sensitive to the same type of impact, then one and the\nsame destructive force can get rid of all of them in one swoop. For\ninstance, if three safety valves are installed in one and the same\nfactory hall, each with the probability 1/1,000 of failure, it does\nnot follow that the probability of all three failing is \\(1 \\times\n10^{-9}\\). The three valves may all be destroyed in the same fire, or\ndamaged by the same mistake in maintenance operations. This is a\ncommon situation for many types of equipment. \n\nInherent safety, safety factors, and multiple barriers have an\nimportant common feature: They all aim at protecting us not only\nagainst risks that can be assigned meaningful probability estimates,\nbut also against dangers that cannot be probabilized, such as the\npossibility that some unanticipated type of event gives rise to an\naccident. It remains, however, for philosophers of technology to\ninvestigate the principles underlying safety engineering more in\ndetail and to clarify how they relate to other principles of\nengineering design (Doorn and Hansson 2015). \n\nMany attempts have been made to predict the risks associated with\nemerging and future technologies. The role of philosophers in these\nendeavours has often been to point out the difficulties and\nuncertainties involved in such predictions (Allhoff 2009; Gordijn\n2005). Experience shows that even after extensive efforts to make a\nnew product safe, there is a need for post market surveillance (PMS)\nin order to discover unexpected problems. For instance, before the\nmassive introduction of automobile air bags around 1990, safety\nengineers performed laboratory tests of different crash scenarios with\ndummies representing a variety of body weights and configurations\n(including infants and pregnant women). But in spite of the\nadjustments of the construction that these tests gave rise to,\ninflated airbags caused a considerable number of (mostly minor)\ninjuries. By carefully analyzing experiences from actual accidents,\nengineers were able to substantially reduce the frequency and severity\nof such injuries (Wetmore 2008). For pharmaceutical drugs and some\nmedical devices, post market surveillance is legally required in many\njurisdictions.\n5. Ethics\n5.1 A difficulty for moral theories\n\nUntil recently, problems of risk have not been treated systematically\nin moral philosophy. A possible defence of this limitation is that\nmoral philosophy can leave it to decision theory to analyse the\ncomplexities that indeterminism and lack of knowledge give rise to in\nreal life. According to the conventional division of labour between\nthe two disciplines, moral philosophy provides assessments of human\nbehaviour in well-determined situations. Decision theory takes\nassessments of these cases for given, adds the available probabilistic\ninformation, and derives assessments for rational behaviour in an\nuncertain and indeterministic world. On this view, no additional input\nof moral values is needed to deal with indeterminism or lack of\nknowledge, since decision theory operates exclusively with criteria of\nrationality. \n\nExamples are easily found that exhibit the problematic nature of this\ndivision between the two disciplines. Compare the act of throwing down\na brick on a person from a high building to the act of throwing down a\nbrick from a high building without first making sure that there is\nnobody beneath who can be hit by the brick. The moral difference\nbetween these two acts is not obviously expressible in a probability\ncalculus. An ethical analysis of the difference will have to refer to\nthe moral aspects of risk impositions as compared to intentional\nill-doing. More generally speaking, a reasonably complete account of\nthe ethics of risk must distinguish between intentional and\nunintentional risk exposure and between voluntary risk-taking, risks\nimposed on a person who accepts them, and risks imposed on a person\nwho does not accept them. This cannot be done in a framework that\ntreats risks as probabilistic mixtures of outcomes. In principle,\nthese outcomes can be so widely defined that they include all relevant\nmoral aspects, including rights infringements as well as\nintentionality and other pertinent mental states. However, this would\nstill not cover the moral implications of risk taking per se,\nsince these are not inherent properties of any of the potential\noutcomes. \n\nMethods of moral analysis are needed that can guide decisions on\nrisk-takings and risk impositions. A first step is to investigate how\nstandard moral theories can deal with problems of risk that are\npresented in the same way as in decision theory, namely as the (moral)\nevaluation of probabilistic mixtures of (deterministic) scenarios.\n5.2 Utilitarianism\n\nIn utilitarian theory, there are two obvious approaches to such\nproblems. One is the actualist solution. It consists in\nassigning to a (probabilistic) mixture of potential outcomes a utility\nthat is equal to the utility of the outcome that actually\nmaterializes. To exemplify this approach, consider a decision whether\nor not to reinforce a bridge before it is used for a single, very\nheavy transport. There is a 50% risk that the bridge will fall down if\nit is not reinforced. Suppose that a decision is made not to reinforce\nthe bridge and that everything goes well; the bridge is not damaged.\nAccording to the actualist approach, the decision was right. This is,\nof course, contrary to common moral intuitions.\n\nThe other established utilitarian approach is the maximization of\nexpected utility. This means that the utility of a mixture of\npotential outcomes is defined as the probability-weighted average of\nthe utilities of these outcomes.\n\nThe expected utility criterion has been criticized along several\nlines. One criticism is that it disallows a common form of\ncautiousness, namely disproportionate avoidance of large disasters.\nFor example, provided that human deaths are valued equally and\nadditively, as most utilitarians are prone to do, this framework does\nnot allow that one prefers a probability of 1 in 1000 that one person\nwill die to a probability of 1 in 100000 that fifty persons will die.\nThe expected utility framework can also be criticized for disallowing\na common expression of strivings for fairness, namely disproportionate\navoidance of high-probability risks for particular individuals. Hence,\nin the choice between exposing one person to a probability of 0.9 to\nbe killed and exposing each of one hundred persons to a probability of\n0.01 of being killed, it requires that the former alternative be\nchosen. In summary, expected utility maximization prohibits what seem\nto be morally reasonable standpoints on risk-taking and risk\nimposition.\n\nHowever, it should be noted that the expected utility criterion does\nnot necessarily follow from utilitarianism. Utilitarianism in a wide\nsense (Scanlon 1982) is compatible with other ways of evaluating\nuncertain outcomes (most notably with actual consequence\nutilitarianism, but in principle also for instance with a maximin\ncriterion). Therefore, criticism directed against expected utility\nmaximization does not necessarily show a defect in utilitarian\nthinking.\n5.3 Rights-based moral theories\n\nThe problem of dealing with risk in rights-based moral theories was\nformulated by Robert Nozick: \u201cImposing how slight a probability\nof a harm that violates someone\u2019s rights also violates his\nrights?\u201d (Nozick 1974, 74).\n\nAn extension of a rights-based moral theory to indeterministic cases\ncan be obtained by prescribing that if A has a right that\nB does not bring about a certain outcome, then A\nalso has a right that B does not perform any action that (at\nall) increases the probability of that outcome. Unfortunately, such a\nstrict extension of rights is untenable in social practice.\nPresumably, A has the right not to be killed by B,\nbut it would not be reasonable to extend this right to all actions by\nB that give rise to a very small increase in the risk that\nA dies \u2014 such as driving a car in the town where\nA lives. Such a strict interpretation would make human\nsociety impossible. \n\nHence, a right not to be risk-exposed will have to be defeasible so\nthat it can be overridden in some (but not necessarily all) cases when\nthe increase in probability is small. However, it remains to find a\ncredible criterion for when it should be overridden. As Nozick\nobserved, a probability limit is not credible in \u201ca tradition\nwhich holds that stealing a penny or a pin or anything from someone\nviolates his rights. That tradition does not select a\nthreshold measure of harm as a lower limit, in the case of harms\ncertain to occur\u201d (Nozick 1974, 75).\n5.4 Deontological moral theories\n\nThe problem of dealing with risks in deontological theories is similar\nto the corresponding problem in rights-based theories. The duty not to\nharm other people can be extended to a duty not to perform actions\nthat increase their risk of being harmed. However, society as we know\nit is not possible without exceptions to this rule. The determination\nof criteria for such exceptions is problematic in the same way as for\nrights-based theories. All reasonable systems of moral obligations\nwill contain a general prohibition against actions that kill another\nperson. Such a prohibition can (and should) be extended to actions\nthat involve a large risk that a person is killed. However, it cannot\nbe extended to all actions that lead to a minuscule increase in the\nrisk that a person is killed, since that would exclude many actions\nand behaviours that few of us would be willing to give up. A limit\nmust be drawn between reasonable and unreasonable impositions of risk.\nIt seems as if such delimitations will have to appeal to concepts,\nsuch as probabilities and/or the size of the benefits obtained by\ntaking a risk, that are not part of the internal resources of\ndeontological theories.\n5.5 Contract theories\n\nContract theories may appear somewhat more promising than the theories\ndiscussed above. The criterion that they offer for the deterministic\ncase, namely consent among all those involved, can also be applied to\nrisky options. It could be claimed that risk impositions are\nacceptable if and only if they are supported by a consensus. Such a\nconsensus, as conceived in contract theories, is either actual or\nhypothetical. \n\nActual consensus is unrealistic in a complex society where everyone\nperforms actions with marginal but additive effects on many\npeople\u2019s lives. According to the criterion of actual consensus,\nany local citizen will have a veto against anyone else who wants to\ndrive a car in the town where she lives. In this way citizens can\nblock each other, creating a society of stalemates. \n\nHypothetical consensus has been developed as a criterion in contract\ntheory in order to deal with inter-individual problems. We are invited\nto consider a hypothetical initial situation in which the social order\nof a future society has not yet been decided. When its future citizens\nmeet to choose a social order, each of them is ignorant of her or his\nposition in any of the social arrangements which they can choose\namong. According to John Rawls\u2019s theory of justice, they will\nthen all opt for a maximin solution, i.e. a social order in which the\nworst position that anyone can have in that society is as good as\npossible. In arguing for that solution, Rawls relied heavily on the\nassumption that none of the participants knows anything at all about\nthe probability that she will end up in one or other of the positions\nin a future social order (Rawls 1957; 1971; 1974). John Harsanyi, who\ndiscussed this problem prior to Rawls, assumed that the probability of\nfinding oneself in a particular social position is equal to the share\nof the population that will have the position in question, and that\nthis is also known by all participants. Hence, if a fifth of the\npopulation in a certain type of society will be migrant workers, then\neach participant in Harsanyi\u2019s initial situation will assume\nthat she has a twenty per cent probability of becoming a migrant\nworker, whereas none of the participants in Rawls\u2019s initial\nsituation will have a clue what that probability can be. In\nHarsanyi\u2019s initial situation, the participants will choose the\nsocial order with the highest expected utility (probability-weighted\nutility), thus taking all potential future positions into account,\nrather than only the least favourable one (Harsanyi 1953; 1955;\n1975).\n\nHowever, in discussions about various risks in our existing societies\nwe do not have much use for the hypothetical initial situations of\ncontract theory. The risks and uncertainties in real life are of quite\na different nature than the hypothetical uncertainty (or ignorance)\nabout one\u2019s own social position and conditions which is a\ncrucial requirement in the initial situation. The thought experiment\nof an initial situation does not seem to provide us with any\nintellectual tools for the moral appraisal of risks in addition to\nthose to which we have access even without trying to think away who we\nare.\n5.6 Summary and outlook\n\nIn summary, the problem of appraising risks from a moral point of view\ndoes not seem to have any satisfactory solution in the common versions\nof the above-mentioned types of moral theories. The following are\nthree possible elements of a solution:\n\nIt may be useful to shift the focus from risks, described\ntwo-dimensionally in terms of probability and severity (or\none-dimensionally as the product of these), to actions of risk-taking\nand risk-imposing. Such actions have many morally relevant properties\nin addition to the two dimensions mentioned, such as who contributes\ncausally to the risk and in what ways and with what intentions, and\nhow the risk and its associated benefits are distributed.\nImportant moral intuitions are accounted for by assuming that each\nperson has a prima facie moral right not to be exposed to risk of\nnegative impact, such as damage to her health or her property, through\nthe actions of others. However, this is a prima facie right that has\nto be overridden in quite a few cases, in order to make social life at\nall possible. Therefore, the recognition of this right gives rise to\nwhat can be called an exemption problem, namely the problem\nof determining when it is rightfully overridden. \nPart of the solution to the exemption problem may be obtained by\nallowing for reciprocal exchanges of risks and benefits. Hence, if\nA is allowed to drive a car, exposing B to certain\nrisks, then in exchange B is allowed to drive a car, exposing\nA to the corresponding risks. In order to deal with the\ncomplexities of modern society, this principle must also be applied to\nexchanges of different types of risks and benefits. Exposure of a\nperson to a risk can then be regarded as acceptable if it is part of\nan equitable social system of risk-taking that works to her advantage.\nSuch a system can be required to contain mechanisms that eliminate, or\ncompensate for, social inequalities that are caused by disease and\ndisability. (Hansson 2003; 2013)\n\n\nDiscussions on the overall issue of risk acceptance can be found in\nMacpherson (2008), Hansson (2013) and Oberdiek (2014). Justice in risk\nimpositions is discussed in Ferretti 2010 and Heyward & Roser\n2016. Issues of rights and risks are discussed in Thomson 1986 and,\nwith a particular emphasis on responsibilities, in Kermisch 2012 and\nvan de Poel, et al. 2012.\n6. Decision theory\n\nDecision theory is concerned with determining the best way to achieve\nas valuable an outcome as possible, given the values that we have. In\ndecision theory, our values and goals are taken as given, and the\nanalysis concerns how best to achieve them to an as high degree as\npossible. Decision-making under risk and uncertainty is one of the\nmajor topics in decision theory. It is usually assumed that if the\nvalues of a set of potential outcomes are known (for instance from\nmoral philosophy), then purely instrumental considerations are\nsufficient for determining how best to act under risk or uncertainty\nin order to achieve the best possible result. (For a critical\ndiscussion of that presumption, see Hansson 2013, 49\u201351.) The\nvalues taken for given in decision-theoretical analysis can, but need\nnot, be moral values of the types that are developed and analyzed in\nmoral philosophy.\n6.1 Decision weights\n\nDecision theory has traditionally had a predilection for\nconsequentialism, whose structure is suitable for most formal models of\ndecision-making. The standard decision-theoretical approach to risk is\nmaximization of expected utility, which can be seen as a smooth\nextension of (act) utilitarianism. In expected utility theory, the value associated with an uncertain situation is equal to the sum of the probability-weighted values of all its possible outcomes. Let \\(p\\) be a function that assigns probabilities to outcomes, and \\(u\\) a function that assigns values to them. Then the value associated with a situation with three possible outcomes \\(x_1\\), \\(x_2\\) and \\(x_3\\), is equal to\n\n\\[p(x_1) \\cdot u(x_1) + p(x_2) \\cdot u(x_2)+ p(x_3) \\cdot u(x_3).\\]\n\n\nHowever, influential proposals have been made for alternative decision\nrules. There are two major types of justification for such endeavours.\nFirst, examples have been put forward in which it would seem\nimplausible to claim that expected utility maximization is the only\nnormatively reasonable decision rule (Allais 1953; Ellsberg 1961).\nSecondly, numerous psychological experiments have shown that human\ndecision-makers tend to deviate substantially from expected utility\nmaximization. The first type of justification puts the normative\nsoundness of expected utility in question, where the second exposes\nits shortcomings as a descriptive model. \n\nIn an important class of alternative decision rules, the probabilities\nused in expected utility calculations are replaced by some other\nnumbers (\u201cdecision weights\u201d). This approach was proposed\nby William Fellner (1961). In most of these constructions, all\nprobabilities are transformed by some transformation function\nr. Instead maximizing the standard expected utility\n\n\\[p(x) \\cdot u(x)\\]\n the agent will then maximize \n\\[r(p(x)) \\cdot\nu(x)\\]\n\n\nSeveral decision rules with this structure have been proposed. One of\nthe earliest was Handa (1977). Currently, the best known proposal in\nthis tradition is prospect theory (Kahneman and Tversky 1979; Tversky\nand Kahneman 1986), which was developed in order to describe\nobservations from psychological decision experiments more accurately\nthan in expected utility theory. Prospect theory a is a fairly complex\ntheory that also deviates in other ways from expected utility theory.\nThe traditional focus on outcomes is replaced by a focus on losses and\ngains, which are treated asymmetrically. (See also Section 7.3.)\n\nA problem with the function \\(r\\), as defined above, is that the\ntransformed probabilities which it gives rise to will not add up to 1\nexcept in the trivial case when r is the identity function\n(Fishburn 1978). To solve this problem, Quiggin (1982) introduced the\nrule of maximizing anticipated utility (also called utility\nwith rank dependent probabilities). Instead of replacing \\(p(x)\\) by a\nfunction of the individual probability, \\(r(p(x))\\), he replaced it by\na function that also depends on the other probabilities and utilities\ninvolved in the problem. The outcomes are first ordered from worst to\nbest, which results in a vector \\(\\langle x_1, x_2 ,\\ldots ,\nx_n\\rangle\\) of outcomes, such that \\(u(x_1) \\leq u(x_2) \\leq \\ldots\n\\leq u(x_n)\\). A decision weight can then be assigned to each outcome,\ntaking into account both its probability and its position in the\nranked sequence of outcomes. Since the decision-weight can be\ndifferent for outcomes with the same probability, Fishburn\u2019s\ntrivialization result does not apply here. There is evidence\nindicating that rank-dependent utility models may be more empirically\nadequate than prospect theory (Harrison and Ross 2017).\n\nOther models have been proposed that replace the probabilities in\nexpected utility maximization by some other type of decision weight\n(Gilboa and Schmeidler 1994; Buchak 2014).\n6.2 Pessimism, cautiousness and the precautionary principle\n\nIn decision-theoretical analysis of risk, it is important to\ndistinguish between two pairs of concepts that are both related to\nrisk: Pessimism vs. optimism and cautiousness vs. risk-taking.\n\nPessimism and optimism concerns a person\u2019s patterns of\nvalue-related beliefs about future events. A person is pessimistic\nabout some future development to the extent that she assigns a higher\nprobability to undesired outcomes, and a lower probability to desired\nones, than the most plausible assignment. She is optimistic to the\nextent that she assigns higher probabilities to desired outcomes, and\nlower probabilities to undesired ones, than the most plausible\nassignment.\n\nCautiousness (risk aversion) and risk-seeking (risk affinity) concern\na person\u2019s decisions and actions, rather than her beliefs. When\nmaking a choice between different action alternatives, a cautious\n(risk-averse) person tends to choose an alternative with as small\nprobabilities as possible for the worst outcomes. She does so even if\nit prevents her from maximizing the expected utility of her choice. In\ncontrast, a risk-seeking person looks for options with a high\nprobability for the best outcomes, and she does this even at the\nexpense of not maximizing the expected utility of her choice. (See\nalso section 7.2.)\n\nThe precautionary principle is often believed to be a general\ninstruction to be cautious. However, this is not what is usually meant\nby the term \u201cthe precautionary principle\u201d. This is a\nprinciple that is defined in international treaties and in European\nlegislation. According to these definitions, the precautionary\nprinciple concerns how we should act when it is scientifically\nreasonable to suspect a risk to health or the environment, but the\nevidence is not strong enough to show conclusively that the risk\nexists. The precautionary principle says that in such cases we may,\nand often should, take measures against the potential danger. This\ndefinition has been the standard at least since 1992, when the\nprecautionary principle was included both in the United Nations\u2019\nRio Declaration and in the European Union\u2019s Treaty of Rome,\nwhich is now called the Treaty on the Functioning of the European\nUnion (Pyh\u00e4l\u00e4 et al., 2010). Its practical application\ndepends on the interpretation of scientific evidence that indicates\nthe presence of a danger, but is not strong enough for scientists to\ntreat the existence of that danger as known. The evaluation of such\nevidence raises methodological and epistemological issues that are in\nneed of further clarification, not least with conceptual tools\ndeveloped in the philosophy of science.\n\nThere is also a still ongoing philosophical discussion about other\ndefinitions of a precautionary principle. Many of these proposals are\ncloser to general (decision-theoretical) cautiousness than the more\nnarrowly defined legal concept (Munthe 2011).\n7. Risk in economic analysis\n\nRisks have a central role in economic activities. In capitalist market\neconomies, taking economic risks is an essential part of the role of\nthe entrepreneur. Decisions on investments and activities on financial\nmarkets can only be understood against the background of the risks\ninvolved. Therefore it is no surprise that modern economic theory,\nwith its emphasis on mathematical models of economic activities, has\ndeveloped several formal models of risk-taking.\n7.1 Measures of economic risks\n\nPortfolio analysis, which was developed in the 1950s by Harry\nMarkowitz (1952), James Tobin (1958) and others, was an important step\nforward in the economic analysis of risk. These authors employed a\nsimple statistical measure, namely the standard deviation (or\nalternatively the variance, that is the square of the standard\ndeviation) as a measure of riskiness. Hence, in a comparison between\ntwo investment alternatives, the one whose economic outcome is\ncalculated to have the largest standard deviation is regarded as the\nmost risky one. In a comparison between different such alternatives,\neach of them can be characterized by two numbers, namely its\nexpectation value and its standard deviation or riskiness. Investors\ntypically prefer investments with as high expectation values and as\nlow riskiness as possible. However, investors differ in the relative\nweight that they assign to expectations respectively risk avoidance.\nGiven these decision weights, an individual\u2019s optimal portfolio\ncan be determined.\n\nSince the late 1960s, alternative measures of risk have been\ndeveloped. Perhaps the most influential of these was provided by\nMichael Rothschild and Joseph Stiglitz (1970): If we move probability\nmass from the centre to the tails of a probability distribution, while\nkeeping its mean unchanged, then we increase the risk associated with\nthe distribution. A measure based on this principle (mean preserving\nspread) can be constructed that has more attractive mathematical\nproperties than those of the older standard deviation measure.\n7.2 Measures of attitudes to risks\n\nWe differ in our attitudes to risk. Some of us willingly take risks\nwhich others deem much too large. The notion of certainty-equivalent\noutcomes can be used to specify such differences. Consider a risky\noutcome X. Another outcome Y is a\ncertainty-equivalent outcome for X if and only if (1)\nY involves no uncertainty and (2) X and Y\nare considered by the agent to be equally good. For instance, let\nX be a lottery ticket with a 50 per cent chance of winning\nsomething to which you assign 10 utiles (utility units) and a 50 per\ncent chance of winning nothing. The expected utility of this ticket is\n5 utiles. Now suppose that you are indifferent between receiving 3\nutiles for sure and receiving X. Then your attitude is risk\naverse. The general criterion for risk aversion (risk avoidance,\ncautiousness) is that \\(CE(X) < EU(X)\\). Similarly, you are risk\nneutral concerning X if \\(CE(X) = EU(X)\\) and you are risk\naffine (risk seeking, risk loving) if \\(EU(X) < CE(X)\\).\n\nIn economics, risk aversion is usually related to money. Let\nX represent a lottery ticket with a 50 per cent chance of\nwinning \u20ac 100, and suppose that you consider this ticket to be\nworth \u20ac 30. We then have \\(EU(X) = u(100)/2\\), and \\(CE(X) =\nu(30)\\). This means that \\(u(30) = u(100)/2\\). If this is a consistent\npattern, then the utility of money, as illustrated in a diagram with\namounts x of money on the x axis and their utilities \\(u(x)\\)\non the y axis, will be represented by a concave (hill-like) curve.\nSimilarly, if a risk affine behaviour will be represented by a convex\n(valley-like curve). Provided that the utility function u is\ntwice continuously differentiable, this can be expressed more\nprecisely in the form of the Arrow-Pratt measure of risk\naversion/affinity, according to which the agent\u2019s risk aversion\nat any point x is equal to \\(-u''(x)/u'(x)\\). Hence, a person\nwith the utility function \\(u_1\\) is more risk averse at a point\nx than one with utility function \\(u_2\\) if and only if\n\n\\[-{u_1}''(x)/{u_1}'(x) > -{u_2}''(x)/{u_2}'(x)\\]\n (Arrow 1965; Pratt 1964). The Arrow-Pratt measure has the\nadvantage of being invariant under transformations of the utility\nfunction that preserve the preference relation that it represents\n(i.e. it is invariant under multiplication of the utility with a\npositive constant and addition of an arbitrary constant). On the other\nhand, it can be questioned on philosophical grounds whether risk\nattitudes can be adequately represented by variations in the utility\nof money. It can be argued that cautiousness and the utility of money\nare two separate issues and that they should therefore have\nindependent representations.\n7.3 Experimental economics\n\nStudies in experimental economics reveal that actual agents often do\nnot conform with theoretically derived rationality criteria. One of\nthe most popular descriptive theories that tries to capture actual\nbehaviour under risk is prospect theory, which was developed by Daniel\nKahneman and Amos Tversky around 1980 (Kahneman & Tversky 1979;\nTversky & Kahneman 1986). It distinguishes between two stages in a\ndecision process. In the first phase, the editing phase,\ngains and losses in the different options are identified. They are\ndefined relative to some neutral reference point that is usually the\ncurrent asset position. In the second phase, the evaluation\nphase, the options are evaluated in a way that resembles expected\nutility analysis, but both utilities and probabilities are replaced by\nother, similar measures. Utility is replaced by a measure that is\nasymmetrical between gains and losses. Objective probabilities are\ntransformed by a function that gives more weight to probability\ndifferences close to the ends than to those near the centre of the\ndistribution. Thus it makes a greater difference to decrease the\nprobability of a negative outcome from 2 to 1 per cent than to\ndecrease it from 51 to 50 percent. \n\nProspect theory can explain some of the ways in which actual behaviour\ndeviates from standard economic models of rational behaviour under\nrisk. Hence the overweighting of probability changes close to zero or\nunity can be used to explain why people both buy insurance and buy\nlottery tickets. However, prospect theory is not plausible as a\nnormative theory for rational behaviour under risk. Probably,\nnormative and descriptive theories of risk will have to go in\ndifferent directions.\n7.4 Risk-benefit analysis\n\nRisk-benefit analysis (RBA), also called cost-benefit analysis (CBA)\nis a collection of decision-aiding techniques that weigh advantages\nagainst disadvantages in numerical terms. In a typical risk-benefit\nanalysis, multi-dimensional problems are reduced to a single\ndimension. This is achieved by assigning monetary values to all\npotential outcomes. Usually, uncertain outcomes are evaluated\naccording to the expected utility model. This means that the disvalue\nof a risk is obtained by multiplying the probability of the undesired\nevent by a monetary value representing its severity (Sen 2000;\nSunstein 2005).\n\nIn order to make overall alternatives comparable in simple numerical\nterms, all consequences, including losses of human lives, have to be\nassigned monetary values. The discussion about assigning monetary\nvalue to human lives seems to have had its origin in criticism by the\nAmerican Air Force against a report from the RAND Corporation in 1950.\nThe report recommended a military strategy that would result in\nmortality rates for crew members that the military considered to be\nunacceptably high. This was because loss of crew members was not\nincluded in the calculations. This omission had the same effect as\nsetting the value of a soldier\u2019s life at zero. In response to\nthe criticism, the RAND Corporation began to include a life value in\ntheir analyses, but they had considerable difficulties in deriving\nthis value (Banzhof 2014). Their employee Thomas Schelling proposed\nthat life values should be derived from a marginal analysis of\nindividuals\u2019 own valuation of risks of death (Schelling 1968).\nIf a person would accept a death risk of 1/10,000 against a\ncompensation of $400, then that is taken to express a life value of\n10,000 \\(\\times\\) $400 or $4,000,000.\n\nMost of the philosophical discussion on risk-benefit analysis has been\nfocused on the assignment of a monetary value to the loss of a human\nlife (MacLean 1994; Heinzerling 2000; 2002). It has been claimed that\nlives and money are incommensurable, and that the determination and\nuse of such \u201clive values\u201d express a lack of respect for\nhuman lives. Defenders of risk-benefit analysis have countered that\nthese values are just technical constructs representing what society\ntends to pay (alternative: ought to pay) in order to save a human\nlife. Risk-benefit analysis can help decision-makers to save as many\nlives as possible, given that they have a fixed amount of resources\nthat they can assign to life-saving policies (Sunstein 2005).\n\nMany of the value assignments used in cost-benefit analysis are based\non estimates or measurements of (hypothetical) willingness to pay.\nSuch estimates will give more influence to affluent people since they\ncan pay more than others to have it their way. This can be corrected\nwith income-based adjustments of the reported willingness to pay.\nHowever, there are considerable problems involved in the performance\nand interpretation of willingness-to-pay studies (Gr\u00fcne-Yanoff\n2009).\n\nRisk-benefit analysis gives rise to several other philosophical\nproblems of considerable philosophical interest (Hansson 2007). Due to\nits quantitative nature, it tends to leave out problems that are\ndifficult to quantify, such as risks of cultural impoverishment,\nsocial isolation, and increased tensions between social strata.\nFurthermore, due to its aggregative structure, risk-benefit analysis\noften leaves out social justice and other distributional aspects,\nalthough these are in fact accessible to quantitative treatment.\n",
    "bibliography": {
        "categories": [],
        "cat_ref_text": {
            "ref_list": [
                "Allais, M., 1953, \u201cLe comportement de l\u2019homme\nrationnel devant le risque: critique des postulats et axiomes de\nl\u2019\u00e9cole Am\u00e9ricaine\u201d, <em>Econometrica</em>,\n21: 503\u2013546.",
                "Allhoff, F., 2009, \u201cRisk, Precaution, and Emerging\nTechnologies\u201d, <em>Studies in Ethics, Law, and Technology</em>,\n3(2), published online 25 June, 2009. doi:10.2202/1941-6008.1078",
                "Arrow, K.J., 1965, <em>Aspects of the Theory of Risk-Bearing.\nYrj\u00f6 Jahnsson Lectures</em>. Helsinki: Yrj\u00f6 Jahnssonin\nS\u00e4\u00e4ti\u00f6.",
                "Banzhaf, H.S., 2014, \u201cRetrospectives: the Cold-War origins\nof the value of statistical life\u201d, <em>Journal of Economic\nPerspectives</em> 28(4): 213\u201326.",
                "Biddle, J.B., 2016, \u201cInductive risk, epistemic risk, and\noverdiagnosis of disease\u201d, <em>Perspectives on Science</em>\n24(2): 192\u2013205.",
                "Boholm, M. et al., 2016, \u201cThe Concepts of Risk, Safety, and\nSecurity: Applications in Everyday Language.\u201d, <em>Risk\nAnalysis</em> 36 (2): 320\u2013338.",
                "Buchak, L., 2014, <em>Risk and Rationality</em>, Oxford: Oxford\nUniversity Press.",
                "Caney, S., 2009, \u201cClimate Change and the Future: Discounting\nfor Time, Wealth, and Risk\u201d, <em>Journal of Social\nPhilosophy</em>, 40(2): 163\u2013186.",
                "Cranor C., 1997, \u201cThe Normative Nature of Risk Assessment:\nFeatures and Possibilities\u201d, <em>Risk: Health, Safety &amp;\nEnvironment</em>, 8: 123\u201336.",
                "\u2013\u2013\u2013, 2005. \u201cThe Science Veil Over Tort Law\nPolicy: How Should Scientific Evidence Be Utilized in Toxic Tort\nLaw?\u201d, <em>Law and Philosophy</em>, 24(2): 139\u2013210.",
                "\u2013\u2013\u2013, 2016, <em>Toxic Torts: Science, Law and the\nPossibility of Justice</em> (second edition), Cambridge: Cambridge\nUniversity Press.",
                "\u2013\u2013\u2013, 2017, <em>Tragic Failures: How and Why We\nAre Harmed by Toxic Chemicals</em>, Oxford: Oxford University Press,\n2017.",
                "Doorn, N., and S.O. Hansson, 2015, \u201cDesign for the value of\nsafety\u201d, in J. van den Hoven, I. van de Poel, and P. Vermaas\n(eds), <em>Handbook of Ethics, Values and Technological Design</em>,\nDordrecht: Springer, 491\u2013511.",
                "Dourson, M.L., &amp; Stara, J.F., 1983, \u201cRegulatory history\nand experimental support of uncertainty (safety) factors\u201d,\n<em>Regulatory Toxicology and Pharmacology</em> 3(3):\n224\u2013238.",
                "Ellsberg, D., 1961, \u201cRisk, Ambiguity, and the Savage\nAxioms\u201d, <em>Quarterly Journal of Economics</em>, 75:\n643\u2013669.",
                "Fellner, W. 1961, \u201cDistortion of subjective probabilities as\na reaction to uncertainty\u201d, <em>Quarterly Journal of\nEconomics</em>, 75: 670\u2013689.",
                "Ferretti, M.P., 2010, \u201cRisk and Distributive Justice: The\nCase of Regulating New Technologies\u201d, <em>Science and\nEngineering Ethics</em>, 16(3): 501\u2013515.",
                "Fishburn, P.C., 1978, \u201cOn Handa\u2019s \u2018New theory of\ncardinal utility\u2019 and the maximization of expected\nreturn\u201d, <em>Journal of Political Economy</em>, 86:\n321\u2013324.",
                "Gigerenzer, G., 2002, <em>Calculated Risks: How to Know When\nNumbers Deceive You</em>, New York: Simon and Schuster.",
                "Gilboa, I. and D. Schmeidler, 1994, \u201cAdditive\nrepresentations of non-additive measures and the Choquet\nintegral\u201d, <em>Annals of Operations Research</em>, 52:\n43\u201365.",
                "Goodwin, W., 2009, \u201cHow Does the Theologizing of Physics\nContribute to Global Warming?\u201d, <em>Environmental\nPhilosophy</em>, 6(2): 21\u201342.",
                "Gordijn, B., 2005, \u201cNanoethics: From Utopian Dreams and\nApocalyptic Nightmares Towards a More Balanced View\u201d,\n<em>Science and Engineering Ethics</em>, 11(4): 521\u2013533.",
                "Gr\u00fcne-Yanoff, T., 2009, \u201cMismeasuring the Value of\nStatistical Life\u201d, <em>Journal of Economic Methodology</em>, 16\n(2): 109\u2013123.",
                "Handa, J., 1977, \u201cRisk, probabilities, and a new theory of\ncardinal utility\u201d, <em>Journal of Political Economy</em>, 85:\n97\u2013122.",
                "Hansson, S. O., 2003, \u201cEthical criteria of risk\nacceptance\u201d, <em>Erkenntnis</em>, 59: 291\u2013309.",
                "\u2013\u2013\u2013, 2004, \u201cWeighing Risks and\nBenefits\u201d, <em>Topoi</em>, 23: 145\u2013152.",
                "\u2013\u2013\u2013, 2006, \u201cEconomic (ir)rationality in\nrisk analysis\u201d, <em>Economics and Philosophy</em>, 22:\n231\u2013241.",
                "\u2013\u2013\u2013, 2007, \u201cPhilosophical Problems in\nCost-Benefit Analysis\u201d, <em>Economics and Philosophy</em> 23:\n163\u2013183.",
                "\u2013\u2013\u2013, 2013, <em>The Ethics of Risk: Ethical\nanalysis in an uncertain world</em>, New York: Palgrave\nMacMillan.",
                "\u2013\u2013\u2013, 2017, \u201cScience denial as a form of\npseudoscience\u201d, <em>Studies in History and Philosophy of\nScience</em>, 63:39\u201347.",
                "Harrison, G.W. and D. Ross, 2017, \u201cThe empirical adequacy of\ncumulative prospect theory and its implications for normative\nassessment\u201d, <em>Journal of Economic Methodology</em>, 24:\n150\u2013165.",
                "Harsanyi, J.C., 1953, \u201cCardinal utility in welfare economics\nand in the theory of risk-taking\u201d, <em>Journal of Political\nEconomy</em>, 61: 434\u2013435.",
                "Harsanyi, J.C., 1955, \u201cCardinal welfare, individualistic\nethics, and interpersonal comparisons of utility\u201d, <em>Journal\nof Political Economy</em>, 63: 309\u2013321.",
                "Harsanyi, J.C., 1975, \u201cCan the maximin principle serve as a\nbasis for morality? A critique of John Rawls\u2019s theory\u201d,\n<em>American Political Science Review</em>, 69: 594\u2013606.",
                "Heinzerling, L., 2000, \u201cThe rights of statistical\npeople.\u201d, <em>Harvard Environmental Law Review</em> 24:\n189\u2013207.",
                "\u2013\u2013\u2013, 2002, \u201cMarkets for arsenic\u201d,\n<em>Georgetown Law Journal</em>, 90: 2311\u20132339.",
                "Hempel, C.G., 1965, <em>Aspects of scientific explanation, and\nother essays in the philosophy of science</em>, New York: Free\nPress.",
                "Heyward, C., and D. Roser (eds.), 2016, <em>Climate Justice in a\nNon-Ideal World</em>, Oxford: Oxford University Press.",
                "Intemann, K., 2015, \u201cDistinguishing Between Legitimate and\nIllegitimate Values in Climate Modeling.\u201d, <em>European Journal\nfor Philosophy of Science</em>, 5(2): 217\u2013232.",
                "Jellinek, S. D., 1981, \u201cOn The Inevitability Of Being\nWrong\u201d, <em>Annals of the New York Academy of Science</em>, 363:\n43\u201347.",
                "John, S., 2017, \u201cFrom Social Values to P-Values: The Social\nEpistemology of the Intergovernmental Panel on Climate Change\u201d,\n<em>Journal of Applied Philosophy</em>, 34 (2): 157\u2013171.",
                "Kahneman, D. and A. Tversky 1979, \u201cProspect theory: An\nanalysis of decision under risk\u201d, <em>Econometrica</em>, 47:\n263\u2013293.",
                "Kermisch, C., 2012, \u201cRisk and Responsibility: A Complex and\nEvolving Relationship\u201d, <em>Science and Engineering Ethics</em>,\n18(1): 91\u2013102.",
                "Lemons, J., et al., 1997, \u201cThe Precautionary Principle:\nScientific Uncertainty and Type I and Type II Errors\u201d,\n<em>Foundations of Science</em>, 2(2): 207\u2013236.",
                "Lewandowsky, S., J. Cook &amp; E. Lloyd, 2018, \u201cThe\n\u2018Alice in Wonderland\u2019 Mechanics of the Rejection of\n(Climate) Science: Simulating Coherence by Conspiracism\u201d,\n<em>Synthese</em> 195 (1):175\u2013196.",
                "MacLean, D. (ed.), 1986, <em>Values at Risk</em> (Maryland Studies\nin Public Philosophy), Totowa, NJ: Rowman and Littlefield.",
                "\u2013\u2013\u2013, 1994, \u201cCost-benefit analysis and\nprocedural values\u201d, <em>Analyse &amp; Kritik</em>, 16(2):\n166\u2013180.",
                "Machina, M.D. and M. Rothschild, 1987, \u201cRisk\u201d, in J.\nEatwell, M. Milgate, and P. Newman (eds.), <em>The New Palgrave: A\nDictionary of Economic Theory and Doctrine</em> (Volume 4), London and\nNew York: Macmillan and Stockton, pp. 201\u2013205.",
                "Macpherson, J.A.E., 2008, \u201cSafety, Risk Acceptability, and\nMorality\u201d, <em>Science and Engineering Ethics</em>, 14(3):\n377\u2013390.",
                "Markowitz, H.M., 1952, \u201cPortfolio Selection\u201d,\n<em>Journal of Finance</em>, 7(1): 77\u201391.",
                "Mayer, L. et al., 2017, \u201cUnderstanding Scientists\u2019\nComputational Modeling Decisions About Climate Risk Management\nStrategies Using Values-Informed Mental Models\u201d, <em>Global\nEnvironmental Change</em> 42:107\u2013116.",
                "McKerlie, D., 1986, \u201cRights and Risk\u201d, <em>Canadian\nJournal of Philosophy</em>, 16: 239\u201352.",
                "McKinney, W.J., 1996, \u201cPrediction and Rolston\u2019s\nEnvironmental Ethics: Lessons From the Philosophy of Science\u201d,\n<em>Science and Engineering Ethics</em>, 2(4): 429\u2013440.",
                "M\u00f6ller, N. and S.O. Hansson, 2008, \u201cPrinciples of\nengineering safety: risk and uncertainty reduction\u201d,\n<em>Reliability Engineering and System Safety</em>, 93:\n776\u2013783.",
                "Munthe, Christian, 2011, <em>The Price of Precaution and the\nEthics of Risk</em>, Dordrecht: Springer.",
                "Ng, Y.-K., 2005, \u201cIntergenerational Impartiality: Replacing\nDiscounting by Probability Weighting\u201d, <em>Journal of\nAgricultural and Environmental Ethics</em>, 18(3): 237\u2013257.",
                "Nozick, Robert, 1974, <em>Anarchy, State, and Utopia</em>, New\nYork: Basic Books.",
                "Oberdiek, J., 2014, <em>Imposing Risk: A Normative Framework</em>,\nOxford: Oxford University Press. ",
                "Pratt, J. W., 1964, \u201cRisk Aversion in the Small and in the\nLarge\u201d, <em>Econometrica</em>, 32: 122\u2013136.",
                "Pressman, P., R. Clemens, W. Hayes, &amp; C. Reddy, 2017,\n\u201cFood additive safety: A review of toxicologic and regulatory\nissues\u201d, <em>Toxicology Research and Application</em>\n1:1\u201322.",
                "Pritchard, D., 2015, \u201cRisk\u201d, <em>Metaphilosophy</em>,\n46(3): 436\u2013461.",
                "\u2013\u2013\u2013, 2016, \u201cEpistemic risk\u201d,\n<em>Journal of Philosophy</em> 113(11): 550\u2013571.",
                "Prothero, D., 2013, \u201cThe Holocaust Denier\u2019s Playbook\nand the Tobacco Smokescreen: Common Threads in the Thinking and\nTactics of Denialists and Pseudoscientists\u201d, pp. 341\u2013358\nin M. Pigliucci and M. Boudry (eds), <em>Philosophy of Pseudoscience.\nReconsidering the demarcation problem</em>, Chicago: University of\nChicago Press.",
                "Pyh\u00e4l\u00e4, M., A.C. Brusendorff &amp; H. Paulom\u00e4ki,\n2010, \u201cThe precautionary principle\u201d, pp. 203\u2013226 in\nM. Fitzmaurice, D. M. Ong &amp; P. Merkouris (eds.), <em>Research\nHandbook on International Environmental Law</em>. Cheltenham: Edward\nElgar.",
                "Quiggin, J., 1982, \u201cA theory of anticipated utility\u201d,\n<em>Journal of Economic Behavior &amp; Organization</em>, 3:\n323\u2013343.",
                "Rasmussen, Norman, <em>et al</em>., 1975, \u201cReactory Safety\nStudy\u201d, <em>WASH-1400</em>, Washington, DC: US NRC.",
                "Rawls, J., 1957, \u201cJustice as Fairness\u201d, <em>Journal of\nPhilosophy</em>, 54: 653\u2013662.",
                "\u2013\u2013\u2013, 1971, <em>A Theory of Justice</em>,\nCambridge, Mass.: Harvard University Press.",
                "\u2013\u2013\u2013, 1974, \u201cSome Reasons for the Maximin\nCriterion\u201d, <em>American Economic Review</em>, 64:\n141\u2013146.",
                "Rechard, R.P., 1999, \u201cHistorical relationship between\nperformance assessment for radioactive waste disposal and other types\nof risk assessment\u201d, <em>Risk Analysis</em>, 19(5):\n763\u2013807.",
                "Rothschild, M. and J. Stiglitz, 1970, \u201cIncreasing risk: 1. A\ndefinition\u201d, <em>Journal of Economic Theory</em>, 2:\n225\u2013243.",
                "Scanlon, T.M., 1982, \u201cContractualism and\nUtilitarianism,\u201d in A. Sen and B. Williams, <em>Utilitarianism\nand Beyond</em>, Cambridge: Cambridge University Press.",
                "Schelling, T.C., 1968, \u201cThe Life You Save May Be Your\nOwn\u201d, in <em>Problems in Public Expenditure Analysis</em>, S.B.\nChase, Jr. (ed.), 127\u2013162. Washington, DC: Brookings\nInstitution.",
                " \u2013\u2013\u2013, 1996, \u201cResearch By Accident\u201d,\n<em>Technological Forecasting And Social Change</em>, 53:\n15\u201320.",
                "Sen, A., 2000, \u201cThe discipline of cost-benefit\nanalysis\u201d, <em>Journal of Legal Studies</em>, 29:\n931\u2013952.",
                "Shrader-Frechette, K., 1991, <em>Risk and Rationality.\nPhilosophical Foundations for Populist Reforms</em>, Berkeley:\nUniversity of California Press.",
                "\u2013\u2013\u2013, 1997, \u201cHydrogeology and Framing\nQuestions Having Policy Consequences\u201d, <em>Philosophy of\nScience</em> (Supplement), 64: S149\u2013S160.",
                "\u2013\u2013\u2013, 2005, \u201cFlawed Attacks on Contemporary\nHuman Rights: Laudan, Sunstein, and the Cost-Benefit State\u201d,\n<em>Human Rights Review</em>, 7(1): 92\u2013110.",
                "\u2013\u2013\u2013, 2005, <em>Environmental Justice: Creating\nEquality, Reclaiming Democracy</em>, New York: Oxford University\nPress.",
                "Sj\u00f6berg, L., 2004, \u201cThe Methodology of Risk Perception\nResearch\u201d, <em>Quality and Quantity</em>, 34:\n407\u2013418.",
                "Sunstein, C.R., 2005, \u201cCost-Benefit Analysis and the\nEnvironment\u201d, <em>Ethics</em>, 115: 351\u2013385.",
                "Thompson, P. B., 1985, \u201cRisking or Being Willing: Hamlet and\nthe DC-10\u201d, <em>Journal of Value Inquiry</em>, 19:\n301\u2013310.",
                "Thomson, J.J., 1985, \u201cImposing Risks\u201d, in <em>To\nBreathe Freely</em>, Mary Gibson (ed.), Totowa, NJ: Rowman and\nAllanheld, 124\u2013140.",
                "\u2013\u2013\u2013, 1986, <em>Rights, Restitution and Risk:\nEssays in Moral Philosophy</em>, Cambridge, Mass.: Harvard University\nPress.",
                "Tobin, J., 1958, \u201cLiquidity preference as behavior towards\nrisk\u201d, <em>Review of Economic Studies</em> 25(2):\n65\u201386.",
                "Tversky, A. and D. Kahneman, 1986, \u201cRational Choice and the\nFraming of Decisions\u201d, <em>Journal of Business</em>, 59:\n251\u2013278.",
                "van den Belt, H. and B. Gremmen., 2002, \u201cBetween\nPrecautionary Principle and \u2018Sound Science\u2019: Distributing\nthe Burdens of Proof.\u201d, <em>Journal of Agricultural and\nEnvironmental Ethics</em>, 15(1): 103\u2013122.",
                "van de Poel, I., et al., 2012, \u201cThe Problem of Many Hands:\nClimate Change as an Example\u201d, <em>Science and Engineering\nEthics</em>, 18(1): 49\u201367.",
                "Wagner, W. E., 1995, \u201cThe Science Charade In Toxic Risk\nRegulation\u201d, <em>Columbia Law Review</em>, 95:\n1613\u20131723.",
                "Weinberg, A. M., 1972, \u201cScience and Trans-Science\u201d,\n<em>Minerva</em>, 10: 209\u2013222. ",
                "Wetmore, J.M., 2008, \u201cEngineering with Uncertainty:\nMonitoring Air Bag Performance\u201d, <em>Science and Engineering\nEthics</em>, 14(2): 201\u2013218."
            ]
        },
        "raw_text": "<div id=\"bibliography\">\n<h2 id=\"Bib\">Bibliography</h2>\n<ul class=\"hanging\">\n<li>Allais, M., 1953, \u201cLe comportement de l\u2019homme\nrationnel devant le risque: critique des postulats et axiomes de\nl\u2019\u00e9cole Am\u00e9ricaine\u201d, <em>Econometrica</em>,\n21: 503\u2013546.</li>\n<li>Allhoff, F., 2009, \u201cRisk, Precaution, and Emerging\nTechnologies\u201d, <em>Studies in Ethics, Law, and Technology</em>,\n3(2), published online 25 June, 2009. doi:10.2202/1941-6008.1078</li>\n<li>Arrow, K.J., 1965, <em>Aspects of the Theory of Risk-Bearing.\nYrj\u00f6 Jahnsson Lectures</em>. Helsinki: Yrj\u00f6 Jahnssonin\nS\u00e4\u00e4ti\u00f6.</li>\n<li>Banzhaf, H.S., 2014, \u201cRetrospectives: the Cold-War origins\nof the value of statistical life\u201d, <em>Journal of Economic\nPerspectives</em> 28(4): 213\u201326.</li>\n<li>Biddle, J.B., 2016, \u201cInductive risk, epistemic risk, and\noverdiagnosis of disease\u201d, <em>Perspectives on Science</em>\n24(2): 192\u2013205.</li>\n<li>Boholm, M. et al., 2016, \u201cThe Concepts of Risk, Safety, and\nSecurity: Applications in Everyday Language.\u201d, <em>Risk\nAnalysis</em> 36 (2): 320\u2013338.</li>\n<li>Buchak, L., 2014, <em>Risk and Rationality</em>, Oxford: Oxford\nUniversity Press.</li>\n<li>Caney, S., 2009, \u201cClimate Change and the Future: Discounting\nfor Time, Wealth, and Risk\u201d, <em>Journal of Social\nPhilosophy</em>, 40(2): 163\u2013186.</li>\n<li>Cranor C., 1997, \u201cThe Normative Nature of Risk Assessment:\nFeatures and Possibilities\u201d, <em>Risk: Health, Safety &amp;\nEnvironment</em>, 8: 123\u201336.</li>\n<li>\u2013\u2013\u2013, 2005. \u201cThe Science Veil Over Tort Law\nPolicy: How Should Scientific Evidence Be Utilized in Toxic Tort\nLaw?\u201d, <em>Law and Philosophy</em>, 24(2): 139\u2013210.</li>\n<li>\u2013\u2013\u2013, 2016, <em>Toxic Torts: Science, Law and the\nPossibility of Justice</em> (second edition), Cambridge: Cambridge\nUniversity Press.</li>\n<li>\u2013\u2013\u2013, 2017, <em>Tragic Failures: How and Why We\nAre Harmed by Toxic Chemicals</em>, Oxford: Oxford University Press,\n2017.</li>\n<li>Doorn, N., and S.O. Hansson, 2015, \u201cDesign for the value of\nsafety\u201d, in J. van den Hoven, I. van de Poel, and P. Vermaas\n(eds), <em>Handbook of Ethics, Values and Technological Design</em>,\nDordrecht: Springer, 491\u2013511.</li>\n<li>Dourson, M.L., &amp; Stara, J.F., 1983, \u201cRegulatory history\nand experimental support of uncertainty (safety) factors\u201d,\n<em>Regulatory Toxicology and Pharmacology</em> 3(3):\n224\u2013238.</li>\n<li>Ellsberg, D., 1961, \u201cRisk, Ambiguity, and the Savage\nAxioms\u201d, <em>Quarterly Journal of Economics</em>, 75:\n643\u2013669.</li>\n<li>Fellner, W. 1961, \u201cDistortion of subjective probabilities as\na reaction to uncertainty\u201d, <em>Quarterly Journal of\nEconomics</em>, 75: 670\u2013689.</li>\n<li>Ferretti, M.P., 2010, \u201cRisk and Distributive Justice: The\nCase of Regulating New Technologies\u201d, <em>Science and\nEngineering Ethics</em>, 16(3): 501\u2013515.</li>\n<li>Fishburn, P.C., 1978, \u201cOn Handa\u2019s \u2018New theory of\ncardinal utility\u2019 and the maximization of expected\nreturn\u201d, <em>Journal of Political Economy</em>, 86:\n321\u2013324.</li>\n<li>Gigerenzer, G., 2002, <em>Calculated Risks: How to Know When\nNumbers Deceive You</em>, New York: Simon and Schuster.</li>\n<li>Gilboa, I. and D. Schmeidler, 1994, \u201cAdditive\nrepresentations of non-additive measures and the Choquet\nintegral\u201d, <em>Annals of Operations Research</em>, 52:\n43\u201365.</li>\n<li>Goodwin, W., 2009, \u201cHow Does the Theologizing of Physics\nContribute to Global Warming?\u201d, <em>Environmental\nPhilosophy</em>, 6(2): 21\u201342.</li>\n<li>Gordijn, B., 2005, \u201cNanoethics: From Utopian Dreams and\nApocalyptic Nightmares Towards a More Balanced View\u201d,\n<em>Science and Engineering Ethics</em>, 11(4): 521\u2013533.</li>\n<li>Gr\u00fcne-Yanoff, T., 2009, \u201cMismeasuring the Value of\nStatistical Life\u201d, <em>Journal of Economic Methodology</em>, 16\n(2): 109\u2013123.</li>\n<li>Handa, J., 1977, \u201cRisk, probabilities, and a new theory of\ncardinal utility\u201d, <em>Journal of Political Economy</em>, 85:\n97\u2013122.</li>\n<li>Hansson, S. O., 2003, \u201cEthical criteria of risk\nacceptance\u201d, <em>Erkenntnis</em>, 59: 291\u2013309.</li>\n<li>\u2013\u2013\u2013, 2004, \u201cWeighing Risks and\nBenefits\u201d, <em>Topoi</em>, 23: 145\u2013152.</li>\n<li>\u2013\u2013\u2013, 2006, \u201cEconomic (ir)rationality in\nrisk analysis\u201d, <em>Economics and Philosophy</em>, 22:\n231\u2013241.</li>\n<li>\u2013\u2013\u2013, 2007, \u201cPhilosophical Problems in\nCost-Benefit Analysis\u201d, <em>Economics and Philosophy</em> 23:\n163\u2013183.</li>\n<li>\u2013\u2013\u2013, 2013, <em>The Ethics of Risk: Ethical\nanalysis in an uncertain world</em>, New York: Palgrave\nMacMillan.</li>\n<li>\u2013\u2013\u2013, 2017, \u201cScience denial as a form of\npseudoscience\u201d, <em>Studies in History and Philosophy of\nScience</em>, 63:39\u201347.</li>\n<li>Harrison, G.W. and D. Ross, 2017, \u201cThe empirical adequacy of\ncumulative prospect theory and its implications for normative\nassessment\u201d, <em>Journal of Economic Methodology</em>, 24:\n150\u2013165.</li>\n<li>Harsanyi, J.C., 1953, \u201cCardinal utility in welfare economics\nand in the theory of risk-taking\u201d, <em>Journal of Political\nEconomy</em>, 61: 434\u2013435.</li>\n<li>Harsanyi, J.C., 1955, \u201cCardinal welfare, individualistic\nethics, and interpersonal comparisons of utility\u201d, <em>Journal\nof Political Economy</em>, 63: 309\u2013321.</li>\n<li>Harsanyi, J.C., 1975, \u201cCan the maximin principle serve as a\nbasis for morality? A critique of John Rawls\u2019s theory\u201d,\n<em>American Political Science Review</em>, 69: 594\u2013606.</li>\n<li>Heinzerling, L., 2000, \u201cThe rights of statistical\npeople.\u201d, <em>Harvard Environmental Law Review</em> 24:\n189\u2013207.</li>\n<li>\u2013\u2013\u2013, 2002, \u201cMarkets for arsenic\u201d,\n<em>Georgetown Law Journal</em>, 90: 2311\u20132339.</li>\n<li>Hempel, C.G., 1965, <em>Aspects of scientific explanation, and\nother essays in the philosophy of science</em>, New York: Free\nPress.</li>\n<li>Heyward, C., and D. Roser (eds.), 2016, <em>Climate Justice in a\nNon-Ideal World</em>, Oxford: Oxford University Press.</li>\n<li>Intemann, K., 2015, \u201cDistinguishing Between Legitimate and\nIllegitimate Values in Climate Modeling.\u201d, <em>European Journal\nfor Philosophy of Science</em>, 5(2): 217\u2013232.</li>\n<li>Jellinek, S. D., 1981, \u201cOn The Inevitability Of Being\nWrong\u201d, <em>Annals of the New York Academy of Science</em>, 363:\n43\u201347.</li>\n<li>John, S., 2017, \u201cFrom Social Values to P-Values: The Social\nEpistemology of the Intergovernmental Panel on Climate Change\u201d,\n<em>Journal of Applied Philosophy</em>, 34 (2): 157\u2013171.</li>\n<li>Kahneman, D. and A. Tversky 1979, \u201cProspect theory: An\nanalysis of decision under risk\u201d, <em>Econometrica</em>, 47:\n263\u2013293.</li>\n<li>Kermisch, C., 2012, \u201cRisk and Responsibility: A Complex and\nEvolving Relationship\u201d, <em>Science and Engineering Ethics</em>,\n18(1): 91\u2013102.</li>\n<li>Lemons, J., et al., 1997, \u201cThe Precautionary Principle:\nScientific Uncertainty and Type I and Type II Errors\u201d,\n<em>Foundations of Science</em>, 2(2): 207\u2013236.</li>\n<li>Lewandowsky, S., J. Cook &amp; E. Lloyd, 2018, \u201cThe\n\u2018Alice in Wonderland\u2019 Mechanics of the Rejection of\n(Climate) Science: Simulating Coherence by Conspiracism\u201d,\n<em>Synthese</em> 195 (1):175\u2013196.</li>\n<li>MacLean, D. (ed.), 1986, <em>Values at Risk</em> (Maryland Studies\nin Public Philosophy), Totowa, NJ: Rowman and Littlefield.</li>\n<li>\u2013\u2013\u2013, 1994, \u201cCost-benefit analysis and\nprocedural values\u201d, <em>Analyse &amp; Kritik</em>, 16(2):\n166\u2013180.</li>\n<li>Machina, M.D. and M. Rothschild, 1987, \u201cRisk\u201d, in J.\nEatwell, M. Milgate, and P. Newman (eds.), <em>The New Palgrave: A\nDictionary of Economic Theory and Doctrine</em> (Volume 4), London and\nNew York: Macmillan and Stockton, pp. 201\u2013205.</li>\n<li>Macpherson, J.A.E., 2008, \u201cSafety, Risk Acceptability, and\nMorality\u201d, <em>Science and Engineering Ethics</em>, 14(3):\n377\u2013390.</li>\n<li>Markowitz, H.M., 1952, \u201cPortfolio Selection\u201d,\n<em>Journal of Finance</em>, 7(1): 77\u201391.</li>\n<li>Mayer, L. et al., 2017, \u201cUnderstanding Scientists\u2019\nComputational Modeling Decisions About Climate Risk Management\nStrategies Using Values-Informed Mental Models\u201d, <em>Global\nEnvironmental Change</em> 42:107\u2013116.</li>\n<li>McKerlie, D., 1986, \u201cRights and Risk\u201d, <em>Canadian\nJournal of Philosophy</em>, 16: 239\u201352.</li>\n<li>McKinney, W.J., 1996, \u201cPrediction and Rolston\u2019s\nEnvironmental Ethics: Lessons From the Philosophy of Science\u201d,\n<em>Science and Engineering Ethics</em>, 2(4): 429\u2013440.</li>\n<li>M\u00f6ller, N. and S.O. Hansson, 2008, \u201cPrinciples of\nengineering safety: risk and uncertainty reduction\u201d,\n<em>Reliability Engineering and System Safety</em>, 93:\n776\u2013783.</li>\n<li>Munthe, Christian, 2011, <em>The Price of Precaution and the\nEthics of Risk</em>, Dordrecht: Springer.</li>\n<li>Ng, Y.-K., 2005, \u201cIntergenerational Impartiality: Replacing\nDiscounting by Probability Weighting\u201d, <em>Journal of\nAgricultural and Environmental Ethics</em>, 18(3): 237\u2013257.</li>\n<li>Nozick, Robert, 1974, <em>Anarchy, State, and Utopia</em>, New\nYork: Basic Books.</li>\n<li>Oberdiek, J., 2014, <em>Imposing Risk: A Normative Framework</em>,\nOxford: Oxford University Press. </li>\n<li>Pratt, J. W., 1964, \u201cRisk Aversion in the Small and in the\nLarge\u201d, <em>Econometrica</em>, 32: 122\u2013136.</li>\n<li>Pressman, P., R. Clemens, W. Hayes, &amp; C. Reddy, 2017,\n\u201cFood additive safety: A review of toxicologic and regulatory\nissues\u201d, <em>Toxicology Research and Application</em>\n1:1\u201322.</li>\n<li>Pritchard, D., 2015, \u201cRisk\u201d, <em>Metaphilosophy</em>,\n46(3): 436\u2013461.</li>\n<li>\u2013\u2013\u2013, 2016, \u201cEpistemic risk\u201d,\n<em>Journal of Philosophy</em> 113(11): 550\u2013571.</li>\n<li>Prothero, D., 2013, \u201cThe Holocaust Denier\u2019s Playbook\nand the Tobacco Smokescreen: Common Threads in the Thinking and\nTactics of Denialists and Pseudoscientists\u201d, pp. 341\u2013358\nin M. Pigliucci and M. Boudry (eds), <em>Philosophy of Pseudoscience.\nReconsidering the demarcation problem</em>, Chicago: University of\nChicago Press.</li>\n<li>Pyh\u00e4l\u00e4, M., A.C. Brusendorff &amp; H. Paulom\u00e4ki,\n2010, \u201cThe precautionary principle\u201d, pp. 203\u2013226 in\nM. Fitzmaurice, D. M. Ong &amp; P. Merkouris (eds.), <em>Research\nHandbook on International Environmental Law</em>. Cheltenham: Edward\nElgar.</li>\n<li>Quiggin, J., 1982, \u201cA theory of anticipated utility\u201d,\n<em>Journal of Economic Behavior &amp; Organization</em>, 3:\n323\u2013343.</li>\n<li>Rasmussen, Norman, <em>et al</em>., 1975, \u201cReactory Safety\nStudy\u201d, <em>WASH-1400</em>, Washington, DC: US NRC.</li>\n<li>Rawls, J., 1957, \u201cJustice as Fairness\u201d, <em>Journal of\nPhilosophy</em>, 54: 653\u2013662.</li>\n<li>\u2013\u2013\u2013, 1971, <em>A Theory of Justice</em>,\nCambridge, Mass.: Harvard University Press.</li>\n<li>\u2013\u2013\u2013, 1974, \u201cSome Reasons for the Maximin\nCriterion\u201d, <em>American Economic Review</em>, 64:\n141\u2013146.</li>\n<li>Rechard, R.P., 1999, \u201cHistorical relationship between\nperformance assessment for radioactive waste disposal and other types\nof risk assessment\u201d, <em>Risk Analysis</em>, 19(5):\n763\u2013807.</li>\n<li>Rothschild, M. and J. Stiglitz, 1970, \u201cIncreasing risk: 1. A\ndefinition\u201d, <em>Journal of Economic Theory</em>, 2:\n225\u2013243.</li>\n<li>Scanlon, T.M., 1982, \u201cContractualism and\nUtilitarianism,\u201d in A. Sen and B. Williams, <em>Utilitarianism\nand Beyond</em>, Cambridge: Cambridge University Press.</li>\n<li>Schelling, T.C., 1968, \u201cThe Life You Save May Be Your\nOwn\u201d, in <em>Problems in Public Expenditure Analysis</em>, S.B.\nChase, Jr. (ed.), 127\u2013162. Washington, DC: Brookings\nInstitution.</li>\n<li> \u2013\u2013\u2013, 1996, \u201cResearch By Accident\u201d,\n<em>Technological Forecasting And Social Change</em>, 53:\n15\u201320.</li>\n<li>Sen, A., 2000, \u201cThe discipline of cost-benefit\nanalysis\u201d, <em>Journal of Legal Studies</em>, 29:\n931\u2013952.</li>\n<li>Shrader-Frechette, K., 1991, <em>Risk and Rationality.\nPhilosophical Foundations for Populist Reforms</em>, Berkeley:\nUniversity of California Press.</li>\n<li>\u2013\u2013\u2013, 1997, \u201cHydrogeology and Framing\nQuestions Having Policy Consequences\u201d, <em>Philosophy of\nScience</em> (Supplement), 64: S149\u2013S160.</li>\n<li>\u2013\u2013\u2013, 2005, \u201cFlawed Attacks on Contemporary\nHuman Rights: Laudan, Sunstein, and the Cost-Benefit State\u201d,\n<em>Human Rights Review</em>, 7(1): 92\u2013110.</li>\n<li>\u2013\u2013\u2013, 2005, <em>Environmental Justice: Creating\nEquality, Reclaiming Democracy</em>, New York: Oxford University\nPress.</li>\n<li>Sj\u00f6berg, L., 2004, \u201cThe Methodology of Risk Perception\nResearch\u201d, <em>Quality and Quantity</em>, 34:\n407\u2013418.</li>\n<li>Sunstein, C.R., 2005, \u201cCost-Benefit Analysis and the\nEnvironment\u201d, <em>Ethics</em>, 115: 351\u2013385.</li>\n<li>Thompson, P. B., 1985, \u201cRisking or Being Willing: Hamlet and\nthe DC-10\u201d, <em>Journal of Value Inquiry</em>, 19:\n301\u2013310.</li>\n<li>Thomson, J.J., 1985, \u201cImposing Risks\u201d, in <em>To\nBreathe Freely</em>, Mary Gibson (ed.), Totowa, NJ: Rowman and\nAllanheld, 124\u2013140.</li>\n<li>\u2013\u2013\u2013, 1986, <em>Rights, Restitution and Risk:\nEssays in Moral Philosophy</em>, Cambridge, Mass.: Harvard University\nPress.</li>\n<li>Tobin, J., 1958, \u201cLiquidity preference as behavior towards\nrisk\u201d, <em>Review of Economic Studies</em> 25(2):\n65\u201386.</li>\n<li>Tversky, A. and D. Kahneman, 1986, \u201cRational Choice and the\nFraming of Decisions\u201d, <em>Journal of Business</em>, 59:\n251\u2013278.</li>\n<li>van den Belt, H. and B. Gremmen., 2002, \u201cBetween\nPrecautionary Principle and \u2018Sound Science\u2019: Distributing\nthe Burdens of Proof.\u201d, <em>Journal of Agricultural and\nEnvironmental Ethics</em>, 15(1): 103\u2013122.</li>\n<li>van de Poel, I., et al., 2012, \u201cThe Problem of Many Hands:\nClimate Change as an Example\u201d, <em>Science and Engineering\nEthics</em>, 18(1): 49\u201367.</li>\n<li>Wagner, W. E., 1995, \u201cThe Science Charade In Toxic Risk\nRegulation\u201d, <em>Columbia Law Review</em>, 95:\n1613\u20131723.</li>\n<li>Weinberg, A. M., 1972, \u201cScience and Trans-Science\u201d,\n<em>Minerva</em>, 10: 209\u2013222. </li>\n<li>Wetmore, J.M., 2008, \u201cEngineering with Uncertainty:\nMonitoring Air Bag Performance\u201d, <em>Science and Engineering\nEthics</em>, 14(2): 201\u2013218.</li>\n</ul>\n</div>"
    },
    "related_entries": {
        "entry_list": [
            "causation: in the law",
            "causation: probabilistic",
            "consequentialism",
            "contractarianism",
            "economics: philosophy of",
            "game theory",
            "luck: justice and bad luck",
            "scientific knowledge: social dimensions of",
            "technology, philosophy of"
        ],
        "entry_link": [
            {
                "../causation-law/": "causation: in the law"
            },
            {
                "../causation-probabilistic/": "causation: probabilistic"
            },
            {
                "../consequentialism/": "consequentialism"
            },
            {
                "../contractarianism/": "contractarianism"
            },
            {
                "../economics/": "economics: philosophy of"
            },
            {
                "../game-theory/": "game theory"
            },
            {
                "../justice-bad-luck/": "luck: justice and bad luck"
            },
            {
                "../scientific-knowledge-social/": "scientific knowledge: social dimensions of"
            },
            {
                "../technology/": "technology, philosophy of"
            }
        ]
    },
    "academic_tools": {
        "listed_text": [
            "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=risk\" target=\"other\">How to cite this entry</a>.",
            "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://leibniz.stanford.edu/friends/preview/risk/\" target=\"other\">Preview the PDF version of this entry</a> at the\n <a href=\"https://leibniz.stanford.edu/friends/\" target=\"other\">Friends of the SEP Society</a>.",
            "<img alt=\"inpho icon\" src=\"../../symbols/inpho.png\"/>",
            "<a href=\"https://www.inphoproject.org/entity?sep=risk&amp;redirect=True\" target=\"other\">Look up topics and thinkers related to this entry</a>\n at the Internet Philosophy Ontology Project (InPhO).",
            "<img alt=\"phil papers icon\" src=\"../../symbols/pp.gif\"/>",
            "<a href=\"https://philpapers.org/sep/risk/\" target=\"other\">Enhanced bibliography for this entry</a>\nat <a href=\"https://philpapers.org/\" target=\"other\">PhilPapers</a>, with links to its database."
        ],
        "listed_links": [
            {
                "https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=risk": "How to cite this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/preview/risk/": "Preview the PDF version of this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/": "Friends of the SEP Society"
            },
            {
                "https://www.inphoproject.org/entity?sep=risk&redirect=True": "Look up topics and thinkers related to this entry"
            },
            {
                "https://philpapers.org/sep/risk/": "Enhanced bibliography for this entry"
            },
            {
                "https://philpapers.org/": "PhilPapers"
            }
        ]
    },
    "other_internet_resources": {
        "listed_text": [],
        "listed_links": []
    }
}