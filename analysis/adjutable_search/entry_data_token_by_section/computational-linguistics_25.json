{
    "main_text": "Statistical NLP\n9. Statistical NLP\n\u201cAll\nthe thousands of times you've heard clause-final auxiliary\nverbs uncontracted strengthen the probability that they're not allowed\nto contract.\u201d\n\n\u2013Geoff Pullum (2011)\nWe have already referred to miscellaneous statistical models and\ntechniques used in various computational tasks, such as\n(in section 2) HMMs in POS tagging,\nprobabilistic grammar modeling and parsing, statistical semantics,\nsemantic disambiguation (word senses, quantifier scope, etc.), plan\nrecognition, discourse modeling, and knowledge extraction from\ntext. Here we try to provide a brief, but slightly more systematic\ntaxonomy of the types of tasks addressed in statistical NLP, and some\nsense of the modeling techniques and algorithms that are most commonly\nused and have made statistical NLP so predominant in recent years,\nchallenging the traditional view of computational linguistics.\nThis traditional view focuses on deriving meaning, and rests on the\nassumption that the syntactic, semantic, pragmatic, and world knowledge\nemployed in this derivation is \u201ccrisp\u201d as opposed to probabilistic;\ni.e., the distributional properties of language are a mere byproduct of\nlinguistic communication, rather than an essential factor in language\nunderstanding, use, or even learning. Thus the emphasis, in this\nview, is on formulating nonprobabilistic syntactic, semantic,\npragmatic, and KR theories to be deployed in language understanding and\nuse. Of course, the problem of ambiguity has always been a focal issue\nin building parsers and language understanding systems, but the\nprevailing assumption was that ambiguity resolution could be\naccomplished by supplementing the interpretive routines with some\ncarefully formulated heuristics expressing syntactic and semantic\npreferences.\nHowever, experience has revealed that the ambiguities that afflict\nthe desired mappings are far too numerous, subtle, and interrelated to\nbe amenable to heuristic arbitration. Instead, linguistic phenomena\nneed to be treated as effectively stochastic, and the distributional\nproperties resulting from these stochastic processes need to be\nsystematically exploited to arrive at reasonably reliable hypotheses\nabout underlying structure. (The Geoff Pullum quote above is relevant\nto this point: The inadmissiblity of contracting the first occurrence\nof I am to I'm in \u201cI'd rather be hated for who\nI am, than loved for who I am not\u201d is not easily ascribed to any\ngrammatical principle, yet\u2014based on positive evidence\nalone\u2014becomes part of our knowledge of English usage.) Thus the\nemphasis has shifted, at least for the time being, to viewing NLP as a\nproblem of uncertain inference and learning in a stochastic\nsetting.\nThis shift is significant from a philosophical perspective, not\njust a practical one: It suggests that traditional thinking about\nlanguage may have been too reliant on introspection. The limitation of\nintrospection is that very little of what goes on in our brains when\nwe comprehend or think about language is accessible to consciousness\n(see for example the discussion of \u201ctwo-channel\nexperiments\u201d in Baars 1997). We consciously register\nthe results of our understanding and thinking, apparently in\nsymbolic form, but not the understanding and\nthinking processes themselves; and these symbolic\nabstractions, to the extent that they lack quantitative or\nprobabilistic dimensions, can lead us to suppose that the underlying\nprocessing is nonquantitative as well. But the successes of\nstatistical NLP, as well as recent developments in cognitive science\n(e.g., Fine et al. 2013; Tenenbaum et al. 2011; Chater and\nOaksford 2008) suggest that language and thinking are not only\nsymbolic, but deeply quantitative and in particular probabilistic.\nFor the first twenty years or so, the primary goals in statistical\nNLP  have been to assign labels, label sequences, syntax trees,\nor translations to linguistic inputs, using statistical language\nmodels trained on large corpora of observed language use. More fully,\nthe types of tasks addressed can be grouped roughly as follows (where\nthe appended keywords indicate typical applications):\n\ntext/document classification: authorship, Reuters news\n  category, sentiment analysis;\nclassification of selected words or phrases in sentential or\n  broader contexts: word sense disambiguation, named entity\n  recognition, multiword expression recognition;\nsequence labeling: acoustic features \u2192 phones\n  \u2192 phonemes \u2192 words \u2192 POS tags;\nstructure assignment to sentences: parsing, semantic\n  role labeling, quantifier scoping;\nsentence transduction: MT, LF computation;\nstructure assignment to multi-sentence texts: discourse\n  relations, anaphora, plan recognition;\nlarge-scale relation extraction: knowledge extraction,\n  paraphrase and entailment relations.\n\nThese\ngroups may seem to differ haphazardly, but as we will further discuss,\ncertain techniques and distinctions are common to many of them, notably\n\nin modeling: numeric and discrete features, vector\n  models, log-linear models, Markov models; generative versus\n  discriminative models, parametric versus non-parametric\n  models;\nin learning from data: maximum likelihood estimation,\n  maximum entropy, expectation maximization, dynamic programming;\n  supervised versus unsupervised learning; and\nin output computation: dynamic programming; unique\n  outputs versus distributions over outputs.\n\nWe now try to provide some intuitive insight into the most\nimportant techniques and distinctions involved in the seven groups of\ntasks above.  For this purpose, we need not comment further on\nquantifier scoping (in the fourth group) or any of the items in the\nsixth and seventh groups, as these are for the most part covered\nelsewhere in this article. In all cases, the two major requirements\nare the development (aided by learning) of a probabilistic model\nrelating linguistic inputs to desired outputs, and the algorithmic use\nof the model in assigning labels or structures to previously unseen\ninputs.  \nText and document classification: In classifying substantial\ndocuments, the features used might be normalized occurrence\nfrequencies of particular words (or word classes) and\npunctuation. Especially for shorter texts, various discrete features\nmay be included as well, such as 0, 1-valued functions indicating the\npresence or absence of certain key words or structural features. In\nthis way documents are represented as numerical vectors, with\nvalues  in a high-dimensional space, with separate classes\npresumably forming somewhat separate clusters in that space. A variety\nof classical pattern recognition techniques are applicable to the\nproblem of learning to assign new documents (as vectors) to the\nappropriate class (e.g., Sebestyen 1962; Duda and Hart 1973). Perhaps\nthe simplest approach (most easily applied when features are binary)\nis a na\u00efve Bayesian one, which assumes that each class generates\nfeature values that are independent of one another. The generative\nfrequencies are estimated from the training data, and class membership\nprobabilities for an unknown document (vector) are computed via Bayes'\nrule (which can be done using successive updates of the prior class\nprobabilities).  Choosing the class with the highest resultant\nposterior probability then provides a decision criterion. A common\ngenerative model for real-valued features, allowing for feature\ninteractions, views the known members of any given class as a sample\nof a multivariate normal (Gaussian) random variable. Learning in this\ncase consists of estimating the mean and covariance matrix of each\nclass (an example of maximum likelihood estimation).\nA traditional discriminative approach, not premised on any\ngenerative model, involves the computation of hyperplanes that\npartition the clusters of known class instances from one another\n(optimizing certain metrics involving in-class and between-class\nvariance); new instances are assigned to the class into whose\npartition they fall. Perceptrons provide a related technique,\ninsofar as they decide class membership on the basis of a linear\ncombination of feature values; their particular advantage is that they\ncan learn incrementally (by adjusting feature weights) as more and\nmore training data become available. Another durable discriminative\napproach\u2014not dependent on linear separability of classes\u2014is the\nk nearest neighbors (kNN) method, which assigns an unknown\ntext or document to the class that is most prevalent among its k\n(e.g., 1\u20135) nearest neighbors in vector space. While all the\npreviously mentioned methods depended on parameter estimation (e.g.,\ngenerative probabilities, Gaussian parameters, or coefficients of\nseparating planes), kNN uses no such parameters\u2014it is\nnonparametric; however, finding a suitable measure of\nproximity or similarity can be challenging, and errors due to\nhaphazard local data point configurations in feature space are hard to\navoid. Another nonparametric discriminative method worth mentioning is\nthe use of decision trees, which can be learned using\ninformation-theoretic techniques; they enable choice of a class by\nfollowing a root-to-leaf path, with branches chosen via tests on\nfeatures of a given input vector. A potentially useful property is\nthat learned decision trees can provide insight into what the most\nimportant features are (such insight can also be provided by\ndimensionality reduction methods). However, decision trees\ntend to converge to nonglobal optima (global optimization is NP-hard),\nand by splitting data, tend to block modeling of feature interactions;\nthis defect can be alleviated to some extent through the use of\ndecision forests. \nHaving mentioned some of the traditional classification methods, we\nnow sketch two techniques that have become particularly prominent in\nstatistical NLP since the 1990s. The first, with mathematical roots\ndating to the 1950s, is maximum entropy (MaxEnt), also called\n(multinomial) logistic regression (e.g., Ratnaparkhi\n1997). Features in this case are any desired 0, 1-valued (binary)\nfunctions of both a given linguistic input and a possible class. (For\ncontinuous features, supervised or unsupervised\ndiscretization methods may be applied, such as entropy-based\npartitioning into some number of intervals.) Training data provide\noccurrence frequencies for these features, and a distribution is\nderived for the conditional probability of a class, given a\nlinguistic input. (As such, it is a discriminative method.) As\nits name implies, this conditional probability function is a\nmaximum-entropy distribution, constrained to conform with the binary\nfeature frequencies observed in the training data. Its form (apart\nfrom a constant multiplier) is an exponential whose exponent is a\nlinear combination of the binary feature values for a given input and\ngiven class. Thus it is a log-linear model (a distribution\nwhose logarithm is linear in the features)\u2014a type of model now\nprevalent in many statistical NLP tasks.  Note that since its\nlogarithm is a linear combination of binary feature values for any\ngiven input and any given class, choosing the maximum-probability\nclass for a given input amounts to linear decision-making, much as in\nsome of the classical methods; however, MaxEnt generally provides\nbetter classification performance, and the classification\nprobabilities it supplies can be useful in further computations (e.g.,\nexpected utilities).\nAnother method important in the emergence and successes of\nstatistical NLP is the support vector machine (SVM) method\n(Boser et al. 1992; Cortes and Vapnik 1995). The great\nadvantage of this method is that it can in principle distinguish\narbitrarily configured classes, by implicitly projecting the original\nvectors into a higher- (or infinite-) dimensional space, where the\nclasses are linearly separable.  The projection is mediated by a\nkernel function\u2014a similarity metric on pairs of vectors,\nsuch as a polynomial in the dot product of the two vectors. Roughly\nspeaking, the components of the higher-dimensional vector correspond\nto terms of the kernel function, if it were expanded out as a sum of\nproducts of the features of the original, unexpanded pair of\nvectors. But no actual expansion is performed, and moreover the\nclassification criterion obtained from a given training corpus only\nrequires calculation of the kernel function for the given feature\nvector (representing the document to be classified) paired with\ncertain special \u201csupport vectors\u201d, and comparison of a linear\ncombination of the resulting values to a threshold. The support\nvectors belong to the training corpus, and define two parallel\nhyperplanes that separate the classes in question as much as possible\n(in the expanded space). (Hence this is a \u201cmax-margin\u201d discriminative\nmethod.) SVMs generally provide excellent accuracy, in part because\nthey allow for nonlinear feature interaction (in the original space),\nand in part because the max-margin method focuses on class separation,\nrather than conditional probability modeling of the classes. On the\nother hand, MaxEnt classifiers are more quickly trainable than SVMs,\nand often provide satisfactory accuracy.  General references covering\nthe classification methods we have sketched are (Duda et al.\n2001; Bishop 2006). \nClassification of selected words or phrases in sentential or\nbroader contexts: As noted earlier, examples include WSD, named\nentity recognition, and sentence boundary detection. The only point of\ndistinction from text/document classification is that it is not a\nchunk of text as a whole, but rather a word or phrase in the context\nof such a chunk that is to be classified. Therefore features are\nchosen to reflect both the features of the target word or phrase (such\nas morphology) and the way it relates to its context, in terms of,\ne.g., surrounding words or word categories, (likely) local syntactic\ndependency relations, and features with broader scope such as word\nfrequencies or document class. Apart from this difference in how\nfeatures are chosen, the same (supervised) learning and classification\nmethods discussed above can be applied.  However, sufficiently\nlarge training corpora may be hard to construct. For example, in\nstatistical WSD (e.g., Yarowsky 1992; Chen et al.\n2009),  since thousands of words have multiple senses in sources\nsuch as WordNet, it is difficult to  construct  a\nsense-annotated training corpus that contains sufficiently many\noccurrences of all of these senses to permit statistical\nlearning. Thus annotations are typically restricted to  the\nsenses of a few polysemous words, and statistical WSD has been shown\nto be feasible for the selected words, but broad-coverage WSD tools\nremain elusive.\n\nSequence labeling: There is a somewhat arbitrary line between\nthe preceding task and sequence labeling. For example, it is quite\npossible to treat POS tagging as a task of classifying words in a text\nin relation to their context.  However, such an approach fails to\nexploit the fact that the classifications of adjacent words are\ninterdependent. For example, in the sentence (from the web) \n\u201cI don't fish like most people\u201d, the occurrence of don't\nshould favor classification of fish as a verb, which in turn\nshould favor classification of like as a preposition. (At\nleast such preferences make sense for declarative sentences; replacing\n\u2018I\u2019 by \u2018why\u2019 would change matters\u2014see\nbelow.) Such cascaded influences are not easily captured through\nsuccessive independent classifications, and they motivate generative\nsequence models such as HMMs. For POS tagging, a labeled training\ncorpus can supply estimates of the probability of any POS for the next\nword, given the POS of the current word. If the corpus is large\nenough, it can also supply estimates of word \u201cemission\u201d probabilities\nfor a large proportion of words generally seen in text, i.e.,\ntheir probability of occurring, given the POS label. (Smoothing\ntechniques are used to fill in non-zero probabilities for unknown\nwords, given a POS.) We previously mentioned the Viterbi\nalgorithm as an efficient dynamic programming algorithm for\napplying an HMM (trained as just mentioned) to the task of assigning a\nmaximum-probability POS tag sequence to the words of a text.  Two\nrelated algorithms, the forward and backward\nalgorithms, can be used to derive probabilities of the\npossible labels at each word position i, which may be more\nuseful than the \u201cbest\u201d label sequence for subsequent higher-level\nprocessing. The forward algorithm in effect (via dynamic\nprogramming) sums the probabilities of all label sequences up to\nposition i that end with a specified label X at word\nposition i and that generate the input up to (and including)\nthat word. The backward algorithm sums the probabilities of\nall label sequences that begin with label X at position\ni, and generate the input from position\ni+1 to the end.  The product of the forward\nand backward probabilities, normalized so that the probabilities of\nthe alternative labels at position i sum to 1, give the\nprobability of X at i, conditioned on the entire\ninput.\n All learning methods referred to so far have been supervised\nlearning methods\u2014a corpus of correctly labeled texts was\nassumed to be available for inferring model parameters. But methods\nhave been developed for unsupervised (or\nsemi-supervised) learning as well. An important unsupervised\nmethod of discovering HMM models for sequence labeling is the\nforward-backward (or Baum-Welch) algorithm. A simple\nversion of this algorithm in the case of POS tagging relies on a\nlexicon containing the possible tags for each word (which are easily\nobtained from a standard lexicon). Some initial, more or less\narbitrarily chosen values of the HMM transition and emission\nprobabilities are then iteratively refined based on a training corpus.\nA caricature of the iterative process would be this: We use the\ncurrent guesses of the HMM parameters to tag the training corpus; then\nwe re-estimate those parameters just as if the corpus were\nhand-tagged. We repeat these two steps till convergence. The actual\nmethod used is more subtle in the way it uses the current HMM\nparameters. (It is a special case of EM\u2014Expectation\nMaximization.) Rather than re-estimating the parameters based on\noccurrence frequencies in the current \u201cbest\u201d tag sequence, it uses the\nexpected number of occurrences of particular pairs of\nsuccessive states (labels), dividing this by the expected\nnumber of occurrences of the first member of the pair. These expected\nvalues are determined by the conditional probability distribution over\ntag sequences, given the training corpus and the current HMM\nparameters, and can be obtained using the forward and backward\nprobabilities as described above (and thus, conditioned on the entire\ncorpus). Revised emission probabilities for any X \u2192\nw can be computed as the sum of probabilities of\nX-labels at all positions where word w occurs in the\ncorpus, divided by the sum of probabilities of X-labels at\nall positions, again using (products of) forward and backward\nprobabilities.\nUnfortunately EM is not guaranteed to find a globally optimal\nmodel.  Thus good results can be achieved only by starting with a\n\u201creasonable\u201d initial HMM, for example assigning very low\nprobabilities to certain transitions (such as determiner \u2192\ndeterminer, determiner \u2192 verb, adjective \u2192 verb).\nSemi-supervised learning might start with a relatively small\nlabeled training corpus, and use the corresponding HMM parameter\nestimates as a starting point for unsupervised learning from further,\nunlabeled texts.  \nA weakness of HMMs themselves is that the Markov assumption\n(independence of non-neighbors, given the neighbors) is violated by\nlonger-range dependencies in text. For example, in the context of a\nrelative clause (signaled by a noun preceding that clause), a\ntransitive verb may well lack an NP complement ( \u201cI collected\nthe money he threw down on the table.\u201d), and as a result,\nwords following the verb may be tagged incorrectly (down as a\nnoun).  A discriminative approach that overcomes this difficulty is\nthe use of conditional random fields (CRFs). Like HMMs (which\nthey subsume), these allow for local interdependence of hidden states,\nbut employ features that depend not only on adjacent pairs of these\nstates, but also on any desired properties of the entire input.\nMathematically, the method is very similar to MaxEnt (as discussed\nabove). The feature coefficients can be learned from training data\neither by gradient ascent or by an incremental dynamic programming\nmethod related to the Baum-Welch algorithm, called improved\niterative scaling (IIS) (Della Pietra et al. 1997; Lafferty et\nal. 2001).  CRFs have been successful in many applications other than\nPOS tagging, such as sentence and word boundary detection (e.g., for\nChinese), WSD, extracting tables from text, named entity recognition,\nand\u2014outside of NLP\u2014in gene prediction and computer\nvision.\nStructure assignment to sentences: The use of probabilistic\ncontext-free grammars (PCFGs) was briefly discussed in \nsection 2. Supervised learning of PCFGs can be\nimplemented much like supervised learning of HMMs for POS tagging. The\nrequired conditional probabilities of phrase expansion are easily\nestimated if a large corpus annotated with phrase bracketings (a\ntreebank) is available (though estimates of POS \u2192 word\nexpansion probabilities are best supplemented with additional data).\nOnce learned, a PCFG can be used to assign probabilistically weighted\nphrase structures to sentences using the chart parsing method\nmentioned in\nsection 2\u2014again a dynamic programming method. \nAlso, unsupervised learning of PCFGs is possible using the EM\napproach. This is important, since it amounts to grammar\ndiscovery.  The only assumption we start with, theoretically,\nis that there is some maximum number of nonterminal symbols, and each\ncan be expanded into any two nonterminals or into any word (Chomsky\nnormal form). Also we associate some more or less arbitrary initial\nexpansion probabilities with these rules. The probabilities are\niteratively revised using expected values of the frequency of\noccurrence of the possible expansions, based on the current PCFG\nmodel, conditioned on the corpus. The analogue of the forward-backward\nalgorithm for computing these expectations is the\ninside-outside algorithm. Inside probabilities specify the\nprobability that a certain proper segment of a given sentence will be\nderived from a specified nonterminal symbol. Outside probabilities\nspecify the probability that all but a certain segment of the\ngiven sentence will be derived from the start (sentence) symbol, where\nthat \u201cmissing\u201d segment remains to be generated from a specified\nnonterminal symbol. The inside and outside probabilities play roles\nanalogous to the backward and forward probabilities in HMM learning\nrespectively. Conceptually, they require summations over exponentially\nmany possible parse trees for a given sentence, but in fact inside\nprobabilities can be computed efficiently by the CYK algorithm\n(section 2), and outside probabilities can also be computed\nefficiently, using a top-down recursive \u201cdivide and conquer\u201d algorithm\nthat makes use of previously computed inside probabilities.\nModest successes have been achieved in learning grammars in this\nway. The complexity is high (cubic-time in the size of the training\ncorpus as well as in the number of nonterminals), and as noted, EM\ndoes not in general find globally optimal models.  Thus it is\nimportant to place some constraints on the initial grammar, e.g.,\nallowing nonterminals to generate either pairs of nonterminals or\nwords, but not both, and also severely limiting the number of allowed\nnonterminals. A method of preferring small rule sets over large ones,\nwithout setting a fixed upper bound, is the use of a Dirichlet\nprocess that supplies a probability distribution over the\nprobabilities of an unbounded number of rules. (This method is\nnonparametric, in the sense that it does not commit to any fixed\nnumber of building blocks or parameters in the modeling.) Whatever\nmethod of bounding the rules is used, the initial PCFG must be\ncarefully chosen if a reasonably good, meaningful rule set is to be\nlearned. One method is to start with a linguistically motivated\ngrammar and to use \u201csymbol splitting\u201d (also called\n\u201cstate splitting\u201d) to generate variants of nonterminals\nthat differ in their expansion rules and\nprobabilities. Recent spectral algorithms offer a relatively\nefficient, and globally optimal, alternative to EM (Cohen et\nal. 2013), and they can be combined with symbol splitting.  \nLike HMMs, PCFGs are generative models, and like them suffer from\ninsufficient sensitivity of local choices to the larger context. CRFs\ncan provide greater context-sensitivity (as in POS tagging and other\ntypes of sequence labeling); though they are not directly suited to\nstructure assignment to text, they can be used to learn shallow\nparsers, which assign phrase types only to nonrecursive phrases (core\nNPs, PPs, VPs, etc.) (Sha and Pereira 2003).\n\nIn the current grammar-learning context, we should also mention\nconnectionist models once more. Such models have shown some capacity\nfor learning to parse from a set of training examples, but achieving\nfull-scale parsing in this way remains a challenge. Also a\ncontroversial issue is the capacity of nonsymbolic NNs to\nexhibit systematicity in unsupervised learning, i.e.,\ndemonstrating a capacity to generalize from unannotated examples. This\nrequires, for example, the ability to accept or generate sentences\nwherein verb arguments appear in positions different from those seen\nin the training set. According to Brakel and Frank (2009),\nsystematicity can be achieved with simple recurrent networks\n(SRNs). However, computational demonstrations have generally been\nrestricted to very simple, English-like artificial languages, at least\nwhen inputs were unannotated word streams.\n\nA structure assignment task that can be viewed as a step towards\nsemantic interpretation is semantic role labeling\n(Palmer et al. 2010). The goal is to assign thematic roles\nsuch as agent, theme, recipient, etc. to core phrases or\nphrasal heads in relation to verbs (and perhaps other\ncomplement-taking words). While this can be approached as a sequence\nlabeling problem, experimental evidence shows that computing parse\ntrees and using resulting structural features for role assignment (or\njointly computing parse trees and roles) improves precision. A\nfrequently used training corpus for such work is PropBank, a version\nof the Penn Treebank annotated with \u201cneutral\u201d roles arg0, arg1,\narg2, etc.\nSentence transduction: The most intensively studied type of\nstatistical sentence transduction to date has been statistical\nMT (SMT) (e.g., Koehn 2010; May 2012). Its successes beginning in\nthe late 1980s and early 90s came as something of a surprise to the\nNLP community, which had been rather pessimistic about MT prospects\never since the report by Bar-Hillel (1960) and the ALPAC report\n(Pierce et al. 1966), negatively assessing the results of a\nmajor post-WW2 funding push in MT by the US government. MT came to be\nviewed as a large-scale engineering enterprise that would not have\nbroad impacts until it could be adequately integrated with semantics\nand knowledge-based inference. The statistical approach emerged in the\nwake of successful application of \u201cnoisy channel\u201d models to speech\nrecognition in the late 1970s and during the 80s, and was propelled\nforward by new developments in machine learning and the increasing\navailability of large machine-readable linguistic corpora, including\nparallel texts in multiple languages.\n The earliest, and simplest type of translation method\nwas word-based. This was grounded in the following sort of\nmodel of how a foreign-language sentence f  (say, in\nFrench) is generated from an English sentence e (which we\nwish to recover, if the target language is English): First, e\nis generated according to some simple model of English, for instance\none based on bigram frequencies. Individual words of e are\nthen assumed to generate individual words of f with some\nprobability, allowing for arbitrary word-order scrambling (or biased\nin some way). In learning such a model, the possible correspondences\nand word-translation probabilities can be estimated from parallel\nEnglish-French corpora, whose sentences and words have\nbeen aligned by hand or by statistical techniques. Such a\nmodel can then be used for \u201cdecoding\u201d a given French\nsentence f into an English sentence e by Bayesian\ninference\u2014we derive e as the English sentence with highest\nposterior probability, given its French \u201cencoding\u201d as f. This\nis accomplished with dynamic programming algorithms, and might use an\nintermediate stage where the n best choices of e are\ncomputed (for some predetermined n), and subsequently\nre-ranked discriminatively using features of e and f\nignored by the generative model.\nHowever, the prevailing SMT systems (such as Google\nTranslate or Yahoo! Babel Fish) are\nphrase-based rather than word-based. Here\n\u201cphrase\u201d refers to single words or groups of words that\ntend to occur adjacent to each other. The idea is that phrases are\nmapped to phrases, for example, the English word pair red\nwine to French phrases vin rouge, du vin rouge, or\nle vin rouge. Also, instead of assuming arbitrary word order\nscrambling, reordering models are used, according to which a\ngiven phrase may tend to be swapped with the left or right neighboring\nphrase or displaced from the neighbors, in the translation\nprocess. Furthermore, instead of relying directly on a Bayesian\nmodel, as in the word-based approach, phrase-based approaches\ntypically use a log-linear model, allowing for incorporation of\nfeatures reflecting not only the language model (such as trigram\nfrequencies), the phrase translation model (such as phrase translation\nfrequencies), and the reordering model, but also miscellaneous\nfeatures such as the number of words created, the number of phrase\ntranslations used, and the number of phrase reorderings (with larger\npenalties for larger displacements).\nWhile phrase-based SMT models have been quite successful, they are\nnonetheless prone to production of syntactically disfluent or\nsemantically odd translations, and much recent research has sought to\nexploit linguistic structure and patterns of meaning to improve\ntranslation quality. Two major approaches to syntactic transfer\nare hierarchical phrase-based translation\nand tree-to-string (TTS) transduction\nmodels.  Hierarchical phrase-based approaches use synchronous\ngrammar rules, which simultaneously expand partial derivations of\ncorresponding sentences in two languages. These are automatically\ninduced from an aligned corpus, and the lowest hierarchical layer\ncorresponds to phrase-to-phrase translation rules like those in\nordinary phrase-based translation. While quite successful, this\napproach provides little assurance that \u201cphrases\u201d in the\nresulting synchronous grammars are semantically coherent units, in the\nlinguistic sense. TTS models obtain better coherency through use of\nparsers trained on phrase-bracketed text corpora (treebanks). The\nencoding of English sentences into French (in keeping with our\npreviously assumed language pair) is conceptualized as beginning with\na parsed English sentence, which is then transformed by (learned)\nrules that progressively expand the original or partially transformed\npattern of phrases and words until all the leaves are French\nwords.\nApart from MT, another important type of sentence transduction\nis semantic parsing, in the sense of mapping sentences in\nsome domain to logical forms usable for question answering. (Note that\nsemantic role labeling, discussed above, can also be viewed as a step\ntowards semantic parsing.) Several studies in this relatively recent\narea have employed supervised learning, based on training corpora\nannotated with LFs (e.g., Mooney 2007; Zettlemoyer & Collins 2007)\nor perhaps syntactic trees along with LFs (e.g., Ge and Mooney\n2009). Typical domains have been QA about geography (where LFs are\ndatabase queries), about Robocup soccer, or about travel\nreservations. Even unsupervised learning has been shown to be possible\nin restricted domains, such as QA based on medical abstracts (Poon and\nDomingos 2009) or the travel reservation domain (Poon 2013). Ideas\nused in this work include forming synonym clusters of nominal terms\nand verbal relations much as in Lin and Pantel's DIRT system, with\ncreation of logical names (reflecting their word origins) for these\nconcepts and relations; and learning (via Markov logic, a\ngeneralization of Markov networks) to annotate the nodes of\ndependency parse trees with database entities, types, and relations on\nthe basis of a travel reservation dialogue corpus (where the data\nneeded for the travel agent's answers are known to lie in the\ndatabase). Whether such methods can be generalized to less restricted\ndomains and forms of language remains to be seen. The recent creation\nof a general corpus annotated with an \u201cabstract meaning\nrepresentation\u201d, AMR, is likely to foster progress in that direction\n(Banarescu et al. 2013).\nThe topics we have touched on in this section are technically\ncomplex, so that our discussion has necessarily been shallow. General\nreferences for statistical language processing are Manning and\nSch\u00fctze 1999 and Jurafsky and Martin 2009. Also the\nstatistical NLP community has developed remarkably comprehensive\ntoolkits for researchers, such as MALLET (MAchine Learning for\nLanguagE Toolkit), which includes brief explanations of many of the\ntechniques.  \nWhat are the prospects for achieving human-like language learning\nin machines?  There is a growing recognition that statistical learning\nwill have to be linked to perceptual and conceptual modeling of the\nworld.  Recent work in the area of grounded language learning\nis moving in that direction. For example, Kim and Mooney (2012)\ndescribe methods of using sentences paired with graph-based\ndescriptions of actions and contexts to hypothesize PCFG rules for\nparsing NL instructions into action representations, while learning\nrule probabilities with the inside-outside algorithm. However, they\nassumed a very restricted domain, and the question remains how far the\nmodeling of perception, concept formation, and of semantic and\nepisodic memory needs to be taken to support unrestricted language\nlearning.  As in the case of world knowledge acquisition by\nmachines (see the preceding section), the modeling capabilities may\nneed to achieve equivalence with those of a newborn, allowing for\nencoding percepts and ideas in symbolic and imagistic languages of\nthought, for taxonomizing entity types,  recognizing animacy and\nintentionality, organizing and abstracting spatial relations and\ncausal chains of events, and more. Providing such capabilities is\nlikely to require, along with advances in our understanding of\ncognitive architecture, resolution of the very issues concerning the\nrepresentation and use of linguistic, semantic, and world knowledge\nthat have been the traditional focus in computational\nlinguistics. \n",
    "section_title": "9. Statistical NLP",
    "entry_title": "Computational Linguistics",
    "hierarchy_title": "Computational Linguistics || Statistical NLP",
    "tokenized_text": [
        "statistical",
        "nlp",
        "statistical",
        "nlp",
        "thousand",
        "time",
        "ve",
        "heard",
        "clausefinal",
        "auxiliary",
        "verb",
        "uncontracted",
        "strengthen",
        "probability",
        "re",
        "allowed",
        "contract",
        "geoff",
        "pullum",
        "already",
        "referred",
        "miscellaneous",
        "statistical",
        "model",
        "technique",
        "used",
        "various",
        "computational",
        "task",
        "section",
        "hmms",
        "po",
        "tagging",
        "probabilistic",
        "grammar",
        "modeling",
        "parsing",
        "statistical",
        "semantics",
        "semantic",
        "disambiguation",
        "word",
        "sens",
        "quantifier",
        "scope",
        "etc",
        "plan",
        "recognition",
        "discourse",
        "modeling",
        "knowledge",
        "extraction",
        "text",
        "try",
        "provide",
        "brief",
        "slightly",
        "systematic",
        "taxonomy",
        "type",
        "task",
        "addressed",
        "statistical",
        "nlp",
        "sense",
        "modeling",
        "technique",
        "algorithm",
        "commonly",
        "used",
        "made",
        "statistical",
        "nlp",
        "predominant",
        "recent",
        "year",
        "challenging",
        "traditional",
        "view",
        "computational",
        "linguistics",
        "traditional",
        "view",
        "focus",
        "deriving",
        "meaning",
        "rest",
        "assumption",
        "syntactic",
        "semantic",
        "pragmatic",
        "world",
        "knowledge",
        "employed",
        "derivation",
        "crisp",
        "opposed",
        "probabilistic",
        "ie",
        "distributional",
        "property",
        "language",
        "mere",
        "byproduct",
        "linguistic",
        "communication",
        "rather",
        "essential",
        "factor",
        "language",
        "understanding",
        "use",
        "even",
        "learning",
        "thus",
        "emphasis",
        "view",
        "formulating",
        "nonprobabilistic",
        "syntactic",
        "semantic",
        "pragmatic",
        "kr",
        "theory",
        "deployed",
        "language",
        "understanding",
        "use",
        "course",
        "problem",
        "ambiguity",
        "always",
        "focal",
        "issue",
        "building",
        "parser",
        "language",
        "understanding",
        "system",
        "prevailing",
        "assumption",
        "ambiguity",
        "resolution",
        "could",
        "accomplished",
        "supplementing",
        "interpretive",
        "routine",
        "carefully",
        "formulated",
        "heuristic",
        "expressing",
        "syntactic",
        "semantic",
        "preference",
        "however",
        "experience",
        "revealed",
        "ambiguity",
        "afflict",
        "desired",
        "mapping",
        "far",
        "numerous",
        "subtle",
        "interrelated",
        "amenable",
        "heuristic",
        "arbitration",
        "instead",
        "linguistic",
        "phenomenon",
        "need",
        "treated",
        "effectively",
        "stochastic",
        "distributional",
        "property",
        "resulting",
        "stochastic",
        "process",
        "need",
        "systematically",
        "exploited",
        "arrive",
        "reasonably",
        "reliable",
        "hypothesis",
        "underlying",
        "structure",
        "geoff",
        "pullum",
        "quote",
        "relevant",
        "point",
        "inadmissiblity",
        "contracting",
        "first",
        "occurrence",
        "m",
        "d",
        "rather",
        "hated",
        "loved",
        "easily",
        "ascribed",
        "grammatical",
        "principle",
        "yetbased",
        "positive",
        "evidence",
        "alonebecomes",
        "part",
        "knowledge",
        "english",
        "usage",
        "thus",
        "emphasis",
        "shifted",
        "least",
        "time",
        "viewing",
        "nlp",
        "problem",
        "uncertain",
        "inference",
        "learning",
        "stochastic",
        "setting",
        "shift",
        "significant",
        "philosophical",
        "perspective",
        "practical",
        "one",
        "suggests",
        "traditional",
        "thinking",
        "language",
        "may",
        "reliant",
        "introspection",
        "limitation",
        "introspection",
        "little",
        "go",
        "brain",
        "comprehend",
        "think",
        "language",
        "accessible",
        "consciousness",
        "see",
        "example",
        "discussion",
        "twochannel",
        "experiment",
        "baars",
        "consciously",
        "register",
        "result",
        "understanding",
        "thinking",
        "apparently",
        "symbolic",
        "form",
        "understanding",
        "thinking",
        "process",
        "symbolic",
        "abstraction",
        "extent",
        "lack",
        "quantitative",
        "probabilistic",
        "dimension",
        "lead",
        "u",
        "suppose",
        "underlying",
        "processing",
        "nonquantitative",
        "well",
        "success",
        "statistical",
        "nlp",
        "well",
        "recent",
        "development",
        "cognitive",
        "science",
        "eg",
        "fine",
        "et",
        "al",
        "tenenbaum",
        "et",
        "al",
        "chater",
        "oaksford",
        "suggest",
        "language",
        "thinking",
        "symbolic",
        "deeply",
        "quantitative",
        "particular",
        "probabilistic",
        "first",
        "twenty",
        "year",
        "primary",
        "goal",
        "statistical",
        "nlp",
        "assign",
        "label",
        "label",
        "sequence",
        "syntax",
        "tree",
        "translation",
        "linguistic",
        "input",
        "using",
        "statistical",
        "language",
        "model",
        "trained",
        "large",
        "corpus",
        "observed",
        "language",
        "use",
        "fully",
        "type",
        "task",
        "addressed",
        "grouped",
        "roughly",
        "follows",
        "appended",
        "keywords",
        "indicate",
        "typical",
        "application",
        "textdocument",
        "classification",
        "authorship",
        "reuters",
        "news",
        "category",
        "sentiment",
        "analysis",
        "classification",
        "selected",
        "word",
        "phrase",
        "sentential",
        "broader",
        "context",
        "word",
        "sense",
        "disambiguation",
        "named",
        "entity",
        "recognition",
        "multiword",
        "expression",
        "recognition",
        "sequence",
        "labeling",
        "acoustic",
        "feature",
        "phone",
        "phoneme",
        "word",
        "po",
        "tag",
        "structure",
        "assignment",
        "sentence",
        "parsing",
        "semantic",
        "role",
        "labeling",
        "quantifier",
        "scoping",
        "sentence",
        "transduction",
        "mt",
        "lf",
        "computation",
        "structure",
        "assignment",
        "multisentence",
        "text",
        "discourse",
        "relation",
        "anaphora",
        "plan",
        "recognition",
        "largescale",
        "relation",
        "extraction",
        "knowledge",
        "extraction",
        "paraphrase",
        "entailment",
        "relation",
        "group",
        "may",
        "seem",
        "differ",
        "haphazardly",
        "discus",
        "certain",
        "technique",
        "distinction",
        "common",
        "many",
        "notably",
        "modeling",
        "numeric",
        "discrete",
        "feature",
        "vector",
        "model",
        "loglinear",
        "model",
        "markov",
        "model",
        "generative",
        "versus",
        "discriminative",
        "model",
        "parametric",
        "versus",
        "nonparametric",
        "model",
        "learning",
        "data",
        "maximum",
        "likelihood",
        "estimation",
        "maximum",
        "entropy",
        "expectation",
        "maximization",
        "dynamic",
        "programming",
        "supervised",
        "versus",
        "unsupervised",
        "learning",
        "output",
        "computation",
        "dynamic",
        "programming",
        "unique",
        "output",
        "versus",
        "distribution",
        "output",
        "try",
        "provide",
        "intuitive",
        "insight",
        "important",
        "technique",
        "distinction",
        "involved",
        "seven",
        "group",
        "task",
        "purpose",
        "need",
        "comment",
        "quantifier",
        "scoping",
        "fourth",
        "group",
        "item",
        "sixth",
        "seventh",
        "group",
        "part",
        "covered",
        "elsewhere",
        "article",
        "case",
        "two",
        "major",
        "requirement",
        "development",
        "aided",
        "learning",
        "probabilistic",
        "model",
        "relating",
        "linguistic",
        "input",
        "desired",
        "output",
        "algorithmic",
        "use",
        "model",
        "assigning",
        "label",
        "structure",
        "previously",
        "unseen",
        "input",
        "text",
        "document",
        "classification",
        "classifying",
        "substantial",
        "document",
        "feature",
        "used",
        "might",
        "normalized",
        "occurrence",
        "frequency",
        "particular",
        "word",
        "word",
        "class",
        "punctuation",
        "especially",
        "shorter",
        "text",
        "various",
        "discrete",
        "feature",
        "may",
        "included",
        "well",
        "valued",
        "function",
        "indicating",
        "presence",
        "absence",
        "certain",
        "key",
        "word",
        "structural",
        "feature",
        "way",
        "document",
        "represented",
        "numerical",
        "vector",
        "value",
        "highdimensional",
        "space",
        "separate",
        "class",
        "presumably",
        "forming",
        "somewhat",
        "separate",
        "cluster",
        "space",
        "variety",
        "classical",
        "pattern",
        "recognition",
        "technique",
        "applicable",
        "problem",
        "learning",
        "assign",
        "new",
        "document",
        "vector",
        "appropriate",
        "class",
        "eg",
        "sebestyen",
        "duda",
        "hart",
        "perhaps",
        "simplest",
        "approach",
        "easily",
        "applied",
        "feature",
        "binary",
        "na\u00efve",
        "bayesian",
        "one",
        "assumes",
        "class",
        "generates",
        "feature",
        "value",
        "independent",
        "one",
        "another",
        "generative",
        "frequency",
        "estimated",
        "training",
        "data",
        "class",
        "membership",
        "probability",
        "unknown",
        "document",
        "vector",
        "computed",
        "via",
        "bayes",
        "rule",
        "done",
        "using",
        "successive",
        "update",
        "prior",
        "class",
        "probability",
        "choosing",
        "class",
        "highest",
        "resultant",
        "posterior",
        "probability",
        "provides",
        "decision",
        "criterion",
        "common",
        "generative",
        "model",
        "realvalued",
        "feature",
        "allowing",
        "feature",
        "interaction",
        "view",
        "known",
        "member",
        "given",
        "class",
        "sample",
        "multivariate",
        "normal",
        "gaussian",
        "random",
        "variable",
        "learning",
        "case",
        "consists",
        "estimating",
        "mean",
        "covariance",
        "matrix",
        "class",
        "example",
        "maximum",
        "likelihood",
        "estimation",
        "traditional",
        "discriminative",
        "approach",
        "premised",
        "generative",
        "model",
        "involves",
        "computation",
        "hyperplanes",
        "partition",
        "cluster",
        "known",
        "class",
        "instance",
        "one",
        "another",
        "optimizing",
        "certain",
        "metric",
        "involving",
        "inclass",
        "betweenclass",
        "variance",
        "new",
        "instance",
        "assigned",
        "class",
        "whose",
        "partition",
        "fall",
        "perceptrons",
        "provide",
        "related",
        "technique",
        "insofar",
        "decide",
        "class",
        "membership",
        "basis",
        "linear",
        "combination",
        "feature",
        "value",
        "particular",
        "advantage",
        "learn",
        "incrementally",
        "adjusting",
        "feature",
        "weight",
        "training",
        "data",
        "become",
        "available",
        "another",
        "durable",
        "discriminative",
        "approachnot",
        "dependent",
        "linear",
        "separability",
        "classesis",
        "k",
        "nearest",
        "neighbor",
        "knn",
        "method",
        "assigns",
        "unknown",
        "text",
        "document",
        "class",
        "prevalent",
        "among",
        "k",
        "eg",
        "nearest",
        "neighbor",
        "vector",
        "space",
        "previously",
        "mentioned",
        "method",
        "depended",
        "parameter",
        "estimation",
        "eg",
        "generative",
        "probability",
        "gaussian",
        "parameter",
        "coefficient",
        "separating",
        "plane",
        "knn",
        "us",
        "parametersit",
        "nonparametric",
        "however",
        "finding",
        "suitable",
        "measure",
        "proximity",
        "similarity",
        "challenging",
        "error",
        "due",
        "haphazard",
        "local",
        "data",
        "point",
        "configuration",
        "feature",
        "space",
        "hard",
        "avoid",
        "another",
        "nonparametric",
        "discriminative",
        "method",
        "worth",
        "mentioning",
        "use",
        "decision",
        "tree",
        "learned",
        "using",
        "informationtheoretic",
        "technique",
        "enable",
        "choice",
        "class",
        "following",
        "roottoleaf",
        "path",
        "branch",
        "chosen",
        "via",
        "test",
        "feature",
        "given",
        "input",
        "vector",
        "potentially",
        "useful",
        "property",
        "learned",
        "decision",
        "tree",
        "provide",
        "insight",
        "important",
        "feature",
        "insight",
        "also",
        "provided",
        "dimensionality",
        "reduction",
        "method",
        "however",
        "decision",
        "tree",
        "tend",
        "converge",
        "nonglobal",
        "optimum",
        "global",
        "optimization",
        "nphard",
        "splitting",
        "data",
        "tend",
        "block",
        "modeling",
        "feature",
        "interaction",
        "defect",
        "alleviated",
        "extent",
        "use",
        "decision",
        "forest",
        "mentioned",
        "traditional",
        "classification",
        "method",
        "sketch",
        "two",
        "technique",
        "become",
        "particularly",
        "prominent",
        "statistical",
        "nlp",
        "since",
        "s",
        "first",
        "mathematical",
        "root",
        "dating",
        "s",
        "maximum",
        "entropy",
        "maxent",
        "also",
        "called",
        "multinomial",
        "logistic",
        "regression",
        "eg",
        "ratnaparkhi",
        "feature",
        "case",
        "desired",
        "valued",
        "binary",
        "function",
        "given",
        "linguistic",
        "input",
        "possible",
        "class",
        "continuous",
        "feature",
        "supervised",
        "unsupervised",
        "discretization",
        "method",
        "may",
        "applied",
        "entropybased",
        "partitioning",
        "number",
        "interval",
        "training",
        "data",
        "provide",
        "occurrence",
        "frequency",
        "feature",
        "distribution",
        "derived",
        "conditional",
        "probability",
        "class",
        "given",
        "linguistic",
        "input",
        "discriminative",
        "method",
        "name",
        "implies",
        "conditional",
        "probability",
        "function",
        "maximumentropy",
        "distribution",
        "constrained",
        "conform",
        "binary",
        "feature",
        "frequency",
        "observed",
        "training",
        "data",
        "form",
        "apart",
        "constant",
        "multiplier",
        "exponential",
        "whose",
        "exponent",
        "linear",
        "combination",
        "binary",
        "feature",
        "value",
        "given",
        "input",
        "given",
        "class",
        "thus",
        "loglinear",
        "model",
        "distribution",
        "whose",
        "logarithm",
        "linear",
        "feature",
        "a",
        "type",
        "model",
        "prevalent",
        "many",
        "statistical",
        "nlp",
        "task",
        "note",
        "since",
        "logarithm",
        "linear",
        "combination",
        "binary",
        "feature",
        "value",
        "given",
        "input",
        "given",
        "class",
        "choosing",
        "maximumprobability",
        "class",
        "given",
        "input",
        "amount",
        "linear",
        "decisionmaking",
        "much",
        "classical",
        "method",
        "however",
        "maxent",
        "generally",
        "provides",
        "better",
        "classification",
        "performance",
        "classification",
        "probability",
        "supply",
        "useful",
        "computation",
        "eg",
        "expected",
        "utility",
        "another",
        "method",
        "important",
        "emergence",
        "success",
        "statistical",
        "nlp",
        "support",
        "vector",
        "machine",
        "svm",
        "method",
        "boser",
        "et",
        "al",
        "cortes",
        "vapnik",
        "great",
        "advantage",
        "method",
        "principle",
        "distinguish",
        "arbitrarily",
        "configured",
        "class",
        "implicitly",
        "projecting",
        "original",
        "vector",
        "higher",
        "infinite",
        "dimensional",
        "space",
        "class",
        "linearly",
        "separable",
        "projection",
        "mediated",
        "kernel",
        "functiona",
        "similarity",
        "metric",
        "pair",
        "vector",
        "polynomial",
        "dot",
        "product",
        "two",
        "vector",
        "roughly",
        "speaking",
        "component",
        "higherdimensional",
        "vector",
        "correspond",
        "term",
        "kernel",
        "function",
        "expanded",
        "sum",
        "product",
        "feature",
        "original",
        "unexpanded",
        "pair",
        "vector",
        "actual",
        "expansion",
        "performed",
        "moreover",
        "classification",
        "criterion",
        "obtained",
        "given",
        "training",
        "corpus",
        "requires",
        "calculation",
        "kernel",
        "function",
        "given",
        "feature",
        "vector",
        "representing",
        "document",
        "classified",
        "paired",
        "certain",
        "special",
        "support",
        "vector",
        "comparison",
        "linear",
        "combination",
        "resulting",
        "value",
        "threshold",
        "support",
        "vector",
        "belong",
        "training",
        "corpus",
        "define",
        "two",
        "parallel",
        "hyperplanes",
        "separate",
        "class",
        "question",
        "much",
        "possible",
        "expanded",
        "space",
        "hence",
        "maxmargin",
        "discriminative",
        "method",
        "svms",
        "generally",
        "provide",
        "excellent",
        "accuracy",
        "part",
        "allow",
        "nonlinear",
        "feature",
        "interaction",
        "original",
        "space",
        "part",
        "maxmargin",
        "method",
        "focus",
        "class",
        "separation",
        "rather",
        "conditional",
        "probability",
        "modeling",
        "class",
        "hand",
        "maxent",
        "classifier",
        "quickly",
        "trainable",
        "svms",
        "often",
        "provide",
        "satisfactory",
        "accuracy",
        "general",
        "reference",
        "covering",
        "classification",
        "method",
        "sketched",
        "duda",
        "et",
        "al",
        "bishop",
        "classification",
        "selected",
        "word",
        "phrase",
        "sentential",
        "broader",
        "context",
        "noted",
        "earlier",
        "example",
        "include",
        "wsd",
        "named",
        "entity",
        "recognition",
        "sentence",
        "boundary",
        "detection",
        "point",
        "distinction",
        "textdocument",
        "classification",
        "chunk",
        "text",
        "whole",
        "rather",
        "word",
        "phrase",
        "context",
        "chunk",
        "classified",
        "therefore",
        "feature",
        "chosen",
        "reflect",
        "feature",
        "target",
        "word",
        "phrase",
        "morphology",
        "way",
        "relates",
        "context",
        "term",
        "eg",
        "surrounding",
        "word",
        "word",
        "category",
        "likely",
        "local",
        "syntactic",
        "dependency",
        "relation",
        "feature",
        "broader",
        "scope",
        "word",
        "frequency",
        "document",
        "class",
        "apart",
        "difference",
        "feature",
        "chosen",
        "supervised",
        "learning",
        "classification",
        "method",
        "discussed",
        "applied",
        "however",
        "sufficiently",
        "large",
        "training",
        "corpus",
        "may",
        "hard",
        "construct",
        "example",
        "statistical",
        "wsd",
        "eg",
        "yarowsky",
        "chen",
        "et",
        "al",
        "since",
        "thousand",
        "word",
        "multiple",
        "sens",
        "source",
        "wordnet",
        "difficult",
        "construct",
        "senseannotated",
        "training",
        "corpus",
        "contains",
        "sufficiently",
        "many",
        "occurrence",
        "sens",
        "permit",
        "statistical",
        "learning",
        "thus",
        "annotation",
        "typically",
        "restricted",
        "sens",
        "polysemous",
        "word",
        "statistical",
        "wsd",
        "shown",
        "feasible",
        "selected",
        "word",
        "broadcoverage",
        "wsd",
        "tool",
        "remain",
        "elusive",
        "sequence",
        "labeling",
        "somewhat",
        "arbitrary",
        "line",
        "preceding",
        "task",
        "sequence",
        "labeling",
        "example",
        "quite",
        "possible",
        "treat",
        "po",
        "tagging",
        "task",
        "classifying",
        "word",
        "text",
        "relation",
        "context",
        "however",
        "approach",
        "fails",
        "exploit",
        "fact",
        "classification",
        "adjacent",
        "word",
        "interdependent",
        "example",
        "sentence",
        "web",
        "nt",
        "fish",
        "like",
        "people",
        "occurrence",
        "nt",
        "favor",
        "classification",
        "fish",
        "verb",
        "turn",
        "favor",
        "classification",
        "like",
        "preposition",
        "least",
        "preference",
        "make",
        "sense",
        "declarative",
        "sentence",
        "replacing",
        "would",
        "change",
        "matterssee",
        "cascaded",
        "influence",
        "easily",
        "captured",
        "successive",
        "independent",
        "classification",
        "motivate",
        "generative",
        "sequence",
        "model",
        "hmms",
        "po",
        "tagging",
        "labeled",
        "training",
        "corpus",
        "supply",
        "estimate",
        "probability",
        "po",
        "next",
        "word",
        "given",
        "po",
        "current",
        "word",
        "corpus",
        "large",
        "enough",
        "also",
        "supply",
        "estimate",
        "word",
        "emission",
        "probability",
        "large",
        "proportion",
        "word",
        "generally",
        "seen",
        "text",
        "ie",
        "probability",
        "occurring",
        "given",
        "po",
        "label",
        "smoothing",
        "technique",
        "used",
        "fill",
        "nonzero",
        "probability",
        "unknown",
        "word",
        "given",
        "po",
        "previously",
        "mentioned",
        "viterbi",
        "algorithm",
        "efficient",
        "dynamic",
        "programming",
        "algorithm",
        "applying",
        "hmm",
        "trained",
        "mentioned",
        "task",
        "assigning",
        "maximumprobability",
        "po",
        "tag",
        "sequence",
        "word",
        "text",
        "two",
        "related",
        "algorithm",
        "forward",
        "backward",
        "algorithm",
        "used",
        "derive",
        "probability",
        "possible",
        "label",
        "word",
        "position",
        "may",
        "useful",
        "best",
        "label",
        "sequence",
        "subsequent",
        "higherlevel",
        "processing",
        "forward",
        "algorithm",
        "effect",
        "via",
        "dynamic",
        "programming",
        "sum",
        "probability",
        "label",
        "sequence",
        "position",
        "end",
        "specified",
        "label",
        "x",
        "word",
        "position",
        "generate",
        "input",
        "including",
        "word",
        "backward",
        "algorithm",
        "sum",
        "probability",
        "label",
        "sequence",
        "begin",
        "label",
        "x",
        "position",
        "generate",
        "input",
        "position",
        "i",
        "end",
        "product",
        "forward",
        "backward",
        "probability",
        "normalized",
        "probability",
        "alternative",
        "label",
        "position",
        "sum",
        "give",
        "probability",
        "x",
        "conditioned",
        "entire",
        "input",
        "learning",
        "method",
        "referred",
        "far",
        "supervised",
        "learning",
        "methodsa",
        "corpus",
        "correctly",
        "labeled",
        "text",
        "assumed",
        "available",
        "inferring",
        "model",
        "parameter",
        "method",
        "developed",
        "unsupervised",
        "semisupervised",
        "learning",
        "well",
        "important",
        "unsupervised",
        "method",
        "discovering",
        "hmm",
        "model",
        "sequence",
        "labeling",
        "forwardbackward",
        "baumwelch",
        "algorithm",
        "simple",
        "version",
        "algorithm",
        "case",
        "po",
        "tagging",
        "relies",
        "lexicon",
        "containing",
        "possible",
        "tag",
        "word",
        "easily",
        "obtained",
        "standard",
        "lexicon",
        "initial",
        "le",
        "arbitrarily",
        "chosen",
        "value",
        "hmm",
        "transition",
        "emission",
        "probability",
        "iteratively",
        "refined",
        "based",
        "training",
        "corpus",
        "caricature",
        "iterative",
        "process",
        "would",
        "use",
        "current",
        "guess",
        "hmm",
        "parameter",
        "tag",
        "training",
        "corpus",
        "reestimate",
        "parameter",
        "corpus",
        "handtagged",
        "repeat",
        "two",
        "step",
        "till",
        "convergence",
        "actual",
        "method",
        "used",
        "subtle",
        "way",
        "us",
        "current",
        "hmm",
        "parameter",
        "special",
        "case",
        "emexpectation",
        "maximization",
        "rather",
        "reestimating",
        "parameter",
        "based",
        "occurrence",
        "frequency",
        "current",
        "best",
        "tag",
        "sequence",
        "us",
        "expected",
        "number",
        "occurrence",
        "particular",
        "pair",
        "successive",
        "state",
        "label",
        "dividing",
        "expected",
        "number",
        "occurrence",
        "first",
        "member",
        "pair",
        "expected",
        "value",
        "determined",
        "conditional",
        "probability",
        "distribution",
        "tag",
        "sequence",
        "given",
        "training",
        "corpus",
        "current",
        "hmm",
        "parameter",
        "obtained",
        "using",
        "forward",
        "backward",
        "probability",
        "described",
        "thus",
        "conditioned",
        "entire",
        "corpus",
        "revised",
        "emission",
        "probability",
        "x",
        "w",
        "computed",
        "sum",
        "probability",
        "xlabels",
        "position",
        "word",
        "w",
        "occurs",
        "corpus",
        "divided",
        "sum",
        "probability",
        "xlabels",
        "position",
        "using",
        "product",
        "forward",
        "backward",
        "probability",
        "unfortunately",
        "em",
        "guaranteed",
        "find",
        "globally",
        "optimal",
        "model",
        "thus",
        "good",
        "result",
        "achieved",
        "starting",
        "reasonable",
        "initial",
        "hmm",
        "example",
        "assigning",
        "low",
        "probability",
        "certain",
        "transition",
        "determiner",
        "determiner",
        "determiner",
        "verb",
        "adjective",
        "verb",
        "semisupervised",
        "learning",
        "might",
        "start",
        "relatively",
        "small",
        "labeled",
        "training",
        "corpus",
        "use",
        "corresponding",
        "hmm",
        "parameter",
        "estimate",
        "starting",
        "point",
        "unsupervised",
        "learning",
        "unlabeled",
        "text",
        "weakness",
        "hmms",
        "markov",
        "assumption",
        "independence",
        "nonneighbors",
        "given",
        "neighbor",
        "violated",
        "longerrange",
        "dependency",
        "text",
        "example",
        "context",
        "relative",
        "clause",
        "signaled",
        "noun",
        "preceding",
        "clause",
        "transitive",
        "verb",
        "may",
        "well",
        "lack",
        "np",
        "complement",
        "collected",
        "money",
        "threw",
        "table",
        "result",
        "word",
        "following",
        "verb",
        "may",
        "tagged",
        "incorrectly",
        "noun",
        "discriminative",
        "approach",
        "overcomes",
        "difficulty",
        "use",
        "conditional",
        "random",
        "field",
        "crfs",
        "like",
        "hmms",
        "subsume",
        "allow",
        "local",
        "interdependence",
        "hidden",
        "state",
        "employ",
        "feature",
        "depend",
        "adjacent",
        "pair",
        "state",
        "also",
        "desired",
        "property",
        "entire",
        "input",
        "mathematically",
        "method",
        "similar",
        "maxent",
        "discussed",
        "feature",
        "coefficient",
        "learned",
        "training",
        "data",
        "either",
        "gradient",
        "ascent",
        "incremental",
        "dynamic",
        "programming",
        "method",
        "related",
        "baumwelch",
        "algorithm",
        "called",
        "improved",
        "iterative",
        "scaling",
        "ii",
        "della",
        "pietra",
        "et",
        "al",
        "lafferty",
        "et",
        "al",
        "crfs",
        "successful",
        "many",
        "application",
        "po",
        "tagging",
        "sentence",
        "word",
        "boundary",
        "detection",
        "eg",
        "chinese",
        "wsd",
        "extracting",
        "table",
        "text",
        "named",
        "entity",
        "recognition",
        "andoutside",
        "nlpin",
        "gene",
        "prediction",
        "computer",
        "vision",
        "structure",
        "assignment",
        "sentence",
        "use",
        "probabilistic",
        "contextfree",
        "grammar",
        "pcfgs",
        "briefly",
        "discussed",
        "section",
        "supervised",
        "learning",
        "pcfgs",
        "implemented",
        "much",
        "like",
        "supervised",
        "learning",
        "hmms",
        "po",
        "tagging",
        "required",
        "conditional",
        "probability",
        "phrase",
        "expansion",
        "easily",
        "estimated",
        "large",
        "corpus",
        "annotated",
        "phrase",
        "bracketings",
        "treebank",
        "available",
        "though",
        "estimate",
        "po",
        "word",
        "expansion",
        "probability",
        "best",
        "supplemented",
        "additional",
        "data",
        "learned",
        "pcfg",
        "used",
        "assign",
        "probabilistically",
        "weighted",
        "phrase",
        "structure",
        "sentence",
        "using",
        "chart",
        "parsing",
        "method",
        "mentioned",
        "section",
        "again",
        "dynamic",
        "programming",
        "method",
        "also",
        "unsupervised",
        "learning",
        "pcfgs",
        "possible",
        "using",
        "em",
        "approach",
        "important",
        "since",
        "amount",
        "grammar",
        "discovery",
        "assumption",
        "start",
        "theoretically",
        "maximum",
        "number",
        "nonterminal",
        "symbol",
        "expanded",
        "two",
        "nonterminals",
        "word",
        "chomsky",
        "normal",
        "form",
        "also",
        "associate",
        "le",
        "arbitrary",
        "initial",
        "expansion",
        "probability",
        "rule",
        "probability",
        "iteratively",
        "revised",
        "using",
        "expected",
        "value",
        "frequency",
        "occurrence",
        "possible",
        "expansion",
        "based",
        "current",
        "pcfg",
        "model",
        "conditioned",
        "corpus",
        "analogue",
        "forwardbackward",
        "algorithm",
        "computing",
        "expectation",
        "insideoutside",
        "algorithm",
        "inside",
        "probability",
        "specify",
        "probability",
        "certain",
        "proper",
        "segment",
        "given",
        "sentence",
        "derived",
        "specified",
        "nonterminal",
        "symbol",
        "outside",
        "probability",
        "specify",
        "probability",
        "certain",
        "segment",
        "given",
        "sentence",
        "derived",
        "start",
        "sentence",
        "symbol",
        "missing",
        "segment",
        "remains",
        "generated",
        "specified",
        "nonterminal",
        "symbol",
        "inside",
        "outside",
        "probability",
        "play",
        "role",
        "analogous",
        "backward",
        "forward",
        "probability",
        "hmm",
        "learning",
        "respectively",
        "conceptually",
        "require",
        "summation",
        "exponentially",
        "many",
        "possible",
        "parse",
        "tree",
        "given",
        "sentence",
        "fact",
        "inside",
        "probability",
        "computed",
        "efficiently",
        "cyk",
        "algorithm",
        "section",
        "outside",
        "probability",
        "also",
        "computed",
        "efficiently",
        "using",
        "topdown",
        "recursive",
        "divide",
        "conquer",
        "algorithm",
        "make",
        "use",
        "previously",
        "computed",
        "inside",
        "probability",
        "modest",
        "success",
        "achieved",
        "learning",
        "grammar",
        "way",
        "complexity",
        "high",
        "cubictime",
        "size",
        "training",
        "corpus",
        "well",
        "number",
        "nonterminals",
        "noted",
        "em",
        "general",
        "find",
        "globally",
        "optimal",
        "model",
        "thus",
        "important",
        "place",
        "constraint",
        "initial",
        "grammar",
        "eg",
        "allowing",
        "nonterminals",
        "generate",
        "either",
        "pair",
        "nonterminals",
        "word",
        "also",
        "severely",
        "limiting",
        "number",
        "allowed",
        "nonterminals",
        "method",
        "preferring",
        "small",
        "rule",
        "set",
        "large",
        "one",
        "without",
        "setting",
        "fixed",
        "upper",
        "bound",
        "use",
        "dirichlet",
        "process",
        "supply",
        "probability",
        "distribution",
        "probability",
        "unbounded",
        "number",
        "rule",
        "method",
        "nonparametric",
        "sense",
        "commit",
        "fixed",
        "number",
        "building",
        "block",
        "parameter",
        "modeling",
        "whatever",
        "method",
        "bounding",
        "rule",
        "used",
        "initial",
        "pcfg",
        "must",
        "carefully",
        "chosen",
        "reasonably",
        "good",
        "meaningful",
        "rule",
        "set",
        "learned",
        "one",
        "method",
        "start",
        "linguistically",
        "motivated",
        "grammar",
        "use",
        "symbol",
        "splitting",
        "also",
        "called",
        "state",
        "splitting",
        "generate",
        "variant",
        "nonterminals",
        "differ",
        "expansion",
        "rule",
        "probability",
        "recent",
        "spectral",
        "algorithm",
        "offer",
        "relatively",
        "efficient",
        "globally",
        "optimal",
        "alternative",
        "em",
        "cohen",
        "et",
        "al",
        "combined",
        "symbol",
        "splitting",
        "like",
        "hmms",
        "pcfgs",
        "generative",
        "model",
        "like",
        "suffer",
        "insufficient",
        "sensitivity",
        "local",
        "choice",
        "larger",
        "context",
        "crfs",
        "provide",
        "greater",
        "contextsensitivity",
        "po",
        "tagging",
        "type",
        "sequence",
        "labeling",
        "though",
        "directly",
        "suited",
        "structure",
        "assignment",
        "text",
        "used",
        "learn",
        "shallow",
        "parser",
        "assign",
        "phrase",
        "type",
        "nonrecursive",
        "phrase",
        "core",
        "np",
        "pps",
        "vps",
        "etc",
        "sha",
        "pereira",
        "current",
        "grammarlearning",
        "context",
        "also",
        "mention",
        "connectionist",
        "model",
        "model",
        "shown",
        "capacity",
        "learning",
        "parse",
        "set",
        "training",
        "example",
        "achieving",
        "fullscale",
        "parsing",
        "way",
        "remains",
        "challenge",
        "also",
        "controversial",
        "issue",
        "capacity",
        "nonsymbolic",
        "nns",
        "exhibit",
        "systematicity",
        "unsupervised",
        "learning",
        "ie",
        "demonstrating",
        "capacity",
        "generalize",
        "unannotated",
        "example",
        "requires",
        "example",
        "ability",
        "accept",
        "generate",
        "sentence",
        "wherein",
        "verb",
        "argument",
        "appear",
        "position",
        "different",
        "seen",
        "training",
        "set",
        "according",
        "brakel",
        "frank",
        "systematicity",
        "achieved",
        "simple",
        "recurrent",
        "network",
        "srns",
        "however",
        "computational",
        "demonstration",
        "generally",
        "restricted",
        "simple",
        "englishlike",
        "artificial",
        "language",
        "least",
        "input",
        "unannotated",
        "word",
        "stream",
        "structure",
        "assignment",
        "task",
        "viewed",
        "step",
        "towards",
        "semantic",
        "interpretation",
        "semantic",
        "role",
        "labeling",
        "palmer",
        "et",
        "al",
        "goal",
        "assign",
        "thematic",
        "role",
        "agent",
        "theme",
        "recipient",
        "etc",
        "core",
        "phrase",
        "phrasal",
        "head",
        "relation",
        "verb",
        "perhaps",
        "complementtaking",
        "word",
        "approached",
        "sequence",
        "labeling",
        "problem",
        "experimental",
        "evidence",
        "show",
        "computing",
        "parse",
        "tree",
        "using",
        "resulting",
        "structural",
        "feature",
        "role",
        "assignment",
        "jointly",
        "computing",
        "parse",
        "tree",
        "role",
        "improves",
        "precision",
        "frequently",
        "used",
        "training",
        "corpus",
        "work",
        "propbank",
        "version",
        "penn",
        "treebank",
        "annotated",
        "neutral",
        "role",
        "arg",
        "arg",
        "arg",
        "etc",
        "sentence",
        "transduction",
        "intensively",
        "studied",
        "type",
        "statistical",
        "sentence",
        "transduction",
        "date",
        "statistical",
        "mt",
        "smt",
        "eg",
        "koehn",
        "may",
        "success",
        "beginning",
        "late",
        "s",
        "early",
        "came",
        "something",
        "surprise",
        "nlp",
        "community",
        "rather",
        "pessimistic",
        "mt",
        "prospect",
        "ever",
        "since",
        "report",
        "barhillel",
        "alpac",
        "report",
        "pierce",
        "et",
        "al",
        "negatively",
        "assessing",
        "result",
        "major",
        "postww",
        "funding",
        "push",
        "mt",
        "u",
        "government",
        "mt",
        "came",
        "viewed",
        "largescale",
        "engineering",
        "enterprise",
        "would",
        "broad",
        "impact",
        "could",
        "adequately",
        "integrated",
        "semantics",
        "knowledgebased",
        "inference",
        "statistical",
        "approach",
        "emerged",
        "wake",
        "successful",
        "application",
        "noisy",
        "channel",
        "model",
        "speech",
        "recognition",
        "late",
        "s",
        "propelled",
        "forward",
        "new",
        "development",
        "machine",
        "learning",
        "increasing",
        "availability",
        "large",
        "machinereadable",
        "linguistic",
        "corpus",
        "including",
        "parallel",
        "text",
        "multiple",
        "language",
        "earliest",
        "simplest",
        "type",
        "translation",
        "method",
        "wordbased",
        "grounded",
        "following",
        "sort",
        "model",
        "foreignlanguage",
        "sentence",
        "f",
        "say",
        "french",
        "generated",
        "english",
        "sentence",
        "e",
        "wish",
        "recover",
        "target",
        "language",
        "english",
        "first",
        "e",
        "generated",
        "according",
        "simple",
        "model",
        "english",
        "instance",
        "one",
        "based",
        "bigram",
        "frequency",
        "individual",
        "word",
        "e",
        "assumed",
        "generate",
        "individual",
        "word",
        "f",
        "probability",
        "allowing",
        "arbitrary",
        "wordorder",
        "scrambling",
        "biased",
        "way",
        "learning",
        "model",
        "possible",
        "correspondence",
        "wordtranslation",
        "probability",
        "estimated",
        "parallel",
        "englishfrench",
        "corpus",
        "whose",
        "sentence",
        "word",
        "aligned",
        "hand",
        "statistical",
        "technique",
        "model",
        "used",
        "decoding",
        "given",
        "french",
        "sentence",
        "f",
        "english",
        "sentence",
        "e",
        "bayesian",
        "inferencewe",
        "derive",
        "e",
        "english",
        "sentence",
        "highest",
        "posterior",
        "probability",
        "given",
        "french",
        "encoding",
        "f",
        "accomplished",
        "dynamic",
        "programming",
        "algorithm",
        "might",
        "use",
        "intermediate",
        "stage",
        "n",
        "best",
        "choice",
        "e",
        "computed",
        "predetermined",
        "n",
        "subsequently",
        "reranked",
        "discriminatively",
        "using",
        "feature",
        "e",
        "f",
        "ignored",
        "generative",
        "model",
        "however",
        "prevailing",
        "smt",
        "system",
        "google",
        "translate",
        "yahoo",
        "babel",
        "fish",
        "phrasebased",
        "rather",
        "wordbased",
        "phrase",
        "refers",
        "single",
        "word",
        "group",
        "word",
        "tend",
        "occur",
        "adjacent",
        "idea",
        "phrase",
        "mapped",
        "phrase",
        "example",
        "english",
        "word",
        "pair",
        "red",
        "wine",
        "french",
        "phrase",
        "vin",
        "rouge",
        "du",
        "vin",
        "rouge",
        "le",
        "vin",
        "rouge",
        "also",
        "instead",
        "assuming",
        "arbitrary",
        "word",
        "order",
        "scrambling",
        "reordering",
        "model",
        "used",
        "according",
        "given",
        "phrase",
        "may",
        "tend",
        "swapped",
        "left",
        "right",
        "neighboring",
        "phrase",
        "displaced",
        "neighbor",
        "translation",
        "process",
        "furthermore",
        "instead",
        "relying",
        "directly",
        "bayesian",
        "model",
        "wordbased",
        "approach",
        "phrasebased",
        "approach",
        "typically",
        "use",
        "loglinear",
        "model",
        "allowing",
        "incorporation",
        "feature",
        "reflecting",
        "language",
        "model",
        "trigram",
        "frequency",
        "phrase",
        "translation",
        "model",
        "phrase",
        "translation",
        "frequency",
        "reordering",
        "model",
        "also",
        "miscellaneous",
        "feature",
        "number",
        "word",
        "created",
        "number",
        "phrase",
        "translation",
        "used",
        "number",
        "phrase",
        "reordering",
        "larger",
        "penalty",
        "larger",
        "displacement",
        "phrasebased",
        "smt",
        "model",
        "quite",
        "successful",
        "nonetheless",
        "prone",
        "production",
        "syntactically",
        "disfluent",
        "semantically",
        "odd",
        "translation",
        "much",
        "recent",
        "research",
        "sought",
        "exploit",
        "linguistic",
        "structure",
        "pattern",
        "meaning",
        "improve",
        "translation",
        "quality",
        "two",
        "major",
        "approach",
        "syntactic",
        "transfer",
        "hierarchical",
        "phrasebased",
        "translation",
        "treetostring",
        "tt",
        "transduction",
        "model",
        "hierarchical",
        "phrasebased",
        "approach",
        "use",
        "synchronous",
        "grammar",
        "rule",
        "simultaneously",
        "expand",
        "partial",
        "derivation",
        "corresponding",
        "sentence",
        "two",
        "language",
        "automatically",
        "induced",
        "aligned",
        "corpus",
        "lowest",
        "hierarchical",
        "layer",
        "corresponds",
        "phrasetophrase",
        "translation",
        "rule",
        "like",
        "ordinary",
        "phrasebased",
        "translation",
        "quite",
        "successful",
        "approach",
        "provides",
        "little",
        "assurance",
        "phrase",
        "resulting",
        "synchronous",
        "grammar",
        "semantically",
        "coherent",
        "unit",
        "linguistic",
        "sense",
        "tt",
        "model",
        "obtain",
        "better",
        "coherency",
        "use",
        "parser",
        "trained",
        "phrasebracketed",
        "text",
        "corpus",
        "treebanks",
        "encoding",
        "english",
        "sentence",
        "french",
        "keeping",
        "previously",
        "assumed",
        "language",
        "pair",
        "conceptualized",
        "beginning",
        "parsed",
        "english",
        "sentence",
        "transformed",
        "learned",
        "rule",
        "progressively",
        "expand",
        "original",
        "partially",
        "transformed",
        "pattern",
        "phrase",
        "word",
        "leaf",
        "french",
        "word",
        "apart",
        "mt",
        "another",
        "important",
        "type",
        "sentence",
        "transduction",
        "semantic",
        "parsing",
        "sense",
        "mapping",
        "sentence",
        "domain",
        "logical",
        "form",
        "usable",
        "question",
        "answering",
        "note",
        "semantic",
        "role",
        "labeling",
        "discussed",
        "also",
        "viewed",
        "step",
        "towards",
        "semantic",
        "parsing",
        "several",
        "study",
        "relatively",
        "recent",
        "area",
        "employed",
        "supervised",
        "learning",
        "based",
        "training",
        "corpus",
        "annotated",
        "lf",
        "eg",
        "mooney",
        "zettlemoyer",
        "collins",
        "perhaps",
        "syntactic",
        "tree",
        "along",
        "lf",
        "eg",
        "ge",
        "mooney",
        "typical",
        "domain",
        "qa",
        "geography",
        "lf",
        "database",
        "query",
        "robocup",
        "soccer",
        "travel",
        "reservation",
        "even",
        "unsupervised",
        "learning",
        "shown",
        "possible",
        "restricted",
        "domain",
        "qa",
        "based",
        "medical",
        "abstract",
        "poon",
        "domingo",
        "travel",
        "reservation",
        "domain",
        "poon",
        "idea",
        "used",
        "work",
        "include",
        "forming",
        "synonym",
        "cluster",
        "nominal",
        "term",
        "verbal",
        "relation",
        "much",
        "lin",
        "pantel",
        "s",
        "dirt",
        "system",
        "creation",
        "logical",
        "name",
        "reflecting",
        "word",
        "origin",
        "concept",
        "relation",
        "learning",
        "via",
        "markov",
        "logic",
        "generalization",
        "markov",
        "network",
        "annotate",
        "node",
        "dependency",
        "parse",
        "tree",
        "database",
        "entity",
        "type",
        "relation",
        "basis",
        "travel",
        "reservation",
        "dialogue",
        "corpus",
        "data",
        "needed",
        "travel",
        "agent",
        "s",
        "answer",
        "known",
        "lie",
        "database",
        "whether",
        "method",
        "generalized",
        "le",
        "restricted",
        "domain",
        "form",
        "language",
        "remains",
        "seen",
        "recent",
        "creation",
        "general",
        "corpus",
        "annotated",
        "abstract",
        "meaning",
        "representation",
        "amr",
        "likely",
        "foster",
        "progress",
        "direction",
        "banarescu",
        "et",
        "al",
        "topic",
        "touched",
        "section",
        "technically",
        "complex",
        "discussion",
        "necessarily",
        "shallow",
        "general",
        "reference",
        "statistical",
        "language",
        "processing",
        "manning",
        "sch\u00fctze",
        "jurafsky",
        "martin",
        "also",
        "statistical",
        "nlp",
        "community",
        "developed",
        "remarkably",
        "comprehensive",
        "toolkits",
        "researcher",
        "mallet",
        "machine",
        "learning",
        "language",
        "toolkit",
        "includes",
        "brief",
        "explanation",
        "many",
        "technique",
        "prospect",
        "achieving",
        "humanlike",
        "language",
        "learning",
        "machine",
        "growing",
        "recognition",
        "statistical",
        "learning",
        "linked",
        "perceptual",
        "conceptual",
        "modeling",
        "world",
        "recent",
        "work",
        "area",
        "grounded",
        "language",
        "learning",
        "moving",
        "direction",
        "example",
        "kim",
        "mooney",
        "describe",
        "method",
        "using",
        "sentence",
        "paired",
        "graphbased",
        "description",
        "action",
        "context",
        "hypothesize",
        "pcfg",
        "rule",
        "parsing",
        "nl",
        "instruction",
        "action",
        "representation",
        "learning",
        "rule",
        "probability",
        "insideoutside",
        "algorithm",
        "however",
        "assumed",
        "restricted",
        "domain",
        "question",
        "remains",
        "far",
        "modeling",
        "perception",
        "concept",
        "formation",
        "semantic",
        "episodic",
        "memory",
        "need",
        "taken",
        "support",
        "unrestricted",
        "language",
        "learning",
        "case",
        "world",
        "knowledge",
        "acquisition",
        "machine",
        "see",
        "preceding",
        "section",
        "modeling",
        "capability",
        "may",
        "need",
        "achieve",
        "equivalence",
        "newborn",
        "allowing",
        "encoding",
        "percept",
        "idea",
        "symbolic",
        "imagistic",
        "language",
        "thought",
        "taxonomizing",
        "entity",
        "type",
        "recognizing",
        "animacy",
        "intentionality",
        "organizing",
        "abstracting",
        "spatial",
        "relation",
        "causal",
        "chain",
        "event",
        "providing",
        "capability",
        "likely",
        "require",
        "along",
        "advance",
        "understanding",
        "cognitive",
        "architecture",
        "resolution",
        "issue",
        "concerning",
        "representation",
        "use",
        "linguistic",
        "semantic",
        "world",
        "knowledge",
        "traditional",
        "focus",
        "computational",
        "linguistics"
    ]
}