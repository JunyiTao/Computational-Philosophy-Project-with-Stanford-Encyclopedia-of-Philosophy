{
    "url": "artificial-intelligence",
    "title": "Artificial Intelligence",
    "authorship": {
        "year": "Copyright \u00a9 2018",
        "author_text": "Selmer Bringsjord\n<Selmer.Bringsjord@gmail.com>\nNaveen Sundar Govindarajulu\n<Naveen.Sundar.G@gmail.com>",
        "author_links": [
            {
                "http://www.rpi.edu/~brings": "Selmer Bringsjord"
            },
            {
                "mailto:Selmer%2eBringsjord%40gmail%2ecom": "Selmer.Bringsjord@gmail.com"
            },
            {
                "mailto:Naveen%2eSundar%2eG%40gmail%2ecom": "Naveen.Sundar.G@gmail.com"
            }
        ],
        "raw_html": "<div id=\"article-copyright\">\n<p>\n<a href=\"../../info.html#c\">Copyright \u00a9 2018</a> by\n\n<br/>\n<a href=\"http://www.rpi.edu/~brings\" target=\"other\">Selmer Bringsjord</a>\n&lt;<a href=\"mailto:Selmer%2eBringsjord%40gmail%2ecom\"><em>Selmer<abbr title=\" dot \">.</abbr>Bringsjord<abbr title=\" at \">@</abbr>gmail<abbr title=\" dot \">.</abbr>com</em></a>&gt;<br/>\nNaveen Sundar Govindarajulu\n&lt;<a href=\"mailto:Naveen%2eSundar%2eG%40gmail%2ecom\"><em>Naveen<abbr title=\" dot \">.</abbr>Sundar<abbr title=\" dot \">.</abbr>G<abbr title=\" at \">@</abbr>gmail<abbr title=\" dot \">.</abbr>com</em></a>&gt;\n    </p>\n</div>"
    },
    "pubinfo": [
        "First published Thu Jul 12, 2018"
    ],
    "preamble": "\n\nArtificial intelligence (AI) is the field devoted to building\nartificial animals (or at least artificial creatures that \u2013 in\nsuitable contexts \u2013 appear to be animals) and, for\nmany, artificial persons (or at least artificial creatures that\n\u2013 in suitable contexts \u2013 appear to be\n persons).[1]\n Such goals immediately ensure that AI is a discipline of considerable\ninterest to many philosophers, and this has been confirmed (e.g.) by\nthe energetic attempt, on the part of numerous philosophers, to show\nthat these goals are in fact un/attainable. On the constructive side,\nmany of the core formalisms and techniques used in AI come out of, and\nare indeed still much used and refined in, philosophy: first-order\nlogic and its extensions; intensional logics suitable for the modeling\nof doxastic attitudes and deontic reasoning; inductive logic,\nprobability theory, and probabilistic reasoning; practical reasoning\nand planning, and so on. In light of this, some philosophers conduct\nAI research and development as philosophy. \n\nIn the present entry, the history of AI is briefly recounted, proposed\ndefinitions of the field are discussed, and an overview of the field\nis provided. In addition, both philosophical AI (AI pursued as and out\nof philosophy) and philosophy of AI are discussed, via\nexamples of both. The entry ends with some de rigueur\nspeculative commentary regarding the future of AI. \n",
    "toc": [
        {
            "#HistAI": "1. The History of AI"
        },
        {
            "#WhatExacAI": "2. What Exactly is AI?"
        },
        {
            "#ApprAI": "3. Approaches to AI"
        },
        {
            "#InteAgenCont": "3.1 The Intelligent Agent Continuum"
        },
        {
            "#LogiBaseAISomeSurgPoin": "3.2 Logic-Based AI: Some Surgical Points"
        },
        {
            "#NonLogiAISumm": "3.3 Non-Logicist AI: A Summary"
        },
        {
            "#AIBeyoClasPara": "3.4 AI Beyond the Clash of Paradigms "
        },
        {
            "#ExplGrowAI": "4. The Explosive Growth of AI"
        },
        {
            "#BlooMachLear": "4.1 Bloom in Machine Learning "
        },
        {
            "#ResuNeurTech": "4.2 The Resurgence of Neurocomputational Techniques"
        },
        {
            "#ResuProbTech": "4.3 The Resurgence of Probabilistic Techniques"
        },
        {
            "#AIWild": "5. AI in the Wild"
        },
        {
            "#MoraAI": "6. Moral AI"
        },
        {
            "#PhilAI": "7. Philosophical AI"
        },
        {
            "#PhilArtiInte": "8. Philosophy of Artificial Intelligence"
        },
        {
            "#StroVersWeakAI": "8.1 \u201cStrong\u201d versus \u201cWeak\u201d AI"
        },
        {
            "#ChinRoomArguAgaiStroAI": "8.2 The Chinese Room Argument Against \u201cStrong AI\u201d"
        },
        {
            "#GodeArguAgaiStroAI": "8.3 The G\u00f6delian Argument Against \u201cStrong AI\u201d"
        },
        {
            "#AddiTopiReadPhilAI": "8.4 Additional Topics and Readings in Philosophy of AI"
        },
        {
            "#Futu": "9. The Future"
        },
        {
            "#Bib": "Bibliography"
        },
        {
            "#Aca": "Academic Tools"
        },
        {
            "#Oth": "Other Internet Resources"
        },
        {
            "#OnliCourAI": " Online Courses on AI "
        },
        {
            "#Rel": "Related Entries"
        }
    ],
    "main_text": "\n1. The History of AI\n\nThe field of artificial intelligence (AI) officially started in 1956,\nlaunched by a small but now-famous\n DARPA-sponsored\n summer conference at Dartmouth College, in Hanover, New Hampshire.\n(The 50-year celebration of this conference,\n AI@50,\n was held in July 2006 at Dartmouth, with five of the original\nparticipants making it\n back.[2]\n What happened at this historic conference figures in the final\nsection of this entry.) Ten thinkers attended, including John McCarthy\n(who was working at Dartmouth in 1956), Claude Shannon, Marvin Minsky,\nArthur Samuel, Trenchard Moore (apparently the lone note-taker at the\noriginal conference), Ray Solomonoff, Oliver Selfridge, Allen Newell,\nand Herbert Simon. From where we stand now, into the start of the new\nmillennium, the Dartmouth conference is memorable for many reasons,\nincluding this pair: one, the term \u2018artificial\nintelligence\u2019 was coined there (and has long been firmly\nentrenched, despite being disliked by some of the attendees, e.g.,\nMoore); two, Newell and Simon revealed a program \u2013 Logic\nTheorist (LT) \u2013 agreed by the attendees (and, indeed, by nearly\nall those who learned of and about it soon after the conference) to be\na remarkable achievement. LT was capable of proving elementary\ntheorems in the propositional\n calculus.[3][4]\n\nThough the term \u2018artificial intelligence\u2019 made\nits advent at the 1956 conference, certainly the field of AI,\noperationally defined (defined, i.e., as a field constituted by\npractitioners who think and act in certain ways), was in operation\nbefore 1956. For example, in a famous Mind paper of 1950,\nAlan Turing argues that the question \u201cCan a machine\nthink?\u201d (and here Turing is talking about standard computing\nmachines: machines capable of computing functions from the natural\nnumbers (or pairs, triples, \u2026 thereof) to the natural numbers\nthat a Turing machine or equivalent can handle) should be replaced\nwith the question \u201cCan a machine be linguistically\nindistinguishable from a human?.\u201d Specifically, he proposes a\ntest, the\n \u201cTuring Test\u201d\n (TT) as it\u2019s now known. In the TT, a woman and a computer are\nsequestered in sealed rooms, and a human judge, in the dark as to\nwhich of the two rooms contains which contestant, asks questions by\nemail (actually, by teletype, to use the original term) of the\ntwo. If, on the strength of returned answers, the judge can do no\nbetter than 50/50 when delivering a verdict as to which room houses\nwhich player, we say that the computer in question has passed\nthe TT. Passing in this sense operationalizes linguistic\nindistinguishability. Later, we shall discuss the role that TT has\nplayed, and indeed continues to play, in attempts to define AI. At the\nmoment, though, the point is that in his paper, Turing explicitly lays\ndown the call for building machines that would provide an existence\nproof of an affirmative answer to his question. The call even includes\na suggestion for how such construction should proceed. (He suggests\nthat \u201cchild machines\u201d be built, and that these machines\ncould then gradually grow up on their own to learn to communicate in\nnatural language at the level of adult humans. This suggestion has\narguably been followed by Rodney Brooks and the philosopher Daniel\nDennett (1994) in the Cog Project. In addition, the Spielberg/Kubrick\nmovie A.I. is at least in part a cinematic exploration of\nTuring\u2019s\n suggestion.[5])\n The TT continues to be at the heart of AI and discussions of its\nfoundations, as confirmed by the appearance of (Moor 2003). In fact,\nthe TT continues to be used to define the field, as in\nNilsson\u2019s (1998) position, expressed in his textbook for the\nfield, that AI simply is the field devoted to building an artifact\nable to negotiate this test. Energy supplied by the dream of\nengineering a computer that can pass TT, or by controversy surrounding\nclaims that it has already been passed, is if anything\nstronger than ever, and the reader has only to do an internet search\nvia the string \n\nturing test passed \n\nto find up-to-the-minute attempts at reaching this dream, and attempts\n(sometimes made by philosophers) to debunk claims that some such\nattempt has succeeded.\n\nReturning to the issue of the historical record, even if one bolsters\nthe claim that AI started at the 1956 conference by adding the proviso\nthat \u2018artificial intelligence\u2019 refers to a nuts-and-bolts\nengineering pursuit (in which case Turing\u2019s\nphilosophical discussion, despite calls for a child machine,\nwouldn\u2019t exactly count as AI per se), one must confront the fact\nthat Turing, and indeed many predecessors, did attempt to build\nintelligent artifacts. In Turing\u2019s case, such building was\nsurprisingly well-understood before the advent of programmable\ncomputers: Turing wrote a program for playing chess before there were\ncomputers to run such programs on, by slavishly following the code\nhimself. He did this well before 1950, and long before Newell (1973)\ngave thought in print to the possibility of a sustained, serious\nattempt at building a good chess-playing\n computer.[6]\n\nFrom the perspective of philosophy, which views the systematic\ninvestigation of mechanical intelligence as meaningful and productive\nseparate from the specific logicist formalisms (e.g., first-order\nlogic) and problems (e.g., the Entscheidungsproblem) that gave\nbirth to computer science, neither the 1956 conference, nor\nTuring\u2019s Mind paper, come close to marking the start of\nAI. This is easy enough to see. For example, Descartes proposed TT\n(not the TT by name, of course) long before Turing was\n born.[7]\n Here\u2019s the relevant passage: \n\nIf there were machines which bore a resemblance to our body and\nimitated our actions as far as it was morally possible to do so, we\nshould always have two very certain tests by which to recognise that,\nfor all that, they were not real men. The first is, that they could\nnever use speech or other signs as we do when placing our thoughts on\nrecord for the benefit of others. For we can easily understand a\nmachine\u2019s being constituted so that it can utter words, and even\nemit some responses to action on it of a corporeal kind, which brings\nabout a change in its organs; for instance, if it is touched in a\nparticular part it may ask what we wish to say to it; if in another\npart it may exclaim that it is being hurt, and so on. But it never\nhappens that it arranges its speech in various ways, in order to reply\nappropriately to everything that may be said in its presence, as even\nthe lowest type of man can do. And the second difference is, that\nalthough machines can perform certain things as well as or perhaps\nbetter than any of us can do, they infallibly fall short in others, by\nwhich means we may discover that they did not act from knowledge, but\nonly for the disposition of their organs. For while reason is a\nuniversal instrument which can serve for all contingencies, these\norgans have need of some special adaptation for every particular\naction. From this it follows that it is morally impossible that there\nshould be sufficient diversity in any machine to allow it to act in\nall the events of life in the same way as our reason causes us to act.\n(Descartes 1637, p. 116)\n\n\nAt the moment, Descartes is certainly carrying the\n day.[8]\n Turing predicted that his test would be passed by 2000, but the\nfireworks across the globe at the start of the new millennium have\nlong since died down, and the most articulate of computers still\ncan\u2019t meaningfully debate a sharp toddler. Moreover, while in\ncertain focussed areas machines out-perform minds (IBM\u2019s famous\nDeep Blue prevailed in chess over Gary Kasparov, e.g.; and more\nrecently, AI systems have prevailed in other games, e.g.\nJeopardy! and Go, about which more will momentarily be said),\nminds have a (Cartesian) capacity for cultivating their expertise in\nvirtually any sphere. (If it were announced to Deep Blue, or\nany current successor, that chess was no longer to be the game of\nchoice, but rather a heretofore unplayed variant of chess, the machine\nwould be trounced by human children of average intelligence having no\nchess expertise.) AI simply hasn\u2019t managed to create\ngeneral intelligence; it hasn\u2019t even managed to produce\nan artifact indicating that eventually it will create such a\nthing. \n\nBut what about IBM Watson\u2019s famous nail-biting victory in the\nJeopardy! game-show\n contest?[9]\n That certainly seems to be a machine triumph over humans on their\n\u201chome field,\u201d since Jeopardy! delivers a\nhuman-level linguistic challenge ranging across many domains. Indeed,\namong many AI cognoscenti, Watson\u2019s success is considered to be\nmuch more impressive than Deep Blue\u2019s, for numerous reasons. One\nreason is that while chess is generally considered to be\nwell-understood from the formal-computational perspective (after all,\nit\u2019s well-known that there exists a perfect strategy for playing\nchess), in open-domain question-answering (QA), as in any\nsignificant natural-language processing task, there is no consensus as\nto what problem, formally speaking, one is trying to solve. Briefly,\nquestion-answering (QA) is what the reader would think it is: one asks\na question of a machine, and gets an answer, where the answer has to\nbe produced via some \u201csignificant\u201d computational process.\n(See Strzalkowski & Harabagiu (2006) for an overview of what QA,\nhistorically, has been as a field.) A bit more precisely, there is no\nagreement as to what underlying function, formally speaking,\nquestion-answering capability computes. This lack of agreement stems\nquite naturally from the fact that there is of course no consensus as\nto what natural languages are, formally\n speaking.[10]\n Despite this murkiness, and in the face of an almost universal belief\nthat open-domain question-answering would remain unsolved for a decade\nor more, Watson decisively beat the two top human Jeopardy!\nchampions on the planet. During the contest, Watson had to answer\nquestions that required not only command of simple factoids\n(Question1), but also of some amount of rudimentary\nreasoning (in the form of temporal reasoning) and commonsense\n(Question2):\n\nQuestion1: The only two consecutive U.S. presidents\nwith the same first name. \n\nQuestion2: In May 1898, Portugal celebrated the\n400th anniversary of this explorer\u2019s arrival in India. \n\nWhile Watson is demonstrably better than humans in\nJeopardy!-style quizzing (a new human Jeopardy! master\ncould arrive on the scene, but as for chess, AI now assumes that a\nsecond round of IBM-level investment would vanquish the new human\nopponent), this approach does not work for the kind of NLP challenge\nthat Descartes described; that is, Watson can\u2019t converse on the\nfly. After all, some questions don\u2019t hinge on sophisticated\ninformation retrieval and machine learning over pre-existing data, but\nrather on intricate reasoning right on the spot. Such questions may\nfor instance involve anaphora resolution, which require even deeper\ndegrees of commonsensical understanding of time, space, history, folk\npsychology, and so on. Levesque (2013) has catalogued some alarmingly\nsimple questions which fall in this category. (Marcus, 2013, gives an\naccount of Levesque\u2019s challenges that is accessible to a wider\naudience.) The other class of question-answering tasks on which Watson\nfails can be characterized as dynamic question-answering. These\nare questions for which answers may not be recorded in textual form\nanywhere at the time of questioning, or for which answers are\ndependent on factors that change with time. Two questions that fall in\nthis category are given below (Govindarajulu et al. 2013):\n\nQuestion3: If I have 4 foos and 5 bars, and if foos\nare not the same as bars, how many foos will I have if I get 3 bazes\nwhich just happen to be foos?\n\nQuestion4: What was IBM\u2019s Sharpe ratio in the\nlast 60 days of trading? \n\nClosely following Watson\u2019s victory, in March 2016,\n Google DeepMind\u2019s AlphaGo\n defeated one of Go\u2019s top-ranked players, Lee Seedol, in four\nout of five matches. This was considered a landmark achievement within\nAI, as it was widely believed in the AI community that computer\nvictory in Go was at least a few decades away, partly due to the\nenormous number of valid sequences of moves in Go compared to that in\n Chess.[11]\n While this is a remarkable achievement, it should be noted that,\ndespite breathless coverage in the popular\n press,[12]\n AlphaGo, while indisputably a great Go player, is just that. For\nexample, neither AlphaGo nor Watson can understand the rules of Go\nwritten in plain-and-simple English and produce a computer program\nthat can play the game. It\u2019s interesting that there is one\nendeavor in AI that tackles a narrow version of this very problem: In\ngeneral game playing, a machine is given a description of a\nbrand new game just before it has to play the game (Genesereth et al.\n2005). However, the description in question is expressed in a formal\nlanguage, and the machine has to manage to play the game from this\ndescription. Note that this is still far from understanding even a\nsimple description of a game in English well enough to play it. \n\nBut what if we consider the history of AI not from the perspective of\nphilosophy, but rather from the perspective of the field with which,\ntoday, it is most closely connected? The reference here is to computer\nscience. From this perspective, does AI run back to well before\nTuring? Interestingly enough, the results are the same: we find that\nAI runs deep into the past, and has always had philosophy in its\nveins. This is true for the simple reason that computer science grew\nout of logic and probability\n theory,[13]\n which in turn grew out of (and is still intertwined with) philosophy.\nComputer science, today, is shot through and through with logic; the\ntwo fields cannot be separated. This phenomenon has become an object\nof study unto itself (Halpern et al. 2001). The situation is no\ndifferent when we are talking not about traditional logic, but rather\nabout probabilistic formalisms, also a significant component of\nmodern-day AI: These formalisms also grew out of philosophy, as nicely\nchronicled, in part, by Glymour (1992). For example, in the one mind\nof Pascal was born a method of rigorously calculating probabilities,\nconditional probability (which plays a particularly large role in AI,\ncurrently), and such fertile philosophico-probabilistic arguments as\n Pascal\u2019s wager,\n according to which it is irrational not to become a Christian. \n\nThat modern-day AI has its roots in philosophy, and in fact that these\nhistorical roots are temporally deeper than even Descartes\u2019\ndistant day, can be seen by looking to the clever, revealing cover of\nthe second edition (the third edition is the current one) of the\ncomprehensive textbook\n Artificial Intelligence: A Modern Approach\n (known in the AI community as simply AIMA2e for Russell &\nNorvig, 2002).\n\n\n\nCover of AIMA2e (Russell & Norvig 2002)\n\n\nWhat you see there is an eclectic collection of memorabilia that might\nbe on and around the desk of some imaginary AI researcher. For\nexample, if you look carefully, you will specifically see: a picture\nof Turing, a view of Big Ben through a window (perhaps R&N are\naware of the fact that Turing famously held at one point that a\nphysical machine with the power of a universal Turing machine is\nphysically impossible: he quipped that it would have to be the size of\nBig Ben), a planning algorithm described in Aristotle\u2019s De\nMotu Animalium,\n Frege\u2019s fascinating notation for first-order logic,\n a glimpse of Lewis Carroll\u2019s (1958) pictorial representation of\nsyllogistic reasoning, Ramon Lull\u2019s concept-generating wheel\nfrom his 13th-century Ars Magna, and a number of\nother pregnant items (including, in a clever, recursive, and\nbordering-on-self-congratulatory touch, a copy of AIMA itself).\nThough there is insufficient space here to make all the historical\nconnections, we can safely infer from the appearance of these items\n(and here we of course refer to the ancient ones: Aristotle conceived\nof planning as information-processing over two-and-a-half millennia\nback; and in addition, as Glymour (1992) notes, Artistotle can also be\ncredited with devising the first knowledge-bases and ontologies, two\ntypes of representation schemes that have long been central to AI)\nthat AI is indeed very, very old. Even those who insist that AI is at\nleast in part an artifact-building enterprise must concede that, in\nlight of these objects, AI is ancient, for it isn\u2019t just\ntheorizing from the perspective that intelligence is at bottom\ncomputational that runs back into the remote past of human history:\nLull\u2019s wheel, for example, marks an attempt to capture\nintelligence not only in computation, but in a physical artifact that\nembodies that\n computation.[14]\n\n\nAIMA has now reached its the third edition, and those interested in\nthe history of AI, and for that matter the history of philosophy of\nmind, will not be disappointed by examination of the cover of the\nthird installment (the cover of the second edition is almost exactly\nlike the first edition). (All the elements of the cover, separately\nlisted and annotated, can be found\n online.)\n One significant addition to the cover of the third edition is a\ndrawing of Thomas Bayes; his appearance reflects the recent rise in\nthe popularity of probabilistic techniques in AI, which we discuss\nlater.\n\nOne final point about the history of AI seems worth making. \n\nIt is generally assumed that the birth of modern-day AI in the 1950s\ncame in large part because of and through the advent of the modern\nhigh-speed digital computer. This assumption accords with\ncommon-sense. After all, AI (and, for that matter, to some degree its\ncousin, cognitive science, particularly computational cognitive\nmodeling, the sub-field of cognitive science devoted to producing\ncomputational simulations of human cognition) is aimed at implementing\nintelligence in a computer, and it stands to reason that such a goal\nwould be inseparably linked with the advent of such devices. However,\nthis is only part of the story: the part that reaches back but to\nTuring and others (e.g., von Neuman) responsible for the first\nelectronic computers. The other part is that, as already mentioned, AI\nhas a particularly strong tie, historically speaking, to reasoning\n(logic-based and, in the need to deal with uncertainty,\ninductive/probabilistic reasoning). In this story, nicely told by\nGlymour (1992), a search for an answer to the question \u201cWhat is\na proof?\u201d eventually led to an answer based on Frege\u2019s\nversion of first-order logic (FOL): a (finitary) mathematical proof\nconsists in a series of step-by-step inferences from one formula of\nfirst-order logic to the next. The obvious extension of this answer\n(and it isn\u2019t a complete answer, given that lots of classical\nmathematics, despite conventional wisdom, clearly can\u2019t be\nexpressed in FOL; even the Peano Axioms, to be expressed as a finite\nset of formulae, require SOL) is to say that not only\nmathematical thinking, but thinking, period, can be expressed in FOL.\n(This extension was entertained by many logicians long before the\nstart of information-processing psychology and cognitive science\n\u2013 a fact some cognitive psychologists and cognitive scientists\noften seem to forget.) Today, logic-based AI is only part of\nAI, but the point is that this part still lives (with help from logics\nmuch more powerful, but much more complicated, than FOL), and it can\nbe traced all the way back to Aristotle\u2019s theory of the\n syllogism.[15]\n In the case of uncertain reasoning, the question isn\u2019t\n\u201cWhat is a proof?\u201d, but rather questions such as\n\u201cWhat is it rational to believe, in light of certain\nobservations and probabilities?\u201d This is a question posed and\ntackled long before the arrival of digital computers. \n2. What Exactly is AI?\n\nSo far we have been proceeding as if we have a firm and precise grasp\nof the nature of AI. But what exactly is AI? Philosophers\narguably know better than anyone that precisely defining a particular\ndiscipline to the satisfaction of all relevant parties (including\nthose working in the discipline itself) can be acutely challenging.\nPhilosophers of science certainly have proposed credible accounts of\nwhat constitutes at least the general shape and texture of a given\nfield of science and/or engineering, but what exactly is the\nagreed-upon definition of physics? What about biology? What, for that\nmatter, is philosophy, exactly? These are remarkably difficult, maybe\neven eternally unanswerable, questions, especially if the target is a\nconsensus definition. Perhaps the most prudent course we can\nmanage here under obvious space constraints is to present in\nencapsulated form some proposed definitions of AI. We do\ninclude a glimpse of recent attempts to define AI in detailed,\nrigorous fashion (and we suspect that such attempts will be of\ninterest to philosophers of science, and those interested in this\nsub-area of philosophy). \n\nRussell and Norvig (1995, 2002, 2009), in their aforementioned\nAIMA text, provide a set of possible answers to the \u201cWhat\nis AI?\u201d question that has considerable currency in the field\nitself. These answers all assume that AI should be defined in terms of\nits goals: a candidate definition thus has the form \u201cAI is the\nfield that aims at building \u2026\u201d The answers all fall under\na quartet of types placed along two dimensions. One dimension is\nwhether the goal is to match human performance, or, instead, ideal\nrationality. The other dimension is whether the goal is to build\nsystems that reason/think, or rather systems that act. The situation\nis summed up in this table:\n\n\n\n\nHuman-Based\nIdeal Rationality \n\nReasoning-Based:\n Systems that think like humans. \n Systems that think rationally. \n\nBehavior-Based:\n Systems that act like humans. \n Systems that act rationally. \n\n\nFour Possible Goals for AI According to AIMA \n\n\nPlease note that this quartet of possibilities does reflect (at least\na significant portion of) the relevant literature. For example,\nphilosopher John Haugeland (1985) falls into the Human/Reasoning\nquadrant when he says that AI is \u201cThe exciting new effort to\nmake computers think \u2026 machines with minds, in the\nfull and literal sense.\u201d (By far, this is the quadrant that most\npopular narratives affirm and explore. The recent\n Westworld\n TV series is a powerful case in point.) Luger and Stubblefield (1993)\nseem to fall into the Ideal/Act quadrant when they write: \u201cThe\nbranch of computer science that is concerned with the automation of\nintelligent behavior.\u201d The Human/Act position is occupied most\nprominently by Turing, whose test is passed only by those systems able\nto act sufficiently like a human. The \u201cthinking\nrationally\u201d position is defended (e.g.) by Winston (1992). While\nit might not be entirely uncontroversial to assert that the four bins\ngiven here are exhaustive, such an assertion appears to be quite\nplausible, even when the literature up to the present moment is\ncanvassed. \n\nIt\u2019s important to know that the contrast between the focus on\nsystems that think/reason versus systems that act, while found, as we\nhave seen, at the heart of the AIMA texts, and at the heart of\nAI itself, should not be interpreted as implying that AI researchers\nview their work as falling all and only within one of these two\ncompartments. Researchers who focus more or less exclusively on\nknowledge representation and reasoning, are also quite prepared to\nacknowledge that they are working on (what they take to be) a central\ncomponent or capability within any one of a family of larger systems\nspanning the reason/act distinction. The clearest case may come from\nthe work on planning \u2013 an AI area traditionally making central\nuse of representation and reasoning. For good or ill, much of this\nresearch is done in abstraction (in vitro, as opposed to in vivo), but\nthe researchers involved certainly intend or at least hope that the\nresults of their work can be embedded into systems that actually do\nthings, such as, for example, execute the plans. \n\nWhat about Russell and Norvig themselves? What is their answer to the\nWhat is AI? question? They are firmly in the the \u201cacting\nrationally\u201d camp. In fact, it\u2019s safe to say both that they\nare the chief proponents of this answer, and that they have been\nremarkably successful evangelists. Their extremely influential\nAIMA series can be viewed as a book-length defense and\nspecification of the Ideal/Act category. We will look a bit later at\nhow Russell and Norvig lay out all of AI in terms of intelligent\nagents, which are systems that act in accordance with various\nideal standards for rationality. But first let\u2019s look a bit\ncloser at the view of intelligence underlying the AIMA text. We\ncan do so by turning to Russell (1997). Here Russell recasts the\n\u201cWhat is AI?\u201d question as the question \u201cWhat is\nintelligence?\u201d (presumably under the assumption that we have a\ngood grasp of what an artifact is), and then he identifies\nintelligence with rationality. More specifically, Russell sees\nAI as the field devoted to building intelligent agents, which\nare functions taking as input tuples of percepts from the external\nenvironment, and producing behavior (actions) on the basis of these\npercepts. Russell\u2019s overall picture is this one:\n\n\n\nThe Basic Picture Underlying Russell\u2019s Account of\nIntelligence/Rationality \n\n\nLet\u2019s unpack this diagram a bit, and take a look, first, at the\naccount of perfect rationality that can be derived from it. The\nbehavior of the agent in the environment \\(E\\) (from a class \\(\\bE\\)\nof environments) produces a sequence of states or snapshots of that\nenvironment. A performance measure \\(U\\) evaluates this sequence;\nnotice the box labeled \u201cPerformance Measure\u201d in the above\nfigure. We let \\(V(f,\\bE,U)\\) denote the expected utility\naccording to \\(U\\) of the agent function \\(f\\) operating on\n \\(\\bE\\).[16]\n Now we identify a perfectly rational agent with the agent function:\n \n\n\\[\\tag{1}\\label{eq1}\nf_{\\opt} = \\argmax_f V(f,\\bE,U)\n\\]\n\n\nAccording to the above equation, a perfectly rational agent can be\ntaken to be the function \\(f_{opt}\\) which produces the maximum\nexpected utility in the environment under consideration. Of course, as\nRussell points out, it\u2019s usually not possible to actually build\nperfectly rational agents. For example, though it\u2019s easy enough\nto specify an algorithm for playing invincible chess, it\u2019s not\nfeasible to implement this algorithm. What traditionally happens in AI\nis that programs that are \u2013 to use Russell\u2019s apt\nterminology \u2013 calculatively rational are constructed\ninstead: these are programs that, if executed infinitely fast,\nwould result in perfectly rational behavior. In the case of chess,\nthis would mean that we strive to write a program that runs an\nalgorithm capable, in principle, of finding a flawless move, but we\nadd features that truncate the search for this move in order to play\nwithin intervals of digestible duration. \n\nRussell himself champions a new brand of intelligence/rationality for\nAI; he calls this brand bounded optimality. To understand\nRussell\u2019s view, first we follow him in introducing a\ndistinction: We say that agents have two components: a program, and a\nmachine upon which the program runs. We write \\(Agent(P, M)\\) to\ndenote the agent function implemented by program \\(P\\) running on\nmachine \\(M\\). Now, let \\(\\mathcal{P}(M)\\) denote the set of all\nprograms \\(P\\) that can run on machine \\(M\\). The bounded\noptimal program \\(P_{\\opt,M}\\) then is: \n\n\\[\nP_{\\opt,M}=\\argmax_{P\\in\\mathcal{P}(M)}V(\\mathit{Agent}(P,M),\\bE,U)\n\\]\n\n\nYou can understand this equation in terms of any of the mathematical\nidealizations for standard computation. For example, machines can be\nidentified with Turing machines minus instructions (i.e., TMs are here\nviewed architecturally only: as having tapes divided into squares upon\nwhich symbols can be written, read/write heads capable of moving up\nand down the tape to write and erase, and control units which are in\none of a finite number of states at any time), and programs can be\nidentified with instructions in the Turing-machine model (telling the\nmachine to write and erase symbols, depending upon what state the\nmachine is in). So, if you are told that you must\n\u201cprogram\u201d within the constraints of a 22-state Turing\nmachine, you could search for the \u201cbest\u201d program given\nthose constraints. In other words, you could strive to find the\noptimal program within the bounds of the 22-state architecture.\nRussell\u2019s (1997) view is thus that AI is the field devoted to\ncreating optimal programs for intelligent agents, under time and space\nconstraints on the machines implementing these\n programs.[17]\n\n\nThe reader must have noticed that in the equation for \\(P_{\\opt,M}\\)\nwe have not elaborated on \\(\\bE\\) and \\(U\\) and how equation\n\\eqref{eq1} might be used to construct an agent if the class of\nenvironments \\(\\bE\\) is quite general, or if the true environment\n\\(E\\) is simply unknown. Depending on the task for which one is\nconstructing an artificial agent, \\(E\\) and \\(U\\) would vary. The\nmathematical form of the environment \\(E\\) and the utility function\n\\(U\\) would vary wildly from, say, chess to Jeopardy!. Of\ncourse, if we were to design a globally intelligent agent, and not\njust a chess-playing agent, we could get away with having just one\npair of \\(E\\) and \\(U\\). What would \\(E\\) look like if we were\nbuilding a generally intelligent agent and not just an agent that is\ngood at a single task? \\(E\\) would be a model of not just a single\ngame or a task, but the entire physical-social-virtual universe\nconsisting of many games, tasks, situations, problems, etc. This\nproject is (at least currently) hopelessly difficult as, obviously, we\nare nowhere near to having such a comprehensive theory-of-everything\nmodel. For further discussion of a theoretical architecture put\nforward for this problem, see the\n Supplement on the AIXI architecture.\n \n\nIt should be mentioned that there is a different, much more\nstraightforward answer to the \u201cWhat is AI?\u201d question. This\nanswer, which goes back to the days of the original Dartmouth\nconference, was expressed by, among others, Newell (1973), one of the\ngrandfathers of modern-day AI (recall that he attended the 1956\nconference); it is: \n\nAI is the field devoted to building artifacts that are intelligent,\nwhere \u2018intelligent\u2019 is operationalized through\nintelligence tests (such as the Wechsler Adult Intelligence Scale),\nand other tests of mental ability (including, e.g., tests of\nmechanical ability, creativity, and so on).\n\n\nThe above definition can be seen as fully specifying a concrete\nversion of Russell and Norvig\u2019s four possible goals. Though few\nare aware of this now, this answer was taken quite seriously for a\nwhile, and in fact underlied one of the most famous programs in the\nhistory of AI: the ANALOGY program of Evans (1968), which solved\ngeometric analogy problems of a type seen in many intelligence tests.\nAn attempt to rigorously define this forgotten form of AI (as what\nthey dub Psychometric AI), and to resurrect it from the days of\nNewell and Evans, is provided by Bringsjord and Schimanski (2003) [see\nalso e.g. (Bringsjord 2011)]. A sizable private investment has been\nmade in the ongoing attempt, now known as\n Project Aristo,\n to build a \u201cdigital Aristotle\u201d, in the form of a machine\nable to excel on standardized tests such at the AP exams tackled by US\nhigh school students (Friedland et al. 2004). (Vibrant work in this\ndirection continues today at the\n Allen Institute for Artificial Intelligence.)[18]\n In addition, researchers at Northwestern have forged a connection\nbetween AI and tests of mechanical ability (Klenk et al. 2005). \n\nIn the end, as is the case with any discipline, to really know\nprecisely what that discipline is requires you to, at least to some\ndegree, dive in and do, or at least dive in and read. Two decades ago\nsuch a dive was quite manageable. Today, because the content that has\ncome to constitute AI has mushroomed, the dive (or at least the swim\nafter it) is a bit more demanding.\n3. Approaches to AI\n\nThere are a number of ways of \u201ccarving up\u201d AI. By far the\nmost prudent and productive way to summarize the field is to turn yet\nagain to the AIMA text given its comprehensive overview of the\nfield. \n3.1 The Intelligent Agent Continuum\n\nAs Russell and Norvig (2009) tell us in the Preface of AIMA:\n\n\nThe main unifying theme is the idea of an intelligent agent. We define\nAI as the study of agents that receive percepts from the environment\nand perform actions. Each such agent implements a function that maps\npercept sequences to actions, and we cover different ways to represent\nthese functions\u2026 (Russell & Norvig 2009, vii)\n\n\nThe basic picture is thus summed up in this figure:\n\n\n\nImpressionistic Overview of an Intelligent Agent\n\n\nThe content of AIMA derives, essentially, from fleshing out\nthis picture; that is, the above figure corresponds to the different\nways of representing the overall function that intelligent agents\nimplement. And there is a progression from the least powerful agents\nup to the more powerful ones. The following figure gives a high-level\nview of a simple kind of agent discussed early in the book. (Though\nsimple, this sort of agent corresponds to the architecture of\nrepresentation-free agents designed and implemented by Rodney Brooks,\n1991.)\n\n\n\nA Simple Reflex Agent\n\n\nAs the book progresses, agents get increasingly sophisticated, and the\nimplementation of the function they represent thus draws from more and\nmore of what AI can currently muster. The following figure gives an\noverview of an agent that is a bit smarter than the simple reflex\nagent. This smarter agent has the ability to internally model the\noutside world, and is therefore not simply at the mercy of what can at\nthe moment be directly sensed.\n\n\n\nA More Sophisticated Reflex Agent\n\n\nThere are seven parts to AIMA. As the reader passes through\nthese parts, she is introduced to agents that take on the powers\ndiscussed in each part. Part I is an introduction to the agent-based\nview. Part II is concerned with giving an intelligent agent the\ncapacity to think ahead a few steps in clearly defined environments.\nExamples here include agents able to successfully play games of\nperfect information, such as chess. Part III deals with agents that\nhave declarative knowledge and can reason in ways that will be quite\nfamiliar to most philosophers and logicians (e.g., knowledge-based\nagents deduce what actions should be taken to secure their goals).\nPart IV of the book outfits agents with the power to handle\nuncertainty by reasoning in probabilistic\n fashion.[19]\n In Part V, agents are given a capacity to learn. The following figure\nshows the overall structure of a learning agent.\n\n\n\nA Learning Agent\n\n\nThe final set of powers agents are given allow them to communicate.\nThese powers are covered in Part VI. \n\nPhilosophers who patiently travel the entire progression of\nincreasingly smart agents will no doubt ask, when reaching the end of\nPart VII, if anything is missing. Are we given enough, in general, to\nbuild an artificial person, or is there enough only to build a mere\nanimal? This question is implicit in the following from Charniak and\nMcDermott (1985): \n\nThe ultimate goal of AI (which we are very far from achieving) is to\nbuild a person, or, more humbly, an animal. (Charniak & McDermott\n1985, 7)\n\n\nTo their credit, Russell & Norvig, in AIMA\u2019s Chapter\n27, \u201cAI: Present and Future,\u201d consider this question, at\nleast to some\n degree.[]\n They do so by considering some challenges to AI that have hitherto\nnot been met. One of these challenges is described by R&N as\nfollows: \n\n[M]achine learning has made very little progress on the important\nproblem of constructing new representations at levels of abstraction\nhigher than the input vocabulary. In computer vision, for example,\nlearning complex concepts such as Classroom and Cafeteria would be\nmade unnecessarily difficult if the agent were forced to work from\npixels as the input representation; instead, the agent needs to be\nable to form intermediate concepts first, such as Desk and Tray,\nwithout explicit human supervision. Similar concepts apply to learning\nbehavior: HavingACupOfTea is a very important high-level step\nin many plans, but how does it get into an action library that\ninitially contains much simpler actions such as RaiseArm and Swallow?\nPerhaps this will incorporate deep belief networks \u2013\nBayesian networks that have multiple layers of hidden variables, as in\nthe work of Hinton et al. (2006), Hawkins and Blakeslee (2004),\nand Bengio and LeCun (2007). \u2026 Unless we understand such\nissues, we are faced with the daunting task of constructing large\ncommonsense knowledge bases by hand, and approach that has not fared\nwell to date. (Russell & Norvig 2009, Ch. 27.1)\n\n\nWhile there has seen some advances in addressing this challenge (in\nthe form of deep learning or representation learning),\nthis specific challenge is actually merely a foothill before a range\nof dizzyingly high mountains that AI must eventually somehow manage to\nclimb. One of those mountains, put simply, is\n reading.[21]\n Despite the fact that, as noted, Part V of AIMA is devoted to\nmachine learning, AI, as it stands, offers next to nothing in the way\nof a mechanization of learning by reading. Yet when you think about\nit, reading is probably the dominant way you learn at this stage in\nyour life. Consider what you\u2019re doing at this very moment.\nIt\u2019s a good bet that you are reading this sentence because,\nearlier, you set yourself the goal of learning about the field of AI.\n Yet\n the formal models of learning provided in AIMA\u2019s Part IV\n(which are all and only the models at play in AI) cannot be applied to\nlearning by\n reading.[22]\n These models all start with a function-based view of learning.\nAccording to this view, to learn is almost invariably to produce an\nunderlying function \\(\\ff\\) on the basis of a restricted set of\npairs \n\n\\[\n \\left\\{\\left\\langle x_1, \\ff(x_1)\\right\\rangle,\\left\\langle x_2, \\ff(x_2)\\right\\rangle, \\ldots, \\left\\langle x_n, \\ff(x_n)\\right\\rangle\\right\\}.\n\\]\n\n\nFor example, consider receiving inputs consisting of 1, 2, 3, 4, and\n5, and corresponding range values of 1, 4, 9, 16, and 25; the goal is\nto \u201clearn\u201d the underlying mapping from natural numbers to\nnatural numbers. In this case, assume that the underlying function is\n\\(n^2\\), and that you do \u201clearn\u201d it. While this narrow\nmodel of learning can be productively applied to a number of\nprocesses, the process of reading isn\u2019t one of them. Learning by\nreading cannot (at least for the foreseeable future) be modeled as\ndivining a function that produces argument-value pairs. Instead, your\nreading about AI can pay dividends only if your knowledge has\nincreased in the right way, and if that knowledge leaves you\npoised to be able to produce behavior taken to confirm sufficient\nmastery of the subject area in question. This behavior can range from\ncorrectly answering and justifying test questions regarding AI, to\nproducing a robust, compelling presentation or paper that signals your\nachievement. \n\nTwo points deserve to be made about machine reading. First, it may not\nbe clear to all readers that reading is an ability that is central to\nintelligence. The centrality derives from the fact that intelligence\nrequires vast knowledge. We have no other means of getting systematic\nknowledge into a system than to get it in from text, whether text on\nthe web, text in libraries, newspapers, and so on. You might even say\nthat the big problem with AI has been that machines really don\u2019t\nknow much compared to humans. That can only be because of the fact\nthat humans read (or hear: illiterate people can listen to text being\nuttered and learn that way). Either machines gain knowledge by humans\nmanually encoding and inserting knowledge, or by reading and\nlistening. These are brute facts. (We leave aside supernatural\ntechniques, of course. Oddly enough, Turing didn\u2019t: he seemed to\nthink ESP should be discussed in connection with the powers of minds\nand machines. See Turing,\n 1950.)[23]\n\nNow for the second point. Humans able to read have invariably also\nlearned a language, and learning languages has been modeled in\nconformity to the function-based approach adumbrated just above\n(Osherson et al. 1986). However, this doesn\u2019t entail that an\nartificial agent able to read, at least to a significant degree, must\nhave really and truly learned a natural language. AI is first and\nforemost concerned with engineering computational artifacts that\nmeasure up to some test (where, yes, sometimes that test is from the\nhuman sphere), not with whether these artifacts process information in\nways that match those present in the human case. It may or may not be\nnecessary, when engineering a machine that can read, to imbue that\nmachine with human-level linguistic competence. The issue is\nempirical, and as time unfolds, and the engineering is pursued, we\nshall no doubt see the issue settled. \n\nTwo additional high mountains facing AI are subjective consciousness\nand creativity, yet it would seem that these great challenges are ones\nthe field apparently hasn\u2019t even come to grips with. Mental\nphenomena of paramount importance to many philosophers of mind and\nneuroscience are simply missing from AIMA. For example,\nconsciousness is only mentioned in passing in AIMA, but\nsubjective consciousness is the most important thing in our lives\n\u2013 indeed we only desire to go on living because we wish to go on\nenjoying subjective states of certain types. Moreover, if human minds\nare the product of evolution, then presumably phenomenal consciousness\nhas great survival value, and would be of tremendous help to a robot\nintended to have at least the behavioral repertoire of the first\ncreatures with brains that match our own (hunter-gatherers; see Pinker\n1997). Of course, subjective consciousness is largely missing from the\nsister fields of cognitive psychology and computational cognitive\nmodeling as well. We discuss some of these challenges in the\n Philosophy of Artificial Intelligence\n section below. For a list of similar challenges to cognitive science,\nsee the relevant\n section of the entry on cognitive science.[24]\n\n\nTo some readers, it might seem in the very least tendentious to point\nto subjective consciousness as a major challenge to AI that it has yet\nto address. These readers might be of the view that pointing to this\nproblem is to look at AI through a distinctively philosophical prism,\nand indeed a controversial philosophical standpoint. \n\nBut as its literature makes clear, AI measures itself by looking to\nanimals and humans and picking out in them remarkable mental powers,\nand by then seeing if these powers can be mechanized. Arguably the\npower most important to humans (the capacity to experience) is nowhere\nto be found on the target list of most AI researchers. There may be a\ngood reason for this (no formalism is at hand, perhaps), but there is\nno denying the state of affairs in question obtains, and that, in\nlight of how AI measures itself, that it\u2019s worrisome. \n\nAs to creativity, it\u2019s quite remarkable that the power we most\npraise in human minds is nowhere to be found in AIMA. Just as\nin (Charniak & McDermott 1985) one cannot find\n\u2018neural\u2019 in the index, \u2018creativity\u2019\ncan\u2019t be found in the index of AIMA. This is particularly\nodd because many AI researchers have in fact worked on creativity\n(especially those coming out of philosophy; e.g., Boden 1994,\nBringsjord & Ferrucci 2000). \n\nAlthough the focus has been on AIMA, any of its counterparts\ncould have been used. As an example, consider Artificial\nIntelligence: A New Synthesis, by Nils Nilsson. As in the case of\nAIMA, everything here revolves around a gradual progression\nfrom the simplest of agents (in Nilsson\u2019s case, reactive\nagents), to ones having more and more of those powers that\ndistinguish persons. Energetic readers can verify that there is a\nstriking parallel between the main sections of Nilsson\u2019s book\nand AIMA. In addition, Nilsson, like Russell and Norvig,\nignores phenomenal consciousness, reading, and creativity. None of the\nthree are even mentioned. Likewise, a recent comprehensive AI textbook\nby Luger (2008) follows the same pattern. \n\nA final point to wrap up this section. It seems quite plausible to\nhold that there is a certain inevitability to the structure of an AI\ntextbook, and the apparent reason is perhaps rather interesting. In\npersonal conversation, Jim Hendler, a well-known AI researcher who is\none of the main innovators behind Semantic Web (Berners-Lee, Hendler,\nLassila 2001), an under-development \u201cAI-ready\u201d version of\nthe World Wide Web, has said that this inevitability can be rather\neasily displayed when teaching Introduction to AI; here\u2019s how.\nBegin by asking students what they think AI is. Invariably, many\nstudents will volunteer that AI is the field devoted to building\nartificial creatures that are intelligent. Next, ask for examples of\nintelligent creatures. Students always respond by giving examples\nacross a continuum: simple multi-cellular organisms, insects, rodents,\nlower mammals, higher mammals (culminating in the great apes), and\nfinally human persons. When students are asked to describe the\ndifferences between the creatures they have cited, they end up\nessentially describing the progression from simple agents to ones\nhaving our (e.g.) communicative powers. This progression gives the\nskeleton of every comprehensive AI textbook. Why does this happen? The\nanswer seems clear: it happens because we can\u2019t resist\nconceiving of AI in terms of the powers of extant creatures with which\nwe are familiar. At least at present, persons, and the creatures who\nenjoy only bits and pieces of personhood, are \u2013 to repeat\n\u2013 the measure of\n AI.[25]\n\n3.2 Logic-Based AI: Some Surgical Points\n\nReasoning based on classical deductive logic is monotonic; that is, if\n\\(\\Phi\\vdash\\phi\\), then for all \\(\\psi\\), \\(\\Phi\\cup\n\\{\\psi\\}\\vdash\\phi\\). Commonsense reasoning is not monotonic. While\nyou may currently believe on the basis of reasoning that your house is\nstill standing, if while at work you see on your computer screen that\na vast tornado is moving through the location of your house, you will\ndrop this belief. The addition of new information causes previous\ninferences to fail. In the simpler example that has become an AI\nstaple, if I tell you that Tweety is a bird, you will infer that\nTweety can fly, but if I then inform you that Tweety is a penguin, the\ninference evaporates, as well it should. Nonmonotonic (or defeasible)\nlogic includes formalisms designed to capture the mechanisms\nunderlying these kinds of examples. See the separate entry on\n logic and artificial intelligence,\n which is focused on nonmonotonic reasoning, and reasoning about time\nand change. It also provides a history of the early days of\nlogic-based AI, making clear the contributions of those who founded\nthe tradition (e.g., John McCarthy and Pat Hayes; see their seminal\n1969 paper). \n\nThe formalisms and techniques of logic-based AI have reached a level\nof impressive maturity \u2013 so much so that in various academic and\ncorporate laboratories, implementations of these formalisms and\ntechniques can be used to engineer robust, real-world software. It is\nstrongly recommend that readers who have an interest to learn where AI\nstands in these areas consult (Mueller 2006), which provides, in one\nvolume, integrated coverage of nonmonotonic reasoning (in the form,\nspecifically, of circumscription), and reasoning about time and change\nin the situation and event calculi. (The former calculus is also\nintroduced by Thomason. In the second, timepoints are included, among\nother things.) The other nice thing about (Mueller 2006) is that the\nlogic used is multi-sorted first-order logic (MSL), which has\nunificatory power that will be known to and appreciated by many\ntechnical philosophers and logicians (Manzano 1996). \n\nWe now turn to three further topics of importance in AI. They are:\n\n The overarching scheme of logicist AI, in the context of the\nattempt to build intelligent artificial agents. \n Common Logic and the intensifying quest for interoperability.\n\n A technique that can be called encoding down, which can\nallow machines to reason efficiently over knowledge that, were it not\nencoded down, would, when reasoned over, lead to paralyzing\ninefficiency. \n\n\nThis trio is covered in order, beginning with the first.\n\nDetailed accounts of logicist AI that fall under the agent-based\nscheme can be found in (Lenat 1983, Lenat & Guha 1990, Nilsson\n1991, Bringsjord & Ferrucci\n 1998).[26].\n The core idea is that an intelligent agent receives percepts from the\nexternal world in the form of formulae in some logical system (e.g.,\nfirst-order logic), and infers, on the basis of these percepts and its\nknowledge base, what actions should be performed to secure the\nagent\u2019s goals. (This is of course a barbaric simplification.\nInformation from the external world is encoded in formulae,\nand transducers to accomplish this feat may be components of the\nagent.) \n\nTo clarify things a bit, we consider, briefly, the logicist view in\nconnection with arbitrary logical systems\n \\(\\mathcal{L}_{X}\\).[27]\n We obtain a particular logical system by setting \\(X\\) in the\nappropriate way. Some examples: If \\(X=I\\), then we have a system at\nthe level of FOL [following the standard notation from model theory;\nsee e.g. (Ebbinghaus et al. 1984)]. \\(\\mathcal{L}_{II}\\) is\nsecond-order logic, and \\(\\mathcal{L}_{\\omega_I\\omega}\\) is a\n\u201csmall system\u201d of infinitary logic (countably infinite\nconjunctions and disjunctions are permitted). These logical systems\nare all extensional, but there are intensional ones as\nwell. For example, we can have logical systems corresponding to those\nseen in standard propositional modal logic (Chellas 1980). One\npossibility, familiar to many philosophers, would be propositional\nKT45, or\n \\(\\mathcal{L}_{KT45}\\).[28]\n In each case, the system in question includes a relevant alphabet\nfrom which well-formed formulae are constructed by way of a formal\ngrammar, a reasoning (or proof) theory, a formal semantics, and at\nleast some meta-theoretical results (soundness, completeness, etc.).\nTaking off from standard notation, we can thus say that a set of\nformulas in some particular logical system \\(\\mathcal{L}_X\\),\n\\(\\Phi_{\\mathcal{L}_X}\\), can be used, in conjunction with some\nreasoning theory, to infer some particular formula\n\\(\\phi_{\\mathcal{L}_X}\\). (The reasoning may be deductive, inductive,\nabductive, and so on. Logicist AI isn\u2019t in the least restricted\nto any particular mode of reasoning.) To say that such a situation\nholds, we write \n\n    \\[\n    \\Phi_{\\mathcal{L}_X} \\vdash_{\\mathcal{L}_X} \\phi_{\\mathcal{L}_X}\n    \\]\n\n \n\nWhen the logical system referred to is clear from context, or when we\ndon\u2019t care about which logical system is involved, we can simply\nwrite \n\n    \\[\n    \\Phi \\vdash \\phi\n    \\]\n\n \n\nEach logical system, in its formal semantics, will include objects\ndesigned to represent ways the world pointed to by formulae in this\nsystem can be. Let these ways be denoted by \\(W^i_{{\\mathcal{L}_X}}\\).\nWhen we aren\u2019t concerned with which logical system is involved,\nwe can simply write \\(W^i\\). To say that such a way models a formula\n\\(\\phi\\) we write \n\n    \\[\n    W_i \\models \\phi\n    \\]\n\n \n\nWe extend this to a set of formulas in the natural way:\n\\(W^i\\models\\Phi\\) means that all the elements of \\(\\Phi\\) are true on\n\\(W^i\\). Now, using the simple machinery we\u2019ve established, we\ncan describe, in broad strokes, the life of an intelligent agent that\nconforms to the logicist point of view. This life conforms to the\nbasic cycle that undergirds intelligent agents in the AIMA\nsense. \n\nTo begin, we assume that the human designer, after studying the world,\nuses the language of a particular logical system to give to our agent\nan initial set of beliefs \\(\\Delta_0\\) about what this world is like.\nIn doing so, the designer works with a formal model of this world,\n\\(W\\), and ensures that \\(W\\models\\Delta_0\\). Following tradition, we\nrefer to \\(\\Delta_0\\) as the agent\u2019s (starting) knowledge\nbase. (This terminology, given that we are talking about the\nagent\u2019s beliefs, is known to be peculiar, but it\npersists.) Next, the agent ADJUSTS its knowlege base to produce\na new one, \\(\\Delta_1\\). We say that adjustment is carried out by way\nof an operation \\(\\mathcal{A}\\); so\n\\(\\mathcal{A}[\\Delta_0]=\\Delta_1\\). How does the adjustment process,\n\\(\\mathcal{A}\\), work? There are many possibilities. Unfortunately,\nmany believe that the simplest possibility (viz.,\n\\(\\mathcal{A}[\\Delta_i]\\) equals the set of all formulas that can be\ndeduced in some elementary manner from \\(\\Delta_i\\)) exhausts\nall the possibilities. The reality is that adjustment, as\nindicated above, can come by way of any mode of reasoning\n\u2013 induction, abduction, and yes, various forms of deduction\ncorresponding to the logical system in play. For present purposes,\nit\u2019s not important that we carefully enumerate all the options.\n\n\nThe cycle continues when the agent ACTS on the environment, in\nan attempt to secure its goals. Acting, of course, can cause changes\nto the environment. At this point, the agent SENSES the\nenvironment, and this new information \\(\\Gamma_1\\) factors into the\nprocess of adjustment, so that\n\\(\\mathcal{A}[\\Delta_1\\cup\\Gamma_1]=\\Delta_2\\). The cycle of SENSES\n\\(\\Rightarrow\\) ADJUSTS \\(\\Rightarrow\\) ACTS continues to produce\nthe life \\(\\Delta_0,\\Delta_1,\\Delta_2,\\Delta_3,\\ldots,\\) \u2026 of\nour agent. \n\nIt may strike you as preposterous that logicist AI be touted as an\napproach taken to replicate all of cognition. Reasoning over\nformulae in some logical system might be appropriate for\ncomputationally capturing high-level tasks like trying to solve a math\nproblem (or devising an outline for an entry in the Stanford\nEncyclopedia of Philosophy), but how could such reasoning apply to\ntasks like those a hawk tackles when swooping down to capture\nscurrying prey? In the human sphere, the task successfully negotiated\nby athletes would seem to be in the same category. Surely, some will\ndeclare, an outfielder chasing down a fly ball doesn\u2019t prove\ntheorems to figure out how to pull off a diving catch to save the\ngame! Two brutally reductionistic arguments can be given in support of\nthis \u201clogicist theory of everything\u201d approach towards\ncognition. The first stems from the fact that a complete proof\ncalculus for just first-order logic can simulate all of Turing-level\ncomputation (Chapter 11, Boolos et al. 2007). The second justification\ncomes from the role logic plays in foundational theories of\nmathematics and mathematical reasoning. Not only are foundational\ntheories of mathematics cast in logic (Potter 2004), but there have\nbeen successful projects resulting in machine verification of ordinary\nnon-trivial theorems, e.g., in the\n Mizar project\n alone around 50,000 theorems have been verified (Naumowicz and\nKornilowicz 2009). The argument goes that if any approach to AI can be\ncast mathematically, then it can be cast in a logicist form. \n\nNeedless to say, such a declaration has been carefully considered by\nlogicists beyond the reductionistic argument given above. For example,\nRosenschein and Kaelbling (1986) describe a method in which logic is\nused to specify finite state machines. These machines are used at\n\u201crun time\u201d for rapid, reactive processing. In this\napproach, though the finite state machines contain no logic in the\ntraditional sense, they are produced by logic and inference. Real\nrobot control via first-order theorem proving has been demonstrated by\nAmir and Maynard-Reid (1999, 2000, 2001). In fact, you can\n download\n version 2.0 of the software that makes this approach real for a Nomad\n200 mobile robot in an office environment. Of course, negotiating an\noffice environment is a far cry from the rapid adjustments an\noutfielder for the Yankees routinely puts on display, but certainly\nit\u2019s an open question as to whether future machines will be able\nto mimic such feats through rapid reasoning. The question is open if\nfor no other reason than that all must concede that the constant\nincrease in reasoning speed of first-order theorem provers is\nbreathtaking. (For up-to-date news on this increase, visit and monitor\nthe\n TPTP site.)\n There is no known reason why the software engineering in question\ncannot continue to produce speed gains that would eventually allow an\nartificial creature to catch a fly ball by processing information in\npurely logicist fashion. \n\nNow we come to the second topic related to logicist AI that warrants\nmention herein: common logic and the intensifying quest for\ninteroperability between logic-based systems using different logics.\nOnly a few brief comments are\n offered.[29]\n Readers wanting more can explore the links provided in the course of\nthe summary. \n\nOne standardization is through what is known as\n Common Logic\n (CL), and variants thereof. (CL is published as an\n ISO standard\n \u2013 ISO is the International Standards Organization.)\nPhilosophers interested in logic, and of course logicians, will find\nCL to be quite fascinating. From an historical perspective, the advent\nof CL is interesting in no small part because the person spearheading\nit is none other than Pat Hayes, the same Hayes who, as we have seen,\nworked with McCarthy to establish logicist AI in the 1960s. Though\nHayes was not at the original 1956 Dartmouth conference, he certainly\nmust be regarded as one of the founders of contemporary AI.) One of\nthe interesting things about CL, at least as we see it, is that it\nsignifies a trend toward the marriage of logics, and programming\nlanguages and environments. Another system that is a logic/programming\nhybrid is\n Athena,\n which can be used as a programming language, and is at the same time\na form of MSL. Athena is based on formal systems known as\ndenotational proof languages (Arkoudas 2000). \n\nHow is interoperability between two systems to be enabled by CL?\nSuppose one of these systems is based on logic \\(L\\), and the other on\n\\(L'\\). (To ease exposition, assume that both logics are first-order.)\nThe idea is that a theory \\(\\Phi_L\\), that is, a set of formulae in\n\\(L\\), can be translated into CL, producing \\(\\Phi_{CL}\\), and then\nthis theory can be translated into \\(\\Phi_L'\\). CL thus becomes an\ninter lingua. Note that what counts as a well-formed formula in\n\\(L\\) can be different than what counts as one in \\(L'\\). The two\nlogics might also have different proof theories. For example,\ninference in \\(L\\) might be based on resolution, while inference in\n\\(L'\\) is of the natural deduction variety. Finally, the symbol sets\nwill be different. Despite these differences, courtesy of the\ntranslations, desired behavior can be produced across the translation.\nThat, at any rate, is the hope. The technical challenges here are\nimmense, but federal monies are increasingly available for attacks on\nthe problem of interoperability.\n\nNow for the third topic in this section: what can be called\nencoding down. The technique is easy to understand. Suppose\nthat we have on hand a set \\(\\Phi\\) of first-order axioms. As is\nwell-known, the problem of deciding, for arbitrary formula \\(\\phi\\),\nwhether or not it\u2019s deducible from \\(\\Phi\\) is\nTuring-undecidable: there is no Turing machine or equivalent that can\ncorrectly return \u201cYes\u201d or \u201cNo\u201d in the general\ncase. However, if the domain in question is finite, we can encode this\nproblem down to the propositional calculus. An assertion that all\nthings have \\(F\\) is of course equivalent to the assertion that\n\\(Fa\\), \\(Fb\\), \\(Fc\\), as long as the domain contains only these\nthree objects. So here a first-order quantified formula becomes a\nconjunction in the propositional calculus. Determining whether such\nconjunctions are provable from axioms themselves expressed in the\npropositional calculus is Turing-decidable, and in addition, in\ncertain clusters of cases, the check can be done very quickly in the\npropositional case; very quickly. Readers interested in\nencoding down to the propositional calculus should consult recent\n DARPA-sponsored work by Bart Selman.\n Please note that the target of encoding down doesn\u2019t need to be\nthe propositional calculus. Because it\u2019s generally harder for\nmachines to find proofs in an intensional logic than in straight\nfirst-order logic, it is often expedient to encode down the former to\nthe latter. For example, propositional modal logic can be encoded in\nmulti-sorted logic (a variant of FOL); see (Arkoudas & Bringsjord\n2005). Prominent usage of such an encoding down can be found in a set\nof systems known as Description Logics, which are a set of\nlogics less expressive than first-order logic but more expressive than\npropositional logic (Baader et al. 2003). Description logics are used\nto reason about ontologies in a given domain and have been\nsuccessfully used, for example, in the biomedical domain (Smith et al.\n2007). \n3.3 Non-Logicist AI: A Summary\n\nIt\u2019s tempting to define non-logicist AI by negation: an approach\nto building intelligent agents that rejects the distinguishing\nfeatures of logicist AI. Such a shortcut would imply that the agents\nengineered by non-logicist AI researchers and developers, whatever the\nvirtues of such agents might be, cannot be said to know that \\(\\phi\\);\n\u2013 for the simple reason that, by negation, the non-logicist\nparadigm would have not even a single declarative proposition that is\na candidate for \\(\\phi\\);. However, this isn\u2019t a particularly\nenlightening way to define non-symbolic AI. A more productive approach\nis to say that non-symbolic AI is AI carried out on the basis of\nparticular formalisms other than logical systems, and to then\nenumerate those formalisms. It will turn out, of course, that these\nformalisms fail to include knowledge in the normal sense. (In\nphilosophy, as is well-known, the normal sense is one according to\nwhich if \\(p\\) is known, \\(p\\) is a declarative statement.) \n\nFrom the standpoint of formalisms other than logical systems,\nnon-logicist AI can be partitioned into symbolic but non-logicist\napproaches, and connectionist/neurocomputational approaches. (AI\ncarried out on the basis of symbolic, declarative structures that, for\nreadability and ease of use, are not treated directly by researchers\nas elements of formal logics, does not count. In this category fall\ntraditional semantic networks, Schank\u2019s (1972) conceptual\ndependency scheme, frame-based schemes, and other such schemes.) The\nformer approaches, today, are probabilistic, and are based on the\nformalisms (Bayesian networks) covered\n below.\n The latter approaches are based, as we have noted, on formalisms that\ncan be broadly termed \u201cneurocomputational.\u201d Given our\nspace constraints, only one of the formalisms in this category is\ndescribed here (and briefly at that): the aforementioned artificial\nneural\n networks.[30].\n Though artificial neural networks, with an appropriate architecture,\ncould be used for arbitrary computation, they are almost exclusively\nused for building learning systems. \n\nNeural nets are composed of units or nodes designed to\nrepresent neurons, which are connected by links designed to\nrepresent dendrites, each of which has a numeric weight.\n\n\n\nA \u201cNeuron\u201d Within an Artificial Neural Network (from\nAIMA3e)\n\n\nIt is usually assumed that some of the units work in symbiosis with\nthe external environment; these units form the sets of input\nand output units. Each unit has a current activation\nlevel, which is its output, and can compute, based on its inputs\nand weights on those inputs, its activation level at the next moment\nin time. This computation is entirely local: a unit takes account of\nbut its neighbors in the net. This local computation is calculated in\ntwo stages. First, the input function, \\(in_i\\), gives the\nweighted sum of the unit\u2019s input values, that is, the sum of the\ninput activations multiplied by their weights:  \n\n\\[\nin_i = \\displaystyle\\sum_j W_{ji} a_j\n\\]\n\n\nIn the second stage, the activation function, \\(g\\), takes the\ninput from the first stage as argument and generates the output, or\nactivation level, \\(a_i\\):  \n\n\\[\na_i = g(in_i) = g \\left(\\displaystyle\\sum_j W_{ji}a_j\\right)\n\\]\n\n\nOne common (and confessedly elementary) choice for the activation\nfunction (which usually governs all units in a given net) is the step\nfunction, which usually has a threshold \\(t\\) that sees to it that a 1\nis output when the input is greater than \\(t\\), and that 0 is output\notherwise. This is supposed to be \u201cbrain-like\u201d to some\ndegree, given that 1 represents the firing of a pulse from a neuron\nthrough an axon, and 0 represents no firing. A simple three-layer\nneural net is shown in the following picture.\n\n\n\nA Simple Three-Layer Artificial Neural Network (from\nAIMA3e)\n\n\nAs you might imagine, there are many different kinds of neural\nnetworks. The main distinction is between feed-forward and\nrecurrent networks. In feed-forward networks like the one\npictured immediately above, as their name suggests, links move\ninformation in one direction, and there are no cycles; recurrent\nnetworks allow for cycling back, and can become rather complicated.\nFor a more detailed presentation, see the\n\nSupplement on Neural Nets.\n \n\nNeural networks were fundamentally plagued by the fact that while they\nare simple and have theoretically efficient learning algorithms, when\nthey are multi-layered and thus sufficiently expressive to represent\nnon-linear functions, they were very hard to train in practice. This\nchanged in the mid 2000s with the advent of methods that exploit\nstate-of-the-art hardware better (Rajat et al. 2009). The\nbackpropagation method for training multi-layered neural networks can\nbe translated into a sequence of repeated simple arithmetic operations\non a large set of numbers. The general trend in computing hardware has\nfavored algorithms that are able to do a large of number of simple\noperations that are not that dependent on each other, versus a small\nof number of complex and intricate operations.\n\nAnother key recent observation is that deep neural networks can be\npre-trained first in an unsupervised phase where they are just fed\ndata without any labels for the data. Each hidden layer is forced to\nrepresent the outputs of the layer below. The outcome of this training\nis a series of layers which represent the input domain with increasing\nlevels of abstraction. For example, if we pre-train the network with\nimages of faces, we would get a first layer which is good at detecting\nedges in images, a second layer which can combine edges to form facial\nfeatures such as eyes, noses etc., a third layer which responds to\ngroups of features, and so on (LeCun et al. 2015).\n\nPerhaps the best technique for teaching students about neural networks\nin the context of other statistical learning formalisms and methods is\nto focus on a specific problem, preferably one that seems unnatural to\ntackle using logicist techniques. The task is then to seek to engineer\na solution to the problem, using any and all techniques\navailable. One nice problem is handwriting recognition (which\nalso happens to have a rich philosophical dimension; see e.g.\nHofstadter & McGraw 1995). For example, consider the problem of\nassigning, given as input a handwritten digit \\(d\\), the correct\ndigit, 0 through 9. Because there is a database of 60,000 labeled\ndigits available to researchers (from the National Institute of\nScience and Technology), this problem has evolved into a benchmark\nproblem for comparing learning algorithms. It turns out that neural\nnetworks currently reign as the best approach to the problem according\nto a recent ranking by Benenson (2016).\n\nReaders interested in AI (and computational cognitive science) pursued\nfrom an overtly brain-based orientation are encouraged to explore the\nwork of Rick Granger (2004a, 2004b) and researchers in his\n Brain Engineering Laboratory\n and\n W. H. Neukom Institute for Computational Sciences.\n The contrast between the \u201cdry\u201d, logicist AI started at\nthe original 1956 conference, and the approach taken here by Granger\nand associates (in which brain circuitry is directly modeled) is\nremarkable. For those interested in computational properties of neural\nnetworks, Hornik et al. (1989) address the general representation\ncapability of neural networks independent of learning. \n3.4 AI Beyond the Clash of Paradigms\n\nAt this point the reader has been exposed to the chief formalisms in\nAI, and may wonder about heterogeneous approaches that bridge them. Is\nthere such research and development in AI? Yes. From an\nengineering standpoint, such work makes irresistibly good\nsense. There is now an understanding that, in order to build\napplications that get the job done, one should choose from a toolbox\nthat includes logicist, probabilistic/Bayesian, and neurocomputational\ntechniques. Given that the original top-down logicist paradigm is\nalive and thriving (e.g., see Brachman & Levesque 2004, Mueller\n2006), and that, as noted, a resurgence of Bayesian and\nneurocomputational approaches has placed these two paradigms on solid,\nfertile footing as well, AI now moves forward, armed with this\nfundamental triad, and it is a virtual certainty that applications\n(e.g., robots) will be engineered by drawing from elements of all\nthree. Watson\u2019s DeepQA architecture is one recent example of an\nengineering system that leverages multiple paradigms. For a detailed\ndiscussion, see the\n\nSupplement on Watson\u2019s DeepQA Architecture.\n \n\nGoogle DeepMind\u2019s AlphaGo is another example of a multi-paradigm\nsystem, although in a much narrower form than Watson. The central\nalgorithmic problem in games such as Go or Chess is to search through\na vast sequence of valid moves. For most non-trivial games, this is\nnot feasible to do so exhaustively. The Monte Carlo tree search (MCTS)\nalgorithm gets around this obstacle by searching through an enormous\nspace of valid moves in a statistical fashion (Browne et al. 2012).\nWhile MCTS is the central algorithm in AlpaGo, there are two neural\nnetworks which help evaluate states in the game and help model how\nexpert opponents play (Silver et al. 2016). It should be noted that\nMCTS is behind almost all the winning submissions in general game\nplaying (Finnsson 2012).\n\nWhat, though, about deep, theoretical integration of the main\nparadigms in AI? Such integration is at present only a possibility for\nthe future, but readers are directed to the research of some striving\nfor such integration. For example: Sun (1994, 2002) has been working\nto demonstrate that human cognition that is on its face symbolic in\nnature (e.g., professional philosophizing in the analytic tradition,\nwhich deals explicitly with arguments and definitions carefully\nsymbolized) can arise from cognition that is neurocomputational in\nnature. Koller (1997) has investigated the marriage between\nprobability theory and logic. And, in general, the very recent arrival\nof so-called human-level AI is being led by theorists seeking\nto genuinely integrate the three paradigms set out above (e.g.,\nCassimatis 2006).\n\nFinally, we note that cognitive architectures such as Soar\n(Laird 2012) and PolyScheme (Cassimatis 2006) are another area where\nintegration of different fields of AI can be found. For example, one\nsuch endeavor striving to build human-level AI is the Companions\nproject (Forbus and Hinrichs 2006). Companions are long-lived systems\nthat strive to be human-level AI systems that function as\ncollaborators with humans. The Companions architecture tries to solve\nmultiple AI problems such as reasoning and learning, interactivity,\nand longevity in one unifying system.\n4. The Explosive Growth of AI\n\nAs we noted above, work on AI has mushroomed over the past couple of\ndecades. Now that we have looked a bit at the content that composes\nAI, we take a quick look at the explosive growth of AI.\n\nFirst, a point of clarification. The growth of which we speak is not a\nshallow sort correlated with amount of funding provided for a given\nsub-field of AI. That kind of thing happens all the time in all\nfields, and can be triggered by entirely political and financial\nchanges designed to grow certain areas, and diminish others. Along the\nsame line, the growth of which we speak is not correlated with the\namount of industrial activity revolving around AI (or a sub-field\nthereof); for this sort of growth too can be driven by forces quite\noutside an expansion in the scientific breadth of\n AI.[31]\n Rather, we are speaking of an explosion of deep content: new\nmaterial which someone intending to be conversant with the field needs\nto know. Relative to other fields, the size of the explosion may or\nmay not be unprecedented. (Though it should perhaps be noted that an\nanalogous increase in philosophy would be marked by the development of\nentirely new formalisms for reasoning, reflected in the fact that,\nsay, longstanding philosophy textbooks like Copi\u2019s (2004)\nIntroduction to Logic are dramatically rewritten and enlarged\nto include these formalisms, rather than remaining anchored to\nessentially immutable core formalisms, with incremental refinement\naround the edges through the years.) But it certainly appears to be\nquite remarkable, and is worth taking note of here, if for no other\nreason than that AI\u2019s near-future will revolve in significant\npart around whether or not the new content in question forms a\nfoundation for new long-lived research and development that would not\notherwise\n obtain.[32]\n\nAI has also witnessed an explosion in its usage in various artifacts\nand applications. While we are nowhere near building a machine with\ncapabilities of a human or one that acts rationally in all scenarios\naccording to the Russell/Hutter definition above, algorithms that have\ntheir origins in AI research are now widely deployed for many tasks in\na variety of domains. \n4.1 Bloom in Machine Learning\n\nA huge part of AI\u2019s growth in applications has been made\npossible through invention of new algorithms in the subfield of\nmachine learning. Machine learning is concerned with building\nsystems that improve their performance on a task when given examples\nof ideal performance on the task, or improve their performance with\nrepeated experience on the task. Algorithms from machine learning have\nbeen used in speech recognition systems, spam filters, online\nfraud-detection systems, product-recommendation systems, etc. The\ncurrent state-of-the-art in machine learning can be divided into three\nareas (Murphy 2013, Alpaydin 2014):\n\nSupervised Learning: A form of learning in which a computer\ntries to learn a function \\(\\ff\\) given examples, the training data\n\\(T\\), of its values at various points in its domain \n\n\\[\nT=\\left\\{\\left\\langle x_1, \\ff(x_1)\\right\\rangle,\\left\\langle x_2, \\ff(x_2)\\right\\rangle, \\ldots, \\left\\langle x_n, \\ff(x_n)\\right\\rangle\\right\\}.\n\\] \n\n A\nsample task would be trying to label images of faces with a\nperson\u2019s name. The supervision in supervised learning comes in\nthe form of the value of the function \\(\\ff(x)\\) at various points\n\\(x\\) in some part of the domain of the function. This is usually\ngiven in the form of a fixed set of input and output pairs for the\nfunction. Let \\(\\hh\\) be the \u201clearned function.\u201d The goal\nof supervised learning is have \\({\\hh}\\) match as closely as possible\nthe true function \\({\\ff}\\) over the same domain. The error is\nusually defined in terms of an error function, for instance, \\(error =\n\\sum_{x\\in T} \\delta(\\ff(x) - \\hh(x))\\), over the training data \\(T\\).\nOther forms of supervision and goals for learning are possible. For\nexample, in active learning the learning algorithm can request\nthe value of the function for arbitrary inputs. Supervised learning\ndominates the field of machine learning and has been used in almost\nall practical applications mentioned just above. \nUnsupervised Learning: Here the machine tries to find\nuseful knowledge or information when given some raw data \\(\\left\\{\nx_1,x_2, \\ldots, x_n \\right\\}\\). There is no function associated with\nthe input that has to be learned. The idea is that the machine helps\nuncover interesting patterns or information that could be hidden in\nthe data. One use of unsupervised learning is data mining,\nwhere large volumes of data are searched for interesting information.\nPageRank, one of the earliest algorithms used by the Google\nsearch engine, can be considered to be an unsupervised learning system\nthat ranks pages without any human supervision (Chapter 14.10, Hastie\net al. 2009). \nReinforcement Learning: Here a machine is set loose in an\nenvironment where it constantly acts and perceives (similar to the\nRussell/Hutter view above) and only occasionally receives\nfeedback on its behavior in the form of rewards or punishments. The\nmachine has to learn to behave rationally from this feedback. One use\nof reinforcement learning has been in building agents to play computer\ngames. The objective here is to build agents that map sensory data\nfrom the game at every time instant to an action that would help win\nin the game or maximize a human player\u2019s enjoyment of the game.\nIn most games, we know how well we are playing only at the end of the\ngame or only at infrequent intervals throughout the game (e.g., a\nchess game that we feel we are winning could quickly turn against us\nat the end). In supervised learning, the training data has ideal\ninput-output pairs. This form of learning is not suitable for building\nagents that have to operate across a length of time and are judged not\non one action but a series of actions and their effects on the\nenvironment. The field of Reinforcement Learning tries to tackle this\nproblem through a variety of methods. Though a bit dated, Sutton and\nBarto (1998) provide a comprehensive introduction to the field. \n\n\nIn addition to being used in domains that are traditionally the ken of\nAI, machine-learning algorithms have also been used in all stages of\nthe scientific process. For example, machine-learning techniques are\nnow routinely applied to analyze large volumes of data generated from\nparticle accelerators. CERN, for instance, generates a petabyte\n(\\(10^{15}\\) bytes) per second, and statistical algorithms that have\ntheir origins in AI are used to filter and analyze this data. Particle\naccelerators are used in fundamental experimental research in physics\nto probe the structure of our physical universe. They work by\ncolliding larger particles together to create much finer particles.\nNot all such events are fruitful. Machine-learning methods have been\nused to select events which are then analyzed further (Whiteson &\nWhiteson 2009 and Baldi et al. 2014). More recently, researchers at\nCERN launched a machine learning\n  competition\n to aid in the analysis of the Higgs Boson. The goal of this challenge\nwas to develop algorithms that separate meaningful events from\nbackground noise given data from the Large Hadron Collider, a particle\naccelerator at CERN.\n\nIn the past few decades, there has been an explosion in data that does\nnot have any explicit semantics attached to it. This data is generated\nby both humans and machines. Most of this data is not easily\nmachine-processable; for example, images, text, video (as opposed to\ncarefully curated data in a knowledge- or data-base). This has given\nrise to a huge industry that applies AI techniques to get usable\ninformation from such enormous data. This field of applying techniques\nderived from AI to large volumes of data goes by names such as\n\u201cdata mining,\u201d \u201cbig data,\u201d\n\u201canalytics,\u201d etc. This field is too vast to even\nmoderately cover in the present article, but we note that there is no\nfull agreement on what constitutes such a \u201cbig-data\u201d\nproblem. One definition, from Madden (2012), is that big data differs\nfrom traditional machine-processable data in that it is too big (for\nmost of the existing state-of-the-art hardware), too quick (generated\nat a fast rate, e.g. online email transactions), or too hard. It is in\nthe too-hard part that AI techniques work quite well. While this\nuniverse is quite varied, we use the Watson\u2019s system later in\nthis article as an AI-relevant exemplar. As we will see later, while\nmost of this new explosion is powered by learning, it isn\u2019t\nentirely limited to just learning. This bloom in learning algorithms\nhas been supported by both a resurgence in neurocomputational\ntechniques and probabilistic techniques.\n4.2 The Resurgence of Neurocomputational Techniques\n\nOne of the remarkable aspects of (Charniak & McDermott 1985) is\nthis: The authors say the central dogma of AI is that \u201cWhat the\nbrain does may be thought of at some level as a kind of\ncomputation\u201d (p. 6). And yet nowhere in the book is brain-like\ncomputation discussed. In fact, you will search the index in vain for\nthe term \u2018neural\u2019 and its variants. Please note that the\nauthors are not to blame for this. A large part of AI\u2019s growth\nhas come from formalisms, tools, and techniques that are, in some\nsense, brain-based, not logic-based. A paper that conveys the\nimportance and maturity of neurocomputation is (Litt et al. 2006).\n(Growth has also come from a return of probabilistic techniques that\nhad withered by the mid-70s and 80s. More about that momentarily, in\nthe next \u201cresurgence\u201d\n section.)\n \n\nOne very prominent class of non-logicist formalism does make an\nexplicit nod in the direction of the brain: viz., artificial neural\nnetworks (or as they are often simply called, neural\nnetworks, or even just neural nets). (The structure of\nneural networks and more recent developments are discussed\n above).\n Because Minsky and Pappert\u2019s (1969) Perceptrons led many\n(including, specifically, many sponsors of AI research and\ndevelopment) to conclude that neural networks didn\u2019t have\nsufficient information-processing power to model human cognition, the\nformalism was pretty much universally dropped from AI. However, Minsky\nand Pappert had only considered very limited neural networks.\nConnectionism, the view that intelligence consists not in\nsymbolic processing, but rather non-symbolic processing at\nleast somewhat like what we find in the brain (at least at the\ncellular level), approximated specifically by artificial neural\nnetworks, came roaring back in the early 1980s on the strength of more\nsophisticated forms of such networks, and soon the situation was (to\nuse a metaphor introduced by John McCarthy) that of two horses in a\nrace toward building truly intelligent agents.\n\nIf one had to pick a year at which connectionism was resurrected, it\nwould certainly be 1986, the year Parallel Distributed\nProcessing (Rumelhart & McClelland 1986) appeared in print.\nThe rebirth of connectionism was specifically fueled by the\nback-propagation (backpropagation) algorithm over neural networks,\nnicely covered in Chapter 20 of AIMA. The\nsymbolicist/connectionist race led to a spate of lively debate in the\nliterature (e.g., Smolensky 1988, Bringsjord 1991), and some AI\nengineers have explicitly championed a methodology marked by a\nrejection of knowledge representation and reasoning. For example,\nRodney Brooks was such an engineer; he wrote the well-known\n\u201cIntelligence Without Representation\u201d (1991), and his Cog\nProject, to which we referred above, is arguably an incarnation of the\npremeditatedly non-logicist approach. Increasingly, however, those in\nthe business of building sophisticated systems find that both\nlogicist and more neurocomputational techniques are required (Wermter\n& Sun\n 2001).[33]\n In addition, the neurocomputational paradigm today includes\nconnectionism only as a proper part, in light of the fact that some of\nthose working on building intelligent systems strive to do so by\nengineering brain-based computation outside the neural network-based\napproach (e.g., Granger 2004a, 2004b). \n\nAnother recent resurgence in neurocomputational techniques has\noccurred in machine learning. The modus operandi in machine learning\nis that given a problem, say recognizing handwritten digits\n\\(\\{0,1,\\ldots,9\\}\\) or faces, from a 2D matrix representing an image\nof the digits or faces, a machine learning or a domain expert would\nconstruct a feature vector representation function for the\ntask. This function is a transformation of the input into a format\nthat tries to throw away irrelevant information in the input and keep\nonly information useful for the task. Inputs transformed by \\(\\rr\\)\nare termed features. For recognizing faces, irrelevant\ninformation could be the amount of lighting in the scene and relevant\ninformation could be information about facial features. The machine is\nthen fed a sequence of inputs represented by the features and the\nideal or ground truth output values for those inputs. This converts\nthe learning challenge from that of having to learn the function\n\\(\\ff\\) from the examples: \\(\\left\\{\\left\\langle x_1,\n\\ff(x_1)\\right\\rangle,\\left\\langle x_2, \\ff(x_2)\\right\\rangle, \\ldots,\n\\left\\langle x_n, \\ff(x_n)\\right\\rangle \\right\\}\\) to having to learn\nfrom possibly easier data: \\(\\left\\{\\left\\langle \\rr(x_1),\n\\ff(x_1)\\right\\rangle,\\left\\langle \\rr(x_2), \\ff(x_2)\\right\\rangle,\n\\ldots, \\left\\langle \\rr(x_n), \\ff(x_n)\\right\\rangle \\right\\}\\). Here\nthe function \\(\\rr\\) is the function that computes the feature vector\nrepresentation of the input. Formally, \\(\\ff\\) is assumed to be a\ncomposition of the functions \\(\\gg\\) and \\(\\rr\\). That is, for any\ninput \\(x\\), \\(f(x) = \\gg\\left(\\rr\\left(x\\right)\\right)\\). This is\ndenoted by \\(\\ff=\\gg\\circ \\rr\\). For any input, the features are first\ncomputed, and then the function \\(\\gg\\) is applied. If the feature\nrepresentation \\(\\rr\\) is provided by the domain expert, the learning\nproblem becomes simpler to the extent the feature representation takes\non the difficulty of the task. At one extreme, the feature vector\ncould hide an easily extractable form of the answer in the input and\nin the other extreme the feature representation could be just the\nplain input.\n\nFor non-trivial problems, choosing the right representation is vital.\nFor instance, one of the drastic changes in the AI landscape was due\nto Minsky and Papert\u2019s (1969) demonstration that the perceptron\ncannot learn even the binary XOR function, but this function\ncan be learnt by the perceptron if we have the right representation.\nFeature engineering has grown to be one of the most labor intensive\ntasks of machine learning, so much so that it is considered to be one\nof the \u201cblack arts\u201d of machine learning. The other\nsignificant black art of learning methods is choosing the right\nparameters. These black arts require significant human expertise and\nexperience, which can be quite difficult to obtain without significant\napprenticeship (Domingos 2012). Another bigger issue is that the task\nof feature engineering is just knowledge representation in a new skin.\n\n\nGiven this state of affairs, there has been a recent resurgence in\nmethods for automatically learning a feature representation function\n\\(\\rr\\); such methods potentially bypass a large part of human labor\nthat is traditionally required. Such methods are based mostly on what\nare now termed deep neural networks. Such networks are simply\nneural networks with two or more hidden layers. These networks allow\nus to learn a feature function \\(\\rr\\) by using one or more of the\nhidden layers to learn \\(\\rr\\). The general form of learning in which\none learns from the raw sensory data without much hand-based feature\nengineering has now its own term: deep learning. A general and\nyet concise definition (Bengio et al. 2015) is: \n\nDeep learning can safely be regarded as the study of models that\neither involve a greater amount of composition of learned functions or\nlearned concepts than traditional machine learning does. (Bengio et\nal. 2015, Chapter 1)\n\n\nThough the idea has been around for decades, recent innovations\nleading to more efficient learning techniques have made the approach\nmore feasible (Bengio et al. 2013). Deep-learning methods have\nrecently produced state-of-the-art results in image recognition (given\nan image containing various objects, label the objects from a given\nset of labels), speech recognition (from audio input, generate a\ntextual representation), and the analysis of data from particle\naccelerators (LeCun et al. 2015). Despite impressive results in tasks\nsuch as these, minor and major issues remain unresolved. A minor issue\nis that significant human expertise is still needed to choose an\narchitecture and set up the right parameters for the architecture; a\nmajor issue is the existence of so-called adversarial inputs,\nwhich are indistinguishable from normal inputs to humans but are\ncomputed in a special manner that makes a neural network regard them\nas different than similar inputs in the training data. The existence\nof such adversarial inputs, which remain stable across training data,\nhas raised doubts about how well performance on benchmarks can\ntranslate into performance in real-world systems with sensory noise\n(Szegedy et al. 2014).\n4.3 The Resurgence of Probabilistic Techniques\n\nThere is a second dimension to the explosive growth of AI: the\nexplosion in popularity of probabilistic methods that aren\u2019t\nneurocomputational in nature, in order to formalize and mechanize a\nform of non-logicist reasoning in the face of uncertainty.\nInterestingly enough, it is Eugene Charniak himself who can be safely\nconsidered one of the leading proponents of an explicit, premeditated\nturn away from logic to statistical techniques. His area of\nspecialization is natural language processing, and whereas his\nintroductory textbook of 1985 gave an accurate sense of his approach\nto parsing at the time (as we have seen, write computer programs that,\ngiven English text as input, ultimately infer meaning expressed in\nFOL), this approach was abandoned in favor of purely statistical\napproaches (Charniak 1993). At the\n AI@50\n conference, Charniak boldly proclaimed, in a talk tellingly entitled\n\u201cWhy Natural Language Processing is Now Statistical Natural\nLanguage Processing,\u201d that logicist AI is moribund, and that the\nstatistical approach is the only promising game in town \u2013 for\nthe next 50\n years.[34]\n\nThe chief source of energy and debate at the conference flowed from\nthe clash between Charniak\u2019s probabilistic orientation, and the\noriginal logicist orientation, upheld at the conference in question by\nJohn McCarthy and others.\n\nAI\u2019s use of probability theory grows out of the standard form of\nthis theory, which grew directly out of technical philosophy and\nlogic. This form will be familiar to many philosophers, but\nlet\u2019s review it quickly now, in order to set a firm stage for\nmaking points about the new probabilistic techniques that have\nenergized AI. \n\nJust as in the case of FOL, in probability theory we are concerned\nwith declarative statements, or propositions, to which degrees\nof belief are applied; we can thus say that both logicist and\nprobabilistic approaches are symbolic in nature. Both approaches also\nagree that statements can either be true or false in the world. In\nbuilding agents, a simplistic logic-based approach requires agents to\nknow the truth-value of all possible statements. This is not\nrealistic, as an agent may not know the truth-value of some\nproposition \\(p\\) due to either ignorance, non-determinism in the\nphysical world, or just plain vagueness in the meaning of the\nstatement. More specifically, the fundamental proposition in\nprobability theory is a random variable, which can be conceived\nof as an aspect of the world whose status is initially unknown to the\nagent. We usually capitalize the names of random variables, though we\nreserve \\(p,q,r, \\ldots\\) as such names as well. For example, in a\nparticular murder investigation centered on whether or not Mr. Barolo\ncommitted the crime, the random variable \\(Guilty\\) might be of\nconcern. The detective may be interested as well in whether or not the\nmurder weapon \u2013 a particular knife, let us assume \u2013\nbelongs to Barolo. In light of this, we might say that \\(\\Weapon =\n\\true\\) if it does, and \\(\\Weapon = \\false\\) if it doesn\u2019t. As a\nnotational convenience, we can write \\(weapon\\) and \\(\\lnot weapon\\)\nand for these two cases, respectively; and we can use this convention\nfor other variables of this type. \n\nThe kind of variables we have described so far are\n\\(\\mathbf{Boolean}\\), because their \\(\\mathbf{domain}\\) is simply\n\\(\\{true,false\\}.\\) But we can generalize and allow\n\\(\\mathbf{discrete}\\) random variables, whose values are from any\ncountable domain. For example, \\(\\PriceTChina\\) might be a variable\nfor the price of (a particular, presumably) tea in China, and its\ndomain might be \\(\\{1,2,3,4,5\\}\\), where each number here is in US\ndollars. A third type of variable is \\(\\mathbf{continous}\\); its\ndomain is either the reals, or some subset thereof. \n\nWe say that an atomic event is an assignment of particular\nvalues from the appropriate domains to all the variables composing the\n(idealized) world. For example, in the simple murder investigation\nworld introduced just above, we have two Boolean variables,\n\\(\\Guilty\\) and \\(\\Weapon\\), and there are just four atomic events.\nNote that atomic events have some obvious properties. For example,\nthey are mutually exclusive, exhaustive, and logically entail the\ntruth or falsity of every proposition. Usually not obvious to\nbeginning students is a fourth property, namely, any proposition is\nlogically equivalent to the disjunction of all atomic events that\nentail that proposition. \n\nPrior probabilities correspond to a degree of belief accorded to a\nproposition in the complete absence of any other information. For\nexample, if the prior probability of Barolo\u2019s guilt is \\(0.2\\),\nwe write \n\n    \\[\n    P\\left(\\Guilty=true\\right)=0.2\n    \\]\n\n \n\nor simply \\(\\P(guilty)=0.2\\). It is often convenient to have a\nnotation allowing one to refer economically to the probabilities of\nall the possible values for a random variable. For example,\nwe can write \n\n    \\[\n    \\P\\left(\\PriceTChina\\right)\n    \\]\n\n \n\nas an abbreviation for the five equations listing all the possible\nprices for tea in China. We can also write \n\n    \\[\n    \\P\\left(\\PriceTChina\\right)=\\langle 1,2,3,4,5\\rangle\n    \\]\n\n \n\nIn addition, as further convenient notation, we can write \\(\n\\mathbf{P}\\left(\\Guilty, \\Weapon\\right)\\) to denote the probabilities\nof all combinations of values of the relevant set of random variables.\nThis is referred to as the joint probability distribution of\n\\(\\Guilty\\) and \\(\\Weapon\\). The full joint probability\ndistribution covers the distribution for all the random variables used\nto describe a world. Given our simple murder world, we have 20 atomic\nevents summed up in the equation \n\n    \\[\n    \\mathbf{P}\\left(\\Guilty, \\Weapon, \\PriceTChina\\right)\n    \\]\n\n \n\nThe final piece of the basic language of probability theory\ncorresponds to conditional probabilities. Where \\(p\\) and \\(q\\)\nare any propositions, the relevant expression is \\(P\\!\\left(p\\given\nq\\right)\\), which can be interpreted as \u201cthe probability of\n\\(p\\), given that all we know is \\(q\\).\u201d For example,\n\n    \\[\n    P\\left(guilty\\ggiven weapon\\right)=0.7\n    \\]\n\n \n\nsays that if the murder weapon belongs to Barolo, and no other\ninformation is available, the probability that Barolo is guilty is\n\\(0.7.\\) \n\nAndrei Kolmogorov showed how to construct probability theory from\nthree axioms that make use of the machinery now introduced, viz., \n\n All probabilities fall between \\(0\\) and \\(1.\\) I.e., \\(\\forall\np. 0 \\leq P(p) \\leq 1\\). \n Valid (in the traditional logicist sense) propositions have a\nprobability of \\(1\\); unsatisfiable (in the traditional logicist\nsense) propositions have a probability of \\(0\\). \n \\(P(p\\lor q) = P(p) +P(q) - P(p\\land q)\\) \n\n\nThese axioms are clearly at bottom logicist. The remainder of\nprobability theory can be erected from this foundation (conditional\nprobabilities are easily defined in terms of prior probabilities). We\ncan thus say that logic is in some fundamental sense still being used\nto characterize the set of beliefs that a rational agent can have. But\nwhere does probabilistic inference enter the picture on this\naccount, since traditional deduction is not used for inference in\nprobability theory?\n\nProbabilistic inference consists in computing, from observed evidence\nexpressed in terms of probability theory, posterior probabilities of\npropositions of interest. For a good long while, there have been\nalgorithms for carrying out such computation. These algorithms precede\nthe resurgence of probabilistic techniques in the 1990s. (Chapter 13\nof AIMA presents a number of them.) For example, given the\nKolmogorov axioms, here is a straightforward way of computing the\nprobability of any proposition, using the full joint distribution\ngiving the probabilities of all atomic events: Where \\(p\\) is some\nproposition, let \\(\\alpha(p)\\) be the disjunction of all atomic events\nin which \\(p\\) holds. Since the probability of a proposition (i.e.,\n\\(P(p)\\)) is equal to the sum of the probabilities of the atomic\nevents in which it holds, we have an equation that provides a method\nfor computing the probability of any proposition \\(p\\), viz., \n\n\\[\n P(p) = \\sum_{e_i\\in\\alpha(p)} P(e_i)\n\\]\n\n\nUnfortunately, there were two serious problems infecting this original\nprobabilistic approach: One, the processing in question needed to take\nplace over paralyzingly large amounts of information (enumeration over\nthe entire distribution is required). And two, the expressivity of the\napproach was merely propositional. (It was by the way the philosopher\nHilary Putnam (1963) who pointed out that there was a price to pay in\nmoving to the first-order level. The issue is not discussed herein.)\nEverything changed with the advent of a new formalism that marks the\nmarriage of probabilism and graph theory: Bayesian networks\n(also called belief nets). The pivotal text was (Pearl 1988).\nFor a more detailed discussion, see the\n\nSupplement on Bayesian Networks.\n \n\nBefore concluding this section, it is probably worth noting that, from\nthe standpoint of philosophy, a situation such as the murder\ninvestigation we have exploited above would often be analyzed into\narguments, and strength factors, not into numbers to be\ncrunched by purely arithmetical procedures. For example, in the\nepistemology of Roderick Chisholm, as presented his Theory of\nKnowledge (1966, 1977), Detective Holmes might classify a\nproposition like  Barolo committed the murder. as\ncounterbalanced if he was unable to find a compelling argument\neither way, or perhaps probable if the murder weapon turned out\nto belong to Barolo. Such categories cannot be found on a continuum\nfrom 0 to 1, and they are used in articulating arguments for or\nagainst Barolo\u2019s guilt. Argument-based approaches to uncertain\nand defeasible reasoning are virtually non-existent in AI. One\nexception is Pollock\u2019s approach, covered below. This approach is\nChisholmian in nature. \n\nIt should also be noted that there have been well-established\nformalisms for dealing with probabilistic reasoning as an instance of\nlogic-based reasoning. E.g., the activity a researcher in\nprobabilistic reasoning undertakes when she proves a theorem \\(\\phi\\)\nabout their domain (e.g. any theorem in (Pearl 1988)) is purely within\nthe realm of traditional logic. Readers interested in logic-flavored\napproaches to probabilistic reasoning can consult (Adams 1996,\nHailperin 1996 & 2010, Halpern 1998). Formalisms marrying\nprobability theory, induction and deductive reasoning, placing them on\nan equal footing, have been on the rise, with Markov logic (Richardson\nand Domingos 2006) being salient among these approaches. \n\nProbabilistic Machine Learning\n\nMachine learning, in the sense given\n above,\n has been associated with probabilistic techniques. Probabilistic\ntechniques have been associated with both the learning of functions\n(e.g. Naive Bayes classification) and the modeling of theoretical\nproperties of learning algorithms. For example, a standard\nreformulation of supervised learning casts it as a Bayesian\nproblem. Assume that we are looking at recognizing digits\n\\([0{-}9]\\) from a given image. One way to cast this problem is to ask\nwhat the probability that the hypothesis \\(H_x\\): \u201cthe digit\nis \\(x\\)\u201d is true given the image \\(d\\) from a sensor. Bayes\ntheorem gives us:  \n\n\\[\n P\\left(H_x\\ggiven  d\\right) = \\frac{P\\left(d\\ggiven  H_x\\right)*P\\left(H_x\\right)}{P\\left(d\\right)}\n\\]\n\n\n\\(P(d\\given H_x)\\) and \\(P(H_x)\\) can be estimated from the given\ntraining dataset. Then the hypothesis with the highest posterior\nprobability is then given as the answer and is given by:\n\\(\\argmax_{x}P\\left(d\\ggiven H_x\\right)*P\\left(H_x\\right) \\) In\naddition to probabilistic methods being used to build algorithms,\nprobability theory has also been used to analyze algorithms which\nmight not have an overt probabilistic or logical formulation. For\nexample, one of the central classes of meta-theorems in learning,\nprobably approximately correct (PAC) theorems, are cast in\nterms of lower bounds of the probability that the mismatch between the\ninduced/learnt fL function and the true function\nfT being less than a certain amount, given that the\nlearnt function fL works well for a certain number\nof cases (see Chapter 18, AIMA). \n5. AI in the Wild\n\nFrom at least its modern inception, AI has always been connected to\ngadgets, often ones produced by corporations, and it would be remiss\nof us not to say a few words about this phenomenon. While there have\nbeen a large number of commercial in-the-wild success stories for AI\nand its sister fields, such as optimization and decision-making, some\napplications are more visible and have been thoroughly battle-tested\nin the wild. In 2014, one of the most visible such domains (one in\nwhich AI has been strikingly successful) is information retrieval,\nincarnated as web search. Another recent success story is pattern\nrecognition. The state-of-the-art in applied pattern recognition\n(e.g., fingerprint/face verification, speech recognition, and\nhandwriting recognition) is robust enough to allow\n\u201chigh-stakes\u201d deployment outside the laboratory. As of mid\n2018, several corporations and research laboratories have begun\ntesting autonomous vehicles on public roads, with even a handful of\njurisdictions making self-driving cars legal to operate. For example,\nGoogle\u2019s autonomous cars have navigated hundreds of thousands of\nmiles in California with minimal human help under non-trivial\nconditions (Guizzo 2011). \n\nComputer games provide a robust test bed for AI techniques as they can\ncapture important parts that might be necessary to test an AI\ntechnique while abstracting or removing details that might beyond the\nscope of core AI research, for example, designing better hardware or\ndealing with legal issues (Laird and VanLent 2001). One subclass of\ngames that has seen quite fruitful for commercial deployment of AI is\nreal-time strategy games. Real-time strategy games are games in which\nplayers manage an army given limited resources. One objective is to\nconstantly battle other players and reduce an opponent\u2019s forces.\nReal-time strategy games differ from strategy games in that players\nplan their actions simultaneously in real-time and do not have to take\nturns playing. Such games have a number of challenges that are\ntantalizing within the grasp of the state-of-the-art. This makes such\ngames an attractive venue in which to deploy simple AI agents. An\noverview of AI used in real-time strategy games can be found in\n(Robertson and Watson 2015). \n\nSome other ventures in AI, despite significant success, have been only\nchugging slowly and humbly along, quietly. For instance, AI-related\nmethods have achieved triumphs in solving open problems in mathematics\nthat have resisted any solution for decades. The most noteworthy\ninstance of such a problem is perhaps a proof of the statement that\n\u201cAll Robbins algebras are Boolean algebras.\u201d This\nwas conjectured in the 1930s, and the proof was finally discovered by\nthe Otter automatic theorem-prover in 1996 after just a few months of\neffort (Kolata 1996, Wos 2013). Sister fields like formal verification\nhave also bloomed to the extent that it is now not too difficult to\nsemi-automatically verify vital hardware/software components (Kaufmann\net al. 2000 and Chajed et al. 2017).\n\nOther related areas, such as (natural) language translation, still\nhave a long way to go, but are good enough to let us use them under\nrestricted conditions. The jury is out on tasks such as machine\ntranslation, which seems to require both statistical methods (Lopez\n2008) and symbolic methods (Espa\u00f1a-Bonet 2011). Both\nmethods now have comparable but limited success in the wild. A\ndeployed translation system at Ford that was initially developed for\ntranslating manufacturing process instructions from English to other\nlanguages initially started out as rule-based system with Ford and\ndomain-specific vocabulary and language. This system then evolved to\nincorporate statistical techniques along with rule-based techniques as\nit gained new uses beyond translating manuals, for example, lay users\nwithin Ford translating their own documents (Rychtyckyj and Plesco\n2012). \n\nAI\u2019s great achievements mentioned above so far have all been in\nlimited, narrow domains. This lack of any success in the unrestricted\ngeneral case has caused a small set of researchers to break away into\nwhat is now called\n  artificial general intelligence\n (Goertzel and Pennachin 2007). The stated goals of this movement\ninclude shifting the focus again to building artifacts that are\ngenerally intelligent and not just capable in one narrow domain. \n6. Moral AI\n\nComputer Ethics has been around for a long time. In this\nsub-field, typically one would consider how one ought to act in a\ncertain class of situations involving computer technology, where the\n\u201cone\u201d here refers to a human being (Moor 1985). So-called\n\u201crobot ethics\u201d is different. In this sub-field (which goes\nby names such as \u201cmoral AI,\u201d \u201cethical AI,\u201d\n\u201cmachine ethics,\u201d \u201cmoral robots,\u201d etc.) one is\nconfronted with such prospects as robots being able to make autonomous\nand weighty decisions \u2013 decisions that might or might not be\nmorally permissible (Wallach & Allen 2010). If one were to attempt\nto engineer a robot with a capacity for sophisticated ethical\nreasoning and decision-making, one would also be doing Philosophical\nAI, as that concept is characterized\n elsewhere\n in the present entry. There can be many different flavors of\napproaches toward Moral AI. Wallach and Allen (2010) provide a\nhigh-level overview of the different approaches. Moral reasoning is\nobviously needed in robots that have the capability for lethal action.\nArkin (2009) provides an introduction to how we can control and\nregulate machines that have the capacity for lethal behavior. Moral AI\ngoes beyond obviously lethal situations, and we can have a spectrum of\nmoral machines. Moor (2006) provides one such spectrum of possible\nmoral agents. An example of a non-lethal but ethically-charged machine\nwould be a lying machine. Clark (2010) uses a computational theory\nof the mind, the ability to represent and reason about other\nagents, to build a lying machine that successfully persuades people\ninto believing falsehoods. Bello & Bringsjord (2013) give a\ngeneral overview of what might be required to build a moral machine,\none of the ingredients being a theory of mind. \n\nThe most general framework for building machines that can reason\nethically consists in endowing the machines with a moral code.\nThis requires that the formal framework used for reasoning by the\nmachine be expressive enough to receive such codes. The field of Moral\nAI, for now, is not concerned with the source or provenance of such\ncodes. The source could be humans, and the machine could receive the\ncode directly (via explicit encoding) or indirectly (reading). Another\npossibility is that the code is inferred by the machine from a more\nbasic set of laws. We assume that the robot has access to some such\ncode, and we then try to engineer the robot to follow that code under\nall circumstances while making sure that the moral code and its\nrepresentation do not lead to unintended consequences. Deontic\nlogics are a class of formal logics that have been studied the\nmost for this purpose. Abstractly, such logics are concerned mainly\nwith what follows from a given moral code. Engineering then studies\nthe match of a given deontic logic to a moral code (i.e., is the logic\nexpressive enough) which has to be balanced with the ease of\nautomation. Bringsjord et al. (2006) provide a blueprint for using\ndeontic logics to build systems that can perform actions in accordance\nwith a moral code. The role deontic logics play in the framework\noffered by Bringsjord et al (which can be considered to be\nrepresentative of the field of deontic logic for moral AI) can be best\nunderstood as striving towards Leibniz\u2019s dream of a universal\nmoral calculus: \n\nWhen controversies arise, there will be no more need for a disputation\nbetween two philosophers than there would be between two accountants\n[computistas]. It would be enough for them to pick up their pens and\nsit at their abacuses, and say to each other (perhaps having summoned\na mutual friend): \u2018Let us calculate.\u2019\n\n\nDeontic logic-based frameworks can also be used in a fashion that is\nanalogous to moral self-reflection. In this mode, logic-based\nverification of the robot\u2019s internal modules can done before the\nrobot ventures out into the real world. Govindarajulu and Bringsjord\n(2015) present an approach, drawing from formal-program\nverification, in which a deontic-logic based system could be used\nto verify that a robot acts in a certain ethically-sanctioned manner\nunder certain conditions. Since formal-verification approaches can be\nused to assert statements about an infinite number of situations and\nconditions, such approaches might be preferred to having the robot\nroam around in an ethically-charged test environment and make a finite\nset of decisions that are then judged for their ethical correctness.\nMore recently, Govindarajulu and Bringsjord (2017) use a deontic logic\nto present a computational model of the\n Doctrine of Double Effect,\n an ethical principle for moral dilemmas that has been studied\nempirically and analyzed extensively by\n philosophers.[35]\n The principle is usually presented and motivated via dilemmas using\ntrolleys and was first presented in this fashion by Foot (1967). \n\nWhile there has been substantial theoretical and philosophical work,\nthe field of machine ethics is still in its infancy. There has been\nsome embryonic work in building ethical machines. One recent such\nexample would be Pereira and Saptawijaya (2016) who use logic\nprogramming and base their work in machine ethics on the ethical\ntheory known as contractualism, set out by Scanlon (1982). And\nwhat about the future? Since artificial agents are bound to get\nsmarter and smarter, and to have more and more autonomy and\nresponsibility, robot ethics is almost certainly going to grow in\nimportance. This endeavor might not be a straightforward application\nof classical ethics. For example, experimental results suggest that\nhumans hold robots to different ethical standards than they expect\nfrom humans under similar conditions (Malle et al.\n 2015).[36]\n\n7. Philosophical AI\n\nNotice that the heading for this section isn\u2019t Philosophy\nof AI. We\u2019ll get to that category momentarily. (For now\nit can be identified with the attempt to answer such questions as\nwhether artificial agents created in AI can ever reach the full\nheights of human intelligence.) Philosophical AI is AI, not\nphilosophy; but it\u2019s AI rooted in and flowing from, philosophy.\nFor example, one could engage, using the tools and techniques of\nphilosophy, a paradox, work out a proposed solution, and then proceed\nto a step that is surely optional for philosophers: expressing the\nsolution in terms that can be translated into a computer program that,\nwhen executed, allows an artificial agent to surmount concrete\ninstances of the original\n paradox.[37]\n Before we ostensively characterize Philosophical AI of this sort\ncourtesy of a particular research program, let us consider first the\nview that AI is in fact simply philosophy, or a part thereof. \n\nDaniel Dennett (1979) has famously claimed not just that there are\nparts of AI intimately bound up with philosophy, but that AI\nis philosophy (and psychology, at least of the cognitive\nsort). (He has made a parallel claim about Artificial Life (Dennett\n1998)). This view will turn out to be incorrect, but the reasons why\nit\u2019s wrong will prove illuminating, and our discussion will pave\nthe way for a discussion of Philosophical AI. \n\nWhat does Dennett say, exactly? This: \n\nI want to claim that AI is better viewed as sharing with traditional\nepistemology the status of being a most general, most abstract asking\nof the top-down question: how is knowledge possible? (Dennett 1979,\n60)\n\n\nElsewhere he says his view is that AI should be viewed \u201cas a\nmost abstract inquiry into the possibility of intelligence or\nknowledge\u201d (Dennett 1979, 64). \n\nIn short, Dennett holds that AI is the attempt to explain\nintelligence, not by studying the brain in the hopes of identifying\ncomponents to which cognition can be reduced, and not by engineering\nsmall information-processing units from which one can build in\nbottom-up fashion to high-level cognitive processes, but rather by\n\u2013 and this is why he says the approach is top-down\n\u2013 designing and implementing abstract algorithms that capture\ncognition. Leaving aside the fact that, at least starting in the early\n1980s, AI includes an approach that is in some sense bottom-up (see\nthe neurocomputational paradigm discussed above, in\n Non-Logicist AI: A Summary;\n and see, specifically, Granger\u2019s (2004a, 2004b) work,\nhyperlinked in text immediately above, a specific counterexample), a\nfatal flaw infects Dennett\u2019s view. Dennett sees the potential\nflaw, as reflected in: \n\nIt has seemed to some philosophers that AI cannot plausibly be so\nconstrued because it takes on an additional burden: it restricts\nitself to mechanistic solutions, and hence its domain is not\nthe Kantian domain of all possible modes of intelligence, but just all\npossible mechanistically realizable modes of intelligence. This, it is\nclaimed, would beg the question against vitalists, dualists, and other\nanti-mechanists. (Dennett 1979, 61)\n\n\nDennett has a ready answer to this objection. He writes: \n\nBut \u2026 the mechanism requirement of AI is not an additional\nconstraint of any moment, for if psychology is possible at all, and if\nChurch\u2019s thesis is true, the constraint of mechanism is no more\nsevere than the constraint against begging the question in psychology,\nand who would wish to evade that? (Dennett 1979, 61)\n\n\nUnfortunately, this is acutely problematic; and examination of the\nproblems throws light on the nature of AI. \n\nFirst, insofar as philosophy and psychology are concerned with the\nnature of mind, they aren\u2019t in the least trammeled by the\npresupposition that mentation consists in computation. AI, at least of\nthe \u201cStrong\u201d variety (we\u2019ll discuss\n\u201cStrong\u201d versus \u201cWeak\u201d AI\n below)\n is indeed an attempt to substantiate, through engineering certain\nimpressive artifacts, the thesis that intelligence is at bottom\ncomputational (at the level of Turing machines and their equivalents,\ne.g., Register machines). So there is a philosophical claim, for sure.\nBut this doesn\u2019t make AI philosophy, any more than some of the\ndeeper, more aggressive claims of some physicists (e.g., that the\n universe is ultimately digital in nature)\n make their field philosophy. Philosophy of physics certainly\nentertains the proposition that the physical universe can be\nperfectly modeled in digital terms (in a series of cellular automata,\ne.g.), but of course philosophy of physics can\u2019t be\nidentified with this doctrine. \n\nSecond, we now know well (and those familiar with the relevant formal\nterrain knew at the time of Dennett\u2019s writing) that information\nprocessing can exceed standard computation, that is, can exceed\ncomputation at and below the level of what a Turing machine can muster\n(Turing-computation, we shall say). (Such information\nprocessing is known as hypercomputation, a term coined by\nphilosopher Jack Copeland, who has himself defined such machines\n(e.g., Copeland 1998). The first machines capable of hypercomputation\nwere trial-and-error machines, introduced in the same famous\nissue of the Journal of Symbolic Logic (Gold 1965; Putnam\n1965). A new hypercomputer is the infinite time Turing machine\n(Hamkins & Lewis 2000).) Dennett\u2019s appeal to Church\u2019s\nthesis thus flies in the face of the mathematical facts: some\nvarieties of information processing exceed standard computation (or\nTuring-computation). Church\u2019s thesis, or more precisely, the\nChurch-Turing thesis, is the view that a function \\(f\\) is effectively\ncomputable if and only if \\(f\\) is Turing-computable (i.e., some\nTuring machine can compute \\(f\\)). Thus, this thesis has nothing to\nsay about information processing that is more demanding than what a\nTuring machine can achieve. (Put another way, there is no\ncounter-example to CTT to be automatically found in an\ninformation-processing device capable of feats beyond the reach of\nTMs.) For all philosophy and psychology know, intelligence, even if\ntied to information processing, exceeds what is Turing-computational\nor\n Turing-mechanical.[38]\n This is especially true because philosophy and psychology, unlike AI,\nare in no way fundamentally charged with engineering artifacts, which\nmakes the physical realizability of hypercomputation irrelevant from\ntheir perspectives. Therefore, contra Dennett, to consider AI\nas psychology or philosophy is to commit a serious error, precisely\nbecause so doing would box these fields into only a speck of the\nentire space of functions from the natural numbers (including tuples\ntherefrom) to the natural numbers. (Only a tiny portion of the\nfunctions in this space are Turing-computable.) AI is without question\nmuch, much narrower than this pair of fields. Of course, it\u2019s\npossible that AI could be replaced by a field devoted not to building\ncomputational artifacts by writing computer programs and running them\non embodied Turing machines. But this new field, by definition, would\nnot be AI. Our exploration of AIMA and other textbooks provide\ndirect empirical confirmation of this. \n\nThird, most AI researchers and developers, in point of fact, are\nsimply concerned with building useful, profitable artifacts, and\ndon\u2019t spend much time reflecting upon the kinds of abstract\ndefinitions of intelligence explored in this entry (e.g.,\n What Exactly is AI?).\n \n\nThough AI isn\u2019t philosophy, there are certainly ways of doing\nreal implementation-focussed AI of the highest caliber that are\nintimately bound up with philosophy. The best way to demonstrate this\nis to simply present such research and development, or at least a\nrepresentative example thereof. While there have been many examples of\nsuch work, the most prominent example in AI is John Pollock\u2019s\nOSCAR project, which stretched over a considerable portion of his\nlifetime. For a detailed presentation and further discussion, see\nthe\n\nSupplement on the OSCAR Project.\n\n\nIt\u2019s important to note at this juncture that the OSCAR project,\nand the information processing that underlies it, are without question\nat once philosophy and technical AI. Given that the work in\nquestion has appeared in the pages of Artificial Intelligence,\na first-rank journal devoted to that field, and not to philosophy,\nthis is undeniable (see, e.g., Pollock 2001, 1992). This point is\nimportant because while it\u2019s certainly appropriate, in the\npresent venue, to emphasize connections between AI and philosophy,\nsome readers may suspect that this emphasis is contrived: they may\nsuspect that the truth of the matter is that page after page of AI\njournals are filled with narrow, technical content far from\nphilosophy. Many such papers do exist. But we must distinguish between\nwritings designed to present the nature of AI, and its core methods\nand goals, versus writings designed to present progress on specific\ntechnical issues. \n\nWritings in the latter category are more often than not quite narrow,\nbut, as the example of Pollock shows, sometimes these specific issues\nare inextricably linked to philosophy. And of course Pollock\u2019s\nwork is a representative example (albeit the most substantive one).\nOne could just as easily have selected work by folks who don\u2019t\nhappen to also produce straight philosophy. For example, for an entire\nbook written within the confines of AI and computer science, but which\nis epistemic logic in action in many ways, suitable for use in\nseminars on that topic, see (Fagin et al. 2004). (It is hard to find\ntechnical work that isn\u2019t bound up with philosophy in some\ndirect way. E.g., AI research on learning is all intimately bound up\nwith philosophical treatments of induction, of how genuinely new\nconcepts not simply defined in terms of prior ones can be learned. One\npossible partial answer offered by AI is inductive logic\nprogramming, discussed in Chapter 19 of AIMA.) \n\nWhat of writings in the former category? Writings in this category,\nwhile by definition in AI venues, not philosophy ones, are nonetheless\nphilosophical. Most textbooks include plenty of material that falls\ninto this latter category, and hence they include discussion of the\nphilosophical nature of AI (e.g., that AI is aimed at building\nartificial intelligences, and that\u2019s why, after all, it\u2019s\ncalled \u2018AI\u2019). \n8. Philosophy of Artificial Intelligence\n8.1 \u201cStrong\u201d versus \u201cWeak\u201d AI\n\nRecall that we earlier discussed proposed definitions of AI, and\nrecall specifically that these proposals were couched in terms of the\ngoals of the field. We can follow this pattern here: We can\ndistinguish between \u201cStrong\u201d and \u201cWeak\u201d AI by\ntaking note of the different goals that these two versions of AI\nstrive to reach. \u201cStrong\u201d AI seeks to create artificial\npersons: machines that have all the mental powers we have, including\nphenomenal consciousness. \u201cWeak\u201d AI, on the other hand,\nseeks to build information-processing machines that appear to\nhave the full mental repertoire of human persons (Searle 1997).\n\u201cWeak\u201d AI can also be defined as the form of AI that aims\nat a system able to pass not just the Turing Test (again, abbreviated\nas TT), but the Total Turing Test (Harnad 1991). In TTT, a\nmachine must muster more than linguistic indistinguishability: it must\npass for a human in all behaviors \u2013 throwing a baseball, eating,\nteaching a class, etc. \n\nIt would certainly seem to be exceedingly difficult for philosophers\nto overthrow \u201cWeak\u201d AI (Bringsjord and Xiao 2000). After\nall, what philosophical reason stands in the way of AI\nproducing artifacts that appear to be animals or even humans?\nHowever, some philosophers have aimed to do in \u201cStrong\u201d\nAI, and we turn now to the most prominent case in point. \n8.2 The Chinese Room Argument Against \u201cStrong AI\u201d\n\nWithout question, the most famous argument in the philosophy of AI is\nJohn Searle\u2019s (1980) Chinese Room Argument (CRA), designed to\noverthrow \u201cStrong\u201d AI. We present a quick summary here and\na \u201creport from the trenches\u201d as to how AI practitioners\nregard the argument. Readers wanting to further study CRA will find an\nexcellent next step in the entry on\n the Chinese Room Argument\n and (Bishop & Preston 2002). \n\nCRA is based on a thought-experiment in which Searle himself stars. He\nis inside a room; outside the room are native Chinese speakers who\ndon\u2019t know that Searle is inside it. Searle-in-the-box, like\nSearle-in-real-life, doesn\u2019t know any Chinese, but is fluent in\nEnglish. The Chinese speakers send cards into the room through a slot;\non these cards are written questions in Chinese. The box, courtesy of\nSearle\u2019s secret work therein, returns cards to the native\nChinese speakers as output. Searle\u2019s output is produced by\nconsulting a rulebook: this book is a lookup table that tells him what\nChinese to produce based on what is sent in. To Searle, the Chinese is\nall just a bunch of \u2013 to use Searle\u2019s language \u2013\nsquiggle-squoggles. The following schematic picture sums up the\nsituation. The labels should be obvious. \\(O\\) denotes the outside\nobservers, in this case the Chinese speakers. Input is denoted by\n\\(i\\) and output by \\(o\\). As you can see, there is an icon for the\nrulebook, and Searle himself is denoted by \\(P\\).\n\n\n\nThe Chinese Room, Schematic View\n\n\nNow, what is the argument based on this thought-experiment? Even if\nyou\u2019ve never heard of CRA before, you doubtless can see the\nbasic idea: that Searle (in the box) is supposed to be everything a\ncomputer can be, and because he doesn\u2019t understand Chinese, no\ncomputer could have such understanding. Searle is mindlessly moving\nsquiggle-squoggles around, and (according to the argument)\nthat\u2019s all computers do,\n fundamentally.[39]\n\nWhere does CRA stand today? As we\u2019ve already indicated, the\nargument would still seem to be alive and well; witness (Bishop &\nPreston 2002). However, there is little doubt that at least among AI\npractitioners, CRA is generally rejected. (This is of course\nthoroughly unsurprising.) Among these practitioners, the philosopher\nwho has offered the most formidable response out of AI itself is\nRapaport (1988), who argues that while AI systems are indeed\nsyntactic, the right syntax can constitute semantics. It should be\nsaid that a common attitude among proponents of \u201cStrong\u201d\nAI is that CRA is not only unsound, but silly, based as it is on a\nfanciful story (CR) far removed from the practice of AI\n\u2013 practice which is year by year moving ineluctably toward\nsophisticated robots that will once and for all silence CRA and its\nproponents. For example, John Pollock (as we\u2019ve noted,\nphilosopher and practitioner of AI) writes: \n\nOnce [my intelligent system] OSCAR is fully functional, the argument\nfrom analogy will lead us inexorably to attribute thoughts and\nfeelings to OSCAR with precisely the same credentials with which we\nattribute them to human beings. Philosophical arguments to the\ncontrary will be pass\u00e9. (Pollock 1995, p. 6)\n\n\nTo wrap up discussion of CRA, we make two quick points, to wit: \n\n Despite the confidence of the likes of Pollock about the eventual\nirrelevance of CRA in the face of the eventual human-level prowess of\nOSCAR (and, by extension, any number of other still-improving AI\nsystems), the brute fact is that deeply semantic natural-language\nprocessing (NLP) is rarely even pursued these days, so proponents of\nCRA are certainly not the ones feeling some discomfort in light of the\ncurrent state of AI. In short, Searle would rightly point to any of\nthe success stories of AI, including the Watson system we have\ndiscussed, and still proclaim that understanding is nowhere to be\nfound \u2013 and he would be well within his philosophical rights in\nsaying this. \n It would appear that the CRA is bubbling back to a level of\nengagement not seen for a number of years, in light of the empirical\nfact that certain thinkers are now issuing explicit warnings to the\neffect that future conscious, malevolent machines may well wish to do\nin our species. In reply, Searle (2014) points out that since CRA is\nsound, there can\u2019t be conscious machines; and if there\ncan\u2019t be conscious machines, there can\u2019t be malevolent\nmachines that wish anything. We return to this at the end of our\nentry; the chief point here is that CRA continues to be quite\nrelevant, and indeed we suspect that Searle\u2019s basis for\nhave-no-fear will be taken up energetically by not only philosophers,\nbut AI experts, futurists, lawyers, and policy-makers. \n\n\nReaders may wonder if there are philosophical debates that AI\nresearchers engage in, in the course of working in their field (as\nopposed to when they might attend a philosophy conference). Surely, AI\nresearchers have philosophical discussions amongst themselves, right?\n\n\nGenerally, one finds that AI researchers do discuss among themselves\ntopics in philosophy of AI, and these topics are usually the very same\nones that occupy philosophers of AI. However, the attitude reflected\nin the quote from Pollock immediately above is by far the dominant\none. That is, in general, the attitude of AI researchers is that\nphilosophizing is sometimes fun, but the upward march of AI\nengineering cannot be stopped, will not fail, and will eventually\nrender such philosophizing otiose. \n\nWe will return to the issue of the future of AI in the\n final section\n of this entry. \n8.3 The G\u00f6delian Argument Against \u201cStrong AI\u201d\n\nFour decades ago, J.R. Lucas (1964) argued that G\u00f6del\u2019s\nfirst incompleteness theorem entails that no machine can ever reach\nhuman-level intelligence. His argument has not proved to be\ncompelling, but Lucas initiated a debate that has produced more\nformidable arguments. One of Lucas\u2019 indefatigable defenders is\nthe physicist Roger Penrose, whose first attempt to vindicate Lucas\nwas a G\u00f6delian attack on \u201cStrong\u201d AI articulated in\nhis The Emperor\u2019s New Mind (1989). This first attempt\nfell short, and Penrose published a more elaborate and more fastidious\nG\u00f6delian case, expressed in Chapters 2 and 3 of his Shadows of\nthe Mind (1994). \n\nIn light of the fact that readers can turn to the\n entry on the G\u00f6del\u2019s Incompleteness Theorems,\n a full review here is not needed. Instead, readers will be given a\ndecent sense of the argument by turning to an online paper in which\nPenrose, writing in response to critics (e.g., the philosopher David\nChalmers, the logician Solomon Feferman, and the computer scientist\nDrew McDermott) of his Shadows of the Mind, distills the\nargument to a couple of\n paragraphs.[40]\n Indeed, in this paper Penrose gives what he takes to be the perfected\nversion of the core G\u00f6delian case given in SOTM. Here is\nthis version, verbatim: \n\nWe try to suppose that the totality of methods of (unassailable)\nmathematical reasoning that are in principle humanly accessible can be\nencapsulated in some (not necessarily computational) sound formal\nsystem \\(F\\). A human mathematician, if presented with \\(F\\), could\nargue as follows (bearing in mind that the phrase \u201cI am\n\\(F\\)\u201d is merely a shorthand for \u201c\\(F\\) encapsulates all\nthe humanly accessible methods of mathematical proof\u201d):\n\n\n(A) \u201cThough I don\u2019t know that I necessarily am \\(F\\), I\nconclude that if I were, then the system \\(F\\) would have to be sound\nand, more to the point, \\(F'\\) would have to be sound, where \\(F'\\) is\n\\(F\\) supplemented by the further assertion \u201cI am \\(F\\).\u201d\nI perceive that it follows from the assumption that I am \\(F\\) that\nthe G\u00f6del statement \\(G(F')\\) would have to be true and,\nfurthermore, that it would not be a consequence of \\(F'\\). But I have\njust perceived that \u201cIf I happened to be \\(F\\), then \\(G(F')\\)\nwould have to be true,\u201d and perceptions of this nature would be\nprecisely what \\(F'\\) is supposed to achieve. Since I am therefore\ncapable of perceiving something beyond the powers of \\(F'\\), I deduce\nthat I cannot be \\(F\\) after all. Moreover, this applies to any other\n(G\u00f6delizable) system, in place of \\(F\\).\u201d (Penrose 1996,\n3.2)\n\n\n\nDoes this argument succeed? A firm answer to this question is not\nappropriate to seek in the present entry. Interested readers are\nencouraged to consult four full-scale treatments of the argument\n(LaForte et. al 1998; Bringsjord and Xiao 2000; Shapiro 2003; Bowie\n1982).\n8.4 Additional Topics and Readings in Philosophy of AI\n\nIn addition to the G\u00f6delian and Searlean arguments covered\nbriefly above, a third attack on \u201cStrong\u201d AI (of the\nsymbolic variety) has been widely discussed (though with the rise of\nstatistical machine learning has come a corresponding decrease in the\nattention paid to it), namely, one given by the philosopher Hubert\nDreyfus (1972, 1992), some incarnations of which have been\nco-articulated with his brother, Stuart Dreyfus (1987), a computer\nscientist. Put crudely, the core idea in this attack is that human\nexpertise is not based on the explicit, disembodied, mechanical\nmanipulation of symbolic information (such as formulae in some logic,\nor probabilities in some Bayesian network), and that AI\u2019s\nefforts to build machines with such expertise are doomed if based on\nthe symbolic paradigm. The genesis of the Dreyfusian attack was a\nbelief that the critique of (if you will) symbol-based philosophy\n(e.g., philosophy in the logic-based, rationalist tradition, as\nopposed to what is called the Continental tradition) from such\nthinkers as Heidegger and Merleau-Ponty could be made against the\nrationalist tradition in AI. After further reading and study of\nDreyfus\u2019 writings, readers may judge whether this critique is\ncompelling, in an information-driven world increasingly managed by\nintelligent agents that carry out symbolic reasoning (albeit not even\nclose to the human level). \n\nFor readers interested in exploring philosophy of AI beyond what Jim\nMoor (in a recent address \u2013 \u201cThe Next Fifty Years of AI:\nFuture Scientific Research vs. Past Philosophical Criticisms\u201d\n\u2013 as the 2006 Barwise Award winner at the annual eastern\nAmerican Philosophical Association meeting) has called the \u201cthe\nbig three\u201d criticisms of AI, there is no shortage of additional\nmaterial, much of it available on the Web. The last chapter of\nAIMA provides a compressed overview of some additional\narguments against \u201cStrong\u201d AI, and is in general not a bad\nnext step. Needless to say, Philosophy of AI today involves much more\nthan the three well-known arguments discussed above, and, inevitably,\nPhilosophy of AI tomorrow will include new debates and problems we\ncan\u2019t see now. Because machines, inevitably, will get smarter\nand smarter (regardless of just how smart they get),\nPhilosophy of AI, pure and simple, is a growth industry. With every\nhuman activity that machines match, the \u201cbig\u201d questions\nwill only attract more attention. \n9. The Future\n\nIf past predictions are any indication, the only thing we know today\nabout tomorrow\u2019s science and technology is that it will be\nradically different than whatever we predict it will be like.\nArguably, in the case of AI, we may also specifically know today that\nprogress will be much slower than what most expect. After all, at the\n1956 kickoff conference (discussed at the start of this entry), Herb\nSimon predicted that thinking machines able to match the human mind\nwere \u201cjust around the corner\u201d (for the relevant quotes and\ninformative discussion, see the first chapter of AIMA). As it\nturned out, the new century would arrive without a single machine able\nto converse at even the toddler level. (Recall that when it comes to\nthe building of machines capable of displaying human-level\nintelligence, Descartes, not Turing, seems today to be the better\nprophet.) Nonetheless, astonishing though it may be, serious thinkers\nin the late 20th century have continued to issue incredibly optimistic\npredictions regarding the progress of AI. For example, Hans Moravec\n(1999), in his Robot: Mere Machine to Transcendent Mind,\ninforms us that because the speed of computer hardware doubles every\n18 months (in accordance with Moore\u2019s Law, which has\napparently held in the past), \u201cfourth generation\u201d\nrobots will soon enough exceed humans in all respects, from running\ncompanies to writing novels. These robots, so the story goes, will\nevolve to such lofty cognitive heights that we will stand to them as\nsingle-cell organisms stand to us\n today.[41]\n\n\nMoravec is by no means singularly Pollyannaish: Many others in AI\npredict the same sensational future unfolding on about the same rapid\nschedule. In fact, at the aforementioned AI@50 conference, Jim Moor\nposed the question \u201cWill human-level AI be achieved within the\nnext 50 years?\u201d to five thinkers who attended the original 1956\nconference: John McCarthy, Marvin Minsky, Oliver Selfridge, Ray\nSolomonoff, and Trenchard Moore. McCarthy and Minsky gave firm,\nunhesitating affirmatives, and Solomonoff seemed to suggest that AI\nprovided the one ray of hope in the face of fact that our species\nseems bent on destroying itself. (Selfridge\u2019s reply was a bit\ncryptic. Moore returned a firm, unambiguous negative, and declared\nthat once his computer is smart enough to interact with him\nconversationally about mathematical problems, he might take this whole\nenterprise more seriously.) It is left to the reader to judge the\naccuracy of such risky predictions as have been given by Moravec,\nMcCarthy, and\n Minsky.[42]\n\n\nThe judgment of the reader in this regard ought to factor in the\nstunning resurgence, very recently, of serious reflection on what is\nknown as \u201cThe Singularity,\u201d (denoted by us simply as\nS) the future point at which artificial intelligence exceeds\nhuman intelligence, whereupon immediately thereafter (as the story\ngoes) the machines make themselves rapidly smarter and smarter and\nsmarter, reaching a superhuman level of intelligence that, stuck as we\nare in the mud of our limited mentation, we can\u2019t fathom. For\nextensive, balanced analysis of S, see Eden et al. (2013).\n\nReaders unfamiliar with the literature on S may be quite\nsurprised to learn the degree to which, among learned folks, this\nhypothetical event is not only taken seriously, but has in fact become\na target for extensive and frequent philosophizing [for a mordant tour\nof the recent thought in question, see Floridi (2015)]. What\narguments support the belief that S is in our future?\nThere are two main arguments at this point: the familiar\nhardware-based one [championed by Moravec, as noted above, and again\nmore recently by Kurzweil (2006)]; and the \u2013 as far as we know\n\u2013 original argument given by mathematician I. J. Good (1965). In\naddition, there is a recent and related doomsayer argument advanced by\nBostrom (2014), which seems to presuppose that S will occur.\nGood\u2019s argument, nicely amplified and adjusted by Chalmers\n(2010), who affirms the tidied-up version of the argument, runs as\nfollows:\n\nPremise 1: There will be AI (created by HI and such that AI\n= HI). \nPremise 2: If there is AI, there will be AI\\(^+\\) (created\nby AI). \nPremise 3: If there is AI\\(^+\\), there will be AI\\(^{++}\\)\n(created by AI\\(^+\\)). \nConclusion: There will be AI\\(^{++}\\) (= S will\noccur). \n\n\nIn this argument, \u2018AI\u2019 is artificial intelligence at the\nlevel of, and created by, human persons, \u2018AI\\(^+\\)\u2019\nartificial intelligence above the level of human persons, and\n\u2018AI\\(^{++}\\)\u2019 super-intelligence constitutive of S.\nThe key process is presumably the creation of one class of\nmachine by another. We have added for convenience \u2018HI\u2019 for\nhuman intelligence; the central idea is then: HI will create AI, the\nlatter at the same level of intelligence as the former; AI will create\nAI\\(^+\\); AI\\(^+\\) will create AI\\(^{++}\\); with the ascension\nproceeding perhaps forever, but at any rate proceeding long enough for\nus to be as ants outstripped by gods. \n\nThe argument certainly appears to be formally valid. Are its three\npremises true? Taking up such a question would fling us far beyond the\nscope of this entry. We point out only that the concept of one class\nof machines creating another, more powerful class of machines is not a\ntransparent one, and neither Good nor Chalmers provides a rigorous\naccount of the concept, which is ripe for philosophical analysis. (As\nto mathematical analysis, some exists, of course. It is for example\nwell-known that a computing machine at level \\(L\\) cannot possibly\ncreate another machine at a higher level \\(L'\\). For instance, a\nlinear-bounded automaton can\u2019t create a Turing machine.)\n\nThe Good-Chalmers argument has a rather clinical air about it; the\nargument doesn\u2019t say anything regarding whether machines in\nthe AI\\(^{++}\\) category will be benign, malicious, or munificent.\nMany others gladly fill this gap with dark, dark pessimism. The\nlocus classicus here is without question a widely read paper by\nBill Joy (2000): \u201cWhy The Future Doesn\u2019t Need Us.\u201d\nJoy believes that the human race is doomed, in no small part because\nit\u2019s busy building smart machines. He writes: \n\n\nThe 21st-century technologies \u2013 genetics, nanotechnology, and\nrobotics (GNR) \u2013 are so powerful that they can spawn whole new\nclasses of accidents and abuses. Most dangerously, for the first time,\nthese accidents and abuses are widely within the reach of individuals\nor small groups. They will not require large facilities or rare raw\nmaterials. Knowledge alone will enable the use of them.\n\nThus we have the possibility not just of weapons of mass destruction\nbut of knowledge-enabled mass destruction (KMD), this destructiveness\nhugely amplified by the power of self-replication. \n\nI think it is no exaggeration to say we are on the cusp of the further\nperfection of extreme evil, an evil whose possibility spreads well\nbeyond that which weapons of mass destruction bequeathed to the\nnation-states, on to a surprising and terrible empowerment of extreme\n individuals.[43]\n\n\n\nPhilosophers would be most interested in arguments for this\nview. What are Joy\u2019s? Well, no small reason for the attention\nlavished on his paper is that, like Raymond Kurzweil (2000), Joy\nrelies heavily on an argument given by none other than the Unabomber\n(Theodore Kaczynski). The idea is that, assuming we succeed in\nbuilding intelligent machines, we will have them do most (if not all)\nwork for us. If we further allow the machines to make decisions for us\n\u2013 even if we retain oversight over the machines \u2013, we will\neventually depend on them to the point where we must simply accept\ntheir decisions. But even if we don\u2019t allow the machines to make\ndecisions, the control of such machines is likely to be held by a\nsmall elite who will view the rest of humanity as unnecessary \u2013\nsince the machines can do any needed work (Joy 2000).\n\nThis isn\u2019t the place to assess this argument. (Having said that,\nthe pattern pushed by the Unabomber and his supporters certainly\nappears to be flatly\n invalid.[44])\n In fact, many readers will doubtless feel that no such place exists\nor will exist, because the reasoning here is amateurish. So then, what\nabout the reasoning of professional philosophers on the matter? \n\nBostrom has recently painted an exceedingly dark picture of a possible\nfuture. He points out that the \u201cfirst superintelligence\u201d\ncould have the capability\n\nto shape the future of Earth-originating life, could easily have\nnon-anthropomorphic final goals, and would likely have instrumental\nreasons to pursue open-ended resource acquisition. If we now reflect\nthat human beings consist of useful resources (such as conveniently\nlocated atoms) and that we depend on many more local resources, we can\nsee that the outcome could easily be one in which humanity quickly\nbecomes extinct. (Bostrom 2014, p. 416)\n\n\n Clearly, the most vulnerable premise in this sort of argument\nis that the \u201cfirst superintelligence\u201d will arrive indeed\narrive. Here perhaps the Good-Chalmers argument provides a basis.\n\n\nSearle (2014) thinks Bostrom\u2019s book is misguided and\nfundamentally mistaken, and that we needn\u2019t worry. His rationale\nis dirt-simple: Machines aren\u2019t conscious; Bostrom is alarmed at\nthe prospect of malicious machines who do us in; a malicious machine\nis by definition a conscious machine; ergo, Bostrom\u2019s argument\ndoesn\u2019t work. Searle writes: \n\nIf the computer can fly airplanes, drive cars, and win at chess, who\ncares if it is totally nonconscious? But if we are worried about a\nmaliciously motivated superintelligence destroying us, then it is\nimportant that the malicious motivation should be real. Without\nconsciousness, there is no possibiity of its being real.\n\n\nThe positively remarkable thing here, it seems to us, is that Searle\nappears to be unaware of the brute fact that most AI engineers are\nperfectly content to build machines on the basis of the AIMA\nview of AI we presented and explained above: the view according to\nwhich machines simply map percepts to actions. On this view, it\ndoesn\u2019t matter whether the machine really has desires;\nwhat matters is whether it acts suitably on the basis of how AI\nscientists engineer formal correlates to desire. An\nautonomous machine with overwhelming destructive power that\nnon-consciously \u201cdecides\u201d to kill doesn\u2019t become\njust a nuisance because genuine, human-level, subjective desire is\nabsent from the machine. If an AI can play the game of chess, and the\ngame of Jeopardy!, it can certainly play the game of war. Just\nas it does little good for a human loser to point out that the\nvictorious machine in a game of chess isn\u2019t conscious, it will\ndo little good for humans being killed by machines to point out that\nthese machines aren\u2019t conscious. (It is interesting to note that\nthe genesis of Joy\u2019s paper was an informal conversation with\nJohn Searle and Raymond Kurzweil. According to Joy, Searle\ndidn\u2019t think there was much to worry about, since he was (and\nis) quite confident that tomorrow\u2019s robots can\u2019t be\n conscious.[45])\n \n\nThere are some things we can safely say about tomorrow.\nCertainly, barring some cataclysmic events (nuclear or biological\nwarfare, global economic depression, a meteorite smashing into Earth,\netc.), we now know that AI will succeed in producing artificial\nanimals. Since even some natural animals (mules, e.g.) can be\neasily trained to work for humans, it stands to reason that artificial\nanimals, designed from scratch with our purposes in mind, will be\ndeployed to work for us. In fact, many jobs currently done by humans\nwill certainly be done by appropriately programmed artificial animals.\nTo pick an arbitrary example, it is difficult to believe that\ncommercial drivers won\u2019t be artificial in the future. (Indeed,\nDaimler is already running commercials in which they tout the ability\nof their automobiles to drive \u201cautonomously,\u201d allowing\nhuman occupants of these vehicles to ignore the road and read.) Other\nexamples would include: cleaners, mail carriers, clerical workers,\nmilitary scouts, surgeons, and pilots. (As to cleaners, probably a\nsignificant number of readers, at this very moment, have robots from\niRobot cleaning the carpets in their homes.) It is hard to see how\nsuch jobs are inseparably bound up with the attributes often taken to\nbe at the core of personhood \u2013 attributes that would be the most\ndifficult for AI to\n replicate.[46]\n\n\nAndy Clark (2003) has another prediction: Humans will gradually\nbecome, at least to an appreciable degree, cyborgs, courtesy of\nartificial limbs and sense organs, and implants. The main driver of\nthis trend will be that while standalone AIs are often desirable, they\nare hard to engineer when the desired level of intelligence is high.\nBut to let humans \u201cpilot\u201d less intelligent machines is a\ngood deal easier, and still very attractive for concrete reasons.\nAnother related prediction is that AI would play the role of a\ncognitive prosthesis for humans (Ford et al. 1997; Hoffman et al.\n2001). The prosthesis view sees AI as a \u201cgreat equalizer\u201d\nthat would lead to less stratification in society, perhaps similar to\nhow the Hindu-Arabic numeral system made arithmetic available to the\nmasses, and to how the Guttenberg press contributed to literacy\nbecoming more universal. \n\nEven if the argument is formally invalid, it leaves us with a question\n\u2013 the cornerstone question about AI and the future: Will AI\nproduce artificial creatures that replicate and exceed human cognition\n(as Kurzweil and Joy believe)? Or is this merely an interesting\nsupposition? \n\nThis is a question not just for scientists and engineers; it is also a\nquestion for philosophers. This is so for two reasons. One, research\nand development designed to validate an affirmative answer must\ninclude philosophy \u2013 for reasons rooted in earlier parts of the\npresent entry. (E.g., philosophy is the place to turn to for robust\nformalisms to model human propositional attitudes in machine terms.)\nTwo, philosophers might well be able to provide arguments that answer\nthe cornerstone question now, definitively. If a version of either of\nthe three arguments against \u201cStrong\u201d AI alluded to above\n(Searle\u2019s CRA; the G\u00f6delian attack; the Dreyfus argument)\nare sound, then of course AI will not manage to produce machines\nhaving the mental powers of persons. No doubt the future holds not\nonly ever-smarter machines, but new arguments pro and con on the\nquestion of whether this progress can reach the human level that\nDescartes declared to be unreachable. \n",
    "bibliography": {
        "categories": [],
        "cat_ref_text": {
            "ref_list": [
                " Adams, E. W., 1996, <i> A Primer of Probability Logic</i>,\nStanford, CA: CSLI. ",
                " Almeida, J., Frade, M., Pinto, J. &amp; de Sousa, S., 2011,\n<i>Rigorous Software Development: An Introduction to Program\nVerification</i>, New York, NY: Spinger. ",
                " Alpaydin, E., 2014, <i>Introduction to Machine Learning</i>,\nCambridge, MA: MIT Press. ",
                " Amir, E. &amp; Maynard-Reid, P., 1999, \u201cLogic-Based\nSubsumption Architecture,\u201d in <i>Proceedings of the\n16<sup>th</sup> International Joint Conference on Artificial\nIntelligence (IJCAI-1999)</i>, (San Francisco, CA: MIT Morgan\nKaufmann), pp. 147\u2013152. ",
                " Amir, E. &amp; Maynard-Reid, P., 2000, \u201cLogic-Based\nSubsumption Architecture: Empirical Evaluation,\u201d in <i>\nProceedings of the AAAI Fall Symposium on Parallel Architectures for\nCognition</i>. ",
                " Amir, E. &amp; Maynard-Reid, P., 2001, \u201cLiSA: A Robot\nDriven by Logical Subsumption,\u201d in <i>Proceedings of the Fifth\nSymposium on the Logical Formalization of Commonsense Reasoning</i>,\n(New York, NY). ",
                " Anderson, C. A., 1983, \u201cThe Paradox of the Knower,\u201d\n<i>The Journal of Philosophy,</i> 80.6: 338\u2013355.",
                " Anderson, J. &amp; Lebiere, C., 2003, \u201cThe Newell Test for\na Theory of Cognition,\u201d <i>Behavioral and Brain Sciences</i>,\n26: 587\u2013640. ",
                " Ashcraft, M., 1994, <i>Human Memory and Cognition</i>, New York,\nNY: HarperCollins. ",
                " Arkin, R., 2009, <i>Governing Lethal Behavior in Autonomous\nRobots</i>, London: Chapman and Hall/CRC Imprint, Taylor and Francis\nGroup. ",
                " Arkoudas, K. &amp; Bringsjord, S., 2005, \u201cVivid: A\nFramework for Heterogeneous Problem Solving,\u201d <i>Artificial\nIntelligence</i>, 173.15: 1367\u20131405. ",
                " Arkoudas, K. &amp; Bringsjord, S., 2005, \u201cMetareasoning for\nMulti-agent Epistemic Logics,\u201d in <i>Fifth International\nConference on Computational Logic In Multi-Agent Systems (CLIMA\n2004)</i>, in the series <i>Lecture Notes in Artificial Intelligence\n(LNAI)</i>, volume 3487, New York, NY: Springer-Verlag, pp.\n111\u2013125. ",
                " Arkoudas, K., 2000, <i>Denotational Proof Languages</i>, PhD\ndissertation, Massachusetts Institute of Technology (Computer\nScience). ",
                " Baader, F., Calvanese, D., McGuinness, D. L., Nardi, D., &amp;\nPatel-Schneider, P. F., eds., 2003, <i>The Description Logic Handbook:\nTheory, Implementation, and Applications</i>, New York, NY: Cambridge\nUniversity Press. ",
                " Smith, B., Ashburner, M., Rosse, C., Bard, J., Bug, W., Ceusters,\nW., Goldberg, L. J., Eilbeck, K., Ireland, A., Mungall, C. J., The OBI\nConsortium, Leontis, N., Rocca-Serra, P., Ruttenberg, A., Sansone, S.,\nScheuermann, R. H., Shah, N., Whetzel, P. L. &amp; Lewis, S., 2007,\n\u201cThe OBO Foundry: Coordinated Evolution of Ontologies to Support\nBiomedical Data Integration,\u201d <i>Nature Biotechnology</i> 25,\n1251\u20131255. ",
                " Barwise, J. &amp; Etchemendy, J., 1999, <i>Language, Proof, and\nLogic</i>, New York, NY: Seven Bridges Press. ",
                " Barwise, J. &amp; Etchemendy, J., 1995, \u201cHeterogeneous\nLogic,\u201d in <i>Diagrammatic Reasoning: Cognitive and\nComputational Perspectives</i>, J. Glasgow, N.H. Narayanan, &amp; B.\nChandrasekaran, eds., Cambridge, MA: MIT Press, pp. 211\u2013234.\n",
                " Baldi, P., Sadowski P. &amp; Whiteson D., 2014, \u201cSearching\nfor Exotic Particles in High-energy Physics with Deep\nLearning,\u201d<i> Nature Communications</i>.\n [<a href=\"http://www.nature.com/ncomms/2014/140702/ncomms5308/full/ncomms5308.html\" target=\"other\">Available online</a>]\n ",
                " Barwise, J. &amp; Etchemendy, J., 1994, <i>Hyperproof</i>,\nStanford, CA: CSLI. ",
                " Barwise, J. &amp; Etchemendy, J., 1990, \u201cInfons and\nInference,\u201d in <i>Situation Theory and its Applications, (Vol\n1)</i>, Cooper, Mukai, and Perry (eds), CSLI Lecture Notes #22, CSLI\nPress, pp. 33\u201378. ",
                " Bello, P. &amp; Bringsjord S., 2013, \u201cOn How to Build a\nMoral Machine,\u201d <i>Topoi,</i> 32.2: 251\u2013266. ",
                " Bengio, Y., Goodfellow, I., &amp; Courville, A., 2016, <i>Deep\nLearning</i>, Cambridge: MIT Press.\n [<a href=\"http://www.deeplearningbook.org\" target=\"other\">Available online</a>]\n ",
                " Bengio, Y., Courville, A. &amp; Vincent, P., 2013,\n\u201cRepresentation Learning: A Review and New Perspectives,\u201d\n<i> Pattern Analysis and Machine Intelligence, IEEE Transactions,</i>\n35.8: 1798\u20131828. ",
                "Berners-Lee, T., Hendler, J. &amp; Lassila, O., 2001, \u201cThe\nSemantic Web,\u201d <i>Scientific American,</i> 284: 34\u201343.\n",
                " Bishop, M. &amp; Preston, J., 2002, <i>Views into the Chinese\nRoom: New Essays on Searle and Artificial Intelligence</i>, Oxford,\nUK: Oxford University Press. ",
                " Boden, M., 1994, \u201cCreativity and Computers,\u201d in\n<i>Artificial Intelligence and Computers</i>, T. Dartnall, ed.,\nDordrecht, The Netherlands: Kluwer, pp. 3\u201326. ",
                " Boolos, G. S., Burgess, J.P., &amp; Jeffrey., R.C., 2007,\n<i>Computability and Logic 5th edition,</i> Cambridge: Cambridge\nUniversity Press. ",
                " Bostrom, N., 2014, <i>Superintelligence: Paths, Dangers,\nStrategies</i>, Oxford, UK: Oxford University Press. ",
                " Bowie, G.L., 1982, \u201cLucas\u2019 Number is Finally\nUp,\u201d <i>Journal of Philosophical Logic</i>, 11: 279\u2013285.\n",
                " Brachman, R. &amp; Levesque, H., 2004, <i>Knowledge\nRepresentation and Reasoning</i>, San Francisco, CA: Morgan\nKaufmann/Elsevier. ",
                " Bringsjord, S., Arkoudas K. &amp; Bello P., 2006, \u201cToward a\nGeneral Logicist Methodology for Engineering Ethically Correct\nRobots,\u201d IEEE Intelligent Systems, 21.4: 38\u201344. ",
                " Bringsjord, S. &amp; Ferrucci, D., 1998, \u201cLogic and\nArtificial Intelligence: Divorced, Still Married,\nSeparated\u2026?\u201d <i>Minds and Machines</i>, 8: 273\u2013308.\n",
                " Bringsjord, S. &amp; Schimanski, B., 2003, \u201cWhat is\nArtificial Intelligence? Psychometric AI as an Answer,\u201d\n<i>Proceedings of the 18<sup>th</sup> International Joint Conference\non Artificial Intelligence (IJCAI-2003)</i>, (San Francisco, CA: MIT\nMorgan Kaufmann), pp. 887\u2013893. ",
                " Bringsjord, S. &amp; Ferrucci, D., 2000, <i>Artificial\nIntelligence and Literary Creativity: Inside the Mind of Brutus, a\nStorytelling Machine</i>, Mahwah, NJ: Lawrence Erlbaum. ",
                " Bringsjord, S. &amp; van Heuveln, B., 2003, \u201cThe Mental Eye\nDefense of an Infinitized Version of Yablo\u2019s Paradox,\u201d\n<i>Analysis</i> 63.1: 61\u201370. ",
                " Bringsjord S. &amp; Xiao, H., 2000, \u201cA Refutation of\nPenrose\u2019s G\u00f6delian Case Against Artificial\nIntelligence,\u201d <i>Journal of Experimental and Theoretical\nArtificial Intelligence</i>, 12: 307\u2013329. ",
                " Bringsjord, S. &amp; Zenzen, M., 2002, \u201cToward a Formal\nPhilosophy of Hypercomputation,\u201d <i>Minds and Machines</i>, 12:\n241\u2013258. ",
                " Bringsjord, S., 2000, \u201cAnimals, Zombanimals, and the Total\nTuring Test: The Essence of Artificial Intelligence,\u201d <i>Journal\nof Logic, Language, and Information</i>, 9: 397\u2013418. ",
                " Bringsjord, S., 1998, \u201cPhilosophy and \u2018Super\u2019\nComputation,\u201d <i>The Digital Phoenix: How Computers are Changing\nPhilosophy</i>, J. Moor and T. Bynam, eds., Oxford, UK: Oxford\nUniversity Press, pp. 231\u2013252. ",
                " Bringsjord, S., 1991, \u201cIs the Connectionist-Logicist Clash\none of AI\u2019s Wonderful Red Herrings?\u201d <i>Journal of\nExperimental &amp; Theoretical AI</i>, 3.4: 319\u2013349. ",
                " Bringsjord, S., Govindarajulu N. S., Eberbach, E. &amp; Yang, Y.,\n2012, \u201cPerhaps the Rigorous Modeling of Economic Phenomena\nRequires Hypercomputation,\u201d <i>International Journal of\nUnconventional Computing,</i> 8.1: 3\u201332.\n [<a href=\"http://kryten.mm.rpi.edu/SB_NSG_EE_YY_28-9-2010.pdf\" target=\"other\">Preprint available online</a>]\n ",
                " Bringsjord, S., 2011, \u201cPsychometric Artificial\nIntelligence,\u201d <i>Journal of Experimental and Theoretical\nArtificial Intelligence</i>, 23.3: 271\u2013277. ",
                " Bringsjord, S. &amp; Govindarajulu N. S., 2012, \u201cGiven the\nWeb, What is Intelligence, Really?\u201d <i>Metaphilosophy</i> 43.12:\n464\u2013479. ",
                " Brooks, R. A., 1991, \u201cIntelligence Without\nRepresentation,\u201d <i>Artificial Intelligence</i>, 47:\n139\u2013159. ",
                " Browne, C. B., Powley, E. &amp; Whitehouse, D., 2012, \u201cA\nSurvey of Monte Carlo Tree Search Methods,\u201d <i>A Survey of Monte\nCarlo Tree Search Methods</i>, 4.1: 1\u201343. ",
                "Buchanan, B. G., 2005, \u201cA (Very) Brief History of Artificial\nIntelligence,\u201d <i>AI Magazine</i>, 26.4: 53\u201360. ",
                " Carroll, L., 1958, <i>Symbolic Logic; Game of Logic</i>, New\nYork, NY: Dover. ",
                " Cassimatis, N., 2006, \u201cCognitive Substrate for Human-Level\nIntelligence,\u201d <i>AI Magazine</i>, 27.2: 71\u201382. ",
                " Chajed, T., Chen, H., Chlipala, A., Kaashoek, F., Zeldovich, N.,\n&amp; Ziegler, D., 2017, \u201cResearch Highlight: Certifying a File\nSystem using Crash Hoare Logic: Correctness in the Presence of\nCrashes,\u201d <i>Communications of the ACM (CACM),</i> 60.4:\n75\u201384. ",
                " Chalmers, D., 2010, \u201cThe Singularity: A Philosophical\nAnalysis,\u201d <i>Journal of Consciousness Studies</i>, 17:\n7\u201365. ",
                " Charniak, E., 1993, <i>Statistical Language Learning</i>,\nCambridge: MIT Press. ",
                " Charniak, E. &amp; McDermott, D., 1985, <i>Introduction to\nArtificial Intelligence</i>, Reading, MA: Addison Wesley. ",
                " Chellas, B., 1980, <i>Modal Logic: An Introduction</i>,\nCambridge, UK: Cambridge University Press. ",
                " Chisholm, R., 1957, <i>Perceiving</i>, Ithaca, NY: Cornell\nUniversity Press. ",
                " Chisholm, R., 1966, <i>Theory of Knowledge</i>, Englewood Cliffs,\nNJ: Prentice-Hall. ",
                " Chisholm, R., 1977, <i>Theory of Knowledge 2nd ed</i>, Englewood\nCliffs, NJ: Prentice-Hall. ",
                " Clark, A., 2003, <i>Natural-Born Cyborgs</i>, Oxford, UK: Oxford\nUniversity Press. ",
                " Clark, M. H., 2010, <i>Cognitive Illusions and the Lying Machine:\nA Blueprint for Sophistic Mendacity</i>, PhD dissertation, Rensselaer\nPolytechnic Institute (Cognitive Science). ",
                " Copeland, B. J., 1998, \u201cSuper Turing Machines,\u201d\n<i>Complexity</i>, 4: 30\u201332. ",
                " Copi, I. &amp; Cohen, C., 2004, <i>Introduction to Logic</i>,\nSaddle River, NJ: Prentice-Hall. ",
                " Dennett, D., 1998, \u201cArtificial Life as Philosophy,\u201d\nin his <i>Brainchildren: Essays on Designing Minds</i>, Cambridge, MA:\nMIT Press, pp. 261\u2013263. ",
                " Dennett, D., 1994, \u201cThe Practical Requirements for Making a\nConscious Robot,\u201d <i>Philosophical Transactions of the Royal\nSociety of London</i>, 349: 133\u2013146. ",
                " Dennett, D., 1979, \u201cArtificial Intelligence as Philosophy\nand as Psychology,\u201d <i>Philosophical Perspectives in Artificial\nIntelligence</i>, M. Ringle, ed., Atlantic Highlands, NJ: Humanities\nPress, pp. 57\u201380. ",
                " Descartes, 1637, R., in Haldane, E. and Ross, G.R.T.,\ntranslators, 1911, <i>The Philosophical Works of Descartes, Volume\n1,</i> Cambridge, UK: Cambridge University Press. ",
                " Dick, P. K., 1968, <i>Do Androids Dream of Electric Sheep?</i>,\nNew York, NY: Doubleday. ",
                " Domingos, P., 2012, \u201cA Few Useful Things to Know about\nMachine Learning,\u201d <i>Communications of the ACM</i>, 55.10:\n78\u201387. ",
                " Dreyfus, H., 1972, <i>What Computers Can\u2019t Do</i>,\nCambridge, MA: MIT Press. ",
                " Dreyfus, H., 1992, <i>What Computers Still Can\u2019t Do</i>,\nCambridge, MA: MIT Press. ",
                " Dreyfus, H. &amp; Dreyfus, S., 1987, <i>Mind Over Machine: The\nPower of Human Intuition and Expertise in the Era of the Computer</i>,\nNew York, NY: Free Press. ",
                " Ebbinghaus, H., Flum, J. &amp; Thomas, W., 1984, <i>Mathematical\nLogic</i>, New York, NY: Springer-Verlag. ",
                " Eden, A., Moor, J., Soraker, J. &amp; Steinhart, E., 2013,\n<i>Singularity Hypotheses: A Scientific and Philosophical\nAssessment</i>, New York, NY: Springer. ",
                " Espa\u00f1a-Bonet, C., Enache, R., Slaski, A., Ranta, A.,\nM\u00e0rquez L. &amp; Gonz\u00e0lez, M., 2011, \u201cPatent\nTranslation within the MOLTO project,\u201d in <i> Proceedings of the\n4th Workshop on Patent Translation, MT Summit XIII</i>, pp.\n70\u201378. ",
                " Evans, G., 1968, \u201cA Program for the Solution of a Class of\nGeometric-Analogy Intelligence-Test Questions,\u201d in M. Minsky,\ned., <i>Semantic Information Processing</i>, Cambridge, MA: MIT Press,\npp. 271\u2013353. ",
                " Fagin, R., Halpern, J. Y., Moses, Y. &amp; Vardi, M., 2004,\n<i>Reasoning About Knowledge</i>, Cambridge, MA: MIT Press. ",
                " Ferrucci, D. &amp; Lally, A., 2004, \u201cUIMA: An Architectural\nApproach to Unstructured Information Processing in the Corporate\nResearch Environment,\u201d <i>Natural Language Engineering</i>,\n10.3\u20134: 327\u2013348. Cambridge, UK: Cambridge University\nPress. ",
                " Ferrucci, D., Brown, E., Chu-Carroll, J., Fan, J., Gondek, D.,\nKalyanpur, A., Lally, A., Murdock, J., Nyberg, E., Prager, J.,\nSchlaefer, N. &amp; Welty, C., 2010, \u201cBuilding Watson: An\nOverview of the DeepQA Project,\u201d <i>AI Magazine</i>, 31.3:\n59\u201379. ",
                " Finnsson, H., 2012, \u201cGeneralized Monte-Carlo Tree Search\nExtensions for General Game Playing,\u201d in <i>Proceedings of the\nTwenty-Sixth AAAI Conference on Artificial Intelligence\n(AAAI-2012)</i>, Toronto, Canda, pp. 1550\u20131556. ",
                " Fitelson, B., 2005, \u201cInductive Logic,\u201d in Pfeifer, J.\nand Sarkar, S., eds., <i>Philosophy of Science: An Encyclopedia</i>,\nLondon, UK: Routledge, pp. 384\u2013394. ",
                " Floridi, L., 2015, \u201cSingularitarians, AItheists, and Why\nthe Problem with Artificial Intelligence is H.A.L. (Humanity At\nLarge), not HAL,\u201d <i>APA Newsletter: Philosophy and\nComputers</i>, 14.2: 8\u201311. ",
                " Foot, P., 1967, \u201cThe Problem of Abortion and the Doctrine\nof the Double Effect,\u201d <i> Oxford Review</i>, 5:\n5\u201315.",
                " Forbus, K. D. &amp; Hinrichs, T. R., 2006, \u201cCompanion\nCognitive Systems: A Step toward Human-Level AI,\u201d <i>AI\nMagazine,</i> 27.2: 83. ",
                " Ford, K. M., Glymour C. &amp; Hayes P., 1997, \u201cOn the Other\nHand \u2026 Cognitive Prostheses,\u201d <i>AI Magazine,</i> 18.3:\n104. ",
                " Friedland, N., Allen, P., Matthews, G., Witbrock, M., Baxter, D.,\nCurtis, J., Shepard, B., Miraglia, P., Angele, J., Staab, S., Moench,\nE., Oppermann, H., Wenke, D., Israel, D., Chaudhri, V., Porter, B.,\nBarker, K., Fan, J., Yi Chaw, S., Yeh, P., Tecuci, D. &amp; Clark, P.,\n2004, \u201cProject Halo: Towards a Digital Aristotle,\u201d <i>AI\nMagazine</i>, 25.4: 29\u201347. ",
                " Genesereth, M., Love, N. &amp; Pell B., 2005, \u201cGeneral Game\nPlaying: Overview of the AAAI Competition,\u201d <i>AI Magazine</i>,\n26.2: 62\u201372. \n [<a href=\"https://www.aaai.org/ocs/index.php/IJCAI/IJCAI-09/paper/viewFile/566/775\" target=\"other\">Available online</a>]\n ",
                " Ginsberg, M., 1993, <i>Essentials of Artificial Intelligence</i>,\nNew York, NY: Morgan Kaufmann. ",
                " Glymour, G., 1992, <i>Thinking Things Through</i>, Cambridge, MA:\nMIT Press. ",
                " Goertzel, B. &amp; Pennachin, C., eds., 2007, <i>Artificial\nGeneral Intelligence</i>, Berlin, Heidelberg: Springer-Verlag. ",
                " Gold, M., 1965, \u201cLimiting Recursion,\u201d <i>Journal of\nSymbolic Logic</i>, 30.1: 28\u201347. ",
                " Goldstine, H. &amp; von Neumann, J., 1947, \u201cPlanning and\nCoding of Problems for an Electronic Computing Instrument,\u201d\n<i>IAS Reports</i> Institute for Advanced Study, Princeton, NJ. [This\nremarkable work is \n <a href=\"https://library.ias.edu/files/pdfs/ecp/planningcodingof0103inst.pdf\">available online</a>\nfrom the Institute for Advanced Study. Please note that this paper is\nPart II of a three-volume set. The first volume was devoted to a\npreliminary discussion, and the first author on it was Arthur Burks,\njoining Goldstine and von Neumann.]\n ",
                " Good, I., 1965, \u201cSpeculations Concerning the First\nUltraintelligent Machines,\u201d in <i>Advances in Computing</i>\n(vol. 6), F. Alt and M. Rubinoff, eds., New York, NY: Academic Press,\npp. 31\u201338. ",
                " Govindarajulu, N. S., Bringsjord, S. &amp; Licato J., 2013,\n\u201cOn Deep Computational Formalization of Natural Language,\u201d\nin <i>Proceedings of the Workshop \u201cFormalizing Mechanisms for\nArtificial General Intelligence and Cognition (Formal\nMAGiC),\u201d</i> Osnabr\u00fcck, Germany: PICS. ",
                " Govindarajulu, N. S., &amp; Bringsjord, S., 2015, \u201cEthical\nRegulation of Robots Must Be Embedded in Their Operating\nSystems\u201d in Trappl, R., ed., <i> A Construction Manual for\nRobot\u2019s Ethical Systems: Requirements, Methods,\nImplementations</i>, Berlin, DE: Springer. ",
                " Govindarajulu, N. S., &amp; Bringsjord, S., 2017, \u201cOn\nAutomating the Doctrine of Double Effect,\u201d in <i>Proceedings of\nthe Twenty-Sixth International Joint Conference on Artificial\nIntelligence (IJCAI-17)</i>, pp. 4722\u20134730. doi:10.24963/ijcai.2017/658\n ",
                " Granger, R., 2004a, \u201cDerivation and Analysis of Basic\nComputational Operations of Thalamocortical Circuits,\u201d\n<i>Journal of Cognitive Neuroscience</i> 16: 856\u2013877. ",
                " Granger, R., 2004b, \u201cBrain Circuit Implementation:\nHigh-precision Computation from Low-Precision Components,\u201d in\n<i>Toward Replacement Parts for the Brain</i>, T. Berger and D.\nGlanzman, eds., Cambridge, MA: MIT Press, pp. 277\u2013294. ",
                " Griewank, A., 2000, <i>Evaluating Derivatives: Principles and\nTechniques of Algorithmic Differentiation</i>, Philadlphia, PA:\nSociety for Industrial and Applied Mathematics (SIAM). ",
                " Guizzo, E., 2011, \u201cHow Google\u2019s Self-driving Car\nWorks,\u201d <i>IEEE Spectrum Online</i>.\n [<a href=\"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/how-google-self-driving-car-works\" target=\"other\">Available online]</a>",
                " Hailperin, T., 1996, <i> Sentential Probability Logic: Origins,\nDevelopment, Current Status, and Technical Applications,</i>\nBethlehem, United States: Lehigh University Press. ",
                " Hailperin, T., 2010, <i> Logic with a Probability Semantics,</i>\nBethlehem, United States: Lehigh University Press. ",
                " Halpern, J. Y., 1990, \u201cAn Analysis of First-order Logics of\nProbability,\u201d <i>Artificial Intelligence,</i> 46: 311\u2013350.\n",
                " Halpern, J., Harper, R., Immerman, N., Kolaitis, P. G., Vardi, M.\n&amp; Vianu, V., 2001, \u201cOn the Unusual Effectiveness of Logic in\nComputer Science,\u201d <i>The Bulletin of Symbolic Logic</i>, 7.2:\n213\u2013236. ",
                " Hamkins, J. &amp; Lewis, A., 2000, \u201cInfinite Time Turing\nMachines,\u201d <i>Journal of Symbolic Logic</i>, 65.2:\n567\u2013604. ",
                " Harnad, S., 1991, \u201cOther Bodies, Other Minds: A Machine\nIncarnation of an Old Philosophical Problem,\u201d <i>Minds and\nMachines</i>, 1.1: 43\u201354. ",
                " Haugeland, J., 1985, <i>Artificial Intelligence: The Very\nIdea</i>, Cambridge, MA: MIT Press. ",
                " Hendler, J. &amp; Jennifer G., 2008, \u201cMetcalfe\u2019s Law,\nWeb 2.0, and the Semantic Web,\u201d <i>Web Semantics: Science,\nServices and Agents on the World Wide Web,</i> 6.1: 14\u201320. ",
                " Hinton, G., Deng, L., Yu, D., Dahl, G.E., Mohamed, A. R., Jaitly,\nN., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T. &amp;\nKingsbury, B., 2012, \u201cDeep Neural Networks for Acoustic Modeling\nin Speech Recognition: The Shared Views of Four Research\nGroups,\u201d <i>IEEE Signal Processing Magazine</i>, 29.6:\n82\u201397. ",
                " Hoffman, R. R., Hayes, P. J. &amp; Ford, K. M., 2001,\n\u201cHuman-Centered Computing: Thinking In and Out of the\nBox,\u201d <i>IEEE Intelligent Systems</i>, 16.5: 76\u201378. ",
                " Hoffman, R. R., Bradshaw J. M., Hayes P. J. &amp; Ford K. M.,\n2003, \u201c The Borg Hypothesis,\u201d<i> IEEE Intelligent\nSystems</i>, 18.5: 73\u201375. ",
                " Hofstadter, D. &amp; McGraw, G., 1995, \u201cLetter Spirit:\nEsthetic Perception and Creative Play in the Rich Microcosm of the\nRoman Alphabet,\u201d in Hofstadter\u2019s <i>Fluid Concepts and\nCreative Analogies: Computer Models of the Fundamental Mechanisms of\nThought</i>, New York, NY: Basic Books, pp. 407\u2013488. ",
                " Hornik, K., Stinchcombe, M. &amp; White, H., 1989,\n\u201cMultilayer Feedforward Networks are Universal\nApproximators,\u201d <i>Neural Networks</i>, 2.5: 359\u2013366.\n",
                " Hutter, M., 2005, <i>Universal Artificial Intelligence</i>,\nBerlin: Springer. ",
                " Joy, W., 2000, \u201cWhy the Future Doesn\u2019t Need\nUs,\u201d <i>Wired</i> 8.4. \n [<a href=\"https://www.wired.com/2000/04/joy-2/\" target=\"other\">Available online</a>]\n ",
                " Kahneman, D., 2013. <i>Thinking, Fast and Slow</i>, New York, NY:\nFarrar, Straus, and Giroux. ",
                " Kaufmann, M., Manolios, P. &amp; Moore, J. S., 2000,\n<i>Computer-Aided Reasoning: ACL2 Case Studies</i>, Dordrecht, The\nNetherlands: Kluwer Academic Publishers. ",
                " Klenk, M., Forbus, K., Tomai, E., Kim,H. &amp; Kyckelhahn, B.,\n2005, \u201cSolving Everyday Physical Reasoning Problems by Analogy\nusing Sketches,\u201d in <i>Proceedings of 20th National Conference\non Artificial Intelligence</i> (AAAI-05), Pittsburgh, PA. ",
                " Kolata, G., 1996, \u201cComputer Math Proof Shows Reasoning\nPower,\u201d in <i>New York Times</i>. \n [<a href=\"http://www.nytimes.com/library/cyber/week/1210math.html\" target=\"other\">Availabe online</a>]\n ",
                " Koller, D., Levy, A. &amp; Pfeffer, A., 1997, \u201cP-CLASSIC: A\nTractable Probablistic Description Logic,\u201d in <i>Proceedings of\nthe AAAI 1997 Meeting</i>, 390\u2013397. ",
                " Kurzweil, R., 2006, <i>The Singularity Is Near: When Humans\nTranscend Biology</i>, New York, NY: Penguin USA. ",
                " Kurzweil, R., 2000, <i>The Age of Spiritual Machines: When\nComputers Exceed Human Intelligence</i>, New York, NY: Penguin USA.\n",
                " LaForte, G., Hayes P. &amp; Ford, K., 1998, \u201cWhy\nG\u00f6del\u2019s Theorem Cannot Refute Computationslism,\u201d\n<i>Artificial Intelligence</i>, 104: 265\u2013286. ",
                " Laird, J. E., 2012, <i>The Soar Cognitive Architecture</i>,\nCambridge, MA: MIT Press. ",
                " Laird, J. &amp; VanLent M., 2001, \u201cHuman-level AI\u2019s\nKiller Application: Interactive Computer Games,\u201d <i>AI\nMagazine</i> 22.2:15\u201326. ",
                " LeCun, Y., Bengio, Y. &amp; Hinton G., 2015, \u201cDeep\nLearning,\u201d <i>Nature</i>, 521: 436\u2013444.",
                " Lenat, D., 1983, \u201cEURISKO: A Program that Learns New Heuristics and Domain Concepts,\u201d <em>Artificial Intelligence</em>, 21(1-2): 61\u201398. doi:10.1016/s0004-3702(83)80005-8",
                " Lenat, D., &amp; Guha, R. V., 1990, <em>Building Large\nKnowledge-Based Systems: Representation and Inference in the Cyc\nProject</em>, Reading, MA: Addison Wesley.",
                " Lenzen, W., 2004, \u201cLeibniz\u2019s Logic,\u201d in Gabbay,\nD., Woods, J. and Kanamori, A., eds., <i>Handbook of the History of\nLogic</i>, Elsevier, Amsterdam, The Netherlands, pp. 1\u201383. ",
                " Lewis, H. &amp; Papadimitriou, C., 1981, <i>Elements of the\nTheory of Computation</i>, Prentice Hall, Englewood Cliffs, NJ:\nPrentice Hall. ",
                " Litt, A., Eliasmith, C., Kroon, F., Weinstein, S. &amp; Thagard,\nP., 2006, \u201cIs the Brain a Quantum Computer?\u201d <i>Cognitive\nScience</i> 30: 593\u2013603. ",
                "Lucas, J. R., 1964, \u201cMinds, Machines, and G\u00f6del,\u201d\nin <i>Minds and Machines</i>, A. R. Anderson, ed., Prentice-Hall, NJ:\nPrentice-Hall, pp. 43\u201359. ",
                " Luger, G., 2008, <i>Artificial Intelligence: Structures and\nStrategies for Complex Problem Solving</i>, New York, NY: Pearson.\n",
                " Luger, G. &amp; Stubblefield, W., 1993, <i>Artificial\nIntelligence: Structures and Strategies for Complex Problem\nSolving</i>, Redwood, CA: Benjamin Cummings. ",
                " Lopez, A., 2008, \u201cStatistical Machine Translation,\u201d\n<i>ACM Computing Surveys</i>, 40.3: 1\u201349. ",
                " Malle, B. F., Scheutz, M., Arnold, T., Voiklis, J. &amp;\nCusimano, C., 2015, \u201cSacrifice One For the Good of Many?: People\nApply Different Moral Norms to Human and Robot Agents,\u201d in\n<i>Proceedings of the Tenth Annual ACM/IEEE International Conference\non Human-Robot Interaction (HRI \u201915)</i> (New York, NY: ACM),\npp. 117\u2013124. ",
                " Manzano, M., 1996, <i>Extensions of First Order Logic</i>,\nCambridge, UK: Cambridge University Press. ",
                " Marcus, G., 2013, \u201cWhy Can\u2019t My Computer Understand\nMe?,\u201d in <i>The New Yorker</i>, August 2013.\n [<a href=\"http://www.newyorker.com/online/blogs/elements/2013/08/why-cant-my-computer-understand-me.html\" target=\"other\">Available online</a>]\n ",
                " McCarthy, J. &amp; Hayes, P., 1969, \u201cSome Philosophical\nProblems from the Standpoint of Artificial Intelligence,\u201d in\n<i>Machine Intelligence 4</i>, B. Meltzer and D. Michie, eds.,\nEdinburgh: Edinburgh University Press, 463\u2013502. ",
                " Mueller, E., 2006, <i>Commonsense Reasoning</i>, San Francisco,\nCA: Morgan Kaufmann. ",
                " Murphy, K. P., 2012, <i>Machine Learning: A Probabilistic\nPerspective</i>, Cambridge, MA: MIT Press. ",
                " Minsky, M. &amp; Pappert, S., 1969, <i>Perceptrons: An\nIntroduction to Computational Geometry</i>, Cambridge, MA: MIT Press.\n",
                " Montague, R., 1970, \u201cUniversal Grammar,\u201d\n<i>Theoria,</i> 36, 373\u2013398.",
                " Moor, J., 2006, \u201cThe Nature, Importance, and Difficulty of\nMachine Ethics\u201d, <i>IEEE Intelligent Systems</i> 21.4:\n18\u201321. ",
                " Moor, J., 1985, \u201cWhat is Computer Ethics?\u201d\n<i>Metaphilosophy</i> 16.4: 266\u2013274. ",
                " Moor, J., ed., 2003, <i>The Turing Test: The Elusive Standard of\nArtificial Intelligence</i>, Dordrecht, The Netherlands: Kluwer\nAcademic Publishers. ",
                " Moravec, H., 1999, <i>Robot: Mere Machine to Transcendant\nMind</i>, Oxford, UK: Oxford University Press, ",
                " Naumowicz, A. &amp; Kornilowicz., A., 2009, \u201cA Brief\nOverview of Mizar,\u201d in <i>Theorem Proving in Higher Order\nLogics,</i> S. Berghofer, T. Nipkow, C. Urban &amp; M. Wenzel, eds.,\nBerlin: Springer, pp. 67\u201372. ",
                " Newell, N., 1973, \u201cYou Can\u2019t Play 20 Questions with\nNature and Win: Projective Comments on the Papers of this\nSymposium\u201d, in <i>Visual Information Processing</i>, W. Chase,\ned., New York, NY: Academic Press, pp. 283\u2013308. ",
                " Nilsson, N., 1998, <i>Artificial Intelligence: A New\nSynthesis</i>, San Francisco, CA: Morgan Kaufmann. ",
                " Nilsson, N., 1987, <i>Principles of Artificial Intelligence</i>,\nNew York, NY: Springer-Verlag. ",
                " Nilsson, N., 1991, \u201cLogic and Artificial\nIntelligence,\u201d <i>Artificial Intelligence</i>, 47: 31\u201356.\n",
                " Nozick, R., 1970, \u201cNewcomb\u2019s Problem and Two\nPrinciples of Choice,\u201d in <i>Essays in Honor of Carl G.\nHempel</i>, N. Rescher, ed., Highlands, NJ: Humanities Press, pp.\n114\u2013146. This appears to be the very first published treatment\nof NP \u2013 though the paradox goes back to its creator: William\nNewcomb, a physicist. ",
                " Osherson, D., Stob, M. &amp; Weinstein, S., 1986, <i>Systems That\nLearn</i>, Cambridge, MA: MIT Press. ",
                " Pearl, J., 1988, <i>Probabilistic Reasoning in Intelligent\nSystems</i>, San Mateo, CA: Morgan Kaufmann. ",
                " Pennington, J., Socher R., &amp; Manning C. D., 2014,\n\u201cGloVe: Global Vectors for Word Representation,\u201d in\n<i>Proceedings of the 2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP 2014)</i>, pp. 1532\u20131543. \n [<a href=\"http://www.aclweb.org/anthology/D14-1162\" target=\"other\">Available online</a>]\n ",
                " Penrose, R., 1989, <i>The Emperor\u2019s New Mind</i>, Oxford,\nUK: Oxford University Press. ",
                " Penrose, R., 1994, <i>Shadows of the Mind</i>, Oxford, UK: Oxford\nUniversity Press. ",
                " Penrose, R., 1996, \u201cBeyond the Doubting of a Shadow: A\nReply to Commentaries on <i>Shadows of the Mind,</i>\u201d\n<i>Psyche</i>, 2.3. This paper is available\n <a href=\"http://www.calculemus.org/MathUniversalis/NS/10/01penrose.html\" target=\"other\"> online.</a>\n",
                "Pereira, L., &amp; Saptawijaya A., 2016, <i>Programming Machine\nEthics,</i> Berlin, Germany: Springer",
                " Pinker, S., 1997, <i>How the Mind Works,</i> New York, NY:\nNorton. ",
                " Pollock, J., 2006, <i>Thinking about Acting: Logical Foundations\nfor Rational Decision Making,</i> Oxford, UK: Oxford University Press.\n",
                " Pollock, J., 2001, \u201cDefeasible Reasoning with Variable\nDegrees of Justification,\u201d <i>Artificial Intelligence</i>, 133,\n233\u2013282. ",
                " Pollock, J., 1995, <i>Cognitive Carpentry: A Blueprint for How to\nBuild a Person</i>, Cambridge, MA: MIT Press. ",
                " Pollock, J., 1992, \u201cHow to Reason Defeasibly,\u201d\n<i>Artificial Intelligence</i>, 57, 1\u201342. ",
                " Pollock, J., 1989, <i>How to Build a Person: A Prolegomenon</i>,\nCambridge, MA: MIT Press. ",
                " Pollock, J., 1974, <i>Knowledge and Justification</i>, Princeton,\nNJ: Princeton University Press. ",
                " Pollock, J., 1967, \u201cCriteria and our Knowledge of the\nMaterial World,\u201d <i>Philosophical Review</i>, 76, 28\u201360.\n",
                " Pollock, J., 1965, <i>Analyticity and Implication,</i> PhD\ndissertation, University of California at Berkeley (Philosophy). ",
                " Potter, M.D., 2004, <i> Set Theory and its Philosophy</i>,\nOxford, UK: Oxford University Press ",
                " Preston, J. &amp; Bishop, M., 2002, <i>Views into the Chinese\nRoom: New Essays on Searle and Artificial Intelligence</i>, Oxford,\nUK: Oxford University Press. ",
                " Putnam, H., 1965, \u201cTrial and Error Predicates and a\nSolution to a Problem of Mostowski,\u201d <i>Journal of Symbolic\nLogic,</i> 30.1, 49\u201357. ",
                " Putnam, H., 1963, \u201cDegree of Confirmation and Inductive\nLogic,\u201d in <i>The Philosophy of Rudolf Carnap</i>, Schilipp, P.,\ned., Open Court, pp. 270\u2013292. ",
                " Rajat, R., Anand, M. &amp; Ng, A. Y., 2009, \u201cLarge-scale\nDeep Unsupervised Learning Using Graphics Processors,\u201d in\n<i>Proceedings of the 26th Annual International Conference on Machine\nLearning</i>, ACM, pp. 873\u2013880. ",
                " Rapaport, W., 1988, \u201cSyntactic Semantics: Foundations of\nComputational Natural-Language Understanding,\u201d in <i>Aspects of\nArtificial Intelligence</i>, J. H. Fetzer ed., Dordrecht, The\nNetherlands: Kluwer Academic Publishers, 81\u2013131. ",
                " Rapaport, W. &amp; Shapiro, S., 1999, \u201cCognition and\nFiction: An Introduction,\u201d <i> Understanding Language\nUnderstanding: Computational Models of Reading</i>, A. Ram &amp; K.\nMoorman, eds., Cambridge, MA: MIT Press, 11\u201325. \n [<a href=\"https://cse.buffalo.edu/~rapaport/Papers/fiction.ashwin.pdf\" target=\"other\">Available online</a>]\n ",
                " Reeke, G. &amp; Edelman, G., 1988, \u201cReal Brains and\nArtificial Intelligence,\u201d in <i>The Artificial Intelligence\nDebate: False Starts, Real Foundations</i>, Cambridge, MA: MIT Press,\npp. 143\u2013173. ",
                " Richardson, M. &amp; Domingos, P., 2006, \u201cMarkov Logic\nNetworks,\u201d <i>Machine Learning,</i> 62.1\u20132:107\u2013136.\n",
                " Robertson, G. &amp; Watson, I., 2015, \u201cA Review of\nReal-Time Strategy Game AI,\u201d <i>AI Magazine</i>, 35.4:\n75\u2013104. ",
                " Rosenschein, S. &amp; Kaelbling, L., 1986, \u201cThe Synthesis\nof Machines with Provable Epistemic Properties,\u201d in\n<i>Proceedings of the 1986 Conference on Theoretical Aspects of\nReasoning About Knowledge</i>, San Mateo, CA: Morgan Kaufmann, pp.\n83\u201398. ",
                " Rumelhart, D. &amp; McClelland, J., 1986, eds., <i>Parallel\nDistributed Processing</i>, Cambridge, MA: MIT Press. ",
                " Russell, S., 1997, \u201cRationality and Intelligence,\u201d\n<i>Artificial Intelligence</i>, 94: 57\u201377. \n [<a href=\"https://people.eecs.berkeley.edu/~russell/papers/aij-cnt.pdf\" target=\"other\">Version available online from author</a>]\n ",
                " Russell, S. &amp; Norvig, P., 1995, <i>Artificial Intelligence: A\nModern Approach</i>, Saddle River, NJ: Prentice Hall. ",
                " Russell, S. &amp; Norvig, P., 2002, <i>Artificial Intelligence: A\nModern Approach 2nd edition</i>, Saddle River, NJ: Prentice Hall.\n",
                " Russell, S. &amp; Norvig, P., 2009, <i>Artificial Intelligence: A\nModern Approach 3rd edition</i>, Saddle River, NJ: Prentice Hall.\n",
                "Rychtyckyj, N. &amp; Plesco, C., 2012, \u201cApplying Automated\nLanguage Translation at a Global Enterprise Level,\u201d <i>AI\nMagazine</i>, 34.1: 43\u201354. ",
                " Scanlon, T. M., 1982, \u201cContractualism and\nUtilitarianism,\u201d in A. Sen and B. Williams, eds., <i>\nUtilitarianism and Beyond,</i> Cambridge: Cambridge University Press,\npp. 103\u2013128. ",
                " Schank, R., 1972, \u201cConceptual Dependency: A Theory of\nNatural Language Understanding,\u201d <i>Cognitive Psychology</i>,\n3.4: 532\u2013631. ",
                " Schaul, T. &amp; Schmidh\u00fcber, J., 2010,\n\u201cMetalearning,\u201d <i>Scholarpedia</i> 5(6): 4650. URL:\n <a href=\"http://www.scholarpedia.org/article/Metalearning\" target=\"other\">http://www.scholarpedia.org/article/Metalearning</a>\n",
                " Schmidh\u00fcber, J., 2009, \u201cUltimate Cognition \u00e0 la\nG\u00f6del,\u201d <i>Cognitive Computation</i> 1.2: 177\u2013193.\n",
                " Searle, J., 1997, <i>The Mystery of Consciousness</i>, New York,\nNY: New York Review of Books. ",
                " Searle, J., 1980, \u201cMinds, Brains and Programs,\u201d\n<i>Behavioral and Brain Sciences</i>, 3: 417\u2013424. \n ",
                " Searle, J., 1984, <i>Minds, Brains and Science,</i> Cambridge,\nMA: Harvard University Press. The Chinese Room Argument is covered in\nChapter Two, \u201cCan Computers Think?\u201d. ",
                " Searle, J., 2014, \u201cWhat Your Computer Can\u2019t\nKnow,\u201d <i>New York Review of Books</i>, October 9. ",
                " Shapiro, S., 2000, \u201cAn Introduction to SNePS 3,\u201d in\n<i>Conceptual Structures: Logical, Linguistic, and Computational\nIssues. Lecture Notes in Artificial Intelligence 1867</i>, B. Ganter\n&amp; G. W. Mineau, eds., Springer-Verlag, 510\u2013524. ",
                " Shapiro, S., 2003, \u201cMechanism, Truth, and Penrose\u2019s\nNew Argument,\u201d <i>Journal of Philosophical Logic</i>, 32.1:\n19\u201342. ",
                " Siegelmann, H., 1999, <i>Neural Networks and Analog Computation:\nBeyond the Turing Limit</i>, Boston, MA: Birkhauser. ",
                " Siegelmann, H. &amp; and Sontag, E., 1994, \u201cAnalog\nComputation Via Neural Nets,\u201d <i>Theoretical Computer\nScience</i>, 131: 331\u2013360. ",
                "Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van\nden Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam,\nV., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N.,\nSutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel T.\n&amp; Hassabis D., 2016, \u201cMastering the Game of Go with Deep\nNeural Networks and Tree Search,\u201d<i> Nature</i>, 529:\n484\u2013489.\n ",
                " Shin, S-J, 2002, <i>The Iconic Logic of Peirce\u2019s\nGraphs,</i> Cambridge, MA: MIT Press. ",
                " Smolensky, P., 1988, \u201cOn the Proper Treatment of\nConnectionism,\u201d <i>Behavioral &amp; Brain Sciences</i>, 11:\n1\u201322. ",
                " Somers, J., 2013, \u201cThe Man Who Would Teach Machines to\nThink,\u201d in <i>The Atlantic</i>. \n [<a href=\"http://theatlantic.com/magazine/archive/2013/11/the-man-who-would-teach-machines-to-think/309529/\" target=\"other\">Available online</a>]\n ",
                " Stanovich, K. &amp; West, R., 2000, \u201cIndividual Differences\nin Reasoning: Implications for the Rationality Debate,\u201d\n<i>Behavioral and Brain Sciences</i>, 23.5: 645\u2013665. ",
                " Strzalkowski, T. &amp; Harabagiu, M. S., 2006, eds., <i> Advances\nin Open Domain Question Answering</i>; in the series Text, Speech and\nLanguage Technology, volume 32, Dordrecht, The Netherlands:\nSpringer-Verlag. ",
                " Sun, R., 2002, <i>Duality of the Mind: A Bottom Up Approach\nToward Cognition</i>, Mahwah, NJ: Lawrence Erlbaum. ",
                " Sun, R., 1994, <i>Integrating Rules and Connectionism for Robust\nCommonsense Reasoning</i>, New York, NY: John Wiley and Sons. ",
                " Sutton R. S. &amp; Barto A. G., 1998, <i>Reinforcement Learning:\nAn Introduction</i>, Cambridge, MA: MIT Press. ",
                " Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D.,\nGoodfellow, I. &amp; Fergus, R., 2014, \u201cIntriguing Properties of\nNeural Networks,\u201d in <i> Second International Conference on\nLearning Representations</i>, Banff, Canada. \n [<a href=\"https://arxiv.org/pdf/1312.6199.pdf\" target=\"other\">Available online</a>]",
                " Hastie, T., Tibshirani, R., &amp; Jerome, F., 2009, <i>The\nElements of Statistical Learning</i>, in the series <i>Springer Series\nin Statistics</i>, New York: Springer. ",
                " Turing, A., 1950, \u201cComputing Machinery and\nIntelligence,\u201d <i>Mind</i>, LIX: 433\u2013460. ",
                " Turing, A., 1936, \u201cOn Computable Numbers with Applications\nto the Entscheidung-Problem,\u201d <i>Proceedings of the London\nMathematical Society</i>, 42: 230\u2013265. ",
                " Vilalta, R. &amp; Drissi, Y., 2002, \u201cA Perspective View and\nSurvey of Meta-learning,\u201d <i> Artificial Intelligence\nReview</i>, 18.2:77\u201395. ",
                " Voronkov, A., 1995, \u201cThe Anatomy of Vampire: Implementing\nBottom-Up Procedures with Code Trees,\u201d <i>Journal of Automated\nReasoning</i>, 15.2. ",
                " Wallach, W. &amp; Allen, C., 2010, <i>Moral Machines: Teaching\nRobots Right from Wrong,</i> Oxford, UK: Oxford University Press.\n",
                " Wermter, S. &amp; Sun, R., 2001 (Spring), \u201cThe Present and\nthe Future of Hybrid Neural Symbolic Systems: Some Reflections from\nthe Neural Information Processing Systems Workshop,\u201d <i>AI\nMagazine</i>, 22.1: 123\u2013125. ",
                " Suppes, P., 1972, <i>Axiomatic Set Theory</i>, New York, NY:\nDover. ",
                " Whiteson, S. &amp; Whiteson, D., 2009, \u201cMachine Learning\nfor Event Selection in High Energy Physics,\u201d <i> Engineering\nApplications of Artificial Intelligence</i> 22.8: 1203\u20131217.\n",
                " Williams, D. E., Hinton G. E., &amp; Williams R. J., 1986\n\u201cLearning Representations by Back-propagating Errors,\u201d\n<i>Nature</i>, 323.10: 533\u2013536. ",
                " Winston, P., 1992, <i>Artificial Intelligence</i>, Reading, MA:\nAddison-Wesley. ",
                " Wos, L., Overbeek, R., Lusk R. &amp; Boyle, J., 1992,\n<i>Automated Reasoning: Introduction and Applications (2nd\nedition)</i>, New York, NY: McGraw-Hill. ",
                " Wos, L., 2013, \u201cThe Legacy of a Great Researcher,\u201d in\n<i>Automated Reasoning and Mathematics: Essays in Memory of William\nMcCune</i>, Bonacina, M.P. &amp; Stickel, M.E., eds., 1\u201314.\nBerlin: Springer. ",
                " Zalta, E., 1988, <i>Intensional Logic and the Metaphysics of\nIntentionality</i>, Cambridge, MA: Bradford Books. "
            ]
        },
        "raw_text": "<div id=\"bibliography\">\n<h2 id=\"Bib\">Bibliography</h2>\n<ul class=\"hanging\">\n<li> Adams, E. W., 1996, <i> A Primer of Probability Logic</i>,\nStanford, CA: CSLI. </li>\n<li> Almeida, J., Frade, M., Pinto, J. &amp; de Sousa, S., 2011,\n<i>Rigorous Software Development: An Introduction to Program\nVerification</i>, New York, NY: Spinger. </li>\n<li> Alpaydin, E., 2014, <i>Introduction to Machine Learning</i>,\nCambridge, MA: MIT Press. </li>\n<li> Amir, E. &amp; Maynard-Reid, P., 1999, \u201cLogic-Based\nSubsumption Architecture,\u201d in <i>Proceedings of the\n16<sup>th</sup> International Joint Conference on Artificial\nIntelligence (IJCAI-1999)</i>, (San Francisco, CA: MIT Morgan\nKaufmann), pp. 147\u2013152. </li>\n<li> Amir, E. &amp; Maynard-Reid, P., 2000, \u201cLogic-Based\nSubsumption Architecture: Empirical Evaluation,\u201d in <i>\nProceedings of the AAAI Fall Symposium on Parallel Architectures for\nCognition</i>. </li>\n<li> Amir, E. &amp; Maynard-Reid, P., 2001, \u201cLiSA: A Robot\nDriven by Logical Subsumption,\u201d in <i>Proceedings of the Fifth\nSymposium on the Logical Formalization of Commonsense Reasoning</i>,\n(New York, NY). </li>\n<li> Anderson, C. A., 1983, \u201cThe Paradox of the Knower,\u201d\n<i>The Journal of Philosophy,</i> 80.6: 338\u2013355.</li>\n<li> Anderson, J. &amp; Lebiere, C., 2003, \u201cThe Newell Test for\na Theory of Cognition,\u201d <i>Behavioral and Brain Sciences</i>,\n26: 587\u2013640. </li>\n<li> Ashcraft, M., 1994, <i>Human Memory and Cognition</i>, New York,\nNY: HarperCollins. </li>\n<li> Arkin, R., 2009, <i>Governing Lethal Behavior in Autonomous\nRobots</i>, London: Chapman and Hall/CRC Imprint, Taylor and Francis\nGroup. </li>\n<li> Arkoudas, K. &amp; Bringsjord, S., 2005, \u201cVivid: A\nFramework for Heterogeneous Problem Solving,\u201d <i>Artificial\nIntelligence</i>, 173.15: 1367\u20131405. </li>\n<li> Arkoudas, K. &amp; Bringsjord, S., 2005, \u201cMetareasoning for\nMulti-agent Epistemic Logics,\u201d in <i>Fifth International\nConference on Computational Logic In Multi-Agent Systems (CLIMA\n2004)</i>, in the series <i>Lecture Notes in Artificial Intelligence\n(LNAI)</i>, volume 3487, New York, NY: Springer-Verlag, pp.\n111\u2013125. </li>\n<li> Arkoudas, K., 2000, <i>Denotational Proof Languages</i>, PhD\ndissertation, Massachusetts Institute of Technology (Computer\nScience). </li>\n<li> Baader, F., Calvanese, D., McGuinness, D. L., Nardi, D., &amp;\nPatel-Schneider, P. F., eds., 2003, <i>The Description Logic Handbook:\nTheory, Implementation, and Applications</i>, New York, NY: Cambridge\nUniversity Press. </li>\n<li> Smith, B., Ashburner, M., Rosse, C., Bard, J., Bug, W., Ceusters,\nW., Goldberg, L. J., Eilbeck, K., Ireland, A., Mungall, C. J., The OBI\nConsortium, Leontis, N., Rocca-Serra, P., Ruttenberg, A., Sansone, S.,\nScheuermann, R. H., Shah, N., Whetzel, P. L. &amp; Lewis, S., 2007,\n\u201cThe OBO Foundry: Coordinated Evolution of Ontologies to Support\nBiomedical Data Integration,\u201d <i>Nature Biotechnology</i> 25,\n1251\u20131255. </li>\n<li> Barwise, J. &amp; Etchemendy, J., 1999, <i>Language, Proof, and\nLogic</i>, New York, NY: Seven Bridges Press. </li>\n<li> Barwise, J. &amp; Etchemendy, J., 1995, \u201cHeterogeneous\nLogic,\u201d in <i>Diagrammatic Reasoning: Cognitive and\nComputational Perspectives</i>, J. Glasgow, N.H. Narayanan, &amp; B.\nChandrasekaran, eds., Cambridge, MA: MIT Press, pp. 211\u2013234.\n</li>\n<li> Baldi, P., Sadowski P. &amp; Whiteson D., 2014, \u201cSearching\nfor Exotic Particles in High-energy Physics with Deep\nLearning,\u201d<i> Nature Communications</i>.\n [<a href=\"http://www.nature.com/ncomms/2014/140702/ncomms5308/full/ncomms5308.html\" target=\"other\">Available online</a>]\n </li>\n<li> Barwise, J. &amp; Etchemendy, J., 1994, <i>Hyperproof</i>,\nStanford, CA: CSLI. </li>\n<li> Barwise, J. &amp; Etchemendy, J., 1990, \u201cInfons and\nInference,\u201d in <i>Situation Theory and its Applications, (Vol\n1)</i>, Cooper, Mukai, and Perry (eds), CSLI Lecture Notes #22, CSLI\nPress, pp. 33\u201378. </li>\n<li> Bello, P. &amp; Bringsjord S., 2013, \u201cOn How to Build a\nMoral Machine,\u201d <i>Topoi,</i> 32.2: 251\u2013266. </li>\n<li> Bengio, Y., Goodfellow, I., &amp; Courville, A., 2016, <i>Deep\nLearning</i>, Cambridge: MIT Press.\n [<a href=\"http://www.deeplearningbook.org\" target=\"other\">Available online</a>]\n </li>\n<li> Bengio, Y., Courville, A. &amp; Vincent, P., 2013,\n\u201cRepresentation Learning: A Review and New Perspectives,\u201d\n<i> Pattern Analysis and Machine Intelligence, IEEE Transactions,</i>\n35.8: 1798\u20131828. </li>\n<li>Berners-Lee, T., Hendler, J. &amp; Lassila, O., 2001, \u201cThe\nSemantic Web,\u201d <i>Scientific American,</i> 284: 34\u201343.\n</li>\n<li> Bishop, M. &amp; Preston, J., 2002, <i>Views into the Chinese\nRoom: New Essays on Searle and Artificial Intelligence</i>, Oxford,\nUK: Oxford University Press. </li>\n<li> Boden, M., 1994, \u201cCreativity and Computers,\u201d in\n<i>Artificial Intelligence and Computers</i>, T. Dartnall, ed.,\nDordrecht, The Netherlands: Kluwer, pp. 3\u201326. </li>\n<li> Boolos, G. S., Burgess, J.P., &amp; Jeffrey., R.C., 2007,\n<i>Computability and Logic 5th edition,</i> Cambridge: Cambridge\nUniversity Press. </li>\n<li> Bostrom, N., 2014, <i>Superintelligence: Paths, Dangers,\nStrategies</i>, Oxford, UK: Oxford University Press. </li>\n<li> Bowie, G.L., 1982, \u201cLucas\u2019 Number is Finally\nUp,\u201d <i>Journal of Philosophical Logic</i>, 11: 279\u2013285.\n</li>\n<li> Brachman, R. &amp; Levesque, H., 2004, <i>Knowledge\nRepresentation and Reasoning</i>, San Francisco, CA: Morgan\nKaufmann/Elsevier. </li>\n<li> Bringsjord, S., Arkoudas K. &amp; Bello P., 2006, \u201cToward a\nGeneral Logicist Methodology for Engineering Ethically Correct\nRobots,\u201d IEEE Intelligent Systems, 21.4: 38\u201344. </li>\n<li> Bringsjord, S. &amp; Ferrucci, D., 1998, \u201cLogic and\nArtificial Intelligence: Divorced, Still Married,\nSeparated\u2026?\u201d <i>Minds and Machines</i>, 8: 273\u2013308.\n</li>\n<li> Bringsjord, S. &amp; Schimanski, B., 2003, \u201cWhat is\nArtificial Intelligence? Psychometric AI as an Answer,\u201d\n<i>Proceedings of the 18<sup>th</sup> International Joint Conference\non Artificial Intelligence (IJCAI-2003)</i>, (San Francisco, CA: MIT\nMorgan Kaufmann), pp. 887\u2013893. </li>\n<li> Bringsjord, S. &amp; Ferrucci, D., 2000, <i>Artificial\nIntelligence and Literary Creativity: Inside the Mind of Brutus, a\nStorytelling Machine</i>, Mahwah, NJ: Lawrence Erlbaum. </li>\n<li> Bringsjord, S. &amp; van Heuveln, B., 2003, \u201cThe Mental Eye\nDefense of an Infinitized Version of Yablo\u2019s Paradox,\u201d\n<i>Analysis</i> 63.1: 61\u201370. </li>\n<li> Bringsjord S. &amp; Xiao, H., 2000, \u201cA Refutation of\nPenrose\u2019s G\u00f6delian Case Against Artificial\nIntelligence,\u201d <i>Journal of Experimental and Theoretical\nArtificial Intelligence</i>, 12: 307\u2013329. </li>\n<li> Bringsjord, S. &amp; Zenzen, M., 2002, \u201cToward a Formal\nPhilosophy of Hypercomputation,\u201d <i>Minds and Machines</i>, 12:\n241\u2013258. </li>\n<li> Bringsjord, S., 2000, \u201cAnimals, Zombanimals, and the Total\nTuring Test: The Essence of Artificial Intelligence,\u201d <i>Journal\nof Logic, Language, and Information</i>, 9: 397\u2013418. </li>\n<li> Bringsjord, S., 1998, \u201cPhilosophy and \u2018Super\u2019\nComputation,\u201d <i>The Digital Phoenix: How Computers are Changing\nPhilosophy</i>, J. Moor and T. Bynam, eds., Oxford, UK: Oxford\nUniversity Press, pp. 231\u2013252. </li>\n<li> Bringsjord, S., 1991, \u201cIs the Connectionist-Logicist Clash\none of AI\u2019s Wonderful Red Herrings?\u201d <i>Journal of\nExperimental &amp; Theoretical AI</i>, 3.4: 319\u2013349. </li>\n<li> Bringsjord, S., Govindarajulu N. S., Eberbach, E. &amp; Yang, Y.,\n2012, \u201cPerhaps the Rigorous Modeling of Economic Phenomena\nRequires Hypercomputation,\u201d <i>International Journal of\nUnconventional Computing,</i> 8.1: 3\u201332.\n [<a href=\"http://kryten.mm.rpi.edu/SB_NSG_EE_YY_28-9-2010.pdf\" target=\"other\">Preprint available online</a>]\n </li>\n<li> Bringsjord, S., 2011, \u201cPsychometric Artificial\nIntelligence,\u201d <i>Journal of Experimental and Theoretical\nArtificial Intelligence</i>, 23.3: 271\u2013277. </li>\n<li> Bringsjord, S. &amp; Govindarajulu N. S., 2012, \u201cGiven the\nWeb, What is Intelligence, Really?\u201d <i>Metaphilosophy</i> 43.12:\n464\u2013479. </li>\n<li> Brooks, R. A., 1991, \u201cIntelligence Without\nRepresentation,\u201d <i>Artificial Intelligence</i>, 47:\n139\u2013159. </li>\n<li> Browne, C. B., Powley, E. &amp; Whitehouse, D., 2012, \u201cA\nSurvey of Monte Carlo Tree Search Methods,\u201d <i>A Survey of Monte\nCarlo Tree Search Methods</i>, 4.1: 1\u201343. </li>\n<li>Buchanan, B. G., 2005, \u201cA (Very) Brief History of Artificial\nIntelligence,\u201d <i>AI Magazine</i>, 26.4: 53\u201360. </li>\n<li> Carroll, L., 1958, <i>Symbolic Logic; Game of Logic</i>, New\nYork, NY: Dover. </li>\n<li> Cassimatis, N., 2006, \u201cCognitive Substrate for Human-Level\nIntelligence,\u201d <i>AI Magazine</i>, 27.2: 71\u201382. </li>\n<li> Chajed, T., Chen, H., Chlipala, A., Kaashoek, F., Zeldovich, N.,\n&amp; Ziegler, D., 2017, \u201cResearch Highlight: Certifying a File\nSystem using Crash Hoare Logic: Correctness in the Presence of\nCrashes,\u201d <i>Communications of the ACM (CACM),</i> 60.4:\n75\u201384. </li>\n<li> Chalmers, D., 2010, \u201cThe Singularity: A Philosophical\nAnalysis,\u201d <i>Journal of Consciousness Studies</i>, 17:\n7\u201365. </li>\n<li> Charniak, E., 1993, <i>Statistical Language Learning</i>,\nCambridge: MIT Press. </li>\n<li> Charniak, E. &amp; McDermott, D., 1985, <i>Introduction to\nArtificial Intelligence</i>, Reading, MA: Addison Wesley. </li>\n<li> Chellas, B., 1980, <i>Modal Logic: An Introduction</i>,\nCambridge, UK: Cambridge University Press. </li>\n<li> Chisholm, R., 1957, <i>Perceiving</i>, Ithaca, NY: Cornell\nUniversity Press. </li>\n<li> Chisholm, R., 1966, <i>Theory of Knowledge</i>, Englewood Cliffs,\nNJ: Prentice-Hall. </li>\n<li> Chisholm, R., 1977, <i>Theory of Knowledge 2nd ed</i>, Englewood\nCliffs, NJ: Prentice-Hall. </li>\n<li> Clark, A., 2003, <i>Natural-Born Cyborgs</i>, Oxford, UK: Oxford\nUniversity Press. </li>\n<li> Clark, M. H., 2010, <i>Cognitive Illusions and the Lying Machine:\nA Blueprint for Sophistic Mendacity</i>, PhD dissertation, Rensselaer\nPolytechnic Institute (Cognitive Science). </li>\n<li> Copeland, B. J., 1998, \u201cSuper Turing Machines,\u201d\n<i>Complexity</i>, 4: 30\u201332. </li>\n<li> Copi, I. &amp; Cohen, C., 2004, <i>Introduction to Logic</i>,\nSaddle River, NJ: Prentice-Hall. </li>\n<li> Dennett, D., 1998, \u201cArtificial Life as Philosophy,\u201d\nin his <i>Brainchildren: Essays on Designing Minds</i>, Cambridge, MA:\nMIT Press, pp. 261\u2013263. </li>\n<li> Dennett, D., 1994, \u201cThe Practical Requirements for Making a\nConscious Robot,\u201d <i>Philosophical Transactions of the Royal\nSociety of London</i>, 349: 133\u2013146. </li>\n<li> Dennett, D., 1979, \u201cArtificial Intelligence as Philosophy\nand as Psychology,\u201d <i>Philosophical Perspectives in Artificial\nIntelligence</i>, M. Ringle, ed., Atlantic Highlands, NJ: Humanities\nPress, pp. 57\u201380. </li>\n<li> Descartes, 1637, R., in Haldane, E. and Ross, G.R.T.,\ntranslators, 1911, <i>The Philosophical Works of Descartes, Volume\n1,</i> Cambridge, UK: Cambridge University Press. </li>\n<li> Dick, P. K., 1968, <i>Do Androids Dream of Electric Sheep?</i>,\nNew York, NY: Doubleday. </li>\n<li> Domingos, P., 2012, \u201cA Few Useful Things to Know about\nMachine Learning,\u201d <i>Communications of the ACM</i>, 55.10:\n78\u201387. </li>\n<li> Dreyfus, H., 1972, <i>What Computers Can\u2019t Do</i>,\nCambridge, MA: MIT Press. </li>\n<li> Dreyfus, H., 1992, <i>What Computers Still Can\u2019t Do</i>,\nCambridge, MA: MIT Press. </li>\n<li> Dreyfus, H. &amp; Dreyfus, S., 1987, <i>Mind Over Machine: The\nPower of Human Intuition and Expertise in the Era of the Computer</i>,\nNew York, NY: Free Press. </li>\n<li> Ebbinghaus, H., Flum, J. &amp; Thomas, W., 1984, <i>Mathematical\nLogic</i>, New York, NY: Springer-Verlag. </li>\n<li> Eden, A., Moor, J., Soraker, J. &amp; Steinhart, E., 2013,\n<i>Singularity Hypotheses: A Scientific and Philosophical\nAssessment</i>, New York, NY: Springer. </li>\n<li> Espa\u00f1a-Bonet, C., Enache, R., Slaski, A., Ranta, A.,\nM\u00e0rquez L. &amp; Gonz\u00e0lez, M., 2011, \u201cPatent\nTranslation within the MOLTO project,\u201d in <i> Proceedings of the\n4th Workshop on Patent Translation, MT Summit XIII</i>, pp.\n70\u201378. </li>\n<li> Evans, G., 1968, \u201cA Program for the Solution of a Class of\nGeometric-Analogy Intelligence-Test Questions,\u201d in M. Minsky,\ned., <i>Semantic Information Processing</i>, Cambridge, MA: MIT Press,\npp. 271\u2013353. </li>\n<li> Fagin, R., Halpern, J. Y., Moses, Y. &amp; Vardi, M., 2004,\n<i>Reasoning About Knowledge</i>, Cambridge, MA: MIT Press. </li>\n<li> Ferrucci, D. &amp; Lally, A., 2004, \u201cUIMA: An Architectural\nApproach to Unstructured Information Processing in the Corporate\nResearch Environment,\u201d <i>Natural Language Engineering</i>,\n10.3\u20134: 327\u2013348. Cambridge, UK: Cambridge University\nPress. </li>\n<li> Ferrucci, D., Brown, E., Chu-Carroll, J., Fan, J., Gondek, D.,\nKalyanpur, A., Lally, A., Murdock, J., Nyberg, E., Prager, J.,\nSchlaefer, N. &amp; Welty, C., 2010, \u201cBuilding Watson: An\nOverview of the DeepQA Project,\u201d <i>AI Magazine</i>, 31.3:\n59\u201379. </li>\n<li> Finnsson, H., 2012, \u201cGeneralized Monte-Carlo Tree Search\nExtensions for General Game Playing,\u201d in <i>Proceedings of the\nTwenty-Sixth AAAI Conference on Artificial Intelligence\n(AAAI-2012)</i>, Toronto, Canda, pp. 1550\u20131556. </li>\n<li> Fitelson, B., 2005, \u201cInductive Logic,\u201d in Pfeifer, J.\nand Sarkar, S., eds., <i>Philosophy of Science: An Encyclopedia</i>,\nLondon, UK: Routledge, pp. 384\u2013394. </li>\n<li> Floridi, L., 2015, \u201cSingularitarians, AItheists, and Why\nthe Problem with Artificial Intelligence is H.A.L. (Humanity At\nLarge), not HAL,\u201d <i>APA Newsletter: Philosophy and\nComputers</i>, 14.2: 8\u201311. </li>\n<li> Foot, P., 1967, \u201cThe Problem of Abortion and the Doctrine\nof the Double Effect,\u201d <i> Oxford Review</i>, 5:\n5\u201315.</li>\n<li> Forbus, K. D. &amp; Hinrichs, T. R., 2006, \u201cCompanion\nCognitive Systems: A Step toward Human-Level AI,\u201d <i>AI\nMagazine,</i> 27.2: 83. </li>\n<li> Ford, K. M., Glymour C. &amp; Hayes P., 1997, \u201cOn the Other\nHand \u2026 Cognitive Prostheses,\u201d <i>AI Magazine,</i> 18.3:\n104. </li>\n<li> Friedland, N., Allen, P., Matthews, G., Witbrock, M., Baxter, D.,\nCurtis, J., Shepard, B., Miraglia, P., Angele, J., Staab, S., Moench,\nE., Oppermann, H., Wenke, D., Israel, D., Chaudhri, V., Porter, B.,\nBarker, K., Fan, J., Yi Chaw, S., Yeh, P., Tecuci, D. &amp; Clark, P.,\n2004, \u201cProject Halo: Towards a Digital Aristotle,\u201d <i>AI\nMagazine</i>, 25.4: 29\u201347. </li>\n<li> Genesereth, M., Love, N. &amp; Pell B., 2005, \u201cGeneral Game\nPlaying: Overview of the AAAI Competition,\u201d <i>AI Magazine</i>,\n26.2: 62\u201372. \n [<a href=\"https://www.aaai.org/ocs/index.php/IJCAI/IJCAI-09/paper/viewFile/566/775\" target=\"other\">Available online</a>]\n </li>\n<li> Ginsberg, M., 1993, <i>Essentials of Artificial Intelligence</i>,\nNew York, NY: Morgan Kaufmann. </li>\n<li> Glymour, G., 1992, <i>Thinking Things Through</i>, Cambridge, MA:\nMIT Press. </li>\n<li> Goertzel, B. &amp; Pennachin, C., eds., 2007, <i>Artificial\nGeneral Intelligence</i>, Berlin, Heidelberg: Springer-Verlag. </li>\n<li> Gold, M., 1965, \u201cLimiting Recursion,\u201d <i>Journal of\nSymbolic Logic</i>, 30.1: 28\u201347. </li>\n<li> Goldstine, H. &amp; von Neumann, J., 1947, \u201cPlanning and\nCoding of Problems for an Electronic Computing Instrument,\u201d\n<i>IAS Reports</i> Institute for Advanced Study, Princeton, NJ. [This\nremarkable work is \n <a href=\"https://library.ias.edu/files/pdfs/ecp/planningcodingof0103inst.pdf\">available online</a>\nfrom the Institute for Advanced Study. Please note that this paper is\nPart II of a three-volume set. The first volume was devoted to a\npreliminary discussion, and the first author on it was Arthur Burks,\njoining Goldstine and von Neumann.]\n </li>\n<li> Good, I., 1965, \u201cSpeculations Concerning the First\nUltraintelligent Machines,\u201d in <i>Advances in Computing</i>\n(vol. 6), F. Alt and M. Rubinoff, eds., New York, NY: Academic Press,\npp. 31\u201338. </li>\n<li> Govindarajulu, N. S., Bringsjord, S. &amp; Licato J., 2013,\n\u201cOn Deep Computational Formalization of Natural Language,\u201d\nin <i>Proceedings of the Workshop \u201cFormalizing Mechanisms for\nArtificial General Intelligence and Cognition (Formal\nMAGiC),\u201d</i> Osnabr\u00fcck, Germany: PICS. </li>\n<li> Govindarajulu, N. S., &amp; Bringsjord, S., 2015, \u201cEthical\nRegulation of Robots Must Be Embedded in Their Operating\nSystems\u201d in Trappl, R., ed., <i> A Construction Manual for\nRobot\u2019s Ethical Systems: Requirements, Methods,\nImplementations</i>, Berlin, DE: Springer. </li>\n<li> Govindarajulu, N. S., &amp; Bringsjord, S., 2017, \u201cOn\nAutomating the Doctrine of Double Effect,\u201d in <i>Proceedings of\nthe Twenty-Sixth International Joint Conference on Artificial\nIntelligence (IJCAI-17)</i>, pp. 4722\u20134730. doi:10.24963/ijcai.2017/658\n </li>\n<li> Granger, R., 2004a, \u201cDerivation and Analysis of Basic\nComputational Operations of Thalamocortical Circuits,\u201d\n<i>Journal of Cognitive Neuroscience</i> 16: 856\u2013877. </li>\n<li> Granger, R., 2004b, \u201cBrain Circuit Implementation:\nHigh-precision Computation from Low-Precision Components,\u201d in\n<i>Toward Replacement Parts for the Brain</i>, T. Berger and D.\nGlanzman, eds., Cambridge, MA: MIT Press, pp. 277\u2013294. </li>\n<li> Griewank, A., 2000, <i>Evaluating Derivatives: Principles and\nTechniques of Algorithmic Differentiation</i>, Philadlphia, PA:\nSociety for Industrial and Applied Mathematics (SIAM). </li>\n<li> Guizzo, E., 2011, \u201cHow Google\u2019s Self-driving Car\nWorks,\u201d <i>IEEE Spectrum Online</i>.\n [<a href=\"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/how-google-self-driving-car-works\" target=\"other\">Available online]</a></li>\n<li> Hailperin, T., 1996, <i> Sentential Probability Logic: Origins,\nDevelopment, Current Status, and Technical Applications,</i>\nBethlehem, United States: Lehigh University Press. </li>\n<li> Hailperin, T., 2010, <i> Logic with a Probability Semantics,</i>\nBethlehem, United States: Lehigh University Press. </li>\n<li> Halpern, J. Y., 1990, \u201cAn Analysis of First-order Logics of\nProbability,\u201d <i>Artificial Intelligence,</i> 46: 311\u2013350.\n</li>\n<li> Halpern, J., Harper, R., Immerman, N., Kolaitis, P. G., Vardi, M.\n&amp; Vianu, V., 2001, \u201cOn the Unusual Effectiveness of Logic in\nComputer Science,\u201d <i>The Bulletin of Symbolic Logic</i>, 7.2:\n213\u2013236. </li>\n<li> Hamkins, J. &amp; Lewis, A., 2000, \u201cInfinite Time Turing\nMachines,\u201d <i>Journal of Symbolic Logic</i>, 65.2:\n567\u2013604. </li>\n<li> Harnad, S., 1991, \u201cOther Bodies, Other Minds: A Machine\nIncarnation of an Old Philosophical Problem,\u201d <i>Minds and\nMachines</i>, 1.1: 43\u201354. </li>\n<li> Haugeland, J., 1985, <i>Artificial Intelligence: The Very\nIdea</i>, Cambridge, MA: MIT Press. </li>\n<li> Hendler, J. &amp; Jennifer G., 2008, \u201cMetcalfe\u2019s Law,\nWeb 2.0, and the Semantic Web,\u201d <i>Web Semantics: Science,\nServices and Agents on the World Wide Web,</i> 6.1: 14\u201320. </li>\n<li> Hinton, G., Deng, L., Yu, D., Dahl, G.E., Mohamed, A. R., Jaitly,\nN., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T. &amp;\nKingsbury, B., 2012, \u201cDeep Neural Networks for Acoustic Modeling\nin Speech Recognition: The Shared Views of Four Research\nGroups,\u201d <i>IEEE Signal Processing Magazine</i>, 29.6:\n82\u201397. </li>\n<li> Hoffman, R. R., Hayes, P. J. &amp; Ford, K. M., 2001,\n\u201cHuman-Centered Computing: Thinking In and Out of the\nBox,\u201d <i>IEEE Intelligent Systems</i>, 16.5: 76\u201378. </li>\n<li> Hoffman, R. R., Bradshaw J. M., Hayes P. J. &amp; Ford K. M.,\n2003, \u201c The Borg Hypothesis,\u201d<i> IEEE Intelligent\nSystems</i>, 18.5: 73\u201375. </li>\n<li> Hofstadter, D. &amp; McGraw, G., 1995, \u201cLetter Spirit:\nEsthetic Perception and Creative Play in the Rich Microcosm of the\nRoman Alphabet,\u201d in Hofstadter\u2019s <i>Fluid Concepts and\nCreative Analogies: Computer Models of the Fundamental Mechanisms of\nThought</i>, New York, NY: Basic Books, pp. 407\u2013488. </li>\n<li> Hornik, K., Stinchcombe, M. &amp; White, H., 1989,\n\u201cMultilayer Feedforward Networks are Universal\nApproximators,\u201d <i>Neural Networks</i>, 2.5: 359\u2013366.\n</li>\n<li> Hutter, M., 2005, <i>Universal Artificial Intelligence</i>,\nBerlin: Springer. </li>\n<li> Joy, W., 2000, \u201cWhy the Future Doesn\u2019t Need\nUs,\u201d <i>Wired</i> 8.4. \n [<a href=\"https://www.wired.com/2000/04/joy-2/\" target=\"other\">Available online</a>]\n </li>\n<li> Kahneman, D., 2013. <i>Thinking, Fast and Slow</i>, New York, NY:\nFarrar, Straus, and Giroux. </li>\n<li> Kaufmann, M., Manolios, P. &amp; Moore, J. S., 2000,\n<i>Computer-Aided Reasoning: ACL2 Case Studies</i>, Dordrecht, The\nNetherlands: Kluwer Academic Publishers. </li>\n<li> Klenk, M., Forbus, K., Tomai, E., Kim,H. &amp; Kyckelhahn, B.,\n2005, \u201cSolving Everyday Physical Reasoning Problems by Analogy\nusing Sketches,\u201d in <i>Proceedings of 20th National Conference\non Artificial Intelligence</i> (AAAI-05), Pittsburgh, PA. </li>\n<li> Kolata, G., 1996, \u201cComputer Math Proof Shows Reasoning\nPower,\u201d in <i>New York Times</i>. \n [<a href=\"http://www.nytimes.com/library/cyber/week/1210math.html\" target=\"other\">Availabe online</a>]\n </li>\n<li> Koller, D., Levy, A. &amp; Pfeffer, A., 1997, \u201cP-CLASSIC: A\nTractable Probablistic Description Logic,\u201d in <i>Proceedings of\nthe AAAI 1997 Meeting</i>, 390\u2013397. </li>\n<li> Kurzweil, R., 2006, <i>The Singularity Is Near: When Humans\nTranscend Biology</i>, New York, NY: Penguin USA. </li>\n<li> Kurzweil, R., 2000, <i>The Age of Spiritual Machines: When\nComputers Exceed Human Intelligence</i>, New York, NY: Penguin USA.\n</li>\n<li> LaForte, G., Hayes P. &amp; Ford, K., 1998, \u201cWhy\nG\u00f6del\u2019s Theorem Cannot Refute Computationslism,\u201d\n<i>Artificial Intelligence</i>, 104: 265\u2013286. </li>\n<li> Laird, J. E., 2012, <i>The Soar Cognitive Architecture</i>,\nCambridge, MA: MIT Press. </li>\n<li> Laird, J. &amp; VanLent M., 2001, \u201cHuman-level AI\u2019s\nKiller Application: Interactive Computer Games,\u201d <i>AI\nMagazine</i> 22.2:15\u201326. </li>\n<li> LeCun, Y., Bengio, Y. &amp; Hinton G., 2015, \u201cDeep\nLearning,\u201d <i>Nature</i>, 521: 436\u2013444.</li>\n<li> Lenat, D., 1983, \u201cEURISKO: A Program that Learns New Heuristics and Domain Concepts,\u201d <em>Artificial Intelligence</em>, 21(1-2): 61\u201398. doi:10.1016/s0004-3702(83)80005-8</li>\n<li> Lenat, D., &amp; Guha, R. V., 1990, <em>Building Large\nKnowledge-Based Systems: Representation and Inference in the Cyc\nProject</em>, Reading, MA: Addison Wesley.</li>\n<li> Lenzen, W., 2004, \u201cLeibniz\u2019s Logic,\u201d in Gabbay,\nD., Woods, J. and Kanamori, A., eds., <i>Handbook of the History of\nLogic</i>, Elsevier, Amsterdam, The Netherlands, pp. 1\u201383. </li>\n<li> Lewis, H. &amp; Papadimitriou, C., 1981, <i>Elements of the\nTheory of Computation</i>, Prentice Hall, Englewood Cliffs, NJ:\nPrentice Hall. </li>\n<li> Litt, A., Eliasmith, C., Kroon, F., Weinstein, S. &amp; Thagard,\nP., 2006, \u201cIs the Brain a Quantum Computer?\u201d <i>Cognitive\nScience</i> 30: 593\u2013603. </li>\n<li>Lucas, J. R., 1964, \u201cMinds, Machines, and G\u00f6del,\u201d\nin <i>Minds and Machines</i>, A. R. Anderson, ed., Prentice-Hall, NJ:\nPrentice-Hall, pp. 43\u201359. </li>\n<li> Luger, G., 2008, <i>Artificial Intelligence: Structures and\nStrategies for Complex Problem Solving</i>, New York, NY: Pearson.\n</li>\n<li> Luger, G. &amp; Stubblefield, W., 1993, <i>Artificial\nIntelligence: Structures and Strategies for Complex Problem\nSolving</i>, Redwood, CA: Benjamin Cummings. </li>\n<li> Lopez, A., 2008, \u201cStatistical Machine Translation,\u201d\n<i>ACM Computing Surveys</i>, 40.3: 1\u201349. </li>\n<li> Malle, B. F., Scheutz, M., Arnold, T., Voiklis, J. &amp;\nCusimano, C., 2015, \u201cSacrifice One For the Good of Many?: People\nApply Different Moral Norms to Human and Robot Agents,\u201d in\n<i>Proceedings of the Tenth Annual ACM/IEEE International Conference\non Human-Robot Interaction (HRI \u201915)</i> (New York, NY: ACM),\npp. 117\u2013124. </li>\n<li> Manzano, M., 1996, <i>Extensions of First Order Logic</i>,\nCambridge, UK: Cambridge University Press. </li>\n<li> Marcus, G., 2013, \u201cWhy Can\u2019t My Computer Understand\nMe?,\u201d in <i>The New Yorker</i>, August 2013.\n [<a href=\"http://www.newyorker.com/online/blogs/elements/2013/08/why-cant-my-computer-understand-me.html\" target=\"other\">Available online</a>]\n </li>\n<li> McCarthy, J. &amp; Hayes, P., 1969, \u201cSome Philosophical\nProblems from the Standpoint of Artificial Intelligence,\u201d in\n<i>Machine Intelligence 4</i>, B. Meltzer and D. Michie, eds.,\nEdinburgh: Edinburgh University Press, 463\u2013502. </li>\n<li> Mueller, E., 2006, <i>Commonsense Reasoning</i>, San Francisco,\nCA: Morgan Kaufmann. </li>\n<li> Murphy, K. P., 2012, <i>Machine Learning: A Probabilistic\nPerspective</i>, Cambridge, MA: MIT Press. </li>\n<li> Minsky, M. &amp; Pappert, S., 1969, <i>Perceptrons: An\nIntroduction to Computational Geometry</i>, Cambridge, MA: MIT Press.\n</li>\n<li> Montague, R., 1970, \u201cUniversal Grammar,\u201d\n<i>Theoria,</i> 36, 373\u2013398.</li>\n<li> Moor, J., 2006, \u201cThe Nature, Importance, and Difficulty of\nMachine Ethics\u201d, <i>IEEE Intelligent Systems</i> 21.4:\n18\u201321. </li>\n<li> Moor, J., 1985, \u201cWhat is Computer Ethics?\u201d\n<i>Metaphilosophy</i> 16.4: 266\u2013274. </li>\n<li> Moor, J., ed., 2003, <i>The Turing Test: The Elusive Standard of\nArtificial Intelligence</i>, Dordrecht, The Netherlands: Kluwer\nAcademic Publishers. </li>\n<li> Moravec, H., 1999, <i>Robot: Mere Machine to Transcendant\nMind</i>, Oxford, UK: Oxford University Press, </li>\n<li> Naumowicz, A. &amp; Kornilowicz., A., 2009, \u201cA Brief\nOverview of Mizar,\u201d in <i>Theorem Proving in Higher Order\nLogics,</i> S. Berghofer, T. Nipkow, C. Urban &amp; M. Wenzel, eds.,\nBerlin: Springer, pp. 67\u201372. </li>\n<li> Newell, N., 1973, \u201cYou Can\u2019t Play 20 Questions with\nNature and Win: Projective Comments on the Papers of this\nSymposium\u201d, in <i>Visual Information Processing</i>, W. Chase,\ned., New York, NY: Academic Press, pp. 283\u2013308. </li>\n<li> Nilsson, N., 1998, <i>Artificial Intelligence: A New\nSynthesis</i>, San Francisco, CA: Morgan Kaufmann. </li>\n<li> Nilsson, N., 1987, <i>Principles of Artificial Intelligence</i>,\nNew York, NY: Springer-Verlag. </li>\n<li> Nilsson, N., 1991, \u201cLogic and Artificial\nIntelligence,\u201d <i>Artificial Intelligence</i>, 47: 31\u201356.\n</li>\n<li> Nozick, R., 1970, \u201cNewcomb\u2019s Problem and Two\nPrinciples of Choice,\u201d in <i>Essays in Honor of Carl G.\nHempel</i>, N. Rescher, ed., Highlands, NJ: Humanities Press, pp.\n114\u2013146. This appears to be the very first published treatment\nof NP \u2013 though the paradox goes back to its creator: William\nNewcomb, a physicist. </li>\n<li> Osherson, D., Stob, M. &amp; Weinstein, S., 1986, <i>Systems That\nLearn</i>, Cambridge, MA: MIT Press. </li>\n<li> Pearl, J., 1988, <i>Probabilistic Reasoning in Intelligent\nSystems</i>, San Mateo, CA: Morgan Kaufmann. </li>\n<li> Pennington, J., Socher R., &amp; Manning C. D., 2014,\n\u201cGloVe: Global Vectors for Word Representation,\u201d in\n<i>Proceedings of the 2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP 2014)</i>, pp. 1532\u20131543. \n [<a href=\"http://www.aclweb.org/anthology/D14-1162\" target=\"other\">Available online</a>]\n </li>\n<li> Penrose, R., 1989, <i>The Emperor\u2019s New Mind</i>, Oxford,\nUK: Oxford University Press. </li>\n<li> Penrose, R., 1994, <i>Shadows of the Mind</i>, Oxford, UK: Oxford\nUniversity Press. </li>\n<li> Penrose, R., 1996, \u201cBeyond the Doubting of a Shadow: A\nReply to Commentaries on <i>Shadows of the Mind,</i>\u201d\n<i>Psyche</i>, 2.3. This paper is available\n <a href=\"http://www.calculemus.org/MathUniversalis/NS/10/01penrose.html\" target=\"other\"> online.</a>\n</li>\n<li>Pereira, L., &amp; Saptawijaya A., 2016, <i>Programming Machine\nEthics,</i> Berlin, Germany: Springer</li>\n<li> Pinker, S., 1997, <i>How the Mind Works,</i> New York, NY:\nNorton. </li>\n<li> Pollock, J., 2006, <i>Thinking about Acting: Logical Foundations\nfor Rational Decision Making,</i> Oxford, UK: Oxford University Press.\n</li>\n<li> Pollock, J., 2001, \u201cDefeasible Reasoning with Variable\nDegrees of Justification,\u201d <i>Artificial Intelligence</i>, 133,\n233\u2013282. </li>\n<li> Pollock, J., 1995, <i>Cognitive Carpentry: A Blueprint for How to\nBuild a Person</i>, Cambridge, MA: MIT Press. </li>\n<li> Pollock, J., 1992, \u201cHow to Reason Defeasibly,\u201d\n<i>Artificial Intelligence</i>, 57, 1\u201342. </li>\n<li> Pollock, J., 1989, <i>How to Build a Person: A Prolegomenon</i>,\nCambridge, MA: MIT Press. </li>\n<li> Pollock, J., 1974, <i>Knowledge and Justification</i>, Princeton,\nNJ: Princeton University Press. </li>\n<li> Pollock, J., 1967, \u201cCriteria and our Knowledge of the\nMaterial World,\u201d <i>Philosophical Review</i>, 76, 28\u201360.\n</li>\n<li> Pollock, J., 1965, <i>Analyticity and Implication,</i> PhD\ndissertation, University of California at Berkeley (Philosophy). </li>\n<li> Potter, M.D., 2004, <i> Set Theory and its Philosophy</i>,\nOxford, UK: Oxford University Press </li>\n<li> Preston, J. &amp; Bishop, M., 2002, <i>Views into the Chinese\nRoom: New Essays on Searle and Artificial Intelligence</i>, Oxford,\nUK: Oxford University Press. </li>\n<li> Putnam, H., 1965, \u201cTrial and Error Predicates and a\nSolution to a Problem of Mostowski,\u201d <i>Journal of Symbolic\nLogic,</i> 30.1, 49\u201357. </li>\n<li> Putnam, H., 1963, \u201cDegree of Confirmation and Inductive\nLogic,\u201d in <i>The Philosophy of Rudolf Carnap</i>, Schilipp, P.,\ned., Open Court, pp. 270\u2013292. </li>\n<li> Rajat, R., Anand, M. &amp; Ng, A. Y., 2009, \u201cLarge-scale\nDeep Unsupervised Learning Using Graphics Processors,\u201d in\n<i>Proceedings of the 26th Annual International Conference on Machine\nLearning</i>, ACM, pp. 873\u2013880. </li>\n<li> Rapaport, W., 1988, \u201cSyntactic Semantics: Foundations of\nComputational Natural-Language Understanding,\u201d in <i>Aspects of\nArtificial Intelligence</i>, J. H. Fetzer ed., Dordrecht, The\nNetherlands: Kluwer Academic Publishers, 81\u2013131. </li>\n<li> Rapaport, W. &amp; Shapiro, S., 1999, \u201cCognition and\nFiction: An Introduction,\u201d <i> Understanding Language\nUnderstanding: Computational Models of Reading</i>, A. Ram &amp; K.\nMoorman, eds., Cambridge, MA: MIT Press, 11\u201325. \n [<a href=\"https://cse.buffalo.edu/~rapaport/Papers/fiction.ashwin.pdf\" target=\"other\">Available online</a>]\n </li>\n<li> Reeke, G. &amp; Edelman, G., 1988, \u201cReal Brains and\nArtificial Intelligence,\u201d in <i>The Artificial Intelligence\nDebate: False Starts, Real Foundations</i>, Cambridge, MA: MIT Press,\npp. 143\u2013173. </li>\n<li> Richardson, M. &amp; Domingos, P., 2006, \u201cMarkov Logic\nNetworks,\u201d <i>Machine Learning,</i> 62.1\u20132:107\u2013136.\n</li>\n<li> Robertson, G. &amp; Watson, I., 2015, \u201cA Review of\nReal-Time Strategy Game AI,\u201d <i>AI Magazine</i>, 35.4:\n75\u2013104. </li>\n<li> Rosenschein, S. &amp; Kaelbling, L., 1986, \u201cThe Synthesis\nof Machines with Provable Epistemic Properties,\u201d in\n<i>Proceedings of the 1986 Conference on Theoretical Aspects of\nReasoning About Knowledge</i>, San Mateo, CA: Morgan Kaufmann, pp.\n83\u201398. </li>\n<li> Rumelhart, D. &amp; McClelland, J., 1986, eds., <i>Parallel\nDistributed Processing</i>, Cambridge, MA: MIT Press. </li>\n<li> Russell, S., 1997, \u201cRationality and Intelligence,\u201d\n<i>Artificial Intelligence</i>, 94: 57\u201377. \n [<a href=\"https://people.eecs.berkeley.edu/~russell/papers/aij-cnt.pdf\" target=\"other\">Version available online from author</a>]\n </li>\n<li> Russell, S. &amp; Norvig, P., 1995, <i>Artificial Intelligence: A\nModern Approach</i>, Saddle River, NJ: Prentice Hall. </li>\n<li> Russell, S. &amp; Norvig, P., 2002, <i>Artificial Intelligence: A\nModern Approach 2nd edition</i>, Saddle River, NJ: Prentice Hall.\n</li>\n<li> Russell, S. &amp; Norvig, P., 2009, <i>Artificial Intelligence: A\nModern Approach 3rd edition</i>, Saddle River, NJ: Prentice Hall.\n</li>\n<li>Rychtyckyj, N. &amp; Plesco, C., 2012, \u201cApplying Automated\nLanguage Translation at a Global Enterprise Level,\u201d <i>AI\nMagazine</i>, 34.1: 43\u201354. </li>\n<li> Scanlon, T. M., 1982, \u201cContractualism and\nUtilitarianism,\u201d in A. Sen and B. Williams, eds., <i>\nUtilitarianism and Beyond,</i> Cambridge: Cambridge University Press,\npp. 103\u2013128. </li>\n<li> Schank, R., 1972, \u201cConceptual Dependency: A Theory of\nNatural Language Understanding,\u201d <i>Cognitive Psychology</i>,\n3.4: 532\u2013631. </li>\n<li> Schaul, T. &amp; Schmidh\u00fcber, J., 2010,\n\u201cMetalearning,\u201d <i>Scholarpedia</i> 5(6): 4650. URL:\n <a href=\"http://www.scholarpedia.org/article/Metalearning\" target=\"other\">http://www.scholarpedia.org/article/Metalearning</a>\n</li>\n<li> Schmidh\u00fcber, J., 2009, \u201cUltimate Cognition \u00e0 la\nG\u00f6del,\u201d <i>Cognitive Computation</i> 1.2: 177\u2013193.\n</li>\n<li> Searle, J., 1997, <i>The Mystery of Consciousness</i>, New York,\nNY: New York Review of Books. </li>\n<li> Searle, J., 1980, \u201cMinds, Brains and Programs,\u201d\n<i>Behavioral and Brain Sciences</i>, 3: 417\u2013424. \n </li>\n<li> Searle, J., 1984, <i>Minds, Brains and Science,</i> Cambridge,\nMA: Harvard University Press. The Chinese Room Argument is covered in\nChapter Two, \u201cCan Computers Think?\u201d. </li>\n<li> Searle, J., 2014, \u201cWhat Your Computer Can\u2019t\nKnow,\u201d <i>New York Review of Books</i>, October 9. </li>\n<li> Shapiro, S., 2000, \u201cAn Introduction to SNePS 3,\u201d in\n<i>Conceptual Structures: Logical, Linguistic, and Computational\nIssues. Lecture Notes in Artificial Intelligence 1867</i>, B. Ganter\n&amp; G. W. Mineau, eds., Springer-Verlag, 510\u2013524. </li>\n<li> Shapiro, S., 2003, \u201cMechanism, Truth, and Penrose\u2019s\nNew Argument,\u201d <i>Journal of Philosophical Logic</i>, 32.1:\n19\u201342. </li>\n<li> Siegelmann, H., 1999, <i>Neural Networks and Analog Computation:\nBeyond the Turing Limit</i>, Boston, MA: Birkhauser. </li>\n<li> Siegelmann, H. &amp; and Sontag, E., 1994, \u201cAnalog\nComputation Via Neural Nets,\u201d <i>Theoretical Computer\nScience</i>, 131: 331\u2013360. </li>\n<li>Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van\nden Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam,\nV., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N.,\nSutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel T.\n&amp; Hassabis D., 2016, \u201cMastering the Game of Go with Deep\nNeural Networks and Tree Search,\u201d<i> Nature</i>, 529:\n484\u2013489.\n </li>\n<li> Shin, S-J, 2002, <i>The Iconic Logic of Peirce\u2019s\nGraphs,</i> Cambridge, MA: MIT Press. </li>\n<li> Smolensky, P., 1988, \u201cOn the Proper Treatment of\nConnectionism,\u201d <i>Behavioral &amp; Brain Sciences</i>, 11:\n1\u201322. </li>\n<li> Somers, J., 2013, \u201cThe Man Who Would Teach Machines to\nThink,\u201d in <i>The Atlantic</i>. \n [<a href=\"http://theatlantic.com/magazine/archive/2013/11/the-man-who-would-teach-machines-to-think/309529/\" target=\"other\">Available online</a>]\n </li>\n<li> Stanovich, K. &amp; West, R., 2000, \u201cIndividual Differences\nin Reasoning: Implications for the Rationality Debate,\u201d\n<i>Behavioral and Brain Sciences</i>, 23.5: 645\u2013665. </li>\n<li> Strzalkowski, T. &amp; Harabagiu, M. S., 2006, eds., <i> Advances\nin Open Domain Question Answering</i>; in the series Text, Speech and\nLanguage Technology, volume 32, Dordrecht, The Netherlands:\nSpringer-Verlag. </li>\n<li> Sun, R., 2002, <i>Duality of the Mind: A Bottom Up Approach\nToward Cognition</i>, Mahwah, NJ: Lawrence Erlbaum. </li>\n<li> Sun, R., 1994, <i>Integrating Rules and Connectionism for Robust\nCommonsense Reasoning</i>, New York, NY: John Wiley and Sons. </li>\n<li> Sutton R. S. &amp; Barto A. G., 1998, <i>Reinforcement Learning:\nAn Introduction</i>, Cambridge, MA: MIT Press. </li>\n<li> Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D.,\nGoodfellow, I. &amp; Fergus, R., 2014, \u201cIntriguing Properties of\nNeural Networks,\u201d in <i> Second International Conference on\nLearning Representations</i>, Banff, Canada. \n [<a href=\"https://arxiv.org/pdf/1312.6199.pdf\" target=\"other\">Available online</a>]</li>\n<li> Hastie, T., Tibshirani, R., &amp; Jerome, F., 2009, <i>The\nElements of Statistical Learning</i>, in the series <i>Springer Series\nin Statistics</i>, New York: Springer. </li>\n<li> Turing, A., 1950, \u201cComputing Machinery and\nIntelligence,\u201d <i>Mind</i>, LIX: 433\u2013460. </li>\n<li> Turing, A., 1936, \u201cOn Computable Numbers with Applications\nto the Entscheidung-Problem,\u201d <i>Proceedings of the London\nMathematical Society</i>, 42: 230\u2013265. </li>\n<li> Vilalta, R. &amp; Drissi, Y., 2002, \u201cA Perspective View and\nSurvey of Meta-learning,\u201d <i> Artificial Intelligence\nReview</i>, 18.2:77\u201395. </li>\n<li> Voronkov, A., 1995, \u201cThe Anatomy of Vampire: Implementing\nBottom-Up Procedures with Code Trees,\u201d <i>Journal of Automated\nReasoning</i>, 15.2. </li>\n<li> Wallach, W. &amp; Allen, C., 2010, <i>Moral Machines: Teaching\nRobots Right from Wrong,</i> Oxford, UK: Oxford University Press.\n</li>\n<li> Wermter, S. &amp; Sun, R., 2001 (Spring), \u201cThe Present and\nthe Future of Hybrid Neural Symbolic Systems: Some Reflections from\nthe Neural Information Processing Systems Workshop,\u201d <i>AI\nMagazine</i>, 22.1: 123\u2013125. </li>\n<li> Suppes, P., 1972, <i>Axiomatic Set Theory</i>, New York, NY:\nDover. </li>\n<li> Whiteson, S. &amp; Whiteson, D., 2009, \u201cMachine Learning\nfor Event Selection in High Energy Physics,\u201d <i> Engineering\nApplications of Artificial Intelligence</i> 22.8: 1203\u20131217.\n</li>\n<li> Williams, D. E., Hinton G. E., &amp; Williams R. J., 1986\n\u201cLearning Representations by Back-propagating Errors,\u201d\n<i>Nature</i>, 323.10: 533\u2013536. </li>\n<li> Winston, P., 1992, <i>Artificial Intelligence</i>, Reading, MA:\nAddison-Wesley. </li>\n<li> Wos, L., Overbeek, R., Lusk R. &amp; Boyle, J., 1992,\n<i>Automated Reasoning: Introduction and Applications (2nd\nedition)</i>, New York, NY: McGraw-Hill. </li>\n<li> Wos, L., 2013, \u201cThe Legacy of a Great Researcher,\u201d in\n<i>Automated Reasoning and Mathematics: Essays in Memory of William\nMcCune</i>, Bonacina, M.P. &amp; Stickel, M.E., eds., 1\u201314.\nBerlin: Springer. </li>\n<li> Zalta, E., 1988, <i>Intensional Logic and the Metaphysics of\nIntentionality</i>, Cambridge, MA: Bradford Books. </li>\n</ul>\n</div>"
    },
    "related_entries": {
        "entry_list": [
            "artificial intelligence: logic and",
            "causation: probabilistic",
            "Chinese room argument",
            "cognitive science",
            "computability and complexity",
            "computing: modern history of",
            "connectionism",
            "epistemology: Bayesian",
            "frame problem",
            "information technology: and moral values",
            "language of thought hypothesis",
            "learning theory, formal",
            "linguistics: computational",
            "mind: computational theory of",
            "reasoning: automated",
            "reasoning: defeasible",
            "statistics, philosophy of",
            "Turing test"
        ],
        "entry_link": [
            {
                "../logic-ai/": "artificial intelligence: logic and"
            },
            {
                "../causation-probabilistic/": "causation: probabilistic"
            },
            {
                "../chinese-room/": "Chinese room argument"
            },
            {
                "../cognitive-science/": "cognitive science"
            },
            {
                "../computability/": "computability and complexity"
            },
            {
                "../computing-history/": "computing: modern history of"
            },
            {
                "../connectionism/": "connectionism"
            },
            {
                "../epistemology-bayesian/": "epistemology: Bayesian"
            },
            {
                "../frame-problem/": "frame problem"
            },
            {
                "../it-moral-values/": "information technology: and moral values"
            },
            {
                "../language-thought/": "language of thought hypothesis"
            },
            {
                "../learning-formal/": "learning theory, formal"
            },
            {
                "../computational-linguistics/": "linguistics: computational"
            },
            {
                "../computational-mind/": "mind: computational theory of"
            },
            {
                "../reasoning-automated/": "reasoning: automated"
            },
            {
                "../reasoning-defeasible/": "reasoning: defeasible"
            },
            {
                "../statistics/": "statistics, philosophy of"
            },
            {
                "../turing-test/": "Turing test"
            }
        ]
    },
    "academic_tools": {
        "listed_text": [
            "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=artificial-intelligence\" target=\"other\">How to cite this entry</a>.",
            "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://leibniz.stanford.edu/friends/preview/artificial-intelligence/\" target=\"other\">Preview the PDF version of this entry</a> at the\n <a href=\"https://leibniz.stanford.edu/friends/\" target=\"other\">Friends of the SEP Society</a>.",
            "<img alt=\"inpho icon\" src=\"../../symbols/inpho.png\"/>",
            "<a href=\"https://www.inphoproject.org/entity?sep=artificial-intelligence&amp;redirect=True\" target=\"other\">Look up topics and thinkers related to this entry</a>\n at the Internet Philosophy Ontology Project (InPhO).",
            "<img alt=\"phil papers icon\" src=\"../../symbols/pp.gif\"/>",
            "<a href=\"http://philpapers.org/sep/artificial-intelligence/\" target=\"other\">Enhanced bibliography for this entry</a>\nat <a href=\"http://philpapers.org/\" target=\"other\">PhilPapers</a>, with links to its database."
        ],
        "listed_links": [
            {
                "https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=artificial-intelligence": "How to cite this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/preview/artificial-intelligence/": "Preview the PDF version of this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/": "Friends of the SEP Society"
            },
            {
                "https://www.inphoproject.org/entity?sep=artificial-intelligence&redirect=True": "Look up topics and thinkers related to this entry"
            },
            {
                "http://philpapers.org/sep/artificial-intelligence/": "Enhanced bibliography for this entry"
            },
            {
                "http://philpapers.org/": "PhilPapers"
            }
        ]
    },
    "other_internet_resources": {
        "listed_text": [
            "<a href=\"http://www.cbsnews.com/news/60-minutes-artificial-intelligence-charlie-rose-robot-sophia/\" target=\"other\">Artificial Intelligence Positioned to be a Game-changer</a>,\nan excellent segment on AI from CBS\u2019s esteemed <i>60 Minutes</i>\nprogram, this gives a popular science level overview of the current\nstate of AI (as of Ocotober, 2016).  The videos in the segment covers\napplications of AI, Watson\u2019s evolution from\nwinning <i>Jeopardy!</i> to fighting cancer and advances in robotics.",
            " <a href=\"http://affect-reason-utility.com/ai/ai.html\" target=\"other\">MacroVU\u2019s Map Coverage of the Great Debates of AI</a>\n",
            "<i>AIMA</i> textbook:\n <ul>\n<a href=\"http://www.cs.berkeley.edu/%7Erussell/aima1e.html\" target=\"other\">web site for first edition (1995)</a>\n<a href=\"http://aima.cs.berkeley.edu/2nd-ed/\" target=\"other\">web site for second edition (2002)</a>\n<a href=\"http://aima.cs.berkeley.edu/\" target=\"other\">web site for the third edition (2009)</a>\n</ul>",
            "<a href=\"http://www.cs.berkeley.edu/%7Erussell/aima1e.html\" target=\"other\">web site for first edition (1995)</a>",
            "<a href=\"http://aima.cs.berkeley.edu/2nd-ed/\" target=\"other\">web site for second edition (2002)</a>",
            "<a href=\"http://aima.cs.berkeley.edu/\" target=\"other\">web site for the third edition (2009)</a>",
            "<a href=\"http://www.aaai.org/\" target=\"other\">Association for the Advancement of Artificial Intelligence</a>",
            "<a href=\"http://www.cognitivesciencesociety.org/\" target=\"other\">Cognitive Science Society</a>",
            "<a href=\"http://ijcai.org/\" target=\"other\">International Joint Conference on Artificial Intelligence</a>",
            "<a href=\"http://www.agi-conference.org/\" target=\"other\">Artificial General Intelligence (AGI) Conference</a>",
            "<a href=\"https://sites.google.com/site/narswang/home/agi-introduction\" target=\"other\"> An introduction and a collection of resources on Artificial General Intelligence</a>",
            "<a href=\"http://agi-conf.org/2010/workshops/#SCSI\" target=\"other\"> AGI 2010 Workshop Call for a Serious Computational Science of Intelligence </a>",
            " Baydin A.G., Pearlmutter, B. A., Radul, A. A. &amp; Siskind J.\nM., 2015, \u201cAutomatic Differentiation in Machine Learning: A\nSurvey,\u201d arXiv:1502.05767 [cs.SC]. URL:\n <a href=\"http://arxiv.org/abs/1502.05767\" target=\"other\">http://arxiv.org/abs/1502.05767</a>\n",
            " Benenson, 2016,\n \u201c<a href=\"http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html\" target=\"other\">Classification Datasets Results,</a>\u201d\n URL =\n <a href=\"http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html\" target=\"other\"> http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html </a>\n (Last accessed in July 2018). ",
            " LeCun, Y., Cortes, C. and Burges, C. J.C, 2017,\n \u201cTHE MNIST DATABASE of handwritten digits,\u201d\n URL =\n <a href=\"http://yann.lecun.com/exdb/mnist/\" target=\"other\">http://yann.lecun.com/exdb/mnist/</a>\n (Last accessed in July 2018). ",
            " Levesque, J. H., 2013, \n \u201c<a href=\"http://www.cs.toronto.edu/%7Ehector/Papers/ijcai-13-paper.pdf\" target=\"other\">On Our Best Behaviour</a>,\u201d\n<i>Speech for the IJCAI 2013 Award for Research Excellence</i>,\nBeijing.\n  ",
            "<a href=\"http://web.stanford.edu/class/cs221/\" target=\"other\"> Artificial Intelligence: Principles and Techniques </a>\n from Stanford",
            "<a href=\"https://www.udacity.com/course/cs271\" target=\"other\"> Aritifical Intelligence</a>\n Online course from Udacity ",
            "<a href=\"https://www.edx.org/course/artificial-intelligence-uc-berkeleyx-cs188-1x-0\" target=\"other\"> Artificial Intelligence</a>\n from Columbia University",
            "\n<a href=\"http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/\" target=\"other\"> Artificial Intelligence</a>\n at MIT (as taugh in Fall 2010) ",
            "<a href=\"https://www.udacity.com/course/cs373\" target=\"other\"> Artificial Intelligence for Robotics: Programming a Robotic Car</a>\n Online course on AI formalisms that are used in mobile robots. "
        ],
        "listed_links": [
            {
                "http://www.cbsnews.com/news/60-minutes-artificial-intelligence-charlie-rose-robot-sophia/": "Artificial Intelligence Positioned to be a Game-changer"
            },
            {
                "http://affect-reason-utility.com/ai/ai.html": "MacroVU\u2019s Map Coverage of the Great Debates of AI"
            },
            {
                "http://www.cs.berkeley.edu/%7Erussell/aima1e.html": "web site for first edition (1995)"
            },
            {
                "http://aima.cs.berkeley.edu/2nd-ed/": "web site for second edition (2002)"
            },
            {
                "http://aima.cs.berkeley.edu/": "web site for the third edition (2009)"
            },
            {
                "http://www.aaai.org/": "Association for the Advancement of Artificial Intelligence"
            },
            {
                "http://www.cognitivesciencesociety.org/": "Cognitive Science Society"
            },
            {
                "http://ijcai.org/": "International Joint Conference on Artificial Intelligence"
            },
            {
                "http://www.agi-conference.org/": "Artificial General Intelligence (AGI) Conference"
            },
            {
                "https://sites.google.com/site/narswang/home/agi-introduction": " An introduction and a collection of resources on Artificial General Intelligence"
            },
            {
                "http://agi-conf.org/2010/workshops/#SCSI": " AGI 2010 Workshop Call for a Serious Computational Science of Intelligence "
            },
            {
                "http://arxiv.org/abs/1502.05767": "http://arxiv.org/abs/1502.05767"
            },
            {
                "http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html": "Classification Datasets Results,"
            },
            {
                "http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html": " http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html "
            },
            {
                "http://yann.lecun.com/exdb/mnist/": "http://yann.lecun.com/exdb/mnist/"
            },
            {
                "http://www.cs.toronto.edu/%7Ehector/Papers/ijcai-13-paper.pdf": "On Our Best Behaviour"
            },
            {
                "http://web.stanford.edu/class/cs221/": " Artificial Intelligence: Principles and Techniques "
            },
            {
                "https://www.udacity.com/course/cs271": " Aritifical Intelligence"
            },
            {
                "https://www.edx.org/course/artificial-intelligence-uc-berkeleyx-cs188-1x-0": " Artificial Intelligence"
            },
            {
                "http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/": " Artificial Intelligence"
            },
            {
                "https://www.udacity.com/course/cs373": " Artificial Intelligence for Robotics: Programming a Robotic Car"
            }
        ]
    }
}