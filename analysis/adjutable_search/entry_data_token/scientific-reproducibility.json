{"url": "scientific-reproducibility", "title": "Reproducibility of Scientific Results", "authorship": {"year": "Copyright \u00a9 2018", "author_text": "Fiona Fidler\n<fidlerfm@unimelb.edu.au>\nJohn Wilcox\n<wilcoxje@stanford.edu>", "author_links": [{"mailto:fidlerfm%40unimelb%2eedu%2eau": "fidlerfm@unimelb.edu.au"}, {"https://johnericwilcox.weebly.com": "John Wilcox"}, {"mailto:wilcoxje%40stanford%2eedu": "wilcoxje@stanford.edu"}], "raw_html": "<div id=\"article-copyright\">\n<p>\n<a href=\"../../info.html#c\">Copyright \u00a9 2018</a> by\n\n<br/>\nFiona Fidler\n&lt;<a href=\"mailto:fidlerfm%40unimelb%2eedu%2eau\"><em>fidlerfm<abbr title=\" at \">@</abbr>unimelb<abbr title=\" dot \">.</abbr>edu<abbr title=\" dot \">.</abbr>au</em></a>&gt;<br/>\n<a href=\"https://johnericwilcox.weebly.com\" target=\"other\">John Wilcox</a>\n&lt;<a href=\"mailto:wilcoxje%40stanford%2eedu\"><em>wilcoxje<abbr title=\" at \">@</abbr>stanford<abbr title=\" dot \">.</abbr>edu</em></a>&gt;\n    </p>\n</div>"}, "pubinfo": ["First published Mon Dec 3, 2018"], "preamble": "\n\nThe terms \u201creproducibility crisis\u201d and \u201creplication\ncrisis\u201d gained currency in conversation and in print over the\nlast decade (e.g., Pashler & Wagenmakers 2012), as disappointing\nresults emerged from large scale reproducibility projects in various\nmedical, life and behavioural sciences (e.g., Open Science\nCollaboration, OSC 2015). In 2016, a poll conducted by the journal\nNature reported that more than half (52%) of scientists\nsurveyed believed science was facing a \u201creplication\ncrisis\u201d (Baker 2016). More recently, some authors have moved to\nmore positive terms for describing this episode in science; for\nexample, Vazire (2018) refers instead to a \u201ccredibility\nrevolution\u201d highlighting the improved methods and open science\npractices it has motivated.\n\nThe crisis often refers collectively to at least the following things:\n\n\n the virtual absence of replication studies in the published\nliterature in many scientific fields (e.g., Makel, Plucker, &\nHegarty 2012), \nwidespread failure to reproduce results of published studies in\nlarge systematic replication projects (e.g., OSC 2015; Begley &\nEllis 2012), \nevidence of publication bias (Fanelli 2010a), \na high prevalence of \u201cquestionable research\npractices\u201d, which inflate the rate of false positives in the\nliterature (Simmons, Nelson, & Simonsohn 2011; John, Loewenstein,\n& Prelec 2012; Agnoli et al. 2017; Fraser et al. 2018), and \nthe documented lack of transparency and completeness in the\nreporting of methods, data and analysis in scientific publication\n(Bakker & Wicherts 2011; Nuijten et al. 2016). \n\n\nThe associated open science reform movement aims to rectify conditions\nthat led to the crisis. This is done by promoting activities such as\ndata sharing and public pre-registration of studies, and by advocating\nstricter editorial policies around statistical reporting including\npublishing replication studies and statistically non-significant\nresults.\n\nThis review consists of four distinct parts. First, we look at the\nterm \u201creproducibility\u201d and related terms like\n\u201crepeatability\u201d and \u201creplication\u201d, presenting\nsome definitions and conceptual discussion about the epistemic\nfunction of different types of replication studies. Second, we\ndescribe the meta-science research that has established and\ncharacterised the reproducibility crisis, including large scale\nreplication projects and surveys of questionable research practices in\nvarious scientific communities. Third, we look at attempts to address\nepistemological questions about the limitations of replication, and\nwhat value it holds for scientific inquiry and the accumulation of\nknowledge. The fourth and final part describes some of the many\ninitiatives the open science reform movement has proposed (and in many\ncases implemented) to improve reproducibility in science. In addition,\nwe reflect there on the values and norms which those reforms embody,\nnoting their relevance to the debate about the role of values in the\nphilosophy of science.\n", "toc": [{"#ReplRepeReprScieResu": "1. Replicating, Repeating, and Reproducing Scientific Results"}, {"#AccSocSci": "1.1 An Account from the Social Sciences"}, {"#IntAcc": "1.2 An Interdisciplinary Account"}, {"#PhiAcc": "1.3 A Philosophical Account"}, {"#MetaScieEstaMoniEvalReprCris": "2. Meta-Science: Establishing, Monitoring, and Evaluating the Reproducibility Crisis"}, {"#ReprProj": "2.1 Reproducibility Projects"}, {"#PublBiasLowStatPoweInflFalsPosiRate": "2.2 Publication Bias, Low Statistical Power and Inflated False Positive Rates"}, {"#QuesResePrac": "2.3 Questionable Research Practices"}, {"#OverReliNullHypoSignTest": "2.4 Over-Reliance on Null Hypothesis Significance Testing"}, {"#ScieFrau": "2.5 Scientific Fraud"}, {"#EpisIssuRelaRepl": "3. Epistemological Issues Related to Replication"}, {"#ExpeRegr": "3.1 The Experimenters\u2019 Regress"}, {"#ReplDistFeatScie": "3.2 Replication as a Distinguishing Feature of Science"}, {"#FormLogiRepl": "3.3 Formalising the Logic of Replication"}, {"#OpenScieRefoValuToneScieNorm": "4. Open Science Reforms: Values, Tone, and Scientific Norms"}, {"#MethTrai": "4.1 Methods and Training"}, {"#RepoDiss": "4.2 Reporting and Dissemination"}, {"#PeerRevi": "4.3 Peer Review"}, {"#InceEval": "4.4 Incentives and Evaluations"}, {"#ValuToneScieNormOpenScieRefo": "4.5 Values, Tone, and Scientific Norms in Open Science Reform"}, {"#Conc": "5. Conclusion"}, {"#Bib": "Bibliography"}, {"#Aca": "Academic Tools"}, {"#Oth": "Other Internet Resources"}, {"#Rel": "Related Entries"}], "main_text": "\n1. Replicating, Repeating, and Reproducing Scientific Results\n\nA starting point in any philosophical exploration of reproducibility\nand related notions is to consider the conceptual question of what\nsuch notions mean. According to some (e.g., Cartwright 1991), the\nterms \u201creplication\u201d, \u201creproduction\u201d and\n\u201crepetition\u201d denote distinct concepts, while others use\nthese terms interchangeably (e.g., Atmanspacher & Maasen 2016a).\nDifferent disciplines can have different understandings of these terms\ntoo. In computational disciplines, for example,\nreproducibility often refers to the ability to reproduce\ncomputations alone, that is, it relates exclusively to sharing and\nsufficiently annotating data and code (e.g., Peng 2011, 2015). In\nthose disciplines, replication describes the redoing of whole\nexperiments (Barba 2017, Other Internet Resources). In psychology and\nother social and life\nsciences, however, reproducibility may refer to either the\nredoing of computations, or the redoing of experiments. The\nReproducibility Projects, coordinated by the Center for Open Science,\nredo entire studies, data collection and analysis. A recent funding\nprogram announcement by DARPA (US Defense Advanced Research Programs\nAgency) distinguished between reproducibility and replicability, where\nthe former refers to computational reproducibility and the latter to\nthe redoing of experiments. Here we use all three\nterms\u2014\u201creplication\u201d, \u201creproduction\u201d and\n\u201crepetition\u201d\u2014interchangeably, unless explicitly\ndescribing the distinctions of other authors.\n\nWhen describing a study as \u201creplicable\u201d, people could have\nin mind either of at least two different things. The first is that the\nstudy is replicable in principle the sense that it can be\ncarried out again, particularly when its methods, procedures and\nanalysis are described in a sufficiently detailed and transparent way.\nThe second is that the study is replicable in that sense that it can\nbe carried out again and, when this happens, the replication study\nwill successfully produce the same or sufficiently\nsimilar results as the original. A study may be replicable in the\nformer sense but not in the second sense: one might be able to\nreplicate the methods, procedures and analysis of a study, but fail to\nsuccessfully replicate the results of the original study. Similarly,\nwhen people talk of a \u201creplication\u201d, they could also have\nin mind two different things: the replication of the methods,\nprocedures and analysis of a study (irrespective of the results) or,\nalternatively, the replication of such methods, procedures and\nanalysis as well as the results.\n\nArguably, most typologies of replication make more or less\nfine-grained distinctions between direct replication (which\nclosely follow the original study to verify results) and\nconceptual replications (which deliberately alter important\nfeatures of the study to generalize findings or to test the underlying\nhypothesis in a new way). As suggested, this distinction may not\nalways be known by these terms. For example, roughly the same\ndistinction is referred to as exact and inexact\nreplication by Keppel (1982); concrete and\nconceptual replication by Sargent (1981), and literal,\noperational and constructive replication by Lykken (1968).\nComputational reproducibility is most often direct (reproducing\nparticular analysis outcomes from the same data set using the same\ncode and software), but it can also be conceptual (analysing the same\nraw data set with alternative approaches, different models or\nstatistical frameworks). For an example of a conceptual computational\nreproducibility study, see Silberzahn and Uhlmann 2015.\n\nWe do not attempt to resolve these disciplinary differences or to\ncreate a new typology of replication, and instead we will provide a\nlimited snapshot of the conceptual terrain by surveying three existing\ntypologies\u2014from Stefan Schmidt (2009), from Omar G\u00f3mez,\nNatalia Juristo, and Sira Vegas (2010) and from Hans Radder.\nSchmidt\u2019s account has been influential and widely-cited in\npsychology and social sciences, where the replication crisis\nliterature is heavily concentrated. G\u00f3mez, Juristo, and\nVegas\u2019s (2010) typology of replication is based on a\nmultidisciplinary survey of over 18 scholarly classifications of\nreplication studies which collectively contain more than 79 types of\nreplication. Finally, Radder\u2019s (1996, 2003, 2006, 2009, 2012)\ntypology is perhaps best known within philosophy of science\nitself.\n1.1 An Account from the Social Sciences\n\nSchmidt outlines five functions of replication studies in the social\nsciences:\n\nFunction 1. Controlling for sampling\nerror\u2014that is, to verify that previous results in a sample were\nnot obtained purely by chance outcomes which paint a distorted picture\nof reality\nFunction 2. Controlling for artifacts\n(internal validity)\u2014that is, ensuring that experimental results\nare a proper test of the hypothesis (i.e., have internal validity) and\ndo not reflect unintended flaws in the study design (such as when a\nmeasurement result is, say, an artifact of a faulty thermometer rather\nthan an actual change in a substance\u2019s temperature)\nFunction 3. Controlling for fraud,\nFunction 4. Enabling\ngeneralizability,\nFunction 5. Enabling verification of the\nunderlying hypothesis.\n\n\nModifying Hendrik\u2019s (1991) classes of variables that define a\nresearch space, Schmidt (2009) presents four classes of variables\nwhich may be altered or held constant in order for a given replication\nstudy to fulfil one of the above functions. The four classes are:\n\nClass 1. Information conveyed to\nparticipants (for example, their task instructions).\nClass 2. Context and background. This is a\nlarge class of variables, and it includes: participant characteristics\n(e.g., age, gender, specific history); the physical setting of the\nresearch; characteristics of the experimenter; incidental\ncharacteristics of materials (e.g., type of font, colour of the\nroom),\nClass 3. Participant recruitment, including\nselection of participants and allocation to conditions (such as\nexperimental or control conditions),\nClass 4. Dependent variable measures (or in\nSchmidt\u2019s terms \u201cprocedures for the constitution of the\ndependent variable\u201d, 2009: 93)\n\n\nSchmidt then systematically works through examples of how each\nfunction can be achieved by altering and/or holding a different class\nor classes of variable constant. For example, to fulfil the function\nof controlling for sampling error\n (Function 1),\n one should alter only variables regarding participant recruitment\n(Class 3), attempting to keep variables in all other classes as close\nto the original study as possible. To control for artefacts\n (Function 2),\n one should alter variables concerning the context and dependent\nvariable measures (variables in Classes 2 and 4 respectively), but\nkeep variables in 1 and 3 (information conveyed to participants and\nparticipant recruitment) as close to the original as possible.\nSchmidt, like most other authors in this area, acknowledges the\npractical limits of being able to hold all else constant. Controlling\nfor fraud\n (Function 3)\n is served by the same arrangements as controlling for artefacts\n (Function 2).\n In Schmidt\u2019s account, controlling for sampling error, artefacts\nand fraud (Functions 1 to 3) are connected by a theme of confirming\nthe results of the original study. Functions 4 and 5 go beyond\nthis\u2014generalizing to new populations\n (Function 4)\n which is served by changes to participant recruitment (Class 3) and\nconfirming the underlying hypothesis\n (Function 5),\n which served by changes to the information conveyed, the context and\ndependant variable measures (Classes 1, 2 and 4 respectively) but not\nchanges to participant recruitment (Class 3, although Schmidt\nacknowledges that holding the latter class of variables constant\nwhilst varying everything else is often practically impossible).\nAttempts to enable verification of the underlying research hypothesis\n(i.e., to fulfil Function 5) alone are what Schmidt classifies as\nconceptual replications, following Rosenthal (1991). Attempts\nto fulfil the other four functions are considered variants of\ndirect replications.\n\nIn summary, for Schmidt, direct replications control for sampling\nerror, artifacts, and fraud, and provide information about the\nreliability and validity of prior empirical work. Conceptual\nreplications help corroborate the underlying theory or substantive (as\nopposed to statistical) hypothesis in question and the extent to which\nthey generalize in new circumstances and situations. In practice,\ndirect and conceptual replications lie on a continuum, with\nreplication studies varying more or less compared to the original on\npotentially a great number of dimensions.\n1.2 An Interdisciplinary Account\n\nG\u00f3mez, Juristo, and Vega\u2019s (2010) survey of the\nliterature in 18 disciplines identified 79 types of replication, not\nall of which they considered entirely distinct. They identify five\nmain ways in which a replication study may diverge from an initial\nstudy. With some similarities to Schimdt\u2019s four classes\nabove:\n\nThe site or spatial location of the\nreplication experiment: replication experiments may be conducted in a\nlocation that is or is not the same as the site of the initial\nstudy.\nThe experimenters conducting a replication may\nbe exclusively the same as the original, exclusively different, or a\ncombination of new and original experimenters\nThe apparatus, including the design,\nmaterials, instruments and other important experimental objects and/or\nprocedures may vary between original and replication studies.\nThe operationalisations employed may differ,\nwhere operationalisation refers to measurement of variables. For\nexample, in psychology this might include using two different scales\nmeasuring for depression (as a dependent variable).\nFinally, studies may vary on population\nproperties.\n\n\nA change in any one or combination of these elements in a replication\nstudy corresponds to different purposes underlying the study, and\nthereby establishes a different kind of validity. Like Schmidt et al.\nthen systematically work through how changes to each of the above work\nto fulfil different epistemic functions.\n\nFunction 1. Conclusion Validity and\nControlling for Sampling Error: If each of the five elements\nabove are unchanged in a replication study, then the purpose of the\nreplication is to control for sampling error, that is, to\nverify that previous results in a sample were not obtained purely by\nchance outcomes which make the sample misleading or unrepresentative.\nThis provides a safeguard against what is known as a type I\nerror: incorrectly failing to reject the null hypothesis\n(that is, the hypothesis that there is no relationship between two\nphenomena under investigation). These studies establish conclusion\nvalidity, that is, the credibility or believability of an\nobserved relationship or phenomenon.\nFunction 2. Internal Validity and Controlling\nfor Artefactual Results: If a replication study differs with\nrespect to the site, experimenters or apparatus, then its purpose is\nto establish that previously observed results are not an artefact of a\nparticular apparatus, lab or so on. These studies establish\ninternal validity, that is, the extent to which results can\nbe attributed to the experimental manipulation itself rather than to\nextraneous variables.\nFunction 3. Construct Validity and Determining\nLimits for Operationalizations: If a replication study differs\nwith respect to operationalisations, then its purpose is to determine\nthe extent to which the effect generalizes across measures of\nmanipulated or dependent variables (e.g., the extent to which the\neffect does not depend on the particular psychometric test one uses to\nevaluate depression or IQ). Such studies fulfil the function of\nestablishing construct validity in that they provide evidence\nthat the effect holds across different ways of measuring the\nconstructs.\nFunction 4. External Validity and Determining\nLimits in the Population Properties: If a replication study\ndiffers with respect to its population properties, then its purpose is\nto ascertain the extent to which the results are generalizable to\ndifferent populations, populations which, in G\u00f3mez, Juristo,\nand Vegas\u2019s view, concern subjects and experimental objects such\nas programs. Such studies reinforce external\nvalidity\u2014the extent to which the results are generalizable\nto different populations.\n\n1.3 A Philosophical Account\n\nRadder (1996, 2003, 2006, 2009, 2012) distinguishes three types of\nreproducibility. One is the reproducibility of what Radder calls an\nexperiment\u2019s material realization. Using one of\nRadder\u2019s own examples as an illustration, two people may carry\nout the same actions to measure the mass of an object. Despite doing\nthe same actions, person A regards themselves as measuring the\nobject\u2019s Newtonian mass while person B regards themselves\nas measuring the object\u2019s Einsteinian mass. Here, then, the\nactions or material realization of the experimental procedure can be\nreproduced, but the theoretical descriptions of their significance\ndiffer. Radder, however, does not specify what is required for one\nmaterial realisation to be a reproduction of another, a pertinent\nquestion, especially since, as Radder himself affirms, no reproduction\nwill be exactly the same as any other reproduction (1996:\n82\u201383).\n\nA second type of reproducibility is the reproducibility of an\nexperiment, given a fixed theoretical description. For\nexample, a social scientist might conduct two experiments to examine\nsocial conformity. In one experiment, a young child might be\ninstructed to give an answer to a question before a group of other\nchildren who are, unknown to the former child, instructed to give\nwrong answers to the same question. In another experiment, an adult\nmight be instructed to give an answer to a question before a group of\nother adults who are, unknown to the former adult, instructed to give\nwrong answers to the same question. If the child and the adult give a\nwrong answer that conforms to the answers of others, then the social\nscientist might interpret the result as exemplifying social\nconformity. For Radder, the theoretical description of the experiment\nmight be fixed, specifying that if some people in a\nparticipant\u2019s surroundings give intentionally false answers to\nthe question, then the genuine participant will conform to the\nbehaviour of their peers. However, the material realization of these\nexperiments differs insofar as one concerns children and the other\nadults. It is difficult to see how, in this example at least, this\ndiffers from what either Schmidt or G\u00f3mez, Juristo, and Vegas\nwould refer to as establishing generalizability to a different\npopulation (Schmidt\u2019s [2009]\n Class 3\n and\n Function 5;\n G\u00f3mez, Juristo, and Vegas\u2019s [2010]\n way 5\n and\n Function 4).\n\nThe third kind of reproducibility is what Radder calls\nreplicability. This is where experimental procedures differ\nto produce the same experimental result (otherwise known as a\nsuccessful replication). For example, Radder notes that multiple\nexperiments might obtain the result \u201ca fluid of type f\nhas a boiling point b\u201d, despite having different kinds of\nthermometers by which to measure this boiling point (2006:\n113\u2013114).\n\nSchmidt (2009) points out that the difference between Radder\u2019s\nsecond and third types of reproducibility is small in comparison to\ntheir differences to the first type. He consequently suggests his\nalternative distinction between direct and conceptual replication,\npresumably intending a conceptual replication to cover Radder\u2019s\nsecond and third types.\n\nIn summary, whilst G\u00f3mez, Juristo, and Vegas\u2019s typology\ndraws distinctions in slightly different places to Schmidt\u2019s,\nits purpose is arguably the same\u2014to explain what types of\nalterations in replication studies fulfil different scientific goals,\nsuch as establishing internal validity or the extent of generalization\nand so on. With the exception of his discussion of reproducing the\nmaterial realization, Radder\u2019s other two categories can perhaps\nbe seen as fitting within the larger range of functions described by\nSchmidt and G\u00f3mez et al., who both acknowledge that in\npractice, direct and conceptual replications lie on a noisy continuum.\n\n2. Meta-Science: Establishing, Monitoring, and Evaluating the Reproducibility Crisis\n\nIn psychology, the origin of the reproducibility crisis is often\nlinked to Daryl Bem\u2019s (2011) paper which reported empirical\nevidence for the existence of \u201cpsi\u201d, otherwise known as\nExtra Sensory Perception (ESP). This paper passed through the standard\npeer review process and was published in the high impact Journal\nof Personality and Social Psychology. The controversial nature of\nthe findings inspired three independent replication studies, each of\nwhich failed to reproduce Bem\u2019s results. However, these\nreplication studies were rejected from four different journals,\nincluding the journal that had originally published Bem\u2019s study,\non the grounds that the replications were not original or novel\nresearch. They were eventually published in PLoS ONE\n(Ritchie, Wiseman, & French 2012). This created controversy in the\nfield, and was interpreted by many as demonstrating how publication\nbias impeded science\u2019s self-correction mechanism. In medicine,\nthe origin of the crisis is often attributed to Ioannidis\u2019\n(2005) paper \u201cWhy most published findings are false\u201d. The\npaper offered formal arguments about inflated rates of false positives\nin the literature\u2014where a \u201cfalse positive\u201d result\nclaims a relationship exists between phenomena when it in fact does\nnot (e.g., a claim that consuming a drug is correlated with symptom\nrelief when it in fact is not). Ioannidis\u2019 (2005) also reported\nvery low (11%) empirical reproducibility rates from a set of\npre-clinical trial replications at Amgen, later independently\npublished by Begley and Ellis (2012). In all disciplines, the\nreplication crisis is also more generally linked to earlier criticisms\nof Null Hypothesis Significance Testing (e.g., Szucs & Ioannidis\n2017), which pointed out the neglect of statistical power (e.g., Cohen\n1962, 1994) and a failure to adequately distinguish statistical and\nsubstantive hypotheses (e.g., Meehl 1967, 1978). This is discussed\nfurther below.\n\nIn response to the events above, a new field identifying as\nmeta-science (or meta-research) has become\nestablished over the last decade (Munaf\u00f2 et al. 2017).\nMunaf\u00f2 et al. define meta-science as \u201cthe scientific\nstudy of science itself\u201d (2017: 1). In October 2015, Ioannidis,\nFanelli, Dunne, and Goodman identified over 800 meta-science papers\npublished in the five-month period from January to May that year, and\nestimated that the relevant literature was accruing at the rate of\napproximately 2,000 papers each year. Referring to the same bodies of\nwork with slightly different terms, Ioannidis et al. define\n\u201cmeta-research\u201d as \n\n\nan evolving scientific discipline that aims to evaluate and improve\nresearch practices. It includes thematic areas of methods, reporting,\nreproducibility, evaluation, and incentives (how to do, report,\nverify, correct, and reward science). (2015: 1) \n\n\nMultiple research centres dedicated to this work now exist, including,\nfor example, the Tilburg University Meta-Research Center in\npsychology, the Meta-Research Innovation Center at Stanford (METRICS),\nand others listed in Ioannidis et al. 2015 (see\n Other Internet Resources).\n Relevant research in medical fields is also covered in Stegenga\n2018.\n\nProjects that self-identify as meta-science or meta-research\ninclude:\n\nLarge, crowd-sourced, direct (or close) replication projects such\nas The Reproducibility Projects in Psychology (OSC 2015) and Cancer\nBiology (Errington et al. 2014) and the Many Labs projects in\npsychology (e.g., Klein et al. 2014);\nComputational reproducibility projects, that is, redoing analysis\nusing the same original data set (e.g., Chang & Li 2015);\nBibliographic studies documenting the extent of publication bias\nin different scientific fields and changes over time (e.g., Fanelli\n2010a, 2010b, 2012);\nSurveys of the use of Questionable Research Practices (QRPs)\namongst researchers and their impact on the publication literature\n(e.g., John, Loewenstein, & Prelec 2012; Fiedler & Schwarz\n2016; Agnoli et al. 2017; Fraser et al. 2018);\nSurveys of the completeness, correctness and transparency of\nmethods and analysis reporting in scientific journals (e.g., Nuijten\net al. 2016; Bakker & Wicherts 2011; Cumming et al. 2007; Fidler\net al. 2006);\nSurvey and interview studies of researchers\u2019 understanding\nof core methodological and statistical concepts, and real and\nperceived obstacles to improving practices (Bakker et al. 2016;\nWashburn et al. 2018; Allen, Dorozenko, & Roberts 2016);\nEvaluation of incentives to change behaviour, thereby improving\nreproducibility and encouraging more open practices (e.g., Kidwell et\nal. 2016).\n\n2.1 Reproducibility Projects\n\nThe most well known of these projects is undoubtedly the\nReproducibility Project: Psychology, coordinated by the now Center for\nOpen Science in Charlottesville, VA (then the Open Science\nCollaboration). It involved 270 crowd sourced researchers in 64\ndifferent institutions in 11 different countries. Researchers\nattempted direct replications of 100 studies published in three\nleading psychology journals in the year 2008. Each study was\nreplicated only once. Replications attempted to follow original\nprotocols as closely as possible, though some differences were\nunavoidable (e.g., some replication studies were done with European\nsamples when the original studies used US samples). In almost all\ncases, replication studies used larger sample sizes that the original\nstudies and therefore had greater statistical power\u2014that is, a\ngreater probability of correctly rejecting the null hypothesis (i.e.,\nthat no relationship exists) when the hypothesis is false. A number of\nmeasures of reproducibility were reported:\n\nThe proportion of studies in which there was a match in the\nstatistical significance between original and replication. (Here, the\nstatistical significance of a result is the probability that it would\noccur given the null hypothesis, and p values are common\nmeasures of such probabilities. A replication study and an original\nstudy would have a match in statistical significance if, for example,\nthey both specified that the probability of the original and\nreplication results occurring given the null hypothesis is less than\n5%\u2014i.e., if the p values for results in both studies are\nbelow 0.05.) Thirty nine percent (36%) of results were successful\nreproduced according to this measure.\nThe proportion of studies in which the Effect Size (ES) of the\nreplication study fell within the 95% Confidence Interval (CI) of the\noriginal. (Here, an ES represents the strength of a relationship\nbetween phenomena\u2014a toy example of which is how strongly\nconsumption of a drug is correlated with symptom relief\u2014and a\nConfidence Interval provides some indication of the probability that\nthe ES of the replication study is close to the ES of the original\nstudy.) Forty seven percent (47%) of results were successfully\nreproduced according to this measure.\nThe correlation between original ES and replication ES.\nReplication study ESs were roughly half the size of original ESs.\nThe proportion of studies for which subjective ratings by\nindependent researchers indicated a match between the replication and\nthe original. Thirty nine percent (39%) were considered successful\nreproductions according to this measure. The closeness of this figure\nto measure 1 suggests that raters relied very heavily on p\nvalues in making their judgements.\n\n\nThere have been objections to the implementation and interpretation of\nthis project, most notably by Gilbert et al. (2016), who took issue\nwith the extent to which the replications studies were indeed direct\nreplications. For example, Gilbert et al. highlighted 6 specific\nexamples of \u201clow fidelity protocols\u201d, that is, where\nreplication studies differed in their view substantially from the\noriginal (in one case, using a European sample rather than a US sample\nof participants). However, Anderson et al. (2016) explained in a reply\nthat in half of those cases, the authors of the original study had\nendorsed the replication as being direct or close to on relevant\ndimensions and that furthermore, that independently rated similarity\nbetween original and replication studies failed to predict replication\nsuccess. Others (e.g., Etz & Vandekerckhove 2016) have applied\nBayesian reanalysis to the OSC\u2019s (2015) data and conclude that\nup to 75% (as opposed to the OSC\u2019s 36\u201347%) of replications\ncould be considered successful. However, they do note that in many\ncases this is only with very weak evidence (i.e., Bayes factors of\n<10). They too conclude that the failure to reproduce many effects\nis indeed explained by the overestimation of effect sizes, itself a\nproduct of publication bias. A Reproducibility Project: Cancer Biology\n(also coordinated by the Center for Open Science) is currently\nunderway (Errington et al. 2014), originally attempting to replicate\n50 of the highest impact studies in Cancer Biology published between\n2010\u20132012. This project has recently announced it will complete\nwith only 18 replication studies, as too few originals reported enough\ninformation to proceed with full replications (Kaiser 2018). Results\nof the first 10 studies are reportedly mixed, with only 5 being\nconsidered \u201cmostly repeatable\u201d (Kaiser 2018).\n\nThe Many Labs project (Klein et al. 2014) coordinated 36 independent\nreplications of 13 classic psychology phenomena (from 12 studies, that\nis, one study tested two effects), including anchoring, sunk cost bias\nand priming, amongst other well-known effects in psychology. In terms\nof matching statistical significance, the project demonstrated that 11\nout of 13 effects could be successful replicated. It also showed great\nvariation in many of the effect sizes across the 36 replications.\n\nIn biomedical research, there have also been a number of large scale\nreproducibility projects. An early one by Begley and Ellis (2012, but\ndiscussed earlier in Ioannidis 2005) attempted to replicate 56\nlandmark pre-clinical trials and reported an alarming reproducibility\nrate of only 11%, that is, only 6 of the 56 results could be\nsuccessfully reproduced. Subsequent attempts at large scale\nreplications in this field have produced more optimistic estimates,\nbut routinely failed to successfully reproduce more than half of the\npublished results. Freedman et al. (2015) report five replication\nprojects by independent groups of researchers which produce\nreproducibility estimates ranging from 22% to 49%. They estimate the\ncost of irreproducible research in US biomedical science alone to be\nin the order of USD$28 billion per year. A reproducibility project in\nExperimental Philosophy is an exception to the general trend,\nreporting reproducibility rates of 70% (Cova et al. forthcoming).\n\nFinally, the Social Science Replication Project (SSRP) redid 21\nexperimental social science studies published in the journals Nature\nand Science between 2010 and 2015. Depending on the measure taken, the\nreplication success rate was 57\u201367% (Camerer et al. 2018).\n2.2 Publication Bias, Low Statistical Power and Inflated False Positive Rates\n\nThe causes of irreproducible results are largely the same across\ndisciplines we have mentioned. This is not surprising given that they\nstem from problems with statistical methods, publishing practices and\nthe incentive structures created in a \u201cpublish or perish\u201d\nresearch culture, all of which are largely shared, at least in the\nlife and behavioral sciences.\n\nWhilst replication is often casually referred to as a cornerstone of\nthe scientific method, direct replication studies (as they might be\nunderstood from Schmidt or G\u00f3mez, Juristo, and Vegas\u2019s\ntypologies above) are a rare event in the published literature of some\nscientific disciplines, most notably the life and social sciences. For\nexample, such replication attempts constitute roughly 1% of the\npublished psychology literature (Makel, Plucker, & Hegarty 2012).\nThe proportion in published ecology and evolution literature is even\nsmaller (Kelly 2017, Other Internet Resources).\n\nThis virtual absence of replication studies in the literature can\nexplained by the fact that many scientific journals have historically\nhad explicit policies against publishing replication studies (Mahoney\n1985)\u2014thus giving rise to a \u201cpublication bias\u201d. Over\n70% of editors from 79 social science journals said they preferred new\nstudies over replications and over 90% said they would did not\nencourage the submission of replication studies (Neuliep &\nCrandall 1990). In addition, many science funding bodies also fund\nonly \u201cnovel\u201d, \u201coriginal\u201d and/or\n\u201cgroundbreaking\u201d research (Schmidt 2009).\n\nA second type of publication bias has also played a substantial role\nin the reproducibility crisis, namely a bias towards\n\u201cstatistically significant\u201d or \u201cpositive\u201d\nresults. Unlike the bias against replication studies, this is rarely\nan explicitly stated policy of a journal. Publication bias towards\nstatistically significant findings has a long history, and was first\ndocumented in psychology by Sterling (1959). Developments in text\nmining techniques have led to more comprehensive estimates. For\nexample, Fanelli\u2019s work has demonstrated the extent of\npublication bias in various disciplines, and the proportions of\nstatistically significant results given below are from his 2010a\npaper. He has also documented the increase of this bias over time\n(2012) and explored the causes of the bias, including the relationship\nbetween publication bias and a publish or perish research culture\n(2010b).\n\nIn many disciplines (e.g., psychology, psychiatry, materials science,\npharmacology and toxicology, clinical medicine, biology and\nbiochemistry, economics and business, microbiology and genetics) the\nproportion of statistically significant results is very high, close to\nor exceeding 90% (Fanelli 2010a). This is despite the fact that in\nmany of these fields, the average statistical power is low\u2014that\nis, the average probability that a study will correctly reject the\nnull hypothesis is low. For example, in psychology the proportion of\npublished results that are statistically significant is 92% despite\nthe fact that the average power of studies in this field to detect\nmedium effect sizes (arguably typical of the discipline) is roughly\n44% (Szucs & Ioannidis 2017). If there was no bias towards\npublishing statistically significant results, the proportion of\nsignificant results should roughly match the average statistical power\nof the discipline. The excess in statistical significance (in this\ncase, the difference between 92% and 44%) is therefore an indicator\nthe strength of the bias. For a second example, in ecology,\nenvironment and plant and animal sciences the proportion of\nstatistically significant results is 74% and 78% respectively,\nadmittedly lower than in psychology. However, the most recent estimate\nof the statistical power, again of medium effect sizes, of ecology and\nanimal behaviour is 23\u201326% (Smith, Hardy, & Gammell 2011)\n(An earlier more optimistic assessment was 40\u201347%, Jennions\n& M\u00f8ller, 2003.) For a third example, the proportion of\nstatistically significant results in neuroscience and behaviour is\n85%. Our best estimate of the statistical power in neuroscience is at\nbest 31%, with a lower bound estimate of 8% (Button et al. 2013). The\nassociated file-drawer problem (Rosenthal 1979)\u2014where\nresearchers relegate failed statistically non-significant studies to\ntheir file drawers, hidden from public view\u2014has long been\nestablished in psychology and others disciplines, and is known to lead\nto distortions in meta-analysis (where a \u201cmeta-analysis\u201d\nis a study which analyses results across multiple other studies).\n2.3 Questionable Research Practices\n\nIn addition to creating the file-drawer problem described above,\npublication bias has been held at least partially responsible for the\nhigh prevalence of Questionable Research Practices (QRPs) uncovered in\nboth self-report survey research (John, Loewenstein, & Prelec\n2012; Agnoli 2017 et al. 2017; Fraser\net al. 2018) and in journal studies that have detected, for example,\nunusual distributions of p values (Masicampo & Lalande\n2012; Hartgerink et al. 2016). Pressure to publish, now ubiquitous\nacross academic institutions, means that researchers often cannot\nafford to simply assign \u201cfailed\u201d or statistically\nnon-significant studies to the file drawer, so instead they p\nhack and cherry-pick results (as discussed below) back to\nsignificance, and back into the published literature. Simmons, Nelson,\nand Simonsohn (2011) explained and demonstrated with simulated results\nhow engaging in such practices inflates the false positive error rate\nof the published literature, leading to a lower rate of reproducible\nresults.\n\n\u201cP hacking\u201d refers to a set of practices which\ninclude: checking the statistical significance of results before\ndeciding whether to collect more data; stopping data collection early\nbecause results have reached statistical significance; deciding\nwhether to exclude data points (e.g., outliers) only after checking\nthe impact on statistical significance and not reporting the impact of\nthe data exclusion; adjusting statistical models, for instance by\nincluding or excluding covariates based on the resulting strength of\nthe main effect of interest; and rounding of a p value to meet\na statistical significance threshold (e.g., presenting 0.053 as\nP < .05). \u201cCherry picking\u201d includes failing to\nreport dependent or response variables or relationships that did not\nreach statistical significance or other threshold and/or failing to\nreport conditions or treatments that did not reach statistical\nsignificance or other threshold. \u201cHARKing\u201d (Hypothesising\nAfter Results are Known) includes presenting ad hoc and/or unexpected\nfindings as though they had been predicted all along (Kerr 1998); and\npresenting exploratory work as though it was confirmatory hypothesis\ntesting (Wagenmakers et al. 2012). Five of the most widespread QRPs\nare listed below in Table 1 (from Fraser et al. 2018), with associated\nsurvey measures of prevalence.\n\n\nTable 1: The prevalence of some common\nQuestionable Research Practices. Percentage (with 95% confidence\nintervals) of researches who reported having used the QRP at least\nonce (adapted from Fraser et al. 2018)\n\n\nQuestionable Research Practice\nPsychology Italy \n\n(Agnoli et al. 2017)\nPsychology USA \n\n(John, Loewenstein, & Prelec 2012) \nEcology \n\n(Fraser et al. 2018)\nEvolution\n\n(Fraser et al. 2018) \n\nNot reporting response (outcome) variables that failed to reach\nstatistical significance# \n47.9\n\n(41.3\u201354.6) \n63.4\n\n(59.1\u201367.7) \n64.1\n\n(59.1\u201368.9) \n63.7\n\n(57.2\u201369.7)  \n\nCollecting more data after inspecting whether the results are\nstatistically significant* \n53.2\n\n(46.6\u201359.7) \n55.9\n\n(51.5\u201360.3) \n36.9\n\n(32.4\u201342.0) \n50.7\n\n(43.9\u201357.6)  \n\nRounding-off a p value or other quantity to meet a\npre-specified threshold* \n22.2\n\n(16.7\u201327.7) \n22.0\n\n(18.4\u201325.7) \n27.3\n\n(23.1\u201332.0) \n17.5\n\n(13.1\u201323.0)  \n\nDeciding to exclude data points after first checking the impact\non statistical significance* \n39.7\n\n(33.3\u201346.2) \n38.2\n\n(33.9\u201342.6) \n24.0\n\n(19.9\u201328.6) \n23.9\n\n(18.5\u201330.2)  \n\nReporting an unexpected finding as having been predicted from\nthe start^ \n37.4\n\n(31.0\u201343.9) \n27.0\n\n(23.1\u201330.9) \n48.5\n\n(43.6\u201353.6) \n54.2\n\n(47.7\u201360.6)  \n\n\n#cherry picking,\n\n*p hacking,\n\n^HARKing\n\n2.4 Over-Reliance on Null Hypothesis Significance Testing\n\nNull Hypothesis Significance Testing (NHST)\u2014discussed\nabove\u2014is a commonly diagnosed cause of the current replication\ncrisis (see Szucs & Ioannidis 2017). The ubiquitous nature of NHST\nin life and behavioural sciences is well documented, most recently by\nCristea and Ioannidis (2018). This is important pre-condition for\nestablishing its role as a cause, since it could not be a cause if its\nactual use was rare. The dichotomous nature of NHST facilitates\npublication bias (Meehl 1967, 1978). For example, the language of\naccept and reject in hypothesis testing maps conveniently on to\nacceptance and rejection of manuscripts, a fact that led Rosnow and\nRosenthal (1989) to decry that \u201csurely God loves the .06 nearly\nas much as the .05\u201d (1989: 1277). Techniques that failed to\nenshrine a dichotomous threshold would be harder to employ in service\nof publication bias. For example, a case has been made that estimation\nusing effect sizes and confidence intervals (introduced above) would\nbe less prone to being used in service of publication bias (Cumming\n2012, Cumming and Calin-Jageman 2017.\n\nAs already mentioned, the average statistical power in various\ndisciplines is low. Not only is power often low, but it is virtually\nnever reported; less than 10% of published studies in psychology\nreport statistical power and even fewer in ecology do (Fidler et al.\n2006). Explanations for the widespread neglect of statistical power\noften highlight the many common misconceptions and fallacies\nassociated with p values (e.g., Haller & Krauss 2002;\nGigerenzer 2018). For example, the inverse probability\n fallacy[1]\n has been used to explain why so many researchers fail to calculate\nand report statistical power (Oakes 1986).\n\nIn 2017, a group of 72 authors proposed in a Nature Human\nBehaviour paper that alpha level in statistical significance\ntesting be lowered to 0.005 (as opposed to the current standard of\n0.05) to improve the reproducibility rate of published research\n(Benjamin et al. 2018). A reply from a different set of 88 authors was\npublished in the same journal, arguing against this proposal and\nstating instead that researchers should justify their alpha level\nbased on context (Lakens et al. 2018). Several other replies have\nfollowed, including a call from Andrew Gelman and colleagues to\nabandon statistical significance altogether (McShane et al. 2018, \nOther Internet Resources). The\nexchange has become known on social media as the Alpha Wars\n(e.g., in the Barely Significant blog,\n Other Internet Resources)).\n Independently, the American Statistical Association released a\nstatement on the use of p values for the first time in its\nhistory, cautioning against their overinterpretation and pointing out\nthe limits of the information they offer about replication (Wasserman\n& Lazar 2016) and devoted their association\u2019s 2017 annual\nconvention to the theme \u201cScientific Method for the\n21st Century: A World Beyond \\(p <0.05\\)\u201d (see\n Other Internet Resources).\n \n2.5 Scientific Fraud\n\nA number of recent high-profile cases of scientific fraud have\ncontributed considerably to the amount of press around the\nreproducibility crisis in science. Often these cases (e.g., Diederik\nStapel in psychology) are used as a hook for media coverage, even\nthough the crisis itself has very little to do with scientific fraud.\n(Note also that the Questionable Research Practices above are not\ntypically counted as \u201cfraud\u201d or even \u201cscientific\nmisconduct\u201d despite their ethically dubious status.) For\nexample, Fang, Grant Steen, and Casadevall (2012) estimated that 43%\nof retracted articles in biomedical research are withdrawn because of\nfraud. However, roughly half a million biomedical articles are\npublished annually and only 400 of those are retracted (Oransky 2016,\nfounder of the website RetractionWatch), so this amounts to a very\nsmall proportion of the literature (approximately 0.1%). There are, of\ncourse, many cases of pharmaceutical companies exercising financial\npressure on scientists and the publishing industry that raise\nspeculation about how many undetected (or unretracted) cases there may\nstill be in the literature. Having said that, there is widespread\nconsensus amongst scientists in the field that the main cause of the\ncurrent reproducibility crisis is the current incentive structure in\nscience (publication bias, publish or perish, non-transparent\nstatistical reporting, lack of rewards for data sharing). Whilst this\nincentive structure can push some to scientific fraud, it appears to\nbe a very small proportion. \n3. Epistemological Issues Related to Replication\n\nMany scientists believe that replication is epistemically valuable in\nsome way, that is to say, that replication serves a useful function in\nenhancing our knowledge, understanding or beliefs about reality. This\nsection first discusses a problem about the epistemic value of\nreplication studies\u2014called the \u201cexperimenters\nregress\u201d\u2014and it then considers the claim that replication\nplays an epistemically valuable role in distinguishing scientific\ninquiry. It lastly examines a recent attempt to formalise the logic of\nreplication in a Bayesian framework.\n3.1 The Experimenters\u2019 Regress\n\nCollins (1985) articulated a widely discussed problem that is now\nknown as the experimenters\u2019 regress. He initially lays\nout the problem in the context of measurement (Collins 1985: 84).\nSuppose a scientist is trying to determine the accuracy of a\nmeasurement device and also the accuracy of a measurement result.\nPerhaps, for example, a scientist is using a thermometer to measure\nthe temperature of a liquid, and it delivers a particular measurement\nresult, say, 12 degrees Celsius.\n\nThe problem arises because of the interdependence of the accuracy of\nthe measurement result and the accuracy of the measurement device: to\nknow whether a particular measurement result is accurate, we need to\ntest it against a measurement result that is previously known to be\naccurate, but to know that the result is accurate, we need to know\nthat it has been obtained via an accurate measuring device, and so on.\nThis, according to Collins, creates a \u201ccircle\u201d which he\nrefers to as the \u201cexperimenters\u2019 regress\u201d.\n\nCollins extends the problem to scientific replication more generally.\nSuppose that an experiment B is a replication study of an\ninitial experiment A, and that B\u2019s result\napparently conflicts with A\u2019s result. This seeming\nconflict may have one of two interpretations:\n\nThe results of A and B deliver genuinely conflicting\nverdicts over the truth of the hypothesis under investigation\n Experiment B was not in fact a proper replication of\nexperiment A.\n\n\nThe regress poses a problem about how to choose between these\ninterpretations, a problem which threatens the epistemic value of\nreplication studies if there are no rational grounds for choosing in a\nparticular way. Determining whether one experiment is a proper\nreplication of another is complicated by the facts that scientific\nwriting conventions often omit precise details of experimental\nmethodology (Collins 2016), and, furthermore, much of the knowledge\nthat scientists require to execute experiments is tacit and\n\u201ccannot be fully explicated or absolutely established\u201d\n(Collins 1985: 73).\n\nIn the context of experimental methodology, Collins wrote:\n\n\nTo know an experiment has been well conducted, one needs to know\nwhether it gives rise to the correct outcome. But to know what the\ncorrect outcome is, one needs to do a well-conducted experiment. But\nto know whether the experiment has been well conducted\u2026! (2016:\n66; ellipses original)\n\n\nCollins holds that in such cases where a conflict of results arises,\nscientists tend to fraction into two groups, each holding opposing\ninterpretations of the results. According to Collins, where such\ngroups are \u201cdetermined\u201d and the \u201ccontroversy runs\ndeep\u201d (Collins 2016: 67), the dispute between the groups cannot\nbe resolved via further experimentation, for each additional result is\nsubject to the problem posed by the experimenters\u2019\n regress.[2]\n In such cases, Collins claims that particular non-epistemic factors\nwill partly determine which interpretation becomes the lasting view:\n\n\n\nthe career, social, and cognitive interests of the scientists, their\nreputations and that of their institutions, and the perceived utility\nfor future work. (Franklin & Collins 2016: 99)\n\n\nFranklin was the most vociferous opponent of Collins, although recent\ncollaboration between the two has fostered some agreement (Collins\n2016). Franklin presented a set of strategies for validating\nexperimental results, all of which relate to \u201crational\nargument\u201d on epistemic grounds (Franklin 1989: 459; 1994).\nExamples include, for instance, appealing to experimental checks on\nmeasurement devices or eliminating potential sources of error in the\nexperiment (Franklin & Collins 2016). He claimed that the fact\nthat such strategies were evidenced in scientific practice\n\u201cargues against those who believe that rational arguments plays\nlittle, if any, role\u201d in such validation (Franklin 1989: 459),\nwith Collins being an example. He interprets Collins as suggesting\nthat the strategies for resolving debates of the validation of results\nare social factors or \u201cculturally accepted practices\u201d\n(Franklin, 1989: 459) which do not provide reasons to underpin\nrational belief about results. Franklin (1994) further claims that\nCollins conflates the difficulty in successfully executing\nexperiments with the difficulty of demonstrating that\nexperiments have been executed, with Feest (2016) interpreting him to\nsay that although such execution requires tacit knowledge, one can\nnevertheless appeal to strategies to demonstrate the validity of\nexperimental findings.\n\nFeest (2016) examines a case study involving debates about the Mozart\neffect in psychology (which, roughly speaking, is the effect whereby\nlistening to Mozart beneficially affects some aspect of intelligence\nor brain structure). Like Collins, she agrees that there is a problem\nin determining whether conflicting results suggest a putative\nreplication experiment is not a proper replication attempt, in part\nbecause there is uncertainty about whether scientific concepts such as\nthe Mozart effect have been appropriately operationalised in earlier\nor later experimental contexts. Unlike Collins (on her\ninterpretation), however, she does not think that this uncertainty\narises because scientists have inescapably tacit knowledge of the\nlinguistic rules about the meaning and application of concepts like\nthe Mozart effect. Rather the uncertainty arises because such concepts\nare still themselves developing and because of assumptions about the\nworld that are required to successfully draw inferences from\nit. Experimental methodology then serves to reveal the previously\ntacit assumptions about the application of concepts and the legitimacy\nof inferences, assumptions which are then susceptible to scrutiny.\n\nFor example, in her study of the Mozart effect, she notes that\nreplication studies of the Mozart effect failed to find that Mozart\nmusic had a beneficial influence on spatial abilities. Rauscher, who\nwas the first to report results supporting the Mozart effect,\nsuggested that the later studies were not proper replications of her\nstudy (Rauscher, Shaw, and Ky 1993, 1995). She clarified that the\nMozart effect applied only to a particular category\nof spatial abilities (spatio-temporal processes) and that the later\nstudies operationalised the Mozart effect in terms of different\nspatial abilities (spatial recognition). Here, then, there was a\ndifficulty in determining whether to interpret failed replication\nresults as evidence against the initial results or rather as an\nindication that the replication studies were not proper replications.\nFeest claims this difficulty arose because of tacit knowledge or\nassumptions: assumptions about the application of the Mozart effect\nconcept to different kinds of spatial abilities, about whether the\nworld is such that Mozart music has an effect on such abilities and\nabout whether the failure of Mozart to impact other kinds of spatial\nabilities warrants the inference that the Mozart effect does not\nexist. Contra Collins, however, experimental methodology enabled the\nexplication and testing of these assumptions, thus allowing scientists\nto overcome the interpretive impasse.\n\nAgainst this background, her overall argument is that scientists often\nare and should be sceptical towards each other\u2019s results.\nHowever, this is not because of inescapably tacit knowledge and the\ninevitable failure of epistemic strategies for validating results.\nRather, it is at least in part because of varying tacit assumptions\nthat researchers have about the meaning of concepts, about the world\nand about what to draw inferences from it. Progressive experimentation\nserves to reveal these tacit assumptions which can then be\nscrutinised, leading to the accumulation of knowledge.\n\nThere is also other philosophical literature on the\nexperimenters\u2019 regress, including Teira\u2019s (2013) paper\narguing that particular experimental debiasing procedures are\ndefensible against the regress from a contractualist perspective,\naccording to which self-interested scientists have reason to adopt\ngood methodological standards.\n3.2 Replication as a Distinguishing Feature of Science\n\nThere is a widespread belief that science is distinct from other\nknowledge accumulation endeavours, and some have suggested that\nreplication distinguishes (or is at least essential to) science in\nthis respect. (See also the entry on \n  science and pseudo-science.).\nAccording to the Open Science Collaboration, \u201cReproducible\nresearch practices are at the heart of sound research and integral to\nthe scientific method.\u201d (OSC 2015: 7). Schmidt echoes this theme:\n\u201cTo confirm results or hypotheses by a repetition procedure is\nat the basis of any scientific conception\u201d (2009: 90). Braude\n(1979) goes so far as to say that reproducibility is a\n\u201cdemarcation criterion between science and nonscience\u201d\n(1979: 2). Similarly, Nosek, Spies, and Motyl state that:\n\n\n[T]he scientific method differentiates itself from other approaches by\npublicly disclosing the basis of evidence for a claim\u2026. In\nprinciple, open sharing of methodology means that the entire body of\nscientific knowledge can be reproduced by anyone. (2012: 618)\n\n\nIf replication played such an essential or distinguishing role in\nscience, we might expect it to be a prominent theme in the history of\nscience. Steinle (2016) considers the extent to which it is such a\ntheme. He presents a variety of cases from the history of science\nwhere replication played very different roles, although he understands\n\u201creplication\u201d narrowly to refer to when an experiment is\nre-run by different researchers. He claims that the role and\nvalue of replication in experimental replication is \u201cmuch more\ncomplex than easy textbook accounts make us believe\u201d (2016: 60),\nparticularly since each scientific inquiry is always tied to a variety\nof contextual considerations that can affect the importance of\nreplication. Such considerations include the relationship between\nexperimental results and the background of accepted theory at the\ntime, the practical and resource constraints on pursuing replication\nand the perceived credibility of the researchers. These contextual\nfactors, he claims, mean that replication was a key or even overriding\ndeterminant of acceptance of research claims in some cases, but not in\nothers.\n\nFor example, sometimes replication was sufficient to embrace a\nresearch claim, even if it conflicted with the background of accepted\ntheory and left theoretical questions unresolved. A case of this is\nhigh-temperature superconductivity, the effect whereby an electric\ncurrent can pass with zero resistance through a conductor at\nrelatively high temperatures. In 1986, physicists Georg Bednorz and\nAlex M\u00fcller reported finding a material which acted as a\nsuperconductor at 35 kelvin (\u2212238 degrees Celsius). Scientists\naround the world successfully replicated the effect, and Bednorz and\nMuller were then awarded with a Nobel Prize in Physics a year after\ntheir announcement. This case is remarkable since not only did their\neffect contradict the accepted physical theory at the time, but there\nis still no extant theory that adequately explains the effects which\nthey reported (Di Bucchianico, 2014).\n\nAs a contrasting example, however, sometimes claims were accepted\nwithout any replication. In the 1650s, German scientist Otto von\nGuericke designed and operated the world\u2019s first vacuum pump\nthat would visibly suck air out of a larger space. He performed\nexperiments with his device to various audiences. Yet the replication\nof his experiments by others would have been very difficult, if not\nimpossible: not only was Guericke\u2019s pump both expensive and\ncomplicated to build, but it was also unlikely that his descriptions\nof it sufficed to enable anyone to build the pump and to consequently\nreplicate his findings. Despite this, Steinle claims that \u201cno\ndoubts were raised about his results\u201d, probably as a results of\nhis \u201cpublic performances that could be witnessed by a large\nnumber of participants\u201d (2016: 55).\n\nSteinle takes such historical cases to provide normative guidance for\nunderstanding the epistemic value as replication as context-sensitive:\nwhether replication is necessary or sufficient for establishing a\nresearch claim will depend on a variety of considerations, such as\nthose mentioned earlier. He consequently eschews wide-reaching claims,\nsuch as those that \u201cit\u2019s all about replicability\u201d or\nthat \u201creplicability does not decide anything\u201d (2016:\n60).\n3.3 Formalising the Logic of Replication\n\nEarp and Trafimow (2015) attempt to formalise the way in which\nreplication is epistemically valuable, and they do this using a\nBayesian framework to explicate the inferences drawn from replication\nstudies. They present the framework in a context similar to that of\nCollins (1985), noting that \u201cit is well-nigh impossible to say\nconclusively what [replication results] mean\u201d (Earp &\nTrafimow, 2015: 3). But while replication studies are often not\nconclusive, they do believe that such studies can be\ninformative, and their Bayesian framework depicts how this is\nso.\n\nThe framework is set out with an example. Suppose an aficionado of\nResearcher A is highly confident that anything said by\nResearcher A is true. Some other researcher, Researcher\nB, then attempts to replicate an experiment by Researcher\nA, and Researcher B find results that conflict with\nthose of Researcher A. Earp and Trafimow claim that the\naficionado might continue to be confident in Researcher\nA\u2019s findings, but the aficionado\u2019s confidence is\nlikely to slightly decrease. As the number of failed replication\nattempts increases, the aficionado\u2019s confidence accordingly\ndecreases, eventually falling below 50% and thereby placing more\nconfidence in the replication failures than in the findings initially\nreported by Researcher A.\n\nHere, then, suppose we are interested in the probability that the\noriginal result reported by Researcher A is true given\nResearcher B\u2019s first replication failure. Earp and\nTrafimow represent this probability with the notation \\(p(T\\mid F)\\)\nwhere p is a probability function, T represents the\nproposition that the original result is true and F represents\nResearcher B\u2019s replication failure. According to\nBayes\u2019s theorem below, this probability is calculable from the\naficionado\u2019s degree of confidence that the original result is\ntrue prior to learning of the replication failure \\(p(T)\\), their\ndegree of expectation of the replication failure on the condition that\nthe original result is true \\(p(T\\mid F)\\), and the degree to which they would\nunconditionally expect a replication failure prior to learning of the\nreplication failure \\(p(F)\\): \n\n\\[\\tag{1}\np(T\\mid F) = \\frac{p(T)p(F\\mid T)}{p(F)}\n\\]\n\n\nRelatedly, we could instead be interested in the confidence ratio that\nthe original result is true or false given the failure to replicate.\nThis ratio is representable as \\(\\frac{p(T\\mid F)}{p(\\nneg T\\mid F)}\\)\nwhere \\(\\nneg T\\) represents the proposition that the original result\nis false. According to the standard Bayesian probability calculus,\nthis ratio in turn is related to a product of ratios concerning \n\nthe confidence that the original result is true\n\\(\\frac{p(T)}{p(\\nneg T)}\\) and \nthe expectation of a replication failure on the condition that the\nresult is true or false \\(\\frac{p(F\\mid T)}{p(F\\mid \\nneg T)}\\). \n\n\nThis relation is expressed in the equation: \n\n\\[\\tag{2}\n\\frac{p(T\\mid F)}{p(\\nneg T\\mid F)} = \\frac{p(T)}{p(\\nneg T)} \\frac{p(F\\mid T)}{p(F\\mid \\nneg T)}\n\\]\n\n\nNow Earp and Trafimow assign some values to the terms on the\nright-hand of the equation for (2). Supposing that the aficionado is\nconfident in the original results, they set the ratio\n\\(\\frac{p(T)}{p(\\nneg T)}\\) to 50, meaning that the aficionado is\ninitially fifty times more confident that the results are true than\nthat the results are false.\n\nThey also set the ratio \\(\\frac{p(F\\mid T)}{p(F\\mid \\nneg T)}\\). about\nthe conditional expectation of a replication failure to 0.5, meaning\nthat the aficionado is considerably less confident that there will be\na replication failure if the original result is true than if it is\nfalse. They point out that the extent to which the aficionado is less\nconfident depends on the quality of so-called auxiliary\nassumptions about the replication experiment. Here, auxiliary\nassumptions are assumptions which enable one to infer that particular\nthings should be observable if the theory under test is true. The\nintuitive idea is that the higher the quality of the assumptions about\na replication study, the more one would expect to observe a successful\nreplication if the original result was true. While they do not specify\nprecisely what makes such auxiliary assumptions have high\n\u201cquality\u201d in this context, presumably this quality\nconcerns the extent to which the assumptions are probably true and the\nextent to which the replication experiment is an appropriate test of\nthe veracity of the original results if the assumptions are true.\n\nOnce the ratios on the right-hand of equation (2) are set as such, one\ncan see that a replication failure would reduce one\u2019s confidence\nin the original results: \n\n\\[\\tag{3}\n\\begin{align}\n\\frac{p(T\\mid F)}{p(\\nneg T\\mid F)} & = \\frac{p(T)}{p(\\nneg T)} \\frac{p(F\\mid T)}{p(F\\mid \\nneg T)} \\\\\n& = (50)(0.5) \\\\\n& = 25\\\\\n\\end{align}\n\\]\n\n\nHere, then, a replication failure would reduce the aficionado\u2019s\nconfidence that the original result was true so that the aficionado\nwould be only 25 times more confident that the result is true given a\nfailure (as per \\(\\frac{p(T\\mid F)}{p(\\nneg T\\mid F)}\\)) rather than\n50 times more confident that it is true (as per \\(\\frac{p(T)}{p(\\nneg\nT)}\\)).\n\nNevertheless, the aficionado may still be confident that the original\nresult is true, but we can see how such confidence would decrease with\nsuccessive replication failures. More formally, let \\(F_N\\) be the\nlast replication failure in a sequence of N replication\nfailures \\(\\langle F_1,F_2,\\ldots,F_N\\rangle\\). Then, the\naficionado\u2019s confidence in the original result given the\nNth replication failure is expressible in the\n equation:[3]\n \n\n\\[\\tag{4}\n\\frac{p(T\\mid F_N)}{p(\\nneg T\\mid F_N)} = \\frac{p(T)}{p(\\nneg T)} \\frac{p(F_1\\mid T)}{p(F_1\\mid \\nneg T)}\n\\frac{p(F_2\\mid T)}{p(F_2\\mid \\nneg T)} \\cdots \\frac{p(F_N\\mid T)}{p(F_N\\mid \\nneg T)} \n\\]\n\n\nFor example, suppose there are 10 replication failures, and so\n\\(N=10\\). Suppose further that the confidence ratios for the\nreplication failures are set such that: \n\n\\[\\tag{5}\n\\begin{multline}\n\\phantom{ab}\\frac{p(F_1\\mid T)}{p(F_1\\mid \\nneg T)}\n\\frac{p(F_2\\mid T)}{p(F_2\\mid \\nneg T)} \\cdots \\frac{p(F_{10}\\mid T)}{p(F_{10}\\mid \\nneg T)}\\\\ \\phantom{ab}  =\n(0.5)(0.8)\n(0.7)(0.65)\n(0.75)(0.56)\n(0.69)(0.54)\n(0.73)(0.52)\n\\end{multline}\n\\]\n\n\nThen, \n\n\\[\\tag{6}\n\\begin{align}\n\\frac{p(T \\mid F_{10})}{p(\\nneg T \\mid F_{10})} & = 0.54 \\\\\n& = \\frac{p(T)}{p(\\nneg T)} \\frac{p(F_1\\mid T)}{p(F_1\\mid \\nneg T)}\n\\frac{p(F_2\\mid T)}{p(F_2\\mid \\nneg T)} \\cdots \\frac{p(F_{10}\\mid T)}{p(F_{10}\\mid \\nneg T)} \\\\\n& = (50)(0.5)(0.8)\\ldots(0.52)\n\\end{align}\n\\]\n\n\nHere, then, the aficionado\u2019s confidence in the original result\ndecreases so that they are more confident that it was false than that\nit was true. Hence, on Earp and Trafimow\u2019s Bayesian account,\nsuccessive replication failures can progressively erode one\u2019s\nconfidence that an original result is true, even if one was initially\nhighly confident in the original result and even if no single\nreplication failure by itself was\n conclusive.[4]\n\nSome putative merits of Earp and Trafimow\u2019s account, then, are\nthat it provides a formalisation whereby replication attempts are\ninformative even if they are not conclusive, and furthermore, the\nformalisation provides a role for both quantity of replication\nattempts as well as auxiliary assumptions about the replications.\n4. Open Science Reforms: Values, Tone, and Scientific Norms\n\nThe aforementioned meta-science has unearthed a range of problems\nwhich give rise to the reproducibility crisis, and the open science\nmovement has proposed or promoted various solutions\u2014or\nreforms\u2014for these problems. These reforms can be grouped into\nfour categories: (a) methods and training, (b) reporting and\ndissemination, (c) peer review processes, and (d) evaluating new\nincentive structures (loosely following the categories used by\nMunaf\u00f2 et al. 2017 and Ioannidis et al. 2015). In subsections\n4.1\u20134.4 below, we present a non-exhaustive list of initiatives\nin each of the above categories. These initiatives are reflections of\nvarious values and norms that are at the heart of the open science\nmovement, and we discuss these values and norms in 4.5.\n4.1 Methods and Training\n\nCombating bias. The development of methods for combating\nbias, for example, masked or blind analysis techniques to combat\nconfirmation bias (e.g., MacCoun & Perlmutter 2017).\nSupport. Providing methodological support for\nresearchers, including published guidelines and statistical\nconsultancy (for example, as offered by the Center for Open Science)\nand large online courses such as that developed by Daniel Lakens (see\n Other Internet Resources).\nCollaboration. Promoting collaboration and team/crowd\nsourced science to combat low power and other methodological\nlimitations of single studies. The Reproducibility Projects themselves\nare an example of this, but there are other initiatives too such\nStudySwap in psychology and the Collective Replication and Education\nProject (CREP, see\n Other Internet Resources\n for both of these , see also Munaf\u00f2 et al. for a more detailed\ndescription) which aims to increase the prevalence of replications\nthrough undergraduate education.\n\n4.2 Reporting and Dissemination\n\nThe TOP Guidelines. The Transparency and Openness\nPromotion (TOP) guidelines (Nosek et al. 2015) have, as at the end of\nMay, 2018, almost 5,000 journals and organizations as signatories.\nDeveloped within psychology, TOP guidelines have formed the basis of\nother disciplinary specific guidelines, such as the Tools for\nTransparency in Ecology and Evolution (TTEE). As the name suggests,\nthese guidelines promote more complete and transparent reporting of\nmethodological and statistical practices. This in turn enables\nauthors, reviewers and editors to consider detailed aspects of their\nsample size planning and design decisions, and to clearly distinguish\nbetween confirmatory (planned) analysis and exploratory (post hoc)\nanalysis.\nPre-registration. In its simplest form, pre-registration\ninvolves making a public, date-stamped statement of predictions and/or\nhypotheses, either before data is collected, viewed or analysed. The\npurpose is to distinguish prediction from postdiction (Nosek et al.\n2018), or what is elsewhere referred to as confirmatory research from\nexploratory research (Wagenmakers et al. 2012) and a distinction\nperhaps more commonly known as hypothesis testing versus hypothesis\ngenerating research. Pre-registration of predictive research helps\ncontrol for HARKing (Kerr 1998) and hindsight bias, and within the\nfrequentist Null Hypothesis Significance Testing, helps contain the\nfalse positive error rate to the set alpha level. There are several\nplatforms that host pre-registrations, such as the Open Science\nFramework (osf.io) and As Predicted (aspredicted.org). The Open\nScience Framework also hosts a \u201cpre-registration\nchallenge\u201d offering monetary rewards for publishing\npre-registered work.\nSpecific Journal Initiatives. Some high impact journals,\nhaving been singled out in the science media as having particularly\nproblematic publishing practices (e.g., Schekman 2013), have taken\nexceptional steps to improve the completeness, transparency and\nreproducibility of the research they publish. For example, since 2013,\nNature and Nature research journals have engaged in a range\nof editorial activities aimed at improving reproducibility of research\npublished in their journals \n (see the editorial announcement, \nNature 496, 398, 25 April 2013,  doi:10.1038/496398a).  In\n2017, they introduced checklists and reporting summaries (published\nalongside articles) in an effort to improve transparency and\nreproducibility. In 2018, they produced discipline specific versions\nfor Nature Human Behaviour and Nature Ecology &\nEvolution. Within psychology, the journal Psychological\nScience (flagship journal of the Association of Psychological\nScience) was the first to adopt open science practices, such the COS\nOpen Science badges described below. Following a meeting of ecology\nand evolution journal editors in 2015, a number of journals in these\nfields have run editorials on this topic, often committing to TTEE\nguidelines (discussed above). Conservation Biology has in\naddition adopted a checklist for associate editors (Parker et\nal. 2016).\n\n4.3 Peer Review\n\nRegistered reports. Registered reports shift the point at\nwhich peer review occurs in the research process, in an effort to\ncombat publication bias against null (negative) results. Manuscripts\nare submitted, reviewed and a publication decision made on the basis\nof the introduction, methods and planned analysis alone. If accepted,\nauthors then have a defined period of time to carry out the planned\nresearch and submit the results. Assuming authors followed their\noriginal plans (or adequately justified deviations from them), the\njournal will honour its decision to publish, regardless of the result\noutcomes. In psychology, the Registered Report format has been\nchampioned by Chris Chambers, with the journal Cortex being\nthe first to adopt the format under Chambers\u2019 editorship\n(Chambers 2013, 2017; Nosek & Lakens 2014). Currently (end of May\n2018), 108 journals in a range of biomedical, psychology and\nneuroscience fields, offer the format (see Registered Reports in\n Other Internet Resources).\nPre-prints. Well-established in some sciences like\nphysics, the use of pre-print servers is relatively new in biological\nand social sciences.\n\n4.4 Incentives and Evaluations\n\nOpen Science badges. A recent review of initiatives for\nimproving data sharing identified the awarding of open data and open\nmaterials badges as the most effective scheme (Rowhani-Farid, Allen,\n& Barnett 2017). One such badge scheme is coordinated by the\nCenter for Open Science who currently award three badges: Open Data,\nOpen Materials and Pre-Registration. Badges are attached to articles\nthat follow a specific set of criteria to engage in these activities.\nKidwell et al. (2016) evaluated the effectiveness of badges in the\njournal Psychological Science and found substantial increases\n(from 3 to 39%) in data sharing over a less than two-year period. Such\nincreases were not found in similar journals without badge schemes\nover the same period.\n\n4.5 Values, Tone, and Scientific Norms in Open Science Reform\n\nThere has long been philosophical debate about what role values do and\nshould play in science (Churchman 1948; Rudner 1953; Douglas 2016),\nand the reproducibility crisis is intimately connected to questions\nabout the operations of, and interconnections between, such values. In\nparticular, Nosek et al. (2017) argue that there is a tension between truth and\npublishability. More specifically, for reasons discussed in section 2\nabove, the accuracy of scientific results are compromised by the value\nwhich journals place on novel and positive results and, consequently,\nby scientists who value career success to seek to exclusively publish\nsuch results in these journals. Many others in addition to Nosek et\nal. (Hackett 2005; Martin 1992; Sovacool 2008) have taken also take issue\nwith the value which journals and funding bodies have placed on\nnovelty.\n\nSome might interpret the tension as a manifestation of how epistemic\nvalues (such as truth and replicability) can be compromised by\n(arguably) non-epistemic values, such the value of novel, interesting\nor surprising results. Epistemic values are typically taken to be\nvalues that, in the words of Steel \u201cpromote the acquisition of\ntrue beliefs\u201d (2010: 18; see also Goldman 1999). Canonical\nexamples of epistemic values include the predictive accuracy and\ninternal consistency of a theory. Epistemic values are often\ncontrasted with putative non-epistemic or non-cognitive values, which\ninclude ethical or social values like, for example, the novelty of a\ntheory or its ability to improve well-being by lessening power\ninequalities (Longino 1996). Of course, there is no complete consensus\nas to precisely what counts as an epistemic or non-epistemic value\n(Rooney 1992; Longino 1996). Longino, for example, claims that, other\nthings being equal, novelty counts in favour of accepting a theory,\nand convincingly argues that, in some contexts, it can serve as a\n\u201cprotection against unconscious perpetuation of the sexism and\nandrocentrism\u201d in traditional science (1997: 22). However, she\ndoes not discuss novelty specifically in the context of the\nreproducibility crisis.\n\nGiner-Sorolla (2012), however, does discuss novelty in the context of\nthe crisis, and he offers another perspective on its value. He claims\nthat one reason novelty has been used to define what is publishable or\nfundable is that it is relatively easy for researchers to establish\nand for reviewers and editors to detect. Yet, Giner-Sorolla argues,\nnovelty for its own sake perhaps should not be valued, and should in\nfact be recognized as merely an operationalisation of a deeper\nconcept, such as \u201cability to advance the field\u201d (567).\nGiner-Sorolla goes on to point out how such shallow\noperationalisations of important concepts often lead to problems, for\nexample, using statistical significance to measure the importance of\nresults, or measuring the quality of research by how well outcomes fit\nwith experimenters\u2019 prior expectations.\n\nValues are closely connected to discussions about norms in the open\nscience movement. Vazire (2018) and others invoke norms of\nscience\u2014communality, universalism, disinterestedness and\norganised skepticism\u2014in setting the goals for open science,\nnorms originally articulated by Robert Merton (1942). Each such norm\narguably reflects a value which Merton advocated, and each norm may be\nopposed by a counternorm which denotes behaviour that is in conflict\nwith the norm. For example, the norm of communality (which Merton\ncalled \u201ccommunism\u201d) reflects the value of collaboration\nand the common ownership of scientific goods since the norm recommends\nsuch collaboration and common ownership. Advocates of open science see\nsuch norms, and the values which they reflect, as an aim for open\nscience. For example, the norm of communality is reflected in sharing\nand making data open, and in open access publishing. In contrast, the\ncounternorm of secrecy is associated with a closed, for profit\npublishing system (Anderson et al. 2010). Likewise, assessing\nscientific work on its merits upholds the norm of\nuniversalism\u2014that the evaluation of research claims should not\ndepend on the socio-demographic characteristics of the proponents of\nsuch claims. In contrast, assessing work by the age, the status, the\ninstitution or the metrics of the journal it is published in reflects\na counternorm of particularism.\n\nVazire (2018) and others have argued that, at the moment, scientific\npractice is dominated by counternorms and that a move to Mertonian\nnorms is a goal of the open science reform movement. In particular,\nself-interestedness, as opposed to the norm of disinterestedness,\nmotivates p-hacking and other Questionable Research Practices.\nSimilarly, a desire to protect one\u2019s professional reputation\nmotivates resistance to having one\u2019s work replicated by others\n(Vazire 2018). This in turn reinforces a counternorm of organized\ndogmatism rather than organized skepticism which, according to Merton,\ninvolves the \u201ctemporary suspension of judgment and the detached\nscrutiny of beliefs\u201d (Merton, 1973).\n\nAnderson et al.\u2019s (2010) focus groups and surveys of scientists\nsuggest that scientists do want to adhere to Merton\u2019s norms but\nthat the current incentive structure of science makes this difficult.\nChanging the structure of penalty and reward systems within science to\npromote communality, universalism, disinterestedness and organized\nskepticism instead of their counternorms is an ongoing challenge for\nthe open science reform movement. As Pashler and Wagenmakers (2012)\nhave said: \n\n\nreplicability problems will not be so easily overcome, as they reflect\ndeep-seated human biases and well-entrenched incentives that shape the\nbehavior of individuals and institutions. (2012: 529)\n\n\nThe effort to promote such values and norms has generated heated\ncontroversy. Some early responses to the Reproducibility Project:\nPsychology and Many Labs projects were highly critical, not just of\nthe substance of the nature and process of the work. Calls for\nopenness were interpreted as reflecting mistrust, and attempts to\nreplicate others\u2019 work as personal attacks (e.g., Schnail 2014\nin\n Other Internet Resources).\n Nosek, Spies, & Motyl (2012) argue that calls for openness should\nnot be interepreted as mistrust: \n\n\nOpening our research process will make us feel accountable to do our\nbest to get it right; and, if we do not get it right, to increase the\nopportunities for others to detect the problems and correct them.\nOpenness is not needed because we are untrustworthy; it is needed\nbecause we are human. (2012: 626)\n\n\nExchanges related to this have become known as the tone\ndebate.[]\n\n5. Conclusion\n\nThe subject of reproducibility is associated with a turbulent period\nin contemporary science. This period has called for a re-evaluation of\nthe values, incentives, practices and structures which underpin\nscientific inquiry. While the meta-science has painted a bleak picture\nof reproducibility in some fields, it has also inspired a parallel\nmovement to strengthen the foundations of science. However, more\nprogress is to be made, especially in understanding the solutions to\nthe reproducibility crisis. In this regard, there are fruitful avenues\nfor future research, including a deeper exploration of the role that\nepistemic and non-epistemic values can or should play in scientific\ninquiry.\n", "bibliography": {"categories": [], "cat_ref_text": {"ref_list": ["Agnoli, Franca, Jelte M. Wicherts, Coosje L. S. Veldkamp, Paolo\nAlbiero, and Roberto Cubelli, 2017, \u201cQuestionable Research\nPractices among Italian Research Psychologists\u201d, Jakob\nPietschnig (ed.), <em>PLoS ONE</em>, 12(3): e0172792.\ndoi:10.1371/journal.pone.0172792", "Allen, Peter J., Kate P. Dorozenko, and Lynne D. Roberts, 2016,\n\u201cDifficult Decisions: A Qualitative Exploration of the\nStatistical Decision Making Process from the Perspectives of\nPsychology Students and Academics\u201d, <em>Frontiers in\nPsychology</em>, 7(February): 188. doi:10.3389/fpsyg.2016.00188", "Anderson, Christopher J., \u0160t\u011bp\u00e1n Bahnik,\nMichael Barnett-Cowan, Frank A. Bosco, Jesse Chandler, C. R. Chartier,\nF. Cheung, et al., 2016, \u201cResponse to Comment on \u2018Estimating\nthe Reproducibility of Psychological Science\u2019\u201d,\n<em>Science</em>, 351(6277): 1037. doi:10.1126/science.aad9163", "Anderson, Melissa S., Emily A. Ronning, Raymond De Vries, and\nBrian C. Martinson, 2010, \u201cExtending the Mertonian Norms:\nScientists\u2019 Subscription to Norms of Research\u201d, <em>The\nJournal of Higher Education</em>, 81(3): 366\u2013393.\ndoi:10.1353/jhe.0.0095", "Atmanspacher, Harald and Sabine Maasen, 2016a,\n\u201cIntroduction\u201d, in Atmanspacher and Maasen 2016b:\n1\u20138. doi:10.1002/9781118865064.ch0", "\u2013\u2013\u2013 (eds.), 2016b, <em>Reproducibility:\nPrinciples, Problems, Practices, and Prospects</em>, Hoboken, NJ: John\nWiley &amp; Sons. doi:10.1002/9781118865064", "Baker, Monya, 2016, \u201c1,500 Scientists Lift the Lid on\nReproducibility\u201d, <em>Nature</em>, 533(7604): 452\u2013454.\ndoi:10.1038/533452a", "Bakker, Marjan, Chris H. J. Hartgerink, Jelte M. Wicherts, and Han\nL. J. van der Maas, 2016, \u201cResearchers\u2019 Intuitions About Power\nin Psychological Research\u201d, <em>Psychological Science</em>,\n27(8): 1069\u20131077. doi:10.1177/0956797616647519", "Bakker, Marjan and Jelte M. Wicherts, 2011, \u201cThe\n(Mis)Reporting of Statistical Results in Psychology Journals\u201d,\n<em>Behavior Research Methods</em>, 43(3): 666\u2013678.\ndoi:10.3758/s13428-011-0089-5", "Begley, C. Glenn and Lee M. Ellis, 2012, \u201cRaise Standards\nfor Preclinical Cancer Research: Drug Development\u201d,\n<em>Nature</em>, 483(7391): 531\u2013533. doi:10.1038/483531a", "Bem, Daryl J., 2011, \u201cFeeling the Future: Experimental\nEvidence for Anomalous Retroactive Influences on Cognition and\nAffect\u201d, <em>Journal of Personality and Social Psychology</em>,\n100(3): 407\u2013425.", "Benjamin, Daniel J., James O. Berger, Magnus Johannesson, Brian A.\nNosek, Eric-Jan Wagenmakers, Richard Berk, Kenneth A. Bollen, et al.,\n2018, \u201cRedefine Statistical Significance\u201d, <em>Nature\nHuman Behaviour</em>, 2(1): 6\u201310.\ndoi:10.1038/s41562-017-0189-z", "Braude, Stephen E., 1979, <em>ESP and Psychokinesis. A\nPhilosophical Examination</em>, Philadelphia: Temple University\nPress.", "Button, Katherine S., John P. A. Ioannidis, Claire Mokrysz, Brian\nA. Nosek, Jonathan Flint, Emma S. J. Robinson, and Marcus R.\nMunaf\u00f2, 2013, \u201cPower Failure: Why Small Sample Size\nUndermines the Reliability of Neuroscience\u201d, <em>Nature Reviews\nNeuroscience</em>, 14(5): 365\u2013376. doi:10.1038/nrn3475", "Camerer C.F., et al., 2018, \u201cEvaluating the replicability of\nsocial science experiments in <em>Nature</em> and <em>Science</em>\nbetween 2010 and 2015\u201d, <em>Nature Human Behaviour</em>, 2:\n637\u2013644. doi: 10.1038/s41562-018-0399-z", "Cartwright, Nancy, 1991, \u201cReplicability, Reproducibility and\nRobustness: Comments on Harry Collins\u201d, <em>History of Political\nEconomy</em>, 23(1): 143\u2013155.", "Chambers, Christopher D., 2013, \u201cRegistered Reports: A New\nPublishing Initiative at Cortex\u201d, <em>Cortex</em>, 49(3):\n609\u2013610. doi:10.1016/j.cortex.2012.12.016", "\u2013\u2013\u2013, 2017, <em>The Seven Deadly Sins of\nPsychology A Manifesto for Reforming the Culture of Scientific\nPractice</em>, Princeton: Princeton University Press.", "Chang, Andrew C. and Phillip Li, 2015, \u201cIs Economics\nResearch Replicable? Sixty Published Papers from Thirteen Journals Say\n\u2018Usually Not\u2019\u201d, <em>Finance and Economics Discussion\nSeries</em>, 2015(83): 1\u201326. doi:10.17016/FEDS.2015.083", "Churchman, C. West, 1948, \u201cStatistics, Pragmatics,\nInduction\u201d, <em>Philosophy of Science</em>, 15(3):\n249\u2013268. doi:10.1086/286991", "Collins, Harry M., 1985, <em>Changing Order: Replication and\nInduction in Scientific Practice</em>, London; Beverly Hills: Sage\nPublications.", "\u2013\u2013\u2013, 2016, \u201cReproducibility of\nexperiments: experiments\u2019 regress, statistical uncertainty\nprinciple, and the replication imperative\u201d in Atmanspacher and\nMaasen 2016b: 65\u201382. doi:10.1002/9781118865064.ch4", "Cohen, Jacob, 1962, \u201cThe Statistical Power of\nAbnormal-Social Psychological Research: A Review\u201d,, <em>The\nJournal of Abnormal and Social Psychology</em>, 65(3): 145\u2013153.\ndoi:10.1037/h0045186", "\u2013\u2013\u2013, 1994, \u201cThe Earth Is Round (\\(p &lt;\n.05\\))\u201d, <em>American Psychologist</em>, 49(12): 997\u20131003,\ndoi:10.1037/0003-066X.49.12.997", "Cova, Florian, Brent Strickland, Angela Abatista, Aur\u00e9lien\nAllard, James Andow, Mario Attie, James Beebe, et al., forthcoming,\n\u201cEstimating the Reproducibility of Experimental\nPhilosophy\u201d, <em>Review of Philosophy and Psychology</em>, early\nonline: 14 June 2018. doi:10.1007/s13164-018-0400-9", "Cristea, Ioana Alina and John P. A. Ioannidis, 2018, \u201cP\nValues in Display Items Are Ubiquitous and Almost Invariably Significant: A Survey of Top Science Journals\u201d, Christos A. Ouzounis (ed.), <em>PLoS ONE</em>, 13(5): e0197440. doi:10.1371/journal.pone.0197440", "Cumming, Geoff, 2012, <em>Understanding the New Statistics: Effect\nSizes, Confidence Intervals, and Meta-Analysis</em>. New York:\nRoutledge.", "Cumming, Geoff and Robert Calin-Jageman, 2017, <em>Introduction to\nthe New Statistics: Estimation, Open Science and Beyond</em>, New\nYork: Routledge.", "Cumming, Geoff, Fiona Fidler, Martine Leonard, Pavel Kalinowski,\nAshton Christiansen, Anita Kleinig, Jessica Lo, Natalie McMenamin, and\nSarah Wilson, 2007, \u201cStatistical Reform in Psychology: Is\nAnything Changing?\u201d, <em>Psychological Science</em>, 18(3):\n230\u2013232. doi:10.1111/j.1467-9280.2007.01881.x", "Di Bucchianico, Marilena, 2014, \u201cA Matter of Phronesis:\nExperiment and Virtue in Physics, A Case Study\u201d, in <em>Virtue\nEpistemology Naturalized</em>, Abrol Fairweather (ed.), Cham: Springer\nInternational Publishing, 291\u2013312.\ndoi:10.1007/978-3-319-04672-3_17", "Dominus, Susan, 2017, \u201cWhen the Revolution Came for Amy\nCuddy\u201d, <em>The New York Times</em>, October 21, Sunday\nMagazine, page 29.", "Douglas, Heather, 2016, \u201cValues in Science\u201d, in Paul\nHumphreys, <em>The Oxford Handbook of Philosophy of Science</em>, New\nYork: Oxford University Press, pp. 609\u2013630.", "Earp, Brian D. and David Trafimow, 2015, \u201cReplication,\nFalsification, and the Crisis of Confidence in Social\nPsychology\u201d, <em>Frontiers in Psychology</em>, 6(May): 621.\ndoi:10.3389/fpsyg.2015.00621", "Errington, Timothy M., Elizabeth Iorns, William Gunn, Fraser\nElisabeth Tan, Joelle Lomax, and Brian A Nosek, 2014, \u201cAn Open\nInvestigation of the Reproducibility of Cancer Biology\nResearch\u201d, <em>ELife</em>, 3(December): e043333.\ndoi:10.7554/eLife.04333", "Etz, Alexander and Joachim Vandekerckhove, 2016, \u201cA Bayesian\nPerspective on the Reproducibility Project: Psychology\u201d, Daniele\nMarinazzo (ed.), <em>PLoS ONE</em>, 11(2): e0149794.\ndoi:10.1371/journal.pone.0149794", "Fanelli, Daniele, 2010a, \u201cDo Pressures to Publish Increase\nScientists\u2019 Bias? An Empirical Support from US States Data\u201d,\nEnrico Scalas (ed.), <em>PLoS ONE</em>, 5(4): e10271.\ndoi:10.1371/journal.pone.0010271", "\u2013\u2013\u2013, 2010b, \u201c\u2018Positive\u2019\nResults Increase Down the Hierarchy of the Sciences\u201d, Enrico\nScalas (ed.), <em>PLoS ONE</em>, 5(4): e10068.\ndoi:10.1371/journal.pone.0010068", "\u2013\u2013\u2013, 2012, \u201cNegative Results Are\nDisappearing from Most Disciplines and Countries\u201d,\n<em>Scientometrics</em>, 90(3): 891\u2013904.\ndoi:10.1007/s11192-011-0494-7", "Fang, Ferric C., R. Grant Steen, and Arturo Casadevall, 2012,\n\u201cMisconduct Accounts for the Majority of Retracted Scientific\nPublications\u201d, <em>Proceedings of the National Academy of\nSciences</em>, 109(42): 17028\u201317033.\ndoi:10.1073/pnas.1212247109", "Feest, Uljana, 2016, \u201cThe Experimenters\u2019 Regress\nReconsidered: Replication, Tacit Knowledge, and the Dynamics of\nKnowledge Generation\u201d, <em>Studies in History and Philosophy of\nScience Part A</em>, 58(August): 34\u201345.\ndoi:10.1016/j.shpsa.2016.04.003", "Fidler, Fiona, Mark A. Burgman, Geoff Cumming, Robert Buttrose,\nand Neil Thomason, 2006, \u201cImpact of Criticism of Null-Hypothesis\nSignificance Testing on Statistical Reporting Practices in\nConservation Biology\u201d, <em>Conservation Biology</em>, 20(5):\n1539\u20131544. doi:10.1111/j.1523-1739.2006.00525.x", "Fidler, Fiona, Yung En Chee, Bonnie C. Wintle, Mark A. Burgman,\nMichael A. McCarthy, and Ascelin Gordon, 2017, \u201cMetaresearch for\nEvaluating Reproducibility in Ecology and Evolution\u201d,\n<em>BioScience</em>, 67(3): 282\u2013289.\ndoi:10.1093/biosci/biw159", "Fiedler, Klaus and Norbert Schwarz, 2016, \u201cQuestionable\nResearch Practices Revisited\u201d, <em>Social Psychological and\nPersonality Science</em>, 7(1): 45\u201352.\ndoi:10.1177/1948550615612150", "Fiske, Susan T., 2016, \u201cA Call to Change Science\u2019s Culture\nof Shaming\u201d, <em>Association for Psychological Science \nObserver</em>, 29(9).\n [<a href=\"https://www.psychologicalscience.org/observer/a-call-to-change-sciences-culture-of-shaming\" target=\"other\">Fiske 2016 available online</a>]", "Franklin, Allan, 1989, \u201cThe Epistemology of\nExperiment\u201d, in David Gooding, Trevor Pinch, and Simon Schaffer\n(eds.), <em>The Uses of Experiment: Studies in the Natural\nSciences</em>, Cambridge: Cambridge University Press, pp.\n437\u2013460.", "\u2013\u2013\u2013, 1994, \u201cHow to Avoid the\nExperimenters\u2019 Regress\u201d, <em>Studies in History and Philosophy\nof Science Part A</em>, 25(3): 463\u2013491.\ndoi:10.1016/0039-3681(94)90062-0", "Franklin, Allan and Harry Collins, 2016, \u201cTwo Kinds of Case\nStudy and a New Agreement\u201d, in <em>The Philosophy of Historical\nCase Studies</em>, Tilman Sauer and Raphael Scholl (eds.), Cham:\nSpringer International Publishing, 319: 95\u2013121.\ndoi:10.1007/978-3-319-30229-4_6", "Fraser, Hannah, Tim Parker, Shinichi Nakagawa, Ashley Barnett, and\nFiona Fidler, 2018, \u201cQuestionable Research Practices in Ecology\nand Evolution\u201d, Jelte M. Wicherts (ed.), <em>PLoS ONE</em>,\n13(7): e0200303. doi:10.1371/journal.pone.0200303", "Freedman, Leonard P., Iain M. Cockburn, and Timothy S. Simcoe,\n2015, \u201cThe Economics of Reproducibility in Preclinical\nResearch\u201d, <em>PLoS Biology</em>, 13(6): e1002165.\ndoi:10.1371/journal.pbio.1002165", "Giner-Sorolla, Roger, 2012, \u201cScience or Art? How Aesthetic\nStandards Grease the Way Through the Publication Bottleneck but\nUndermine Science\u201d, <em>Perspectives on Psychological\nScience</em>, 7(6): 562\u2013571. doi:10.1177/1745691612457576", "Gigerenzer, Gerd, 2018, \u201cStatistical Rituals: The\nReplication Delusion and How We Got There\u201d, <em>Advances in\nMethods and Practices in Psychological Science</em>, 1(2):\n198\u2013218. doi:10.1177/2515245918771329", "Gilbert, Daniel T., Gary King, Stephen Pettigrew, and Timothy D.\nWilson, 2016, \u201cComment on \u2018Estimating the Reproducibility\nof Psychological Science\u2019\u201d, <em>Science</em>, 351(6277):\n1037\u20131037. doi:10.1126/science.aad7243", "Goldman, Alvin I., 1999, <em>Knowledge in a Social World</em>,\nOxford: Clarendon. doi:10.1093/0198238207.001.0001", "G\u00f3mez, Omar S., Natalia Juristo, and Sira Vegas, 2010,\n\u201cReplications Types in Experimental Disciplines\u201d, in\n<em>Proceedings of the 2010 ACM-IEEE International Symposium on\nEmpirical Software Engineering and Measurement - ESEM \u201910</em>,\nBolzano-Bozen, Italy: ACM Press. doi:10.1145/1852786.1852790", "Hackett, B., 2005, \u201cEssential tensions: Identity, control, and\nrisk in research\u201d, <em>Social Studies of Science</em>, 35(5):\n787\u2013826. doi:10.1177/0306312705056045", "Haller, Heiko, and Stefan Krauss, 2002, \u201cMisinterpretations\nof Significance: a Problem Students Share with Their Teachers?\u201d\n<em>Methods of Psychological Research\u2014Online</em>, 7(1):\n1\u201320.\n [<a href=\"https://www.researchgate.net/publication/27262211_Misinterpretations_of_Significance_A_Problem_Students_Share_with_Their_Teachers\" target=\"other\">Haller &amp; Kraus 2002 available online</a>]", "Hartgerink, Chris H.J., Robbie C.M. van Aert, Mich\u00e8le B.\nNuijten, Jelte M. Wicherts, and Marcel A.L.M. van Assen, 2016,\n\u201cDistributions of <i>p</i>-Values Smaller than .05 in\nPsychology: What Is Going On?\u201d, <em>PeerJ</em>, 4(April): e1935.\ndoi:10.7717/peerj.1935", "Hendrick, Clyde, 1991. \u201cReplication, Strict Replications,\nand Conceptual Replications: Are They Important?\u201d, in Neuliep\n1991: 41\u201349.", "Ioannidis, John P. A., 2005, \u201cWhy Most Published Research\nFindings Are False\u201d, <em>PLoS Medicine</em>, 2(8): e124.\ndoi:10.1371/journal.pmed.0020124", "Ioannidis, John P. A., Daniele Fanelli, Debbie Drake Dunne, and\nSteven N. Goodman, 2015, \u201cMeta-Research: Evaluation and\nImprovement of Research Methods and Practices\u201d, <em>PLOS\nBiology</em>, 13(10): e1002264. doi:10.1371/journal.pbio.1002264", "Jennions, Michael D. and Anders Pape M\u00f8ller, 2003, \u201cA\nSurvey of the Statistical Power of Research in Behavioral Ecology and\nAnimal Behavior\u201d, <em>Behavioral Ecology</em>, 14(3):\n438\u2013445. doi:10.1093/beheco/14.3.438", "John, Leslie K., George Loewenstein, and Drazen Prelec, 2012,\n\u201cMeasuring the Prevalence of Questionable Research Practices\nWith Incentives for Truth Telling\u201d, <em>Psychological\nScience</em>, 23(5): 524\u2013532. doi:10.1177/0956797611430953", "Kaiser, Jocelyn, 2018, \u201cPlan to Replicate 50 High-Impact\nCancer Papers Shrinks to Just 18\u201d, <em>Science</em>, 31 July\n2018. doi:10.1126/science.aau9619", "Keppel, Geoffrey, 1982, <em>Design and Analysis. A\nResearcher\u2019s Handbook</em>, second edition, Englewood Cliffs,\nNJ: Prentice-Hall.", "Kerr, Norbert L., 1998, \u201cHARKing: Hypothesizing After the\nResults Are Known\u201d, <em>Personality and Social Psychology\nReview</em>, 2(3): 196\u2013217. doi:10.1207/s15327957pspr0203_4", "Kidwell, Mallory C., Ljiljana B. Lazarevi\u0107, Erica Baranski,\nTom E. Hardwicke, Sarah Piechowski, Lina-Sophia Falkenberg, Curtis\nKennett, et al., 2016, \u201cBadges to Acknowledge Open Practices: A\nSimple, Low-Cost, Effective Method for Increasing Transparency\u201d,\nMalcolm R Macleod (ed.), <em>PLOS Biology</em>, 14(5): e1002456.\ndoi:10.1371/journal.pbio.1002456", "Klein, Richard A., Kate A. Ratliff, Michelangelo Vianello,\nReginald B. Adams, \u0160t\u011bp\u00e1n Bahn\u00edk, Michael J.\nBernstein, Konrad Bocian, et al., 2014, \u201cInvestigating Variation\nin Replicability: A \u2018Many Labs\u2019 Replication\nProject\u201d, <em>Social Psychology</em>, 45(3): 142\u2013152.\ndoi:10.1027/1864-9335/a000178", "Lakens, Daniel, Federico G. Adolfi, Casper J. Albers, Farid\nAnvari, Matthew A. J. Apps, Shlomo E. Argamon, Thom Baguley, et al.,\n2018, \u201cJustify Your Alpha\u201d, <em>Nature Human\nBehaviour</em>, 2(3): 168\u2013171.\ndoi:10.1038/s41562-018-0311-x", "Longino, Helen E., 1990, <em>Science as Social Knowledge: Values\nand Objectivity in Scientific Inquiry</em>, Princeton: Princeton\nUniversity Press.", "\u2013\u2013\u2013, 1996, \u201cCognitive and Non-Cognitive\nValues in Science: Rethinking the Dichotomy\u201d, in <em>Feminism,\nScience, and the Philosophy of Science</em>, Lynn Hankinson Nelson and\nJack Nelson (eds.), Dordrecht: Springer Netherlands, 39\u201358.\ndoi:10.1007/978-94-009-1742-2_3", "\u2013\u2013\u2013, 1997, \u201cFeminist Epistemology as a\nLocal Epistemology: Helen E. Longino\u201d, <em>Aristotelian Society\nSupplementary Volume</em>, 71(1): 19\u201335.\ndoi:10.1111/1467-8349.00017", "Lykken, David T., 1968, \u201cStatistical Significance in\nPsychological Research\u201d, <em>Psychological Bulletin</em>, 70(3,\nPt.1): 151\u2013159. doi:10.1037/h0026141", "Madden, Charles S., Richard W. Easley, and Mark G. Dunn, 1995,\n\u201cHow Journal Editors View Replication Research\u201d,\n<em>Journal of Advertising</em>, 24(December): 77\u201387.\ndoi:10.1080/00913367.1995.10673490", "Makel, Matthew C., Jonathan A. Plucker, and Boyd Hegarty, 2012,\n\u201cReplications in Psychology Research: How Often Do They Really\nOccur?\u201d, <em>Perspectives on Psychological Science</em>, 7(6):\n537\u2013542. doi:10.1177/1745691612460688", "MacCoun, Robert J. and Saul Perlmutter, 2017, \u201cBlind\nAnalysis as a Correction for Confirmatory Bias in Physics and in\nPsychology\u201d, in <em>Psychological Science Under Scrutiny</em>,\nScott O. Lilienfeld and Irwin D. Waldman (eds.), Hoboken, NJ: John\nWiley &amp; Sons, pp. 295\u2013322.\ndoi:10.1002/9781119095910.ch15", "Martin, B., 1992, \u201cScientific fraud and the power structure\nof science\u201d, <em>Prometheus</em>, 10(1):\n83\u201398. doi:10.1080/08109029208629515", "Masicampo, E.J. and Daniel R. Lalande, 2012, \u201cA Peculiar\nPrevalence of <i>p</i> Values Just below .05\u201d, <em>Quarterly\nJournal of Experimental Psychology</em>, 65(11): 2271\u20132279.\ndoi:10.1080/17470218.2012.711335", "Mahoney, Michael J., 1985, \u201cOpen Exchange and Epistemic\nProgress\u201d,, <em>American Psychologist</em>, 40(1): 29\u201339. doi:10.1037/0003-066X.40.1.29", "Meehl, Paul E., 1967, \u201cTheory-Testing in Psychology and\nPhysics: A Methodological Paradox\u201d, <em>Philosophy of\nScience</em>, 34(2): 103\u2013115. doi:10.1086/288135", "\u2013\u2013\u2013, 1978, \u201cTheoretical Risks and Tabular\nAsterisks: Sir Karl, Sir Ronald, and the Slow Progress of Soft\nPsychology\u201d, <em>Journal of Consulting and Clinical\nPsychology</em>, 46(4): 806\u2013834.\ndoi:10.1037/0022-006X.46.4.806", "Merton, Robert K., 1942 [1973], \u201cA Note on Science and\nTechnology in a Democratic Order\u201d, <em>Journal of Legal and\nPolitical Sociology</em>, 1(1\u20132): 115\u2013126; reprinted\nas \u201cThe Normative Structure of Science\u201d, in Robert K.\nMerton (ed.) <em>The Sociology of Science: Theoretical and Empirical\nInvestigations</em>, Chicago, IL: University of Chicago Press.", "Munaf\u00f2, Marcus R., Brian A. Nosek, Dorothy V. M. Bishop,\nKatherine S. Button, Christopher D. Chambers, Nathalie Percie du Sert,\nUri Simonsohn, Eric-Jan Wagenmakers, Jennifer J. Ware, and John P. A.\nIoannidis, 2017, \u201cA Manifesto for Reproducible Science\u201d,\n<em>Nature Human Behaviour</em>, 1(1): 0021.\ndoi:10.1038/s41562-016-0021", "Neuliep, James William (ed.), 1991, <em>Replication Research in\nthe Social Sciences</em>, (Journal of social behavior and personality;\n8: 6), Newbury Park, CA: Sage Publications.", "Neuliep, James W. and Rick Crandall, 1990, \u201cEditorial Bias\nAgainst Replication Research\u201d, <em>Journal of Social Behavior\nand Personality</em>, 5(4): 85\u201390", "Nosek, Brian A. and Dani\u00ebl Lakens, 2014, \u201cRegistered\nReports: A Method to Increase the Credibility of Published\nResults\u201d, <em>Social Psychology</em>, 45(3): 137\u2013141.\ndoi:10.1027/1864-9335/a000192", "Nosek, Brian A., Jeffrey R. Spies, and Matt Motyl, 2012,\n\u201cScientific Utopia: II. Restructuring Incentives and Practices\nto Promote Truth Over Publishability\u201d, <em>Perspectives on\nPsychological Science</em>, 7(6): 615\u2013631.\ndoi:10.1177/1745691612459058", "Nosek, B. A., G. Alter, G. C. Banks, D. Borsboom, S. D. Bowman, S.\nJ. Breckler, S. Buck, et al., 2015, \u201cPromoting an Open Research\nCulture\u201d, <em>Science</em>, 348(6242): 1422\u20131425.\ndoi:10.1126/science.aab2374,", "Nosek, Brian A., Charles R. Ebersole, Alexander C. DeHaven, and\nDavid T. Mellor, 2018, \u201cThe Preregistration Revolution\u201d,\n<em>Proceedings of the National Academy of Sciences</em>, 115(11):\n2600\u20132606. doi:10.1073/pnas.1708274114", "Nuijten, Mich\u00e8le B., Chris H. J. Hartgerink, Marcel A. L.\nM. van Assen, Sacha Epskamp, and Jelte M. Wicherts, 2016, \u201cThe\nPrevalence of Statistical Reporting Errors in Psychology\n(1985\u20132013)\u201d, <em>Behavior Research Methods</em>, 48(4):\n1205\u20131226. doi:10.3758/s13428-015-0664-2", "Oakes, Michael, 1986, <em>Statistical Inference: A Commentary for\nthe Social and Behavioral Sciences</em>, New York: Wiley.", "Open Science Collaboration (OSC), 2015, \u201cEstimating the\nReproducibility of Psychological Science\u201d, <em>Science</em>,\n349(6251): 943\u2013951. doi:10.1126/science.aac4716", "Oransky, Ivan, 2016, \u201cHalf of Biomedical Studies Don\u2019t\nStand up to Scrutiny and What We Need to Do about That\u201d, <em>The\nConversation</em>, 11 November 2016.\n [<a href=\"http://theconversation.com/half-of-biomedical-research-studies-dont-stand-up-to-scrutiny-and-what-we-need-to-do-about-that-45149\" target=\"other\">Oransky 2016 available online</a>]", "Parker, T.H., E. Main, S. Nakagawa, J. Gurevitch, F. Jarrad, and\nM. Burgman, 2016, \u201cPromoting Transparency in Conservation\nScience: Editorial\u201d, <em>Conservation Biology</em>, 30(6):\n1149\u20131150. doi:10.1111/cobi.12760", "Pashler, Harold and Eric-Jan Wagenmakers, 2012,\n\u201cEditors\u2019 Introduction to the Special Section on Replicability\nin Psychological Science: A Crisis of Confidence?\u201d,\n<em>Perspectives on Psychological Science</em>, 7(6): 528\u2013530.\ndoi:10.1177/1745691612465253", "Peng, Roger D., 2011, \u201cReproducible Research in\nComputational Science\u201d, <em>Science</em>, 334(6060):\n1226\u20131227. doi:10.1126/science.1213847", "\u2013\u2013\u2013, 2015, \u201cThe Reproducibility Crisis in\nScience: A Statistical Counterattack\u201d, <em>Significance</em>,\n12(3): 30\u201332. doi:10.1111/j.1740-9713.2015.00827.x", "Radder, Hans, 1996, <em>In And About The World: Philosophical\nStudies Of Science And Technology</em>, Albany, NY: State University\nof New York Press.", "\u2013\u2013\u2013, 2003, \u201cTechnology and Theory in\nExperimental Science\u201d, in Hans Radder (ed.), <em>The Philosophy\nof Scientific Experimentation</em>, Pittsburgh: University of\nPittsburgh Press, pp. 152\u2013173.", "\u2013\u2013\u2013, 2006, <em>The World Observed/The World\nConceived</em>, Pittsburgh, PA: University of Pittsburgh Press.", "\u2013\u2013\u2013, 2009, \u201cScience, Technology and the\nScience-Technology Relationship\u201d, in Anthonie Meijers (ed.),\n<em>Philosophy of Technology and Engineering Sciences</em>, Amsterdam:\nElsevier, pp. 65\u201391. doi:10.1016/B978-0-444-51667-1.50007-0", "\u2013\u2013\u2013, 2012, <em>The Material Realization of\nScience: From Habermas to Experimentation and Referential\nRealism</em>, Boston: Springer. doi:10.1007/978-94-007-4107-2", "Rauscher, Frances H., Gordon L. Shaw, and Catherine N. Ky, 1993,\n\u201cMusic and Spatial Task Performance\u201d, <em>Nature</em>,\n365(6447): 611\u2013611. doi:10.1038/365611a0", "Rauscher, Frances H., Gordon L. Shaw, and Katherine N. Ky, 1995,\n\u201cListening to Mozart Enhances Spatial-Temporal Reasoning:\nTowards a Neurophysiological Basis\u201d, <em>Neuroscience\nLetters</em>, 185(1): 44\u201347.\ndoi:10.1016/0304-3940(94)11221-4", "Ritchie, Stuart J., Richard Wiseman, and Christopher C. French,\n2012, \u201cFailing the Future: Three Unsuccessful Attempts to\nReplicate Bem\u2019s \u2018Retroactive Facilitation of Recall\u2019\nEffect\u201d, Sam Gilbert (ed.), <em>PLoS ONE</em>, 7(3): e33423.\ndoi:10.1371/journal.pone.0033423", "Rooney, Phyllis, 1992, \u201cOn Values in Science: Is the\nEpistemic/Non-Epistemic Distinction Useful?\u201d, <em>PSA:\nProceedings of the Biennial Meeting of the Philosophy of Science\nAssociation</em>, 1992(1): 13\u201322.\ndoi:10.1086/psaprocbienmeetp.1992.1.192740", "Rosenthal, Robert, 1979, \u201cThe File Drawer Problem and\nTolerance for Null Results\u201d, <em>Psychological Bulletin</em>,\n86(3): 638\u2013641. doi:10.1037/0033-2909.86.3.638", "\u2013\u2013\u2013, 1991, \u201cReplication in Behavioral\nResearch\u201d, in Neuliep 1991: 1\u201339.", "Rosnow, Ralph L. and Robert Rosenthal, 1989, \u201cStatistical\nProcedures and the Justification of Knowledge in Psychological\nScience\u201d,, <em>American Psychologist</em>, 44(10):\n1276\u20131284. doi:10.1037/0003-066X.44.10.1276", "Rowhani-Farid, Anisa, Michelle Allen, and Adrian G. Barnett, 2017,\n\u201cWhat Incentives Increase Data Sharing in Health and Medical\nResearch? A Systematic Review\u201d, <em>Research Integrity and Peer\nReview</em>, 2: 4. doi:10.1186/s41073-017-0028-9", "Rudner, Richard, 1953, \u201cThe Scientist Qua Scientist Makes\nValue Judgments\u201d, <em>Philosophy of Science</em>, 20(1):\n1\u20136. doi:10.1086/287231", "Sargent, C.L., 1981, \u201cThe Repeatability Of Significance And\nThe Significance Of Repeatability\u201d, <em>European Journal of\nParapsychology</em>, 3: 423\u2013433.", "Schekman, Randy, 2013, \u201cHow Journals like Nature, Cell and\nScience Are Damaging Science | Randy Schekman\u201d, <em>The\nGuardian</em>, December 9, sec. Opinion,\n [<a href=\"https://www.theguardian.com/commentisfree/2013/dec/09/how-journals-nature-science-cell-damage-science\" target=\"other\">Schekman 2013 available online</a>]", "Schmidt, Stefan, 2009, \u201cShall We Really Do It Again? The\nPowerful Concept of Replication Is Neglected in the Social\nSciences\u201d, <em>Review of General Psychology</em>, 13(2):\n90\u2013100. doi:10.1037/a0015108", "Silberzahn, Raphael,and Uhlmann, Eric L., 2015, \u201cMany hands\nmake tight work: crowdsourcing research can balance discussions,\nvalidate findings and better inform policy\u201d, <em>Nature</em>, 526(7572):\n189\u2013192.", "Simmons, Joseph P., Leif D. Nelson, and Uri Simonsohn, 2011,\n\u201cFalse-Positive Psychology: Undisclosed Flexibility in Data\nCollection and Analysis Allows Presenting Anything as\nSignificant\u201d, <em>Psychological Science</em>, 22(11):\n1359\u20131366. doi:10.1177/0956797611417632", "Smith, Daniel R., Ian C.W. Hardy, and Martin P. Gammell, 2011,\n\u201cPower Rangers: No Improvement in the Statistical Power of\nAnalyses Published in Animal Behaviour\u201d, <em>Animal\nBehaviour</em>, 81(1): 347\u2013352.\ndoi:10.1016/j.anbehav.2010.09.026", "Sovacool, B. K., 2008, \u201cExploring scientific misconduct:\nIsolated individuals, impure institutions, or an inevitable idiom of\nmodern science?\u201d <em>Journal of Bioethical Inquiry</em>, 5:\n271\u2013282. doi: 10.1007/s11673-008-9113-6", "Steel, Daniel, 2010, \u201cEpistemic Values and the Argument from\nInductive Risk*\u201d, <em>Philosophy of Science</em>, 77(1):\n14\u201334. doi:10.1086/650206", "Stegenga, Jacob, 2018, <em>Medical Nihilism</em>, Oxford: Oxford\nUniversity Press.", "Steinle, Friedrich, 2016, \u201cStability and Replication of\nExperimental Results: A Historical Perspective\u201d, in Atmanspacher\nand Maasen 2016b: 39\u201368. doi:10.1002/9781118865064.ch3", "Sterling, Theodore D., 1959, \u201cPublication Decisions and\nTheir Possible Effects on Inferences Drawn from Tests of Significance\n\u2013 or Vice Versa\u201d, <em>Journal of the American Statistical\nAssociation</em>, 54(285): 30\u201334.\ndoi:10.1080/01621459.1959.10501497", "Sutton, Jon, 2018, \u201cTone Deaf?\u201d, <em>The\nPsychologist</em>, 31: 12\u201313.\n [<a href=\"https://thepsychologist.bps.org.uk/volume-31/march-2018/tone-deaf\" target=\"other\">Sutton 2018 available online</a>]", "Szucs, Denes and John P. A. Ioannidis, 2017, \u201cEmpirical\nAssessment of Published Effect Sizes and Power in the Recent Cognitive\nNeuroscience and Psychology Literature\u201d, Eric-Jan Wagenmakers\n(ed.), <em>PLoS Biology</em>, 15(3): e2000797.\ndoi:10.1371/journal.pbio.2000797", "Teira, David, 2013, \u201cA Contractarian Solution to the\nExperimenter\u2019s Regress\u201d, <em>Philosophy of Science</em>,\n80(5): 709\u2013720. doi:10.1086/673717", "Vazire, Simine, 2018, \u201cImplications of the Credibility\nRevolution for Productivity, Creativity, and Progress\u201d,\n<em>Perspectives on Psychological Science</em>, 13(4): 411\u2013417.\ndoi:10.1177/1745691617751884", "Wagenmakers, Eric-Jan, Ruud Wetzels, Denny Borsboom, Han L. J. van\nder Maas, and Rogier A. Kievit, 2012, \u201cAn Agenda for Purely\nConfirmatory Research\u201d, <em>Perspectives on Psychological\nScience</em>, 7(6): 632\u2013638. doi:10.1177/1745691612463078", "Washburn, Anthony N., Brittany E. Hanson, Matt Motyl, Linda J.\nSkitka, Caitlyn Yantis, Kendal M. Wong, Jiaqing Sun, et al., 2018,\n\u201cWhy Do Some Psychology Researchers Resist Adopting Proposed\nReforms to Research Practices? A Description of Researchers\u2019\nRationales\u201d, <em>Advances in Methods and Practices in\nPsychological Science</em>, 1(2): 166\u2013173.\ndoi:10.1177/2515245918757427", "Wasserstein, Ronald L. and Nicole A. Lazar, 2016, \u201cThe\nASA\u2019s Statement on p-Values: Context, Process, and Purpose\u201d,\n<em>The American Statistician</em>, 70(2): 129\u2013133.\ndoi:10.1080/00031305.2016.1154108"]}, "raw_text": "<div id=\"bibliography\">\n<h2 id=\"Bib\">Bibliography</h2>\n<ul class=\"hanging\">\n<li>Agnoli, Franca, Jelte M. Wicherts, Coosje L. S. Veldkamp, Paolo\nAlbiero, and Roberto Cubelli, 2017, \u201cQuestionable Research\nPractices among Italian Research Psychologists\u201d, Jakob\nPietschnig (ed.), <em>PLoS ONE</em>, 12(3): e0172792.\ndoi:10.1371/journal.pone.0172792</li>\n<li>Allen, Peter J., Kate P. Dorozenko, and Lynne D. Roberts, 2016,\n\u201cDifficult Decisions: A Qualitative Exploration of the\nStatistical Decision Making Process from the Perspectives of\nPsychology Students and Academics\u201d, <em>Frontiers in\nPsychology</em>, 7(February): 188. doi:10.3389/fpsyg.2016.00188</li>\n<li>Anderson, Christopher J., \u0160t\u011bp\u00e1n Bahnik,\nMichael Barnett-Cowan, Frank A. Bosco, Jesse Chandler, C. R. Chartier,\nF. Cheung, et al., 2016, \u201cResponse to Comment on \u2018Estimating\nthe Reproducibility of Psychological Science\u2019\u201d,\n<em>Science</em>, 351(6277): 1037. doi:10.1126/science.aad9163</li>\n<li>Anderson, Melissa S., Emily A. Ronning, Raymond De Vries, and\nBrian C. Martinson, 2010, \u201cExtending the Mertonian Norms:\nScientists\u2019 Subscription to Norms of Research\u201d, <em>The\nJournal of Higher Education</em>, 81(3): 366\u2013393.\ndoi:10.1353/jhe.0.0095</li>\n<li>Atmanspacher, Harald and Sabine Maasen, 2016a,\n\u201cIntroduction\u201d, in Atmanspacher and Maasen 2016b:\n1\u20138. doi:10.1002/9781118865064.ch0</li>\n<li>\u2013\u2013\u2013 (eds.), 2016b, <em>Reproducibility:\nPrinciples, Problems, Practices, and Prospects</em>, Hoboken, NJ: John\nWiley &amp; Sons. doi:10.1002/9781118865064</li>\n<li>Baker, Monya, 2016, \u201c1,500 Scientists Lift the Lid on\nReproducibility\u201d, <em>Nature</em>, 533(7604): 452\u2013454.\ndoi:10.1038/533452a</li>\n<li>Bakker, Marjan, Chris H. J. Hartgerink, Jelte M. Wicherts, and Han\nL. J. van der Maas, 2016, \u201cResearchers\u2019 Intuitions About Power\nin Psychological Research\u201d, <em>Psychological Science</em>,\n27(8): 1069\u20131077. doi:10.1177/0956797616647519</li>\n<li>Bakker, Marjan and Jelte M. Wicherts, 2011, \u201cThe\n(Mis)Reporting of Statistical Results in Psychology Journals\u201d,\n<em>Behavior Research Methods</em>, 43(3): 666\u2013678.\ndoi:10.3758/s13428-011-0089-5</li>\n<li>Begley, C. Glenn and Lee M. Ellis, 2012, \u201cRaise Standards\nfor Preclinical Cancer Research: Drug Development\u201d,\n<em>Nature</em>, 483(7391): 531\u2013533. doi:10.1038/483531a</li>\n<li>Bem, Daryl J., 2011, \u201cFeeling the Future: Experimental\nEvidence for Anomalous Retroactive Influences on Cognition and\nAffect\u201d, <em>Journal of Personality and Social Psychology</em>,\n100(3): 407\u2013425.</li>\n<li>Benjamin, Daniel J., James O. Berger, Magnus Johannesson, Brian A.\nNosek, Eric-Jan Wagenmakers, Richard Berk, Kenneth A. Bollen, et al.,\n2018, \u201cRedefine Statistical Significance\u201d, <em>Nature\nHuman Behaviour</em>, 2(1): 6\u201310.\ndoi:10.1038/s41562-017-0189-z</li>\n<li>Braude, Stephen E., 1979, <em>ESP and Psychokinesis. A\nPhilosophical Examination</em>, Philadelphia: Temple University\nPress.</li>\n<li>Button, Katherine S., John P. A. Ioannidis, Claire Mokrysz, Brian\nA. Nosek, Jonathan Flint, Emma S. J. Robinson, and Marcus R.\nMunaf\u00f2, 2013, \u201cPower Failure: Why Small Sample Size\nUndermines the Reliability of Neuroscience\u201d, <em>Nature Reviews\nNeuroscience</em>, 14(5): 365\u2013376. doi:10.1038/nrn3475</li>\n<li>Camerer C.F., et al., 2018, \u201cEvaluating the replicability of\nsocial science experiments in <em>Nature</em> and <em>Science</em>\nbetween 2010 and 2015\u201d, <em>Nature Human Behaviour</em>, 2:\n637\u2013644. doi: 10.1038/s41562-018-0399-z</li>\n<li>Cartwright, Nancy, 1991, \u201cReplicability, Reproducibility and\nRobustness: Comments on Harry Collins\u201d, <em>History of Political\nEconomy</em>, 23(1): 143\u2013155.</li>\n<li>Chambers, Christopher D., 2013, \u201cRegistered Reports: A New\nPublishing Initiative at Cortex\u201d, <em>Cortex</em>, 49(3):\n609\u2013610. doi:10.1016/j.cortex.2012.12.016</li>\n<li>\u2013\u2013\u2013, 2017, <em>The Seven Deadly Sins of\nPsychology A Manifesto for Reforming the Culture of Scientific\nPractice</em>, Princeton: Princeton University Press.</li>\n<li>Chang, Andrew C. and Phillip Li, 2015, \u201cIs Economics\nResearch Replicable? Sixty Published Papers from Thirteen Journals Say\n\u2018Usually Not\u2019\u201d, <em>Finance and Economics Discussion\nSeries</em>, 2015(83): 1\u201326. doi:10.17016/FEDS.2015.083</li>\n<li>Churchman, C. West, 1948, \u201cStatistics, Pragmatics,\nInduction\u201d, <em>Philosophy of Science</em>, 15(3):\n249\u2013268. doi:10.1086/286991</li>\n<li>Collins, Harry M., 1985, <em>Changing Order: Replication and\nInduction in Scientific Practice</em>, London; Beverly Hills: Sage\nPublications.</li>\n<li>\u2013\u2013\u2013, 2016, \u201cReproducibility of\nexperiments: experiments\u2019 regress, statistical uncertainty\nprinciple, and the replication imperative\u201d in Atmanspacher and\nMaasen 2016b: 65\u201382. doi:10.1002/9781118865064.ch4</li>\n<li>Cohen, Jacob, 1962, \u201cThe Statistical Power of\nAbnormal-Social Psychological Research: A Review\u201d,, <em>The\nJournal of Abnormal and Social Psychology</em>, 65(3): 145\u2013153.\ndoi:10.1037/h0045186</li>\n<li>\u2013\u2013\u2013, 1994, \u201cThe Earth Is Round (\\(p &lt;\n.05\\))\u201d, <em>American Psychologist</em>, 49(12): 997\u20131003,\ndoi:10.1037/0003-066X.49.12.997</li>\n<li>Cova, Florian, Brent Strickland, Angela Abatista, Aur\u00e9lien\nAllard, James Andow, Mario Attie, James Beebe, et al., forthcoming,\n\u201cEstimating the Reproducibility of Experimental\nPhilosophy\u201d, <em>Review of Philosophy and Psychology</em>, early\nonline: 14 June 2018. doi:10.1007/s13164-018-0400-9</li>\n<li>Cristea, Ioana Alina and John P. A. Ioannidis, 2018, \u201cP\nValues in Display Items Are Ubiquitous and Almost Invariably Significant: A Survey of Top Science Journals\u201d, Christos A. Ouzounis (ed.), <em>PLoS ONE</em>, 13(5): e0197440. doi:10.1371/journal.pone.0197440</li>\n<li>Cumming, Geoff, 2012, <em>Understanding the New Statistics: Effect\nSizes, Confidence Intervals, and Meta-Analysis</em>. New York:\nRoutledge.</li>\n<li>Cumming, Geoff and Robert Calin-Jageman, 2017, <em>Introduction to\nthe New Statistics: Estimation, Open Science and Beyond</em>, New\nYork: Routledge.</li>\n<li>Cumming, Geoff, Fiona Fidler, Martine Leonard, Pavel Kalinowski,\nAshton Christiansen, Anita Kleinig, Jessica Lo, Natalie McMenamin, and\nSarah Wilson, 2007, \u201cStatistical Reform in Psychology: Is\nAnything Changing?\u201d, <em>Psychological Science</em>, 18(3):\n230\u2013232. doi:10.1111/j.1467-9280.2007.01881.x</li>\n<li>Di Bucchianico, Marilena, 2014, \u201cA Matter of Phronesis:\nExperiment and Virtue in Physics, A Case Study\u201d, in <em>Virtue\nEpistemology Naturalized</em>, Abrol Fairweather (ed.), Cham: Springer\nInternational Publishing, 291\u2013312.\ndoi:10.1007/978-3-319-04672-3_17</li>\n<li>Dominus, Susan, 2017, \u201cWhen the Revolution Came for Amy\nCuddy\u201d, <em>The New York Times</em>, October 21, Sunday\nMagazine, page 29.</li>\n<li>Douglas, Heather, 2016, \u201cValues in Science\u201d, in Paul\nHumphreys, <em>The Oxford Handbook of Philosophy of Science</em>, New\nYork: Oxford University Press, pp. 609\u2013630.</li>\n<li>Earp, Brian D. and David Trafimow, 2015, \u201cReplication,\nFalsification, and the Crisis of Confidence in Social\nPsychology\u201d, <em>Frontiers in Psychology</em>, 6(May): 621.\ndoi:10.3389/fpsyg.2015.00621</li>\n<li>Errington, Timothy M., Elizabeth Iorns, William Gunn, Fraser\nElisabeth Tan, Joelle Lomax, and Brian A Nosek, 2014, \u201cAn Open\nInvestigation of the Reproducibility of Cancer Biology\nResearch\u201d, <em>ELife</em>, 3(December): e043333.\ndoi:10.7554/eLife.04333</li>\n<li>Etz, Alexander and Joachim Vandekerckhove, 2016, \u201cA Bayesian\nPerspective on the Reproducibility Project: Psychology\u201d, Daniele\nMarinazzo (ed.), <em>PLoS ONE</em>, 11(2): e0149794.\ndoi:10.1371/journal.pone.0149794</li>\n<li>Fanelli, Daniele, 2010a, \u201cDo Pressures to Publish Increase\nScientists\u2019 Bias? An Empirical Support from US States Data\u201d,\nEnrico Scalas (ed.), <em>PLoS ONE</em>, 5(4): e10271.\ndoi:10.1371/journal.pone.0010271</li>\n<li>\u2013\u2013\u2013, 2010b, \u201c\u2018Positive\u2019\nResults Increase Down the Hierarchy of the Sciences\u201d, Enrico\nScalas (ed.), <em>PLoS ONE</em>, 5(4): e10068.\ndoi:10.1371/journal.pone.0010068</li>\n<li>\u2013\u2013\u2013, 2012, \u201cNegative Results Are\nDisappearing from Most Disciplines and Countries\u201d,\n<em>Scientometrics</em>, 90(3): 891\u2013904.\ndoi:10.1007/s11192-011-0494-7</li>\n<li>Fang, Ferric C., R. Grant Steen, and Arturo Casadevall, 2012,\n\u201cMisconduct Accounts for the Majority of Retracted Scientific\nPublications\u201d, <em>Proceedings of the National Academy of\nSciences</em>, 109(42): 17028\u201317033.\ndoi:10.1073/pnas.1212247109</li>\n<li>Feest, Uljana, 2016, \u201cThe Experimenters\u2019 Regress\nReconsidered: Replication, Tacit Knowledge, and the Dynamics of\nKnowledge Generation\u201d, <em>Studies in History and Philosophy of\nScience Part A</em>, 58(August): 34\u201345.\ndoi:10.1016/j.shpsa.2016.04.003</li>\n<li>Fidler, Fiona, Mark A. Burgman, Geoff Cumming, Robert Buttrose,\nand Neil Thomason, 2006, \u201cImpact of Criticism of Null-Hypothesis\nSignificance Testing on Statistical Reporting Practices in\nConservation Biology\u201d, <em>Conservation Biology</em>, 20(5):\n1539\u20131544. doi:10.1111/j.1523-1739.2006.00525.x</li>\n<li>Fidler, Fiona, Yung En Chee, Bonnie C. Wintle, Mark A. Burgman,\nMichael A. McCarthy, and Ascelin Gordon, 2017, \u201cMetaresearch for\nEvaluating Reproducibility in Ecology and Evolution\u201d,\n<em>BioScience</em>, 67(3): 282\u2013289.\ndoi:10.1093/biosci/biw159</li>\n<li>Fiedler, Klaus and Norbert Schwarz, 2016, \u201cQuestionable\nResearch Practices Revisited\u201d, <em>Social Psychological and\nPersonality Science</em>, 7(1): 45\u201352.\ndoi:10.1177/1948550615612150</li>\n<li>Fiske, Susan T., 2016, \u201cA Call to Change Science\u2019s Culture\nof Shaming\u201d, <em>Association for Psychological Science \nObserver</em>, 29(9).\n [<a href=\"https://www.psychologicalscience.org/observer/a-call-to-change-sciences-culture-of-shaming\" target=\"other\">Fiske 2016 available online</a>]</li>\n<li>Franklin, Allan, 1989, \u201cThe Epistemology of\nExperiment\u201d, in David Gooding, Trevor Pinch, and Simon Schaffer\n(eds.), <em>The Uses of Experiment: Studies in the Natural\nSciences</em>, Cambridge: Cambridge University Press, pp.\n437\u2013460.</li>\n<li>\u2013\u2013\u2013, 1994, \u201cHow to Avoid the\nExperimenters\u2019 Regress\u201d, <em>Studies in History and Philosophy\nof Science Part A</em>, 25(3): 463\u2013491.\ndoi:10.1016/0039-3681(94)90062-0</li>\n<li>Franklin, Allan and Harry Collins, 2016, \u201cTwo Kinds of Case\nStudy and a New Agreement\u201d, in <em>The Philosophy of Historical\nCase Studies</em>, Tilman Sauer and Raphael Scholl (eds.), Cham:\nSpringer International Publishing, 319: 95\u2013121.\ndoi:10.1007/978-3-319-30229-4_6</li>\n<li>Fraser, Hannah, Tim Parker, Shinichi Nakagawa, Ashley Barnett, and\nFiona Fidler, 2018, \u201cQuestionable Research Practices in Ecology\nand Evolution\u201d, Jelte M. Wicherts (ed.), <em>PLoS ONE</em>,\n13(7): e0200303. doi:10.1371/journal.pone.0200303</li>\n<li>Freedman, Leonard P., Iain M. Cockburn, and Timothy S. Simcoe,\n2015, \u201cThe Economics of Reproducibility in Preclinical\nResearch\u201d, <em>PLoS Biology</em>, 13(6): e1002165.\ndoi:10.1371/journal.pbio.1002165</li>\n<li>Giner-Sorolla, Roger, 2012, \u201cScience or Art? How Aesthetic\nStandards Grease the Way Through the Publication Bottleneck but\nUndermine Science\u201d, <em>Perspectives on Psychological\nScience</em>, 7(6): 562\u2013571. doi:10.1177/1745691612457576</li>\n<li>Gigerenzer, Gerd, 2018, \u201cStatistical Rituals: The\nReplication Delusion and How We Got There\u201d, <em>Advances in\nMethods and Practices in Psychological Science</em>, 1(2):\n198\u2013218. doi:10.1177/2515245918771329</li>\n<li>Gilbert, Daniel T., Gary King, Stephen Pettigrew, and Timothy D.\nWilson, 2016, \u201cComment on \u2018Estimating the Reproducibility\nof Psychological Science\u2019\u201d, <em>Science</em>, 351(6277):\n1037\u20131037. doi:10.1126/science.aad7243</li>\n<li>Goldman, Alvin I., 1999, <em>Knowledge in a Social World</em>,\nOxford: Clarendon. doi:10.1093/0198238207.001.0001</li>\n<li>G\u00f3mez, Omar S., Natalia Juristo, and Sira Vegas, 2010,\n\u201cReplications Types in Experimental Disciplines\u201d, in\n<em>Proceedings of the 2010 ACM-IEEE International Symposium on\nEmpirical Software Engineering and Measurement - ESEM \u201910</em>,\nBolzano-Bozen, Italy: ACM Press. doi:10.1145/1852786.1852790</li>\n<li>Hackett, B., 2005, \u201cEssential tensions: Identity, control, and\nrisk in research\u201d, <em>Social Studies of Science</em>, 35(5):\n787\u2013826. doi:10.1177/0306312705056045</li>\n<li>Haller, Heiko, and Stefan Krauss, 2002, \u201cMisinterpretations\nof Significance: a Problem Students Share with Their Teachers?\u201d\n<em>Methods of Psychological Research\u2014Online</em>, 7(1):\n1\u201320.\n [<a href=\"https://www.researchgate.net/publication/27262211_Misinterpretations_of_Significance_A_Problem_Students_Share_with_Their_Teachers\" target=\"other\">Haller &amp; Kraus 2002 available online</a>]</li>\n<li>Hartgerink, Chris H.J., Robbie C.M. van Aert, Mich\u00e8le B.\nNuijten, Jelte M. Wicherts, and Marcel A.L.M. van Assen, 2016,\n\u201cDistributions of <i>p</i>-Values Smaller than .05 in\nPsychology: What Is Going On?\u201d, <em>PeerJ</em>, 4(April): e1935.\ndoi:10.7717/peerj.1935</li>\n<li>Hendrick, Clyde, 1991. \u201cReplication, Strict Replications,\nand Conceptual Replications: Are They Important?\u201d, in Neuliep\n1991: 41\u201349.</li>\n<li>Ioannidis, John P. A., 2005, \u201cWhy Most Published Research\nFindings Are False\u201d, <em>PLoS Medicine</em>, 2(8): e124.\ndoi:10.1371/journal.pmed.0020124</li>\n<li>Ioannidis, John P. A., Daniele Fanelli, Debbie Drake Dunne, and\nSteven N. Goodman, 2015, \u201cMeta-Research: Evaluation and\nImprovement of Research Methods and Practices\u201d, <em>PLOS\nBiology</em>, 13(10): e1002264. doi:10.1371/journal.pbio.1002264</li>\n<li>Jennions, Michael D. and Anders Pape M\u00f8ller, 2003, \u201cA\nSurvey of the Statistical Power of Research in Behavioral Ecology and\nAnimal Behavior\u201d, <em>Behavioral Ecology</em>, 14(3):\n438\u2013445. doi:10.1093/beheco/14.3.438</li>\n<li>John, Leslie K., George Loewenstein, and Drazen Prelec, 2012,\n\u201cMeasuring the Prevalence of Questionable Research Practices\nWith Incentives for Truth Telling\u201d, <em>Psychological\nScience</em>, 23(5): 524\u2013532. doi:10.1177/0956797611430953</li>\n<li>Kaiser, Jocelyn, 2018, \u201cPlan to Replicate 50 High-Impact\nCancer Papers Shrinks to Just 18\u201d, <em>Science</em>, 31 July\n2018. doi:10.1126/science.aau9619</li>\n<li>Keppel, Geoffrey, 1982, <em>Design and Analysis. A\nResearcher\u2019s Handbook</em>, second edition, Englewood Cliffs,\nNJ: Prentice-Hall.</li>\n<li>Kerr, Norbert L., 1998, \u201cHARKing: Hypothesizing After the\nResults Are Known\u201d, <em>Personality and Social Psychology\nReview</em>, 2(3): 196\u2013217. doi:10.1207/s15327957pspr0203_4</li>\n<li>Kidwell, Mallory C., Ljiljana B. Lazarevi\u0107, Erica Baranski,\nTom E. Hardwicke, Sarah Piechowski, Lina-Sophia Falkenberg, Curtis\nKennett, et al., 2016, \u201cBadges to Acknowledge Open Practices: A\nSimple, Low-Cost, Effective Method for Increasing Transparency\u201d,\nMalcolm R Macleod (ed.), <em>PLOS Biology</em>, 14(5): e1002456.\ndoi:10.1371/journal.pbio.1002456</li>\n<li>Klein, Richard A., Kate A. Ratliff, Michelangelo Vianello,\nReginald B. Adams, \u0160t\u011bp\u00e1n Bahn\u00edk, Michael J.\nBernstein, Konrad Bocian, et al., 2014, \u201cInvestigating Variation\nin Replicability: A \u2018Many Labs\u2019 Replication\nProject\u201d, <em>Social Psychology</em>, 45(3): 142\u2013152.\ndoi:10.1027/1864-9335/a000178</li>\n<li>Lakens, Daniel, Federico G. Adolfi, Casper J. Albers, Farid\nAnvari, Matthew A. J. Apps, Shlomo E. Argamon, Thom Baguley, et al.,\n2018, \u201cJustify Your Alpha\u201d, <em>Nature Human\nBehaviour</em>, 2(3): 168\u2013171.\ndoi:10.1038/s41562-018-0311-x</li>\n<li>Longino, Helen E., 1990, <em>Science as Social Knowledge: Values\nand Objectivity in Scientific Inquiry</em>, Princeton: Princeton\nUniversity Press.</li>\n<li>\u2013\u2013\u2013, 1996, \u201cCognitive and Non-Cognitive\nValues in Science: Rethinking the Dichotomy\u201d, in <em>Feminism,\nScience, and the Philosophy of Science</em>, Lynn Hankinson Nelson and\nJack Nelson (eds.), Dordrecht: Springer Netherlands, 39\u201358.\ndoi:10.1007/978-94-009-1742-2_3</li>\n<li>\u2013\u2013\u2013, 1997, \u201cFeminist Epistemology as a\nLocal Epistemology: Helen E. Longino\u201d, <em>Aristotelian Society\nSupplementary Volume</em>, 71(1): 19\u201335.\ndoi:10.1111/1467-8349.00017</li>\n<li>Lykken, David T., 1968, \u201cStatistical Significance in\nPsychological Research\u201d, <em>Psychological Bulletin</em>, 70(3,\nPt.1): 151\u2013159. doi:10.1037/h0026141</li>\n<li>Madden, Charles S., Richard W. Easley, and Mark G. Dunn, 1995,\n\u201cHow Journal Editors View Replication Research\u201d,\n<em>Journal of Advertising</em>, 24(December): 77\u201387.\ndoi:10.1080/00913367.1995.10673490</li>\n<li>Makel, Matthew C., Jonathan A. Plucker, and Boyd Hegarty, 2012,\n\u201cReplications in Psychology Research: How Often Do They Really\nOccur?\u201d, <em>Perspectives on Psychological Science</em>, 7(6):\n537\u2013542. doi:10.1177/1745691612460688</li>\n<li>MacCoun, Robert J. and Saul Perlmutter, 2017, \u201cBlind\nAnalysis as a Correction for Confirmatory Bias in Physics and in\nPsychology\u201d, in <em>Psychological Science Under Scrutiny</em>,\nScott O. Lilienfeld and Irwin D. Waldman (eds.), Hoboken, NJ: John\nWiley &amp; Sons, pp. 295\u2013322.\ndoi:10.1002/9781119095910.ch15</li>\n<li>Martin, B., 1992, \u201cScientific fraud and the power structure\nof science\u201d, <em>Prometheus</em>, 10(1):\n83\u201398. doi:10.1080/08109029208629515</li>\n<li>Masicampo, E.J. and Daniel R. Lalande, 2012, \u201cA Peculiar\nPrevalence of <i>p</i> Values Just below .05\u201d, <em>Quarterly\nJournal of Experimental Psychology</em>, 65(11): 2271\u20132279.\ndoi:10.1080/17470218.2012.711335</li>\n<li>Mahoney, Michael J., 1985, \u201cOpen Exchange and Epistemic\nProgress\u201d,, <em>American Psychologist</em>, 40(1): 29\u201339. doi:10.1037/0003-066X.40.1.29</li>\n<li>Meehl, Paul E., 1967, \u201cTheory-Testing in Psychology and\nPhysics: A Methodological Paradox\u201d, <em>Philosophy of\nScience</em>, 34(2): 103\u2013115. doi:10.1086/288135</li>\n<li>\u2013\u2013\u2013, 1978, \u201cTheoretical Risks and Tabular\nAsterisks: Sir Karl, Sir Ronald, and the Slow Progress of Soft\nPsychology\u201d, <em>Journal of Consulting and Clinical\nPsychology</em>, 46(4): 806\u2013834.\ndoi:10.1037/0022-006X.46.4.806</li>\n<li>Merton, Robert K., 1942 [1973], \u201cA Note on Science and\nTechnology in a Democratic Order\u201d, <em>Journal of Legal and\nPolitical Sociology</em>, 1(1\u20132): 115\u2013126; reprinted\nas \u201cThe Normative Structure of Science\u201d, in Robert K.\nMerton (ed.) <em>The Sociology of Science: Theoretical and Empirical\nInvestigations</em>, Chicago, IL: University of Chicago Press.</li>\n<li>Munaf\u00f2, Marcus R., Brian A. Nosek, Dorothy V. M. Bishop,\nKatherine S. Button, Christopher D. Chambers, Nathalie Percie du Sert,\nUri Simonsohn, Eric-Jan Wagenmakers, Jennifer J. Ware, and John P. A.\nIoannidis, 2017, \u201cA Manifesto for Reproducible Science\u201d,\n<em>Nature Human Behaviour</em>, 1(1): 0021.\ndoi:10.1038/s41562-016-0021</li>\n<li>Neuliep, James William (ed.), 1991, <em>Replication Research in\nthe Social Sciences</em>, (Journal of social behavior and personality;\n8: 6), Newbury Park, CA: Sage Publications.</li>\n<li>Neuliep, James W. and Rick Crandall, 1990, \u201cEditorial Bias\nAgainst Replication Research\u201d, <em>Journal of Social Behavior\nand Personality</em>, 5(4): 85\u201390</li>\n<li>Nosek, Brian A. and Dani\u00ebl Lakens, 2014, \u201cRegistered\nReports: A Method to Increase the Credibility of Published\nResults\u201d, <em>Social Psychology</em>, 45(3): 137\u2013141.\ndoi:10.1027/1864-9335/a000192</li>\n<li>Nosek, Brian A., Jeffrey R. Spies, and Matt Motyl, 2012,\n\u201cScientific Utopia: II. Restructuring Incentives and Practices\nto Promote Truth Over Publishability\u201d, <em>Perspectives on\nPsychological Science</em>, 7(6): 615\u2013631.\ndoi:10.1177/1745691612459058</li>\n<li>Nosek, B. A., G. Alter, G. C. Banks, D. Borsboom, S. D. Bowman, S.\nJ. Breckler, S. Buck, et al., 2015, \u201cPromoting an Open Research\nCulture\u201d, <em>Science</em>, 348(6242): 1422\u20131425.\ndoi:10.1126/science.aab2374,</li>\n<li>Nosek, Brian A., Charles R. Ebersole, Alexander C. DeHaven, and\nDavid T. Mellor, 2018, \u201cThe Preregistration Revolution\u201d,\n<em>Proceedings of the National Academy of Sciences</em>, 115(11):\n2600\u20132606. doi:10.1073/pnas.1708274114</li>\n<li>Nuijten, Mich\u00e8le B., Chris H. J. Hartgerink, Marcel A. L.\nM. van Assen, Sacha Epskamp, and Jelte M. Wicherts, 2016, \u201cThe\nPrevalence of Statistical Reporting Errors in Psychology\n(1985\u20132013)\u201d, <em>Behavior Research Methods</em>, 48(4):\n1205\u20131226. doi:10.3758/s13428-015-0664-2</li>\n<li>Oakes, Michael, 1986, <em>Statistical Inference: A Commentary for\nthe Social and Behavioral Sciences</em>, New York: Wiley.</li>\n<li>Open Science Collaboration (OSC), 2015, \u201cEstimating the\nReproducibility of Psychological Science\u201d, <em>Science</em>,\n349(6251): 943\u2013951. doi:10.1126/science.aac4716</li>\n<li>Oransky, Ivan, 2016, \u201cHalf of Biomedical Studies Don\u2019t\nStand up to Scrutiny and What We Need to Do about That\u201d, <em>The\nConversation</em>, 11 November 2016.\n [<a href=\"http://theconversation.com/half-of-biomedical-research-studies-dont-stand-up-to-scrutiny-and-what-we-need-to-do-about-that-45149\" target=\"other\">Oransky 2016 available online</a>]</li>\n<li>Parker, T.H., E. Main, S. Nakagawa, J. Gurevitch, F. Jarrad, and\nM. Burgman, 2016, \u201cPromoting Transparency in Conservation\nScience: Editorial\u201d, <em>Conservation Biology</em>, 30(6):\n1149\u20131150. doi:10.1111/cobi.12760</li>\n<li>Pashler, Harold and Eric-Jan Wagenmakers, 2012,\n\u201cEditors\u2019 Introduction to the Special Section on Replicability\nin Psychological Science: A Crisis of Confidence?\u201d,\n<em>Perspectives on Psychological Science</em>, 7(6): 528\u2013530.\ndoi:10.1177/1745691612465253</li>\n<li>Peng, Roger D., 2011, \u201cReproducible Research in\nComputational Science\u201d, <em>Science</em>, 334(6060):\n1226\u20131227. doi:10.1126/science.1213847</li>\n<li>\u2013\u2013\u2013, 2015, \u201cThe Reproducibility Crisis in\nScience: A Statistical Counterattack\u201d, <em>Significance</em>,\n12(3): 30\u201332. doi:10.1111/j.1740-9713.2015.00827.x</li>\n<li>Radder, Hans, 1996, <em>In And About The World: Philosophical\nStudies Of Science And Technology</em>, Albany, NY: State University\nof New York Press.</li>\n<li>\u2013\u2013\u2013, 2003, \u201cTechnology and Theory in\nExperimental Science\u201d, in Hans Radder (ed.), <em>The Philosophy\nof Scientific Experimentation</em>, Pittsburgh: University of\nPittsburgh Press, pp. 152\u2013173.</li>\n<li>\u2013\u2013\u2013, 2006, <em>The World Observed/The World\nConceived</em>, Pittsburgh, PA: University of Pittsburgh Press.</li>\n<li>\u2013\u2013\u2013, 2009, \u201cScience, Technology and the\nScience-Technology Relationship\u201d, in Anthonie Meijers (ed.),\n<em>Philosophy of Technology and Engineering Sciences</em>, Amsterdam:\nElsevier, pp. 65\u201391. doi:10.1016/B978-0-444-51667-1.50007-0</li>\n<li>\u2013\u2013\u2013, 2012, <em>The Material Realization of\nScience: From Habermas to Experimentation and Referential\nRealism</em>, Boston: Springer. doi:10.1007/978-94-007-4107-2</li>\n<li>Rauscher, Frances H., Gordon L. Shaw, and Catherine N. Ky, 1993,\n\u201cMusic and Spatial Task Performance\u201d, <em>Nature</em>,\n365(6447): 611\u2013611. doi:10.1038/365611a0</li>\n<li>Rauscher, Frances H., Gordon L. Shaw, and Katherine N. Ky, 1995,\n\u201cListening to Mozart Enhances Spatial-Temporal Reasoning:\nTowards a Neurophysiological Basis\u201d, <em>Neuroscience\nLetters</em>, 185(1): 44\u201347.\ndoi:10.1016/0304-3940(94)11221-4</li>\n<li>Ritchie, Stuart J., Richard Wiseman, and Christopher C. French,\n2012, \u201cFailing the Future: Three Unsuccessful Attempts to\nReplicate Bem\u2019s \u2018Retroactive Facilitation of Recall\u2019\nEffect\u201d, Sam Gilbert (ed.), <em>PLoS ONE</em>, 7(3): e33423.\ndoi:10.1371/journal.pone.0033423</li>\n<li>Rooney, Phyllis, 1992, \u201cOn Values in Science: Is the\nEpistemic/Non-Epistemic Distinction Useful?\u201d, <em>PSA:\nProceedings of the Biennial Meeting of the Philosophy of Science\nAssociation</em>, 1992(1): 13\u201322.\ndoi:10.1086/psaprocbienmeetp.1992.1.192740</li>\n<li>Rosenthal, Robert, 1979, \u201cThe File Drawer Problem and\nTolerance for Null Results\u201d, <em>Psychological Bulletin</em>,\n86(3): 638\u2013641. doi:10.1037/0033-2909.86.3.638</li>\n<li>\u2013\u2013\u2013, 1991, \u201cReplication in Behavioral\nResearch\u201d, in Neuliep 1991: 1\u201339.</li>\n<li>Rosnow, Ralph L. and Robert Rosenthal, 1989, \u201cStatistical\nProcedures and the Justification of Knowledge in Psychological\nScience\u201d,, <em>American Psychologist</em>, 44(10):\n1276\u20131284. doi:10.1037/0003-066X.44.10.1276</li>\n<li>Rowhani-Farid, Anisa, Michelle Allen, and Adrian G. Barnett, 2017,\n\u201cWhat Incentives Increase Data Sharing in Health and Medical\nResearch? A Systematic Review\u201d, <em>Research Integrity and Peer\nReview</em>, 2: 4. doi:10.1186/s41073-017-0028-9</li>\n<li>Rudner, Richard, 1953, \u201cThe Scientist Qua Scientist Makes\nValue Judgments\u201d, <em>Philosophy of Science</em>, 20(1):\n1\u20136. doi:10.1086/287231</li>\n<li>Sargent, C.L., 1981, \u201cThe Repeatability Of Significance And\nThe Significance Of Repeatability\u201d, <em>European Journal of\nParapsychology</em>, 3: 423\u2013433.</li>\n<li>Schekman, Randy, 2013, \u201cHow Journals like Nature, Cell and\nScience Are Damaging Science | Randy Schekman\u201d, <em>The\nGuardian</em>, December 9, sec. Opinion,\n [<a href=\"https://www.theguardian.com/commentisfree/2013/dec/09/how-journals-nature-science-cell-damage-science\" target=\"other\">Schekman 2013 available online</a>]</li>\n<li>Schmidt, Stefan, 2009, \u201cShall We Really Do It Again? The\nPowerful Concept of Replication Is Neglected in the Social\nSciences\u201d, <em>Review of General Psychology</em>, 13(2):\n90\u2013100. doi:10.1037/a0015108</li>\n<li>Silberzahn, Raphael,and Uhlmann, Eric L., 2015, \u201cMany hands\nmake tight work: crowdsourcing research can balance discussions,\nvalidate findings and better inform policy\u201d, <em>Nature</em>, 526(7572):\n189\u2013192.</li>\n<li>Simmons, Joseph P., Leif D. Nelson, and Uri Simonsohn, 2011,\n\u201cFalse-Positive Psychology: Undisclosed Flexibility in Data\nCollection and Analysis Allows Presenting Anything as\nSignificant\u201d, <em>Psychological Science</em>, 22(11):\n1359\u20131366. doi:10.1177/0956797611417632</li>\n<li>Smith, Daniel R., Ian C.W. Hardy, and Martin P. Gammell, 2011,\n\u201cPower Rangers: No Improvement in the Statistical Power of\nAnalyses Published in Animal Behaviour\u201d, <em>Animal\nBehaviour</em>, 81(1): 347\u2013352.\ndoi:10.1016/j.anbehav.2010.09.026</li>\n<li>Sovacool, B. K., 2008, \u201cExploring scientific misconduct:\nIsolated individuals, impure institutions, or an inevitable idiom of\nmodern science?\u201d <em>Journal of Bioethical Inquiry</em>, 5:\n271\u2013282. doi: 10.1007/s11673-008-9113-6</li>\n<li>Steel, Daniel, 2010, \u201cEpistemic Values and the Argument from\nInductive Risk*\u201d, <em>Philosophy of Science</em>, 77(1):\n14\u201334. doi:10.1086/650206</li>\n<li>Stegenga, Jacob, 2018, <em>Medical Nihilism</em>, Oxford: Oxford\nUniversity Press.</li>\n<li>Steinle, Friedrich, 2016, \u201cStability and Replication of\nExperimental Results: A Historical Perspective\u201d, in Atmanspacher\nand Maasen 2016b: 39\u201368. doi:10.1002/9781118865064.ch3</li>\n<li>Sterling, Theodore D., 1959, \u201cPublication Decisions and\nTheir Possible Effects on Inferences Drawn from Tests of Significance\n\u2013 or Vice Versa\u201d, <em>Journal of the American Statistical\nAssociation</em>, 54(285): 30\u201334.\ndoi:10.1080/01621459.1959.10501497</li>\n<li>Sutton, Jon, 2018, \u201cTone Deaf?\u201d, <em>The\nPsychologist</em>, 31: 12\u201313.\n [<a href=\"https://thepsychologist.bps.org.uk/volume-31/march-2018/tone-deaf\" target=\"other\">Sutton 2018 available online</a>]</li>\n<li>Szucs, Denes and John P. A. Ioannidis, 2017, \u201cEmpirical\nAssessment of Published Effect Sizes and Power in the Recent Cognitive\nNeuroscience and Psychology Literature\u201d, Eric-Jan Wagenmakers\n(ed.), <em>PLoS Biology</em>, 15(3): e2000797.\ndoi:10.1371/journal.pbio.2000797</li>\n<li>Teira, David, 2013, \u201cA Contractarian Solution to the\nExperimenter\u2019s Regress\u201d, <em>Philosophy of Science</em>,\n80(5): 709\u2013720. doi:10.1086/673717</li>\n<li>Vazire, Simine, 2018, \u201cImplications of the Credibility\nRevolution for Productivity, Creativity, and Progress\u201d,\n<em>Perspectives on Psychological Science</em>, 13(4): 411\u2013417.\ndoi:10.1177/1745691617751884</li>\n<li>Wagenmakers, Eric-Jan, Ruud Wetzels, Denny Borsboom, Han L. J. van\nder Maas, and Rogier A. Kievit, 2012, \u201cAn Agenda for Purely\nConfirmatory Research\u201d, <em>Perspectives on Psychological\nScience</em>, 7(6): 632\u2013638. doi:10.1177/1745691612463078</li>\n<li>Washburn, Anthony N., Brittany E. Hanson, Matt Motyl, Linda J.\nSkitka, Caitlyn Yantis, Kendal M. Wong, Jiaqing Sun, et al., 2018,\n\u201cWhy Do Some Psychology Researchers Resist Adopting Proposed\nReforms to Research Practices? A Description of Researchers\u2019\nRationales\u201d, <em>Advances in Methods and Practices in\nPsychological Science</em>, 1(2): 166\u2013173.\ndoi:10.1177/2515245918757427</li>\n<li>Wasserstein, Ronald L. and Nicole A. Lazar, 2016, \u201cThe\nASA\u2019s Statement on p-Values: Context, Process, and Purpose\u201d,\n<em>The American Statistician</em>, 70(2): 129\u2013133.\ndoi:10.1080/00031305.2016.1154108</li>\n</ul>\n</div>"}, "related_entries": {"entry_list": ["Bayes\u2019 Theorem", "epistemology: Bayesian", "measurement: in science", "operationalism", "science: theory and observation in", "scientific knowledge: social dimensions of", "scientific method", "scientific research and big data"], "entry_link": [{"../bayes-theorem/": "Bayes\u2019 Theorem"}, {"../epistemology-bayesian/": "epistemology: Bayesian"}, {"../measurement-science/": "measurement: in science"}, {"../operationalism/": "operationalism"}, {"../science-theory-observation/": "science: theory and observation in"}, {"../scientific-knowledge-social/": "scientific knowledge: social dimensions of"}, {"../scientific-method/": "scientific method"}, {"../science-big-data/": "scientific research and big data"}]}, "academic_tools": {"listed_text": ["<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>", "<a href=\"https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=scientific-reproducibility\" target=\"other\">How to cite this entry</a>.", "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>", "<a href=\"https://leibniz.stanford.edu/friends/preview/scientific-reproducibility/\" target=\"other\">Preview the PDF version of this entry</a> at the\n <a href=\"https://leibniz.stanford.edu/friends/\" target=\"other\">Friends of the SEP Society</a>.", "<img alt=\"inpho icon\" src=\"../../symbols/inpho.png\"/>", "<a href=\"https://www.inphoproject.org/entity?sep=scientific-reproducibility&amp;redirect=True\" target=\"other\">Look up topics and thinkers related to this entry</a>\n at the Internet Philosophy Ontology Project (InPhO).", "<img alt=\"phil papers icon\" src=\"../../symbols/pp.gif\"/>", "<a href=\"http://philpapers.org/sep/scientific-reproducibility/\" target=\"other\">Enhanced bibliography for this entry</a>\nat <a href=\"http://philpapers.org/\" target=\"other\">PhilPapers</a>, with links to its database."], "listed_links": [{"https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=scientific-reproducibility": "How to cite this entry"}, {"https://leibniz.stanford.edu/friends/preview/scientific-reproducibility/": "Preview the PDF version of this entry"}, {"https://leibniz.stanford.edu/friends/": "Friends of the SEP Society"}, {"https://www.inphoproject.org/entity?sep=scientific-reproducibility&redirect=True": "Look up topics and thinkers related to this entry"}, {"http://philpapers.org/sep/scientific-reproducibility/": "Enhanced bibliography for this entry"}, {"http://philpapers.org/": "PhilPapers"}]}, "other_internet_resources": {"listed_text": ["Barba, Lorena A., 2017, \n\u201c<a href=\"https://figshare.com/articles/Science_Reproducibility_Taxonomy/5248273\" target=\"other\">Science Reproducibility Taxonomy</a>\u201d, Presentation slides for the \n<em>2017 Workshop on Reproducibility Taxonomies for Computing and\nComputational Science</em>. ", "Kelly, Clint, 2017, \u201cRedux: Do Behavioral Ecologists\nReplicate Their Studies?\u201d, presented at Ignite Session 12,\nEcological Society of America, Portland, Oregon, 8 August.\n [<a href=\"https://eco.confex.com/eco/2017/webprogram/Paper63392.html\" target=\"other\">Kelly 2017 abstract available online</a>]", "McShane, Blakeley B., David Gal, Andrew Gelman, Christian Robert,\nand Jennifer L. Tackett, 2018, \n \u201c<a href=\"http://arxiv.org/abs/1709.07588\" target=\"other\">Abandon Statistical Significance</a>\u201d,\narXiv.org, first version 22 September 2017; latest revision, 8\nSeptember 2018.", "Schnall, Simone, 2014, \n\u201c<a href=\"https://www.psychol.cam.ac.uk/cece/blog\" target=\"other\">Social Media and the Crowd-Sourcing of Social Psychology</a>\u201d, Blog <em>Department of Psychology,\nCambridge University</em>, November 18.", "<a href=\"https://metaresearch.nl/\" target=\"other\">Tilburg University Meta-Research Center</a>", "<a href=\"https://metrics.stanford.edu/\" target=\"other\">Meta-Research Innovation Center at Stanford (METRICS)</a>", "<a href=\"http://www.barelysignificant.com/post/summersaga/\" target=\"other\">The saga of the summer 2017, a.k.a. \u2018the alpha wars\u2019</a>,\n Barely Significant blog by Ladislas Nalborczyk.", "<a href=\"http://ww2.amstat.org/meetings/ssi/2017/conferenceinfo.cfm\" target=\"other\">2017 American Statistical Association Symposium on Statistical Inference: Scientific Method for the 21<sup>st</sup> Century: A World Beyond \\(p &lt;0.05\\)</a>", "<a href=\"https://www.coursera.org/learn/statistical-inferences\" target=\"other\">Improving Your Statistical Inferences,</a>\n David Lakens, 2018, Coursera, ", "<a href=\"https://osf.io/view/StudySwap/\" target=\"other\">StudySwap: A Platform for Interlab Replication, Collaboration, and Research Resource Exchange</a>,\n Open Science Framework", "<a href=\"https://osf.io/wfc6u/\" target=\"other\">Collaborative Replications and Education Project (CREP)</a>,\n Open Science Framework", "<a href=\"https://cos.io/rr/\" target=\"other\">Registered Reports: Peer review before results are known to align scientific values and practices</a>,\n Center for Open Science"], "listed_links": [{"https://figshare.com/articles/Science_Reproducibility_Taxonomy/5248273": "Science Reproducibility Taxonomy"}, {"https://eco.confex.com/eco/2017/webprogram/Paper63392.html": "Kelly 2017 abstract available online"}, {"http://arxiv.org/abs/1709.07588": "Abandon Statistical Significance"}, {"https://www.psychol.cam.ac.uk/cece/blog": "Social Media and the Crowd-Sourcing of Social Psychology"}, {"https://metaresearch.nl/": "Tilburg University Meta-Research Center"}, {"https://metrics.stanford.edu/": "Meta-Research Innovation Center at Stanford (METRICS)"}, {"http://www.barelysignificant.com/post/summersaga/": "The saga of the summer 2017, a.k.a. \u2018the alpha wars\u2019"}, {"http://ww2.amstat.org/meetings/ssi/2017/conferenceinfo.cfm": "2017 American Statistical Association Symposium on Statistical Inference: Scientific Method for the 21st Century: A World Beyond \\(p <0.05\\)"}, {"https://www.coursera.org/learn/statistical-inferences": "Improving Your Statistical Inferences,"}, {"https://osf.io/view/StudySwap/": "StudySwap: A Platform for Interlab Replication, Collaboration, and Research Resource Exchange"}, {"https://osf.io/wfc6u/": "Collaborative Replications and Education Project (CREP)"}, {"https://cos.io/rr/": "Registered Reports: Peer review before results are known to align scientific values and practices"}]}, "tokenized_text": ["1", "replicating", "repeating", "reproducing", "scientific", "result", "starting", "point", "philosophical", "exploration", "reproducibility", "related", "notion", "consider", "conceptual", "question", "notion", "mean", "according", "eg", "cartwright", "1991", "term", "replication", "reproduction", "repetition", "denote", "distinct", "concept", "others", "use", "term", "interchangeably", "eg", "atmanspacher", "maasen", "2016a", "different", "discipline", "different", "understanding", "term", "computational", "discipline", "example", "reproducibility", "often", "refers", "ability", "reproduce", "computation", "alone", "relates", "exclusively", "sharing", "sufficiently", "annotating", "data", "code", "eg", "peng", "2011", "2015", "discipline", "replication", "describes", "redoing", "whole", "experiment", "barba", "2017", "internet", "resource", "psychology", "social", "life", "science", "however", "reproducibility", "may", "refer", "either", "redoing", "computation", "redoing", "experiment", "reproducibility", "project", "coordinated", "center", "open", "science", "redo", "entire", "study", "data", "collection", "analysis", "recent", "funding", "program", "announcement", "darpa", "u", "defense", "advanced", "research", "program", "agency", "distinguished", "reproducibility", "replicability", "former", "refers", "computational", "reproducibility", "latter", "redoing", "experiment", "use", "three", "terms", "replication", "reproduction", "repetition", "interchangeably", "unless", "explicitly", "describing", "distinction", "author", "describing", "study", "replicable", "people", "could", "mind", "either", "least", "two", "different", "thing", "first", "study", "replicable", "principle", "sense", "carried", "particularly", "method", "procedure", "analysis", "described", "sufficiently", "detailed", "transparent", "way", "second", "study", "replicable", "sense", "carried", "happens", "replication", "study", "successfully", "produce", "sufficiently", "similar", "result", "original", "study", "may", "replicable", "former", "sense", "second", "sense", "one", "might", "able", "replicate", "method", "procedure", "analysis", "study", "fail", "successfully", "replicate", "result", "original", "study", "similarly", "people", "talk", "replication", "could", "also", "mind", "two", "different", "thing", "replication", "method", "procedure", "analysis", "study", "irrespective", "result", "alternatively", "replication", "method", "procedure", "analysis", "well", "result", "arguably", "typology", "replication", "make", "le", "finegrained", "distinction", "direct", "replication", "closely", "follow", "original", "study", "verify", "result", "conceptual", "replication", "deliberately", "alter", "important", "feature", "study", "generalize", "finding", "test", "underlying", "hypothesis", "new", "way", "suggested", "distinction", "may", "always", "known", "term", "example", "roughly", "distinction", "referred", "exact", "inexact", "replication", "keppel", "1982", "concrete", "conceptual", "replication", "sargent", "1981", "literal", "operational", "constructive", "replication", "lykken", "1968", "computational", "reproducibility", "often", "direct", "reproducing", "particular", "analysis", "outcome", "data", "set", "using", "code", "software", "also", "conceptual", "analysing", "raw", "data", "set", "alternative", "approach", "different", "model", "statistical", "framework", "example", "conceptual", "computational", "reproducibility", "study", "see", "silberzahn", "uhlmann", "2015", "attempt", "resolve", "disciplinary", "difference", "create", "new", "typology", "replication", "instead", "provide", "limited", "snapshot", "conceptual", "terrain", "surveying", "three", "existing", "typologiesfrom", "stefan", "schmidt", "2009", "omar", "g\u00f3mez", "natalia", "juristo", "sira", "vega", "2010", "han", "radder", "schmidt", "account", "influential", "widelycited", "psychology", "social", "science", "replication", "crisis", "literature", "heavily", "concentrated", "g\u00f3mez", "juristo", "vega", "2010", "typology", "replication", "based", "multidisciplinary", "survey", "18", "scholarly", "classification", "replication", "study", "collectively", "contain", "79", "type", "replication", "finally", "radder", "1996", "2003", "2006", "2009", "2012", "typology", "perhaps", "best", "known", "within", "philosophy", "science", "11", "account", "social", "science", "schmidt", "outline", "five", "function", "replication", "study", "social", "science", "function", "1", "controlling", "sampling", "errorthat", "verify", "previous", "result", "sample", "obtained", "purely", "chance", "outcome", "paint", "distorted", "picture", "reality", "function", "2", "controlling", "artifact", "internal", "validity", "that", "ensuring", "experimental", "result", "proper", "test", "hypothesis", "ie", "internal", "validity", "reflect", "unintended", "flaw", "study", "design", "measurement", "result", "say", "artifact", "faulty", "thermometer", "rather", "actual", "change", "substance", "temperature", "function", "3", "controlling", "fraud", "function", "4", "enabling", "generalizability", "function", "5", "enabling", "verification", "underlying", "hypothesis", "modifying", "hendrik", "1991", "class", "variable", "define", "research", "space", "schmidt", "2009", "present", "four", "class", "variable", "may", "altered", "held", "constant", "order", "given", "replication", "study", "fulfil", "one", "function", "four", "class", "class", "1", "information", "conveyed", "participant", "example", "task", "instruction", "class", "2", "context", "background", "large", "class", "variable", "includes", "participant", "characteristic", "eg", "age", "gender", "specific", "history", "physical", "setting", "research", "characteristic", "experimenter", "incidental", "characteristic", "material", "eg", "type", "font", "colour", "room", "class", "3", "participant", "recruitment", "including", "selection", "participant", "allocation", "condition", "experimental", "control", "condition", "class", "4", "dependent", "variable", "measure", "schmidt", "term", "procedure", "constitution", "dependent", "variable", "2009", "93", "schmidt", "systematically", "work", "example", "function", "achieved", "altering", "andor", "holding", "different", "class", "class", "variable", "constant", "example", "fulfil", "function", "controlling", "sampling", "error", "function", "1", "one", "alter", "variable", "regarding", "participant", "recruitment", "class", "3", "attempting", "keep", "variable", "class", "close", "original", "study", "possible", "control", "artefact", "function", "2", "one", "alter", "variable", "concerning", "context", "dependent", "variable", "measure", "variable", "class", "2", "4", "respectively", "keep", "variable", "1", "3", "information", "conveyed", "participant", "participant", "recruitment", "close", "original", "possible", "schmidt", "like", "author", "area", "acknowledges", "practical", "limit", "able", "hold", "else", "constant", "controlling", "fraud", "function", "3", "served", "arrangement", "controlling", "artefact", "function", "2", "schmidt", "account", "controlling", "sampling", "error", "artefact", "fraud", "function", "1", "3", "connected", "theme", "confirming", "result", "original", "study", "function", "4", "5", "go", "beyond", "thisgeneralizing", "new", "population", "function", "4", "served", "change", "participant", "recruitment", "class", "3", "confirming", "underlying", "hypothesis", "function", "5", "served", "change", "information", "conveyed", "context", "dependant", "variable", "measure", "class", "1", "2", "4", "respectively", "change", "participant", "recruitment", "class", "3", "although", "schmidt", "acknowledges", "holding", "latter", "class", "variable", "constant", "whilst", "varying", "everything", "else", "often", "practically", "impossible", "attempt", "enable", "verification", "underlying", "research", "hypothesis", "ie", "fulfil", "function", "5", "alone", "schmidt", "classifies", "conceptual", "replication", "following", "rosenthal", "1991", "attempt", "fulfil", "four", "function", "considered", "variant", "direct", "replication", "summary", "schmidt", "direct", "replication", "control", "sampling", "error", "artifact", "fraud", "provide", "information", "reliability", "validity", "prior", "empirical", "work", "conceptual", "replication", "help", "corroborate", "underlying", "theory", "substantive", "opposed", "statistical", "hypothesis", "question", "extent", "generalize", "new", "circumstance", "situation", "practice", "direct", "conceptual", "replication", "lie", "continuum", "replication", "study", "varying", "le", "compared", "original", "potentially", "great", "number", "dimension", "12", "interdisciplinary", "account", "g\u00f3mez", "juristo", "vega", "2010", "survey", "literature", "18", "discipline", "identified", "79", "type", "replication", "considered", "entirely", "distinct", "identify", "five", "main", "way", "replication", "study", "may", "diverge", "initial", "study", "similarity", "schimdt", "four", "class", "site", "spatial", "location", "replication", "experiment", "replication", "experiment", "may", "conducted", "location", "site", "initial", "study", "experimenter", "conducting", "replication", "may", "exclusively", "original", "exclusively", "different", "combination", "new", "original", "experimenter", "apparatus", "including", "design", "material", "instrument", "important", "experimental", "object", "andor", "procedure", "may", "vary", "original", "replication", "study", "operationalisations", "employed", "may", "differ", "operationalisation", "refers", "measurement", "variable", "example", "psychology", "might", "include", "using", "two", "different", "scale", "measuring", "depression", "dependent", "variable", "finally", "study", "may", "vary", "population", "property", "change", "one", "combination", "element", "replication", "study", "corresponds", "different", "purpose", "underlying", "study", "thereby", "establishes", "different", "kind", "validity", "like", "schmidt", "et", "al", "systematically", "work", "change", "work", "fulfil", "different", "epistemic", "function", "function", "1", "conclusion", "validity", "controlling", "sampling", "error", "five", "element", "unchanged", "replication", "study", "purpose", "replication", "control", "sampling", "error", "verify", "previous", "result", "sample", "obtained", "purely", "chance", "outcome", "make", "sample", "misleading", "unrepresentative", "provides", "safeguard", "known", "type", "error", "incorrectly", "failing", "reject", "null", "hypothesis", "hypothesis", "relationship", "two", "phenomenon", "investigation", "study", "establish", "conclusion", "validity", "credibility", "believability", "observed", "relationship", "phenomenon", "function", "2", "internal", "validity", "controlling", "artefactual", "result", "replication", "study", "differs", "respect", "site", "experimenter", "apparatus", "purpose", "establish", "previously", "observed", "result", "artefact", "particular", "apparatus", "lab", "study", "establish", "internal", "validity", "extent", "result", "attributed", "experimental", "manipulation", "rather", "extraneous", "variable", "function", "3", "construct", "validity", "determining", "limit", "operationalizations", "replication", "study", "differs", "respect", "operationalisations", "purpose", "determine", "extent", "effect", "generalizes", "across", "measure", "manipulated", "dependent", "variable", "eg", "extent", "effect", "depend", "particular", "psychometric", "test", "one", "us", "evaluate", "depression", "iq", "study", "fulfil", "function", "establishing", "construct", "validity", "provide", "evidence", "effect", "hold", "across", "different", "way", "measuring", "construct", "function", "4", "external", "validity", "determining", "limit", "population", "property", "replication", "study", "differs", "respect", "population", "property", "purpose", "ascertain", "extent", "result", "generalizable", "different", "population", "population", "g\u00f3mez", "juristo", "vega", "view", "concern", "subject", "experimental", "object", "program", "study", "reinforce", "external", "validitythe", "extent", "result", "generalizable", "different", "population", "13", "philosophical", "account", "radder", "1996", "2003", "2006", "2009", "2012", "distinguishes", "three", "type", "reproducibility", "one", "reproducibility", "radder", "call", "experiment", "material", "realization", "using", "one", "radder", "example", "illustration", "two", "people", "may", "carry", "action", "measure", "mass", "object", "despite", "action", "person", "regard", "measuring", "object", "newtonian", "mass", "person", "b", "regard", "measuring", "object", "einsteinian", "mass", "action", "material", "realization", "experimental", "procedure", "reproduced", "theoretical", "description", "significance", "differ", "radder", "however", "specify", "required", "one", "material", "realisation", "reproduction", "another", "pertinent", "question", "especially", "since", "radder", "affirms", "reproduction", "exactly", "reproduction", "1996", "8283", "second", "type", "reproducibility", "reproducibility", "experiment", "given", "fixed", "theoretical", "description", "example", "social", "scientist", "might", "conduct", "two", "experiment", "examine", "social", "conformity", "one", "experiment", "young", "child", "might", "instructed", "give", "answer", "question", "group", "child", "unknown", "former", "child", "instructed", "give", "wrong", "answer", "question", "another", "experiment", "adult", "might", "instructed", "give", "answer", "question", "group", "adult", "unknown", "former", "adult", "instructed", "give", "wrong", "answer", "question", "child", "adult", "give", "wrong", "answer", "conforms", "answer", "others", "social", "scientist", "might", "interpret", "result", "exemplifying", "social", "conformity", "radder", "theoretical", "description", "experiment", "might", "fixed", "specifying", "people", "participant", "surroundings", "give", "intentionally", "false", "answer", "question", "genuine", "participant", "conform", "behaviour", "peer", "however", "material", "realization", "experiment", "differs", "insofar", "one", "concern", "child", "adult", "difficult", "see", "example", "least", "differs", "either", "schmidt", "g\u00f3mez", "juristo", "vega", "would", "refer", "establishing", "generalizability", "different", "population", "schmidt", "2009", "class", "3", "function", "5", "g\u00f3mez", "juristo", "vega", "2010", "way", "5", "function", "4", "third", "kind", "reproducibility", "radder", "call", "replicability", "experimental", "procedure", "differ", "produce", "experimental", "result", "otherwise", "known", "successful", "replication", "example", "radder", "note", "multiple", "experiment", "might", "obtain", "result", "fluid", "type", "f", "boiling", "point", "b", "despite", "different", "kind", "thermometer", "measure", "boiling", "point", "2006", "113114", "schmidt", "2009", "point", "difference", "radder", "second", "third", "type", "reproducibility", "small", "comparison", "difference", "first", "type", "consequently", "suggests", "alternative", "distinction", "direct", "conceptual", "replication", "presumably", "intending", "conceptual", "replication", "cover", "radder", "second", "third", "type", "summary", "whilst", "g\u00f3mez", "juristo", "vega", "typology", "draw", "distinction", "slightly", "different", "place", "schmidt", "purpose", "arguably", "sameto", "explain", "type", "alteration", "replication", "study", "fulfil", "different", "scientific", "goal", "establishing", "internal", "validity", "extent", "generalization", "exception", "discussion", "reproducing", "material", "realization", "radder", "two", "category", "perhaps", "seen", "fitting", "within", "larger", "range", "function", "described", "schmidt", "g\u00f3mez", "et", "al", "acknowledge", "practice", "direct", "conceptual", "replication", "lie", "noisy", "continuum", "2", "metascience", "establishing", "monitoring", "evaluating", "reproducibility", "crisis", "psychology", "origin", "reproducibility", "crisis", "often", "linked", "daryl", "bem", "2011", "paper", "reported", "empirical", "evidence", "existence", "psi", "otherwise", "known", "extra", "sensory", "perception", "esp", "paper", "passed", "standard", "peer", "review", "process", "published", "high", "impact", "journal", "personality", "social", "psychology", "controversial", "nature", "finding", "inspired", "three", "independent", "replication", "study", "failed", "reproduce", "bem", "result", "however", "replication", "study", "rejected", "four", "different", "journal", "including", "journal", "originally", "published", "bem", "study", "ground", "replication", "original", "novel", "research", "eventually", "published", "plo", "one", "ritchie", "wiseman", "french", "2012", "created", "controversy", "field", "interpreted", "many", "demonstrating", "publication", "bias", "impeded", "science", "selfcorrection", "mechanism", "medicine", "origin", "crisis", "often", "attributed", "ioannidis", "2005", "paper", "published", "finding", "false", "paper", "offered", "formal", "argument", "inflated", "rate", "false", "positive", "literaturewhere", "false", "positive", "result", "claim", "relationship", "exists", "phenomenon", "fact", "eg", "claim", "consuming", "drug", "correlated", "symptom", "relief", "fact", "ioannidis", "2005", "also", "reported", "low", "11", "empirical", "reproducibility", "rate", "set", "preclinical", "trial", "replication", "amgen", "later", "independently", "published", "begley", "elli", "2012", "discipline", "replication", "crisis", "also", "generally", "linked", "earlier", "criticism", "null", "hypothesis", "significance", "testing", "eg", "szucs", "ioannidis", "2017", "pointed", "neglect", "statistical", "power", "eg", "cohen", "1962", "1994", "failure", "adequately", "distinguish", "statistical", "substantive", "hypothesis", "eg", "meehl", "1967", "1978", "discussed", "response", "event", "new", "field", "identifying", "metascience", "metaresearch", "become", "established", "last", "decade", "munaf\u00f2", "et", "al", "2017", "munaf\u00f2", "et", "al", "define", "metascience", "scientific", "study", "science", "2017", "1", "october", "2015", "ioannidis", "fanelli", "dunne", "goodman", "identified", "800", "metascience", "paper", "published", "fivemonth", "period", "january", "may", "year", "estimated", "relevant", "literature", "accruing", "rate", "approximately", "2000", "paper", "year", "referring", "body", "work", "slightly", "different", "term", "ioannidis", "et", "al", "define", "metaresearch", "evolving", "scientific", "discipline", "aim", "evaluate", "improve", "research", "practice", "includes", "thematic", "area", "method", "reporting", "reproducibility", "evaluation", "incentive", "report", "verify", "correct", "reward", "science", "2015", "1", "multiple", "research", "centre", "dedicated", "work", "exist", "including", "example", "tilburg", "university", "metaresearch", "center", "psychology", "metaresearch", "innovation", "center", "stanford", "metric", "others", "listed", "ioannidis", "et", "al", "2015", "see", "internet", "resource", "relevant", "research", "medical", "field", "also", "covered", "stegenga", "2018", "project", "selfidentify", "metascience", "metaresearch", "include", "large", "crowdsourced", "direct", "close", "replication", "project", "reproducibility", "project", "psychology", "osc", "2015", "cancer", "biology", "errington", "et", "al", "2014", "many", "lab", "project", "psychology", "eg", "klein", "et", "al", "2014", "computational", "reproducibility", "project", "redoing", "analysis", "using", "original", "data", "set", "eg", "chang", "li", "2015", "bibliographic", "study", "documenting", "extent", "publication", "bias", "different", "scientific", "field", "change", "time", "eg", "fanelli", "2010a", "2010b", "2012", "survey", "use", "questionable", "research", "practice", "qrps", "amongst", "researcher", "impact", "publication", "literature", "eg", "john", "loewenstein", "prelec", "2012", "fiedler", "schwarz", "2016", "agnoli", "et", "al", "2017", "fraser", "et", "al", "2018", "survey", "completeness", "correctness", "transparency", "method", "analysis", "reporting", "scientific", "journal", "eg", "nuijten", "et", "al", "2016", "bakker", "wicherts", "2011", "cumming", "et", "al", "2007", "fidler", "et", "al", "2006", "survey", "interview", "study", "researcher", "understanding", "core", "methodological", "statistical", "concept", "real", "perceived", "obstacle", "improving", "practice", "bakker", "et", "al", "2016", "washburn", "et", "al", "2018", "allen", "dorozenko", "robert", "2016", "evaluation", "incentive", "change", "behaviour", "thereby", "improving", "reproducibility", "encouraging", "open", "practice", "eg", "kidwell", "et", "al", "2016", "21", "reproducibility", "project", "well", "known", "project", "undoubtedly", "reproducibility", "project", "psychology", "coordinated", "center", "open", "science", "charlottesville", "va", "open", "science", "collaboration", "involved", "270", "crowd", "sourced", "researcher", "64", "different", "institution", "11", "different", "country", "researcher", "attempted", "direct", "replication", "100", "study", "published", "three", "leading", "psychology", "journal", "year", "2008", "study", "replicated", "replication", "attempted", "follow", "original", "protocol", "closely", "possible", "though", "difference", "unavoidable", "eg", "replication", "study", "done", "european", "sample", "original", "study", "used", "u", "sample", "almost", "case", "replication", "study", "used", "larger", "sample", "size", "original", "study", "therefore", "greater", "statistical", "powerthat", "greater", "probability", "correctly", "rejecting", "null", "hypothesis", "ie", "relationship", "exists", "hypothesis", "false", "number", "measure", "reproducibility", "reported", "proportion", "study", "match", "statistical", "significance", "original", "replication", "statistical", "significance", "result", "probability", "would", "occur", "given", "null", "hypothesis", "p", "value", "common", "measure", "probability", "replication", "study", "original", "study", "would", "match", "statistical", "significance", "example", "specified", "probability", "original", "replication", "result", "occurring", "given", "null", "hypothesis", "le", "5", "ie", "p", "value", "result", "study", "005", "thirty", "nine", "percent", "36", "result", "successful", "reproduced", "according", "measure", "proportion", "study", "effect", "size", "e", "replication", "study", "fell", "within", "95", "confidence", "interval", "ci", "original", "e", "represents", "strength", "relationship", "phenomenaa", "toy", "example", "strongly", "consumption", "drug", "correlated", "symptom", "reliefand", "confidence", "interval", "provides", "indication", "probability", "e", "replication", "study", "close", "e", "original", "study", "forty", "seven", "percent", "47", "result", "successfully", "reproduced", "according", "measure", "correlation", "original", "e", "replication", "e", "replication", "study", "es", "roughly", "half", "size", "original", "es", "proportion", "study", "subjective", "rating", "independent", "researcher", "indicated", "match", "replication", "original", "thirty", "nine", "percent", "39", "considered", "successful", "reproduction", "according", "measure", "closeness", "figure", "measure", "1", "suggests", "raters", "relied", "heavily", "p", "value", "making", "judgement", "objection", "implementation", "interpretation", "project", "notably", "gilbert", "et", "al", "2016", "took", "issue", "extent", "replication", "study", "indeed", "direct", "replication", "example", "gilbert", "et", "al", "highlighted", "6", "specific", "example", "low", "fidelity", "protocol", "replication", "study", "differed", "view", "substantially", "original", "one", "case", "using", "european", "sample", "rather", "u", "sample", "participant", "however", "anderson", "et", "al", "2016", "explained", "reply", "half", "case", "author", "original", "study", "endorsed", "replication", "direct", "close", "relevant", "dimension", "furthermore", "independently", "rated", "similarity", "original", "replication", "study", "failed", "predict", "replication", "success", "others", "eg", "etz", "vandekerckhove", "2016", "applied", "bayesian", "reanalysis", "osc", "2015", "data", "conclude", "75", "opposed", "osc", "3647", "replication", "could", "considered", "successful", "however", "note", "many", "case", "weak", "evidence", "ie", "bayes", "factor", "10", "conclude", "failure", "reproduce", "many", "effect", "indeed", "explained", "overestimation", "effect", "size", "product", "publication", "bias", "reproducibility", "project", "cancer", "biology", "also", "coordinated", "center", "open", "science", "currently", "underway", "errington", "et", "al", "2014", "originally", "attempting", "replicate", "50", "highest", "impact", "study", "cancer", "biology", "published", "20102012", "project", "recently", "announced", "complete", "18", "replication", "study", "original", "reported", "enough", "information", "proceed", "full", "replication", "kaiser", "2018", "result", "first", "10", "study", "reportedly", "mixed", "5", "considered", "mostly", "repeatable", "kaiser", "2018", "many", "lab", "project", "klein", "et", "al", "2014", "coordinated", "36", "independent", "replication", "13", "classic", "psychology", "phenomenon", "12", "study", "one", "study", "tested", "two", "effect", "including", "anchoring", "sunk", "cost", "bias", "priming", "amongst", "wellknown", "effect", "psychology", "term", "matching", "statistical", "significance", "project", "demonstrated", "11", "13", "effect", "could", "successful", "replicated", "also", "showed", "great", "variation", "many", "effect", "size", "across", "36", "replication", "biomedical", "research", "also", "number", "large", "scale", "reproducibility", "project", "early", "one", "begley", "elli", "2012", "discussed", "earlier", "ioannidis", "2005", "attempted", "replicate", "56", "landmark", "preclinical", "trial", "reported", "alarming", "reproducibility", "rate", "11", "6", "56", "result", "could", "successfully", "reproduced", "subsequent", "attempt", "large", "scale", "replication", "field", "produced", "optimistic", "estimate", "routinely", "failed", "successfully", "reproduce", "half", "published", "result", "freedman", "et", "al", "2015", "report", "five", "replication", "project", "independent", "group", "researcher", "produce", "reproducibility", "estimate", "ranging", "22", "49", "estimate", "cost", "irreproducible", "research", "u", "biomedical", "science", "alone", "order", "usd", "28", "billion", "per", "year", "reproducibility", "project", "experimental", "philosophy", "exception", "general", "trend", "reporting", "reproducibility", "rate", "70", "cova", "et", "al", "forthcoming", "finally", "social", "science", "replication", "project", "ssrp", "redid", "21", "experimental", "social", "science", "study", "published", "journal", "nature", "science", "2010", "2015", "depending", "measure", "taken", "replication", "success", "rate", "5767", "camerer", "et", "al", "2018", "22", "publication", "bias", "low", "statistical", "power", "inflated", "false", "positive", "rate", "cause", "irreproducible", "result", "largely", "across", "discipline", "mentioned", "surprising", "given", "stem", "problem", "statistical", "method", "publishing", "practice", "incentive", "structure", "created", "publish", "perish", "research", "culture", "largely", "shared", "least", "life", "behavioral", "science", "whilst", "replication", "often", "casually", "referred", "cornerstone", "scientific", "method", "direct", "replication", "study", "might", "understood", "schmidt", "g\u00f3mez", "juristo", "vega", "typology", "rare", "event", "published", "literature", "scientific", "discipline", "notably", "life", "social", "science", "example", "replication", "attempt", "constitute", "roughly", "1", "published", "psychology", "literature", "makel", "plucker", "hegarty", "2012", "proportion", "published", "ecology", "evolution", "literature", "even", "smaller", "kelly", "2017", "internet", "resource", "virtual", "absence", "replication", "study", "literature", "explained", "fact", "many", "scientific", "journal", "historically", "explicit", "policy", "publishing", "replication", "study", "mahoney", "1985", "thus", "giving", "rise", "publication", "bias", "70", "editor", "79", "social", "science", "journal", "said", "preferred", "new", "study", "replication", "90", "said", "would", "encourage", "submission", "replication", "study", "neuliep", "crandall", "1990", "addition", "many", "science", "funding", "body", "also", "fund", "novel", "original", "andor", "groundbreaking", "research", "schmidt", "2009", "second", "type", "publication", "bias", "also", "played", "substantial", "role", "reproducibility", "crisis", "namely", "bias", "towards", "statistically", "significant", "positive", "result", "unlike", "bias", "replication", "study", "rarely", "explicitly", "stated", "policy", "journal", "publication", "bias", "towards", "statistically", "significant", "finding", "long", "history", "first", "documented", "psychology", "sterling", "1959", "development", "text", "mining", "technique", "led", "comprehensive", "estimate", "example", "fanelli", "work", "demonstrated", "extent", "publication", "bias", "various", "discipline", "proportion", "statistically", "significant", "result", "given", "2010a", "paper", "also", "documented", "increase", "bias", "time", "2012", "explored", "cause", "bias", "including", "relationship", "publication", "bias", "publish", "perish", "research", "culture", "2010b", "many", "discipline", "eg", "psychology", "psychiatry", "material", "science", "pharmacology", "toxicology", "clinical", "medicine", "biology", "biochemistry", "economics", "business", "microbiology", "genetics", "proportion", "statistically", "significant", "result", "high", "close", "exceeding", "90", "fanelli", "2010a", "despite", "fact", "many", "field", "average", "statistical", "power", "lowthat", "average", "probability", "study", "correctly", "reject", "null", "hypothesis", "low", "example", "psychology", "proportion", "published", "result", "statistically", "significant", "92", "despite", "fact", "average", "power", "study", "field", "detect", "medium", "effect", "size", "arguably", "typical", "discipline", "roughly", "44", "szucs", "ioannidis", "2017", "bias", "towards", "publishing", "statistically", "significant", "result", "proportion", "significant", "result", "roughly", "match", "average", "statistical", "power", "discipline", "excess", "statistical", "significance", "case", "difference", "92", "44", "therefore", "indicator", "strength", "bias", "second", "example", "ecology", "environment", "plant", "animal", "science", "proportion", "statistically", "significant", "result", "74", "78", "respectively", "admittedly", "lower", "psychology", "however", "recent", "estimate", "statistical", "power", "medium", "effect", "size", "ecology", "animal", "behaviour", "2326", "smith", "hardy", "gammell", "2011", "earlier", "optimistic", "assessment", "4047", "jennions", "m\u00f8ller", "2003", "third", "example", "proportion", "statistically", "significant", "result", "neuroscience", "behaviour", "85", "best", "estimate", "statistical", "power", "neuroscience", "best", "31", "lower", "bound", "estimate", "8", "button", "et", "al", "2013", "associated", "filedrawer", "problem", "rosenthal", "1979", "where", "researcher", "relegate", "failed", "statistically", "nonsignificant", "study", "file", "drawer", "hidden", "public", "viewhas", "long", "established", "psychology", "others", "discipline", "known", "lead", "distortion", "metaanalysis", "metaanalysis", "study", "analysis", "result", "across", "multiple", "study", "23", "questionable", "research", "practice", "addition", "creating", "filedrawer", "problem", "described", "publication", "bias", "held", "least", "partially", "responsible", "high", "prevalence", "questionable", "research", "practice", "qrps", "uncovered", "selfreport", "survey", "research", "john", "loewenstein", "prelec", "2012", "agnoli", "2017", "et", "al", "2017", "fraser", "et", "al", "2018", "journal", "study", "detected", "example", "unusual", "distribution", "p", "value", "masicampo", "lalande", "2012", "hartgerink", "et", "al", "2016", "pressure", "publish", "ubiquitous", "across", "academic", "institution", "mean", "researcher", "often", "afford", "simply", "assign", "failed", "statistically", "nonsignificant", "study", "file", "drawer", "instead", "p", "hack", "cherrypick", "result", "discussed", "back", "significance", "back", "published", "literature", "simmons", "nelson", "simonsohn", "2011", "explained", "demonstrated", "simulated", "result", "engaging", "practice", "inflates", "false", "positive", "error", "rate", "published", "literature", "leading", "lower", "rate", "reproducible", "result", "p", "hacking", "refers", "set", "practice", "include", "checking", "statistical", "significance", "result", "deciding", "whether", "collect", "data", "stopping", "data", "collection", "early", "result", "reached", "statistical", "significance", "deciding", "whether", "exclude", "data", "point", "eg", "outlier", "checking", "impact", "statistical", "significance", "reporting", "impact", "data", "exclusion", "adjusting", "statistical", "model", "instance", "including", "excluding", "covariates", "based", "resulting", "strength", "main", "effect", "interest", "rounding", "p", "value", "meet", "statistical", "significance", "threshold", "eg", "presenting", "0053", "p", "05", "cherry", "picking", "includes", "failing", "report", "dependent", "response", "variable", "relationship", "reach", "statistical", "significance", "threshold", "andor", "failing", "report", "condition", "treatment", "reach", "statistical", "significance", "threshold", "harking", "hypothesising", "result", "known", "includes", "presenting", "ad", "hoc", "andor", "unexpected", "finding", "though", "predicted", "along", "kerr", "1998", "presenting", "exploratory", "work", "though", "confirmatory", "hypothesis", "testing", "wagenmakers", "et", "al", "2012", "five", "widespread", "qrps", "listed", "table", "1", "fraser", "et", "al", "2018", "associated", "survey", "measure", "prevalence", "table", "1", "prevalence", "common", "questionable", "research", "practice", "percentage", "95", "confidence", "interval", "research", "reported", "used", "qrp", "least", "adapted", "fraser", "et", "al", "2018", "questionable", "research", "practice", "psychology", "italy", "agnoli", "et", "al", "2017", "psychology", "usa", "john", "loewenstein", "prelec", "2012", "ecology", "fraser", "et", "al", "2018", "evolution", "fraser", "et", "al", "2018", "reporting", "response", "outcome", "variable", "failed", "reach", "statistical", "significance", "479", "413546", "634", "591677", "641", "591689", "637", "572697", "collecting", "data", "inspecting", "whether", "result", "statistically", "significant", "532", "466597", "559", "515603", "369", "324420", "507", "439576", "roundingoff", "p", "value", "quantity", "meet", "prespecified", "threshold", "222", "167277", "220", "184257", "273", "231320", "175", "131230", "deciding", "exclude", "data", "point", "first", "checking", "impact", "statistical", "significance", "397", "333462", "382", "339426", "240", "199286", "239", "185302", "reporting", "unexpected", "finding", "predicted", "start", "374", "310439", "270", "231309", "485", "436536", "542", "477606", "cherry", "picking", "p", "hacking", "harking", "24", "overreliance", "null", "hypothesis", "significance", "testing", "null", "hypothesis", "significance", "testing", "nhst", "discussed", "aboveis", "commonly", "diagnosed", "cause", "current", "replication", "crisis", "see", "szucs", "ioannidis", "2017", "ubiquitous", "nature", "nhst", "life", "behavioural", "science", "well", "documented", "recently", "cristea", "ioannidis", "2018", "important", "precondition", "establishing", "role", "cause", "since", "could", "cause", "actual", "use", "rare", "dichotomous", "nature", "nhst", "facilitates", "publication", "bias", "meehl", "1967", "1978", "example", "language", "accept", "reject", "hypothesis", "testing", "map", "conveniently", "acceptance", "rejection", "manuscript", "fact", "led", "rosnow", "rosenthal", "1989", "decry", "surely", "god", "love", "06", "nearly", "much", "05", "1989", "1277", "technique", "failed", "enshrine", "dichotomous", "threshold", "would", "harder", "employ", "service", "publication", "bias", "example", "case", "made", "estimation", "using", "effect", "size", "confidence", "interval", "introduced", "would", "le", "prone", "used", "service", "publication", "bias", "cumming", "2012", "cumming", "calinjageman", "2017", "already", "mentioned", "average", "statistical", "power", "various", "discipline", "low", "power", "often", "low", "virtually", "never", "reported", "le", "10", "published", "study", "psychology", "report", "statistical", "power", "even", "fewer", "ecology", "fidler", "et", "al", "2006", "explanation", "widespread", "neglect", "statistical", "power", "often", "highlight", "many", "common", "misconception", "fallacy", "associated", "p", "value", "eg", "haller", "krauss", "2002", "gigerenzer", "2018", "example", "inverse", "probability", "fallacy", "1", "used", "explain", "many", "researcher", "fail", "calculate", "report", "statistical", "power", "oakes", "1986", "2017", "group", "72", "author", "proposed", "nature", "human", "behaviour", "paper", "alpha", "level", "statistical", "significance", "testing", "lowered", "0005", "opposed", "current", "standard", "005", "improve", "reproducibility", "rate", "published", "research", "benjamin", "et", "al", "2018", "reply", "different", "set", "88", "author", "published", "journal", "arguing", "proposal", "stating", "instead", "researcher", "justify", "alpha", "level", "based", "context", "lakens", "et", "al", "2018", "several", "reply", "followed", "including", "call", "andrew", "gelman", "colleague", "abandon", "statistical", "significance", "altogether", "mcshane", "et", "al", "2018", "internet", "resource", "exchange", "become", "known", "social", "medium", "alpha", "war", "eg", "barely", "significant", "blog", "internet", "resource", "independently", "american", "statistical", "association", "released", "statement", "use", "p", "value", "first", "time", "history", "cautioning", "overinterpretation", "pointing", "limit", "information", "offer", "replication", "wasserman", "lazar", "2016", "devoted", "association", "2017", "annual", "convention", "theme", "scientific", "method", "21st", "century", "world", "beyond", "p", "005", "see", "internet", "resource", "25", "scientific", "fraud", "number", "recent", "highprofile", "case", "scientific", "fraud", "contributed", "considerably", "amount", "press", "around", "reproducibility", "crisis", "science", "often", "case", "eg", "diederik", "stapel", "psychology", "used", "hook", "medium", "coverage", "even", "though", "crisis", "little", "scientific", "fraud", "note", "also", "questionable", "research", "practice", "typically", "counted", "fraud", "even", "scientific", "misconduct", "despite", "ethically", "dubious", "status", "example", "fang", "grant", "steen", "casadevall", "2012", "estimated", "43", "retracted", "article", "biomedical", "research", "withdrawn", "fraud", "however", "roughly", "half", "million", "biomedical", "article", "published", "annually", "400", "retracted", "oransky", "2016", "founder", "website", "retractionwatch", "amount", "small", "proportion", "literature", "approximately", "01", "course", "many", "case", "pharmaceutical", "company", "exercising", "financial", "pressure", "scientist", "publishing", "industry", "raise", "speculation", "many", "undetected", "unretracted", "case", "may", "still", "literature", "said", "widespread", "consensus", "amongst", "scientist", "field", "main", "cause", "current", "reproducibility", "crisis", "current", "incentive", "structure", "science", "publication", "bias", "publish", "perish", "nontransparent", "statistical", "reporting", "lack", "reward", "data", "sharing", "whilst", "incentive", "structure", "push", "scientific", "fraud", "appears", "small", "proportion", "3", "epistemological", "issue", "related", "replication", "many", "scientist", "believe", "replication", "epistemically", "valuable", "way", "say", "replication", "serf", "useful", "function", "enhancing", "knowledge", "understanding", "belief", "reality", "section", "first", "discus", "problem", "epistemic", "value", "replication", "studiescalled", "experimenter", "regress", "and", "considers", "claim", "replication", "play", "epistemically", "valuable", "role", "distinguishing", "scientific", "inquiry", "lastly", "examines", "recent", "attempt", "formalise", "logic", "replication", "bayesian", "framework", "31", "experimenter", "regress", "collins", "1985", "articulated", "widely", "discussed", "problem", "known", "experimenter", "regress", "initially", "lay", "problem", "context", "measurement", "collins", "1985", "84", "suppose", "scientist", "trying", "determine", "accuracy", "measurement", "device", "also", "accuracy", "measurement", "result", "perhaps", "example", "scientist", "using", "thermometer", "measure", "temperature", "liquid", "delivers", "particular", "measurement", "result", "say", "12", "degree", "celsius", "problem", "arises", "interdependence", "accuracy", "measurement", "result", "accuracy", "measurement", "device", "know", "whether", "particular", "measurement", "result", "accurate", "need", "test", "measurement", "result", "previously", "known", "accurate", "know", "result", "accurate", "need", "know", "obtained", "via", "accurate", "measuring", "device", "according", "collins", "creates", "circle", "refers", "experimenter", "regress", "collins", "extends", "problem", "scientific", "replication", "generally", "suppose", "experiment", "b", "replication", "study", "initial", "experiment", "b", "result", "apparently", "conflict", "result", "seeming", "conflict", "may", "one", "two", "interpretation", "result", "b", "deliver", "genuinely", "conflicting", "verdict", "truth", "hypothesis", "investigation", "experiment", "b", "fact", "proper", "replication", "experiment", "a", "regress", "pose", "problem", "choose", "interpretation", "problem", "threatens", "epistemic", "value", "replication", "study", "rational", "ground", "choosing", "particular", "way", "determining", "whether", "one", "experiment", "proper", "replication", "another", "complicated", "fact", "scientific", "writing", "convention", "often", "omit", "precise", "detail", "experimental", "methodology", "collins", "2016", "furthermore", "much", "knowledge", "scientist", "require", "execute", "experiment", "tacit", "fully", "explicated", "absolutely", "established", "collins", "1985", "73", "context", "experimental", "methodology", "collins", "wrote", "know", "experiment", "well", "conducted", "one", "need", "know", "whether", "give", "rise", "correct", "outcome", "know", "correct", "outcome", "one", "need", "wellconducted", "experiment", "know", "whether", "experiment", "well", "conducted", "2016", "66", "ellipsis", "original", "collins", "hold", "case", "conflict", "result", "arises", "scientist", "tend", "fraction", "two", "group", "holding", "opposing", "interpretation", "result", "according", "collins", "group", "determined", "controversy", "run", "deep", "collins", "2016", "67", "dispute", "group", "resolved", "via", "experimentation", "additional", "result", "subject", "problem", "posed", "experimenter", "regress", "2", "case", "collins", "claim", "particular", "nonepistemic", "factor", "partly", "determine", "interpretation", "becomes", "lasting", "view", "career", "social", "cognitive", "interest", "scientist", "reputation", "institution", "perceived", "utility", "future", "work", "franklin", "collins", "2016", "99", "franklin", "vociferous", "opponent", "collins", "although", "recent", "collaboration", "two", "fostered", "agreement", "collins", "2016", "franklin", "presented", "set", "strategy", "validating", "experimental", "result", "relate", "rational", "argument", "epistemic", "ground", "franklin", "1989", "459", "1994", "example", "include", "instance", "appealing", "experimental", "check", "measurement", "device", "eliminating", "potential", "source", "error", "experiment", "franklin", "collins", "2016", "claimed", "fact", "strategy", "evidenced", "scientific", "practice", "argues", "believe", "rational", "argument", "play", "little", "role", "validation", "franklin", "1989", "459", "collins", "example", "interprets", "collins", "suggesting", "strategy", "resolving", "debate", "validation", "result", "social", "factor", "culturally", "accepted", "practice", "franklin", "1989", "459", "provide", "reason", "underpin", "rational", "belief", "result", "franklin", "1994", "claim", "collins", "conflates", "difficulty", "successfully", "executing", "experiment", "difficulty", "demonstrating", "experiment", "executed", "feest", "2016", "interpreting", "say", "although", "execution", "requires", "tacit", "knowledge", "one", "nevertheless", "appeal", "strategy", "demonstrate", "validity", "experimental", "finding", "feest", "2016", "examines", "case", "study", "involving", "debate", "mozart", "effect", "psychology", "roughly", "speaking", "effect", "whereby", "listening", "mozart", "beneficially", "affect", "aspect", "intelligence", "brain", "structure", "like", "collins", "agrees", "problem", "determining", "whether", "conflicting", "result", "suggest", "putative", "replication", "experiment", "proper", "replication", "attempt", "part", "uncertainty", "whether", "scientific", "concept", "mozart", "effect", "appropriately", "operationalised", "earlier", "later", "experimental", "context", "unlike", "collins", "interpretation", "however", "think", "uncertainty", "arises", "scientist", "inescapably", "tacit", "knowledge", "linguistic", "rule", "meaning", "application", "concept", "like", "mozart", "effect", "rather", "uncertainty", "arises", "concept", "still", "developing", "assumption", "world", "required", "successfully", "draw", "inference", "experimental", "methodology", "serf", "reveal", "previously", "tacit", "assumption", "application", "concept", "legitimacy", "inference", "assumption", "susceptible", "scrutiny", "example", "study", "mozart", "effect", "note", "replication", "study", "mozart", "effect", "failed", "find", "mozart", "music", "beneficial", "influence", "spatial", "ability", "rauscher", "first", "report", "result", "supporting", "mozart", "effect", "suggested", "later", "study", "proper", "replication", "study", "rauscher", "shaw", "ky", "1993", "1995", "clarified", "mozart", "effect", "applied", "particular", "category", "spatial", "ability", "spatiotemporal", "process", "later", "study", "operationalised", "mozart", "effect", "term", "different", "spatial", "ability", "spatial", "recognition", "difficulty", "determining", "whether", "interpret", "failed", "replication", "result", "evidence", "initial", "result", "rather", "indication", "replication", "study", "proper", "replication", "feest", "claim", "difficulty", "arose", "tacit", "knowledge", "assumption", "assumption", "application", "mozart", "effect", "concept", "different", "kind", "spatial", "ability", "whether", "world", "mozart", "music", "effect", "ability", "whether", "failure", "mozart", "impact", "kind", "spatial", "ability", "warrant", "inference", "mozart", "effect", "exist", "contra", "collins", "however", "experimental", "methodology", "enabled", "explication", "testing", "assumption", "thus", "allowing", "scientist", "overcome", "interpretive", "impasse", "background", "overall", "argument", "scientist", "often", "sceptical", "towards", "result", "however", "inescapably", "tacit", "knowledge", "inevitable", "failure", "epistemic", "strategy", "validating", "result", "rather", "least", "part", "varying", "tacit", "assumption", "researcher", "meaning", "concept", "world", "draw", "inference", "progressive", "experimentation", "serf", "reveal", "tacit", "assumption", "scrutinised", "leading", "accumulation", "knowledge", "also", "philosophical", "literature", "experimenter", "regress", "including", "teira", "2013", "paper", "arguing", "particular", "experimental", "debiasing", "procedure", "defensible", "regress", "contractualist", "perspective", "according", "selfinterested", "scientist", "reason", "adopt", "good", "methodological", "standard", "32", "replication", "distinguishing", "feature", "science", "widespread", "belief", "science", "distinct", "knowledge", "accumulation", "endeavour", "suggested", "replication", "distinguishes", "least", "essential", "science", "respect", "see", "also", "entry", "science", "pseudoscience", "according", "open", "science", "collaboration", "reproducible", "research", "practice", "heart", "sound", "research", "integral", "scientific", "method", "osc", "2015", "7", "schmidt", "echo", "theme", "confirm", "result", "hypothesis", "repetition", "procedure", "basis", "scientific", "conception", "2009", "90", "braude", "1979", "go", "far", "say", "reproducibility", "demarcation", "criterion", "science", "nonscience", "1979", "2", "similarly", "nosek", "spy", "motyl", "state", "scientific", "method", "differentiates", "approach", "publicly", "disclosing", "basis", "evidence", "claim", "principle", "open", "sharing", "methodology", "mean", "entire", "body", "scientific", "knowledge", "reproduced", "anyone", "2012", "618", "replication", "played", "essential", "distinguishing", "role", "science", "might", "expect", "prominent", "theme", "history", "science", "steinle", "2016", "considers", "extent", "theme", "present", "variety", "case", "history", "science", "replication", "played", "different", "role", "although", "understands", "replication", "narrowly", "refer", "experiment", "rerun", "different", "researcher", "claim", "role", "value", "replication", "experimental", "replication", "much", "complex", "easy", "textbook", "account", "make", "u", "believe", "2016", "60", "particularly", "since", "scientific", "inquiry", "always", "tied", "variety", "contextual", "consideration", "affect", "importance", "replication", "consideration", "include", "relationship", "experimental", "result", "background", "accepted", "theory", "time", "practical", "resource", "constraint", "pursuing", "replication", "perceived", "credibility", "researcher", "contextual", "factor", "claim", "mean", "replication", "key", "even", "overriding", "determinant", "acceptance", "research", "claim", "case", "others", "example", "sometimes", "replication", "sufficient", "embrace", "research", "claim", "even", "conflicted", "background", "accepted", "theory", "left", "theoretical", "question", "unresolved", "case", "hightemperature", "superconductivity", "effect", "whereby", "electric", "current", "pas", "zero", "resistance", "conductor", "relatively", "high", "temperature", "1986", "physicist", "georg", "bednorz", "alex", "m\u00fcller", "reported", "finding", "material", "acted", "superconductor", "35", "kelvin", "238", "degree", "celsius", "scientist", "around", "world", "successfully", "replicated", "effect", "bednorz", "muller", "awarded", "nobel", "prize", "physic", "year", "announcement", "case", "remarkable", "since", "effect", "contradict", "accepted", "physical", "theory", "time", "still", "extant", "theory", "adequately", "explains", "effect", "reported", "di", "bucchianico", "2014", "contrasting", "example", "however", "sometimes", "claim", "accepted", "without", "replication", "1650s", "german", "scientist", "otto", "von", "guericke", "designed", "operated", "world", "first", "vacuum", "pump", "would", "visibly", "suck", "air", "larger", "space", "performed", "experiment", "device", "various", "audience", "yet", "replication", "experiment", "others", "would", "difficult", "impossible", "guericke", "pump", "expensive", "complicated", "build", "also", "unlikely", "description", "sufficed", "enable", "anyone", "build", "pump", "consequently", "replicate", "finding", "despite", "steinle", "claim", "doubt", "raised", "result", "probably", "result", "public", "performance", "could", "witnessed", "large", "number", "participant", "2016", "55", "steinle", "take", "historical", "case", "provide", "normative", "guidance", "understanding", "epistemic", "value", "replication", "contextsensitive", "whether", "replication", "necessary", "sufficient", "establishing", "research", "claim", "depend", "variety", "consideration", "mentioned", "earlier", "consequently", "eschews", "widereaching", "claim", "replicability", "replicability", "decide", "anything", "2016", "60", "33", "formalising", "logic", "replication", "earp", "trafimow", "2015", "attempt", "formalise", "way", "replication", "epistemically", "valuable", "using", "bayesian", "framework", "explicate", "inference", "drawn", "replication", "study", "present", "framework", "context", "similar", "collins", "1985", "noting", "wellnigh", "impossible", "say", "conclusively", "replication", "result", "mean", "earp", "trafimow", "2015", "3", "replication", "study", "often", "conclusive", "believe", "study", "informative", "bayesian", "framework", "depicts", "framework", "set", "example", "suppose", "aficionado", "researcher", "highly", "confident", "anything", "said", "researcher", "true", "researcher", "researcher", "b", "attempt", "replicate", "experiment", "researcher", "researcher", "b", "find", "result", "conflict", "researcher", "a", "earp", "trafimow", "claim", "aficionado", "might", "continue", "confident", "researcher", "finding", "aficionado", "confidence", "likely", "slightly", "decrease", "number", "failed", "replication", "attempt", "increase", "aficionado", "confidence", "accordingly", "decrease", "eventually", "falling", "50", "thereby", "placing", "confidence", "replication", "failure", "finding", "initially", "reported", "researcher", "a", "suppose", "interested", "probability", "original", "result", "reported", "researcher", "true", "given", "researcher", "b", "first", "replication", "failure", "earp", "trafimow", "represent", "probability", "notation", "p", "tmid", "f", "p", "probability", "function", "represents", "proposition", "original", "result", "true", "f", "represents", "researcher", "b", "replication", "failure", "according", "bayes", "theorem", "probability", "calculable", "aficionado", "degree", "confidence", "original", "result", "true", "prior", "learning", "replication", "failure", "p", "degree", "expectation", "replication", "failure", "condition", "original", "result", "true", "p", "tmid", "f", "degree", "would", "unconditionally", "expect", "replication", "failure", "prior", "learning", "replication", "failure", "p", "f", "tag", "1", "p", "tmid", "f", "frac", "p", "p", "fmid", "p", "f", "relatedly", "could", "instead", "interested", "confidence", "ratio", "original", "result", "true", "false", "given", "failure", "replicate", "ratio", "representable", "frac", "p", "tmid", "f", "p", "nneg", "tmid", "f", "nneg", "t", "represents", "proposition", "original", "result", "false", "according", "standard", "bayesian", "probability", "calculus", "ratio", "turn", "related", "product", "ratio", "concerning", "confidence", "original", "result", "true", "frac", "p", "p", "nneg", "expectation", "replication", "failure", "condition", "result", "true", "false", "frac", "p", "fmid", "p", "fmid", "nneg", "relation", "expressed", "equation", "tag", "2", "frac", "p", "tmid", "f", "p", "nneg", "tmid", "f", "frac", "p", "p", "nneg", "frac", "p", "fmid", "p", "fmid", "nneg", "earp", "trafimow", "assign", "value", "term", "righthand", "equation", "2", "supposing", "aficionado", "confident", "original", "result", "set", "ratio", "frac", "p", "p", "nneg", "50", "meaning", "aficionado", "initially", "fifty", "time", "confident", "result", "true", "result", "false", "also", "set", "ratio", "frac", "p", "fmid", "p", "fmid", "nneg", "conditional", "expectation", "replication", "failure", "05", "meaning", "aficionado", "considerably", "le", "confident", "replication", "failure", "original", "result", "true", "false", "point", "extent", "aficionado", "le", "confident", "depends", "quality", "socalled", "auxiliary", "assumption", "replication", "experiment", "auxiliary", "assumption", "assumption", "enable", "one", "infer", "particular", "thing", "observable", "theory", "test", "true", "intuitive", "idea", "higher", "quality", "assumption", "replication", "study", "one", "would", "expect", "observe", "successful", "replication", "original", "result", "true", "specify", "precisely", "make", "auxiliary", "assumption", "high", "quality", "context", "presumably", "quality", "concern", "extent", "assumption", "probably", "true", "extent", "replication", "experiment", "appropriate", "test", "veracity", "original", "result", "assumption", "true", "ratio", "righthand", "equation", "2", "set", "one", "see", "replication", "failure", "would", "reduce", "one", "confidence", "original", "result", "tag", "3", "begin", "align", "frac", "p", "tmid", "f", "p", "nneg", "tmid", "f", "frac", "p", "p", "nneg", "frac", "p", "fmid", "p", "fmid", "nneg", "50", "05", "25", "end", "align", "replication", "failure", "would", "reduce", "aficionado", "confidence", "original", "result", "true", "aficionado", "would", "25", "time", "confident", "result", "true", "given", "failure", "per", "frac", "p", "tmid", "f", "p", "nneg", "tmid", "f", "rather", "50", "time", "confident", "true", "per", "frac", "p", "p", "nneg", "nevertheless", "aficionado", "may", "still", "confident", "original", "result", "true", "see", "confidence", "would", "decrease", "successive", "replication", "failure", "formally", "let", "f_n", "last", "replication", "failure", "sequence", "n", "replication", "failure", "langle", "f_1", "f_2", "ldots", "f_nrangle", "aficionado", "confidence", "original", "result", "given", "nth", "replication", "failure", "expressible", "equation", "3", "tag", "4", "frac", "p", "tmid", "f_n", "p", "nneg", "tmid", "f_n", "frac", "p", "p", "nneg", "frac", "p", "f_1mid", "p", "f_1mid", "nneg", "frac", "p", "f_2mid", "p", "f_2mid", "nneg", "cdots", "frac", "p", "f_nmid", "p", "f_nmid", "nneg", "example", "suppose", "10", "replication", "failure", "n10", "suppose", "confidence", "ratio", "replication", "failure", "set", "tag", "5", "begin", "multline", "phantom", "ab", "frac", "p", "f_1mid", "p", "f_1mid", "nneg", "frac", "p", "f_2mid", "p", "f_2mid", "nneg", "cdots", "frac", "p", "f_", "10", "mid", "p", "f_", "10", "mid", "nneg", "phantom", "ab", "05", "08", "07", "065", "075", "056", "069", "054", "073", "052", "end", "multline", "tag", "6", "begin", "align", "frac", "p", "mid", "f_", "10", "p", "nneg", "mid", "f_", "10", "054", "frac", "p", "p", "nneg", "frac", "p", "f_1mid", "p", "f_1mid", "nneg", "frac", "p", "f_2mid", "p", "f_2mid", "nneg", "cdots", "frac", "p", "f_", "10", "mid", "p", "f_", "10", "mid", "nneg", "50", "05", "08", "ldots", "052", "end", "align", "aficionado", "confidence", "original", "result", "decrease", "confident", "false", "true", "hence", "earp", "trafimow", "bayesian", "account", "successive", "replication", "failure", "progressively", "erode", "one", "confidence", "original", "result", "true", "even", "one", "initially", "highly", "confident", "original", "result", "even", "single", "replication", "failure", "conclusive", "4", "putative", "merit", "earp", "trafimow", "account", "provides", "formalisation", "whereby", "replication", "attempt", "informative", "even", "conclusive", "furthermore", "formalisation", "provides", "role", "quantity", "replication", "attempt", "well", "auxiliary", "assumption", "replication", "4", "open", "science", "reform", "value", "tone", "scientific", "norm", "aforementioned", "metascience", "unearthed", "range", "problem", "give", "rise", "reproducibility", "crisis", "open", "science", "movement", "proposed", "promoted", "various", "solutionsor", "reformsfor", "problem", "reform", "grouped", "four", "category", "method", "training", "b", "reporting", "dissemination", "c", "peer", "review", "process", "evaluating", "new", "incentive", "structure", "loosely", "following", "category", "used", "munaf\u00f2", "et", "al", "2017", "ioannidis", "et", "al", "2015", "subsection", "4144", "present", "nonexhaustive", "list", "initiative", "category", "initiative", "reflection", "various", "value", "norm", "heart", "open", "science", "movement", "discus", "value", "norm", "45", "41", "method", "training", "combating", "bias", "development", "method", "combating", "bias", "example", "masked", "blind", "analysis", "technique", "combat", "confirmation", "bias", "eg", "maccoun", "perlmutter", "2017", "support", "providing", "methodological", "support", "researcher", "including", "published", "guideline", "statistical", "consultancy", "example", "offered", "center", "open", "science", "large", "online", "course", "developed", "daniel", "lakens", "see", "internet", "resource", "collaboration", "promoting", "collaboration", "teamcrowd", "sourced", "science", "combat", "low", "power", "methodological", "limitation", "single", "study", "reproducibility", "project", "example", "initiative", "studyswap", "psychology", "collective", "replication", "education", "project", "crep", "see", "internet", "resource", "see", "also", "munaf\u00f2", "et", "al", "detailed", "description", "aim", "increase", "prevalence", "replication", "undergraduate", "education", "42", "reporting", "dissemination", "top", "guideline", "transparency", "openness", "promotion", "top", "guideline", "nosek", "et", "al", "2015", "end", "may", "2018", "almost", "5000", "journal", "organization", "signatory", "developed", "within", "psychology", "top", "guideline", "formed", "basis", "disciplinary", "specific", "guideline", "tool", "transparency", "ecology", "evolution", "ttee", "name", "suggests", "guideline", "promote", "complete", "transparent", "reporting", "methodological", "statistical", "practice", "turn", "enables", "author", "reviewer", "editor", "consider", "detailed", "aspect", "sample", "size", "planning", "design", "decision", "clearly", "distinguish", "confirmatory", "planned", "analysis", "exploratory", "post", "hoc", "analysis", "preregistration", "simplest", "form", "preregistration", "involves", "making", "public", "datestamped", "statement", "prediction", "andor", "hypothesis", "either", "data", "collected", "viewed", "analysed", "purpose", "distinguish", "prediction", "postdiction", "nosek", "et", "al", "2018", "elsewhere", "referred", "confirmatory", "research", "exploratory", "research", "wagenmakers", "et", "al", "2012", "distinction", "perhaps", "commonly", "known", "hypothesis", "testing", "versus", "hypothesis", "generating", "research", "preregistration", "predictive", "research", "help", "control", "harking", "kerr", "1998", "hindsight", "bias", "within", "frequentist", "null", "hypothesis", "significance", "testing", "help", "contain", "false", "positive", "error", "rate", "set", "alpha", "level", "several", "platform", "host", "preregistrations", "open", "science", "framework", "osfio", "predicted", "aspredictedorg", "open", "science", "framework", "also", "host", "preregistration", "challenge", "offering", "monetary", "reward", "publishing", "preregistered", "work", "specific", "journal", "initiative", "high", "impact", "journal", "singled", "science", "medium", "particularly", "problematic", "publishing", "practice", "eg", "schekman", "2013", "taken", "exceptional", "step", "improve", "completeness", "transparency", "reproducibility", "research", "publish", "example", "since", "2013", "nature", "nature", "research", "journal", "engaged", "range", "editorial", "activity", "aimed", "improving", "reproducibility", "research", "published", "journal", "see", "editorial", "announcement", "nature", "496", "398", "25", "april", "2013", "doi101038496398a", "2017", "introduced", "checklist", "reporting", "summary", "published", "alongside", "article", "effort", "improve", "transparency", "reproducibility", "2018", "produced", "discipline", "specific", "version", "nature", "human", "behaviour", "nature", "ecology", "evolution", "within", "psychology", "journal", "psychological", "science", "flagship", "journal", "association", "psychological", "science", "first", "adopt", "open", "science", "practice", "co", "open", "science", "badge", "described", "following", "meeting", "ecology", "evolution", "journal", "editor", "2015", "number", "journal", "field", "run", "editorial", "topic", "often", "committing", "ttee", "guideline", "discussed", "conservation", "biology", "addition", "adopted", "checklist", "associate", "editor", "parker", "et", "al", "2016", "43", "peer", "review", "registered", "report", "registered", "report", "shift", "point", "peer", "review", "occurs", "research", "process", "effort", "combat", "publication", "bias", "null", "negative", "result", "manuscript", "submitted", "reviewed", "publication", "decision", "made", "basis", "introduction", "method", "planned", "analysis", "alone", "accepted", "author", "defined", "period", "time", "carry", "planned", "research", "submit", "result", "assuming", "author", "followed", "original", "plan", "adequately", "justified", "deviation", "journal", "honour", "decision", "publish", "regardless", "result", "outcome", "psychology", "registered", "report", "format", "championed", "chris", "chamber", "journal", "cortex", "first", "adopt", "format", "chamber", "editorship", "chamber", "2013", "2017", "nosek", "lakens", "2014", "currently", "end", "may", "2018", "108", "journal", "range", "biomedical", "psychology", "neuroscience", "field", "offer", "format", "see", "registered", "report", "internet", "resource", "preprints", "wellestablished", "science", "like", "physic", "use", "preprint", "server", "relatively", "new", "biological", "social", "science", "44", "incentive", "evaluation", "open", "science", "badge", "recent", "review", "initiative", "improving", "data", "sharing", "identified", "awarding", "open", "data", "open", "material", "badge", "effective", "scheme", "rowhanifarid", "allen", "barnett", "2017", "one", "badge", "scheme", "coordinated", "center", "open", "science", "currently", "award", "three", "badge", "open", "data", "open", "material", "preregistration", "badge", "attached", "article", "follow", "specific", "set", "criterion", "engage", "activity", "kidwell", "et", "al", "2016", "evaluated", "effectiveness", "badge", "journal", "psychological", "science", "found", "substantial", "increase", "3", "39", "data", "sharing", "le", "twoyear", "period", "increase", "found", "similar", "journal", "without", "badge", "scheme", "period", "45", "value", "tone", "scientific", "norm", "open", "science", "reform", "long", "philosophical", "debate", "role", "value", "play", "science", "churchman", "1948", "rudner", "1953", "douglas", "2016", "reproducibility", "crisis", "intimately", "connected", "question", "operation", "interconnection", "value", "particular", "nosek", "et", "al", "2017", "argue", "tension", "truth", "publishability", "specifically", "reason", "discussed", "section", "2", "accuracy", "scientific", "result", "compromised", "value", "journal", "place", "novel", "positive", "result", "consequently", "scientist", "value", "career", "success", "seek", "exclusively", "publish", "result", "journal", "many", "others", "addition", "nosek", "et", "al", "hackett", "2005", "martin", "1992", "sovacool", "2008", "taken", "also", "take", "issue", "value", "journal", "funding", "body", "placed", "novelty", "might", "interpret", "tension", "manifestation", "epistemic", "value", "truth", "replicability", "compromised", "arguably", "nonepistemic", "value", "value", "novel", "interesting", "surprising", "result", "epistemic", "value", "typically", "taken", "value", "word", "steel", "promote", "acquisition", "true", "belief", "2010", "18", "see", "also", "goldman", "1999", "canonical", "example", "epistemic", "value", "include", "predictive", "accuracy", "internal", "consistency", "theory", "epistemic", "value", "often", "contrasted", "putative", "nonepistemic", "noncognitive", "value", "include", "ethical", "social", "value", "like", "example", "novelty", "theory", "ability", "improve", "wellbeing", "lessening", "power", "inequality", "longino", "1996", "course", "complete", "consensus", "precisely", "count", "epistemic", "nonepistemic", "value", "rooney", "1992", "longino", "1996", "longino", "example", "claim", "thing", "equal", "novelty", "count", "favour", "accepting", "theory", "convincingly", "argues", "context", "serve", "protection", "unconscious", "perpetuation", "sexism", "androcentrism", "traditional", "science", "1997", "22", "however", "discus", "novelty", "specifically", "context", "reproducibility", "crisis", "ginersorolla", "2012", "however", "discus", "novelty", "context", "crisis", "offer", "another", "perspective", "value", "claim", "one", "reason", "novelty", "used", "define", "publishable", "fundable", "relatively", "easy", "researcher", "establish", "reviewer", "editor", "detect", "yet", "ginersorolla", "argues", "novelty", "sake", "perhaps", "valued", "fact", "recognized", "merely", "operationalisation", "deeper", "concept", "ability", "advance", "field", "567", "ginersorolla", "go", "point", "shallow", "operationalisations", "important", "concept", "often", "lead", "problem", "example", "using", "statistical", "significance", "measure", "importance", "result", "measuring", "quality", "research", "well", "outcome", "fit", "experimenter", "prior", "expectation", "value", "closely", "connected", "discussion", "norm", "open", "science", "movement", "vazire", "2018", "others", "invoke", "norm", "sciencecommunality", "universalism", "disinterestedness", "organised", "skepticismin", "setting", "goal", "open", "science", "norm", "originally", "articulated", "robert", "merton", "1942", "norm", "arguably", "reflects", "value", "merton", "advocated", "norm", "may", "opposed", "counternorm", "denotes", "behaviour", "conflict", "norm", "example", "norm", "communality", "merton", "called", "communism", "reflects", "value", "collaboration", "common", "ownership", "scientific", "good", "since", "norm", "recommends", "collaboration", "common", "ownership", "advocate", "open", "science", "see", "norm", "value", "reflect", "aim", "open", "science", "example", "norm", "communality", "reflected", "sharing", "making", "data", "open", "open", "access", "publishing", "contrast", "counternorm", "secrecy", "associated", "closed", "profit", "publishing", "system", "anderson", "et", "al", "2010", "likewise", "assessing", "scientific", "work", "merit", "upholds", "norm", "universalismthat", "evaluation", "research", "claim", "depend", "sociodemographic", "characteristic", "proponent", "claim", "contrast", "assessing", "work", "age", "status", "institution", "metric", "journal", "published", "reflects", "counternorm", "particularism", "vazire", "2018", "others", "argued", "moment", "scientific", "practice", "dominated", "counternorms", "move", "mertonian", "norm", "goal", "open", "science", "reform", "movement", "particular", "selfinterestedness", "opposed", "norm", "disinterestedness", "motivates", "phacking", "questionable", "research", "practice", "similarly", "desire", "protect", "one", "professional", "reputation", "motivates", "resistance", "one", "work", "replicated", "others", "vazire", "2018", "turn", "reinforces", "counternorm", "organized", "dogmatism", "rather", "organized", "skepticism", "according", "merton", "involves", "temporary", "suspension", "judgment", "detached", "scrutiny", "belief", "merton", "1973", "anderson", "et", "al", "2010", "focus", "group", "survey", "scientist", "suggest", "scientist", "want", "adhere", "merton", "norm", "current", "incentive", "structure", "science", "make", "difficult", "changing", "structure", "penalty", "reward", "system", "within", "science", "promote", "communality", "universalism", "disinterestedness", "organized", "skepticism", "instead", "counternorms", "ongoing", "challenge", "open", "science", "reform", "movement", "pashler", "wagenmakers", "2012", "said", "replicability", "problem", "easily", "overcome", "reflect", "deepseated", "human", "bias", "wellentrenched", "incentive", "shape", "behavior", "individual", "institution", "2012", "529", "effort", "promote", "value", "norm", "generated", "heated", "controversy", "early", "response", "reproducibility", "project", "psychology", "many", "lab", "project", "highly", "critical", "substance", "nature", "process", "work", "call", "openness", "interpreted", "reflecting", "mistrust", "attempt", "replicate", "others", "work", "personal", "attack", "eg", "schnail", "2014", "internet", "resource", "nosek", "spy", "motyl", "2012", "argue", "call", "openness", "interepreted", "mistrust", "opening", "research", "process", "make", "u", "feel", "accountable", "best", "get", "right", "get", "right", "increase", "opportunity", "others", "detect", "problem", "correct", "openness", "needed", "untrustworthy", "needed", "human", "2012", "626", "exchange", "related", "become", "known", "tone", "debate", "5", "conclusion", "subject", "reproducibility", "associated", "turbulent", "period", "contemporary", "science", "period", "called", "reevaluation", "value", "incentive", "practice", "structure", "underpin", "scientific", "inquiry", "metascience", "painted", "bleak", "picture", "reproducibility", "field", "also", "inspired", "parallel", "movement", "strengthen", "foundation", "science", "however", "progress", "made", "especially", "understanding", "solution", "reproducibility", "crisis", "regard", "fruitful", "avenue", "future", "research", "including", "deeper", "exploration", "role", "epistemic", "nonepistemic", "value", "play", "scientific", "inquiry"]}