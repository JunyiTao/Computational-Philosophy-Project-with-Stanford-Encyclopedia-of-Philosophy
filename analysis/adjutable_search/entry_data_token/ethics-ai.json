{"url": "ethics-ai", "title": "Ethics of Artificial Intelligence and Robotics", "authorship": {"year": "Copyright \u00a9 2020", "author_text": "Vincent C. M\u00fcller\n<vincent.c.mueller@fau.de>", "author_links": [{"http://www.sophia.de": "Vincent C. M\u00fcller"}, {"mailto:vincent%2ec%2emueller%40fau%2ede": "vincent.c.mueller@fau.de"}], "raw_html": "<div id=\"article-copyright\">\n<p>\n<a href=\"../../info.html#c\">Copyright \u00a9 2020</a> by\n\n<br/>\n<a href=\"http://www.sophia.de\" target=\"other\">Vincent C. M\u00fcller</a>\n&lt;<a href=\"mailto:vincent%2ec%2emueller%40fau%2ede\"><em>vincent<abbr title=\" dot \">.</abbr>c<abbr title=\" dot \">.</abbr>mueller<abbr title=\" at \">@</abbr>fau<abbr title=\" dot \">.</abbr>de</em></a>&gt;\n    </p>\n</div>"}, "pubinfo": ["First published Thu Apr 30, 2020"], "preamble": "\n\nArtificial intelligence (AI) and robotics are digital technologies\nthat will have significant impact on the development of humanity in\nthe near future. They have raised fundamental questions about what we\nshould do with these systems, what the systems themselves should do,\nwhat risks they involve, and how we can control these.\n\nAfter the Introduction to the field (\u00a71), the main themes\n(\u00a72) of this article are: Ethical issues that arise with AI\nsystems as objects, i.e., tools made and used by humans. This\nincludes issues of privacy (\u00a72.1) and manipulation (\u00a72.2),\nopacity (\u00a72.3) and bias (\u00a72.4), human-robot interaction\n(\u00a72.5), employment (\u00a72.6), and the effects of autonomy\n(\u00a72.7). Then AI systems as subjects, i.e., ethics for\nthe AI systems themselves in machine ethics (\u00a72.8) and artificial\nmoral agency (\u00a72.9). Finally, the problem of a possible future AI\nsuperintelligence leading to a \u201csingularity\u201d (\u00a72.10).\nWe close with a remark on the vision of AI (\u00a73).\n\nFor each section within these themes, we provide a general explanation\nof the ethical issues, outline existing positions\nand arguments, then analyse how these play out with current\ntechnologies and finally, what policy consequences\nmay be drawn.\n", "toc": [{"#Intr": "1. Introduction"}, {"#BackFiel": "1.1 Background of the Field"}, {"#AIRobo": "1.2 AI & Robotics"}, {"#NotePoli": "1.3 A Note on Policy"}, {"#MainDeba": "2. Main Debates"}, {"#PrivSurv": "2.1 Privacy & Surveillance"}, {"#ManiBeha": "2.2 Manipulation of Behaviour"}, {"#OpacAISyst": "2.3 Opacity of AI Systems"}, {"#BiasDeciSyst": "2.4 Bias in Decision Systems"}, {"#HumaRoboInte": "2.5 Human-Robot Interaction"}, {"#AutoEmpl": "2.6 Automation and Employment"}, {"#AutoSyst": "2.7 Autonomous Systems"}, {"#MachEthi": "2.8 Machine Ethics"}, {"#ArtiMoraAgen": "2.9 Artificial Moral Agents"}, {"#Sing": "2.10 Singularity"}, {"#Clos": "3. Closing"}, {"#Bib": "Bibliography"}, {"#Aca": "Academic Tools"}, {"#Oth": "Other Internet Resources"}, {"#Refe": "References"}, {"#ReseOrga": "Research Organizations"}, {"#Conf": "Conferences"}, {"#PoliDocu": "Policy Documents"}, {"#OtheRelePage": "Other Relevant pages"}, {"#Rel": "Related Entries"}], "main_text": "\n1. Introduction\n1.1 Background of the Field\n\nThe ethics of AI and robotics is often focused on\n\u201cconcerns\u201d of various sorts, which is a typical response\nto new technologies. Many such concerns turn out to be rather quaint\n(trains are too fast for souls); some are predictably wrong when they\nsuggest that the technology will fundamentally change humans\n(telephones will destroy personal communication, writing will destroy\nmemory, video cassettes will make going out redundant); some are\nbroadly correct but moderately relevant (digital technology will\ndestroy industries that make photographic film, cassette tapes, or\nvinyl records); but some are broadly correct and deeply relevant (cars\nwill kill children and fundamentally change the landscape). The task\nof an article such as this is to analyse the issues and to deflate the\nnon-issues.\n\nSome technologies, like nuclear power, cars, or plastics, have caused\nethical and political discussion and significant policy efforts to\ncontrol the trajectory these technologies, usually only once some\ndamage is done. In addition to such \u201cethical concerns\u201d,\nnew technologies challenge current norms and conceptual systems, which\nis of particular interest to philosophy. Finally, once we have\nunderstood a technology in its context, we need to shape our societal\nresponse, including regulation and law. All these features also exist\nin the case of new AI and Robotics technologies\u2014plus the more\nfundamental fear that they may end the era of human control on\nEarth.\n\nThe ethics of AI and robotics has seen significant press coverage in\nrecent years, which supports related research, but also may end up\nundermining it: the press often talks as if the issues under\ndiscussion were just predictions of what future technology will bring,\nand as though we already know what would be most ethical and how to\nachieve that. Press coverage thus focuses on risk, security (Brundage\net al. 2018, in the\n Other Internet Resources\n section below, hereafter [OIR]), and prediction of impact (e.g., on\nthe job market). The result is a discussion of essentially technical\nproblems that focus on how to achieve a desired outcome. Current\ndiscussions in policy and industry are also motivated by image and\npublic relations, where the label \u201cethical\u201d is really not\nmuch more than the new \u201cgreen\u201d, perhaps used for\n\u201cethics washing\u201d. For a problem to qualify as a problem\nfor AI ethics would require that we do not readily know what\nthe right thing to do is. In this sense, job loss, theft, or killing\nwith AI is not a problem in ethics, but whether these are permissible\nunder certain circumstances is a problem. This article\nfocuses on the genuine problems of ethics where we do not readily know\nwhat the answers are.\n\nA last caveat: The ethics of AI and robotics is a very young field\nwithin applied ethics, with significant dynamics, but few\nwell-established issues and no authoritative overviews\u2014though\nthere is a promising outline (European Group on Ethics in Science and\nNew Technologies 2018) and there are beginnings on societal impact\n(Floridi et al. 2018; Taddeo and Floridi 2018; S. Taylor et al. 2018;\nWalsh 2018; Bryson 2019; Gibert 2019; \nWhittlestone et al. 2019), and policy recommendations (AI HLEG 2019\n[OIR]; IEEE 2019). So this article cannot merely reproduce what the\ncommunity has achieved thus far, but must propose an ordering where\nlittle order exists.\n1.2 AI & Robotics\n\nThe notion of \u201cartificial intelligence\u201d (AI) is understood\nbroadly as any kind of artificial computational system that shows\nintelligent behaviour, i.e., complex behaviour that is conducive to\nreaching goals. In particular, we do not wish to restrict\n\u201cintelligence\u201d to what would require intelligence if done\nby humans, as Minsky had suggested (1985). This means we\nincorporate a range of machines, including those in \u201ctechnical\nAI\u201d, that show only limited abilities in learning or reasoning\nbut excel at the automation of particular tasks, as well as machines\nin \u201cgeneral AI\u201d that aim to create a generally intelligent\nagent.\n\nAI somehow gets closer to our skin than other technologies\u2014thus\nthe field of \u201cphilosophy of AI\u201d. Perhaps this is because\nthe project of AI is to create machines that have a feature central to\nhow we humans see ourselves, namely as feeling, thinking, intelligent\nbeings. The main purposes of an artificially intelligent agent\nprobably involve sensing, modelling, planning and action, but current\nAI applications also include perception, text analysis, natural\nlanguage processing (NLP), logical reasoning, game-playing, decision\nsupport systems, data analytics, predictive analytics, as well as\nautonomous vehicles and other forms of robotics (P. Stone et al.\n2016). AI may involve any number of computational techniques to\nachieve these aims, be that classical symbol-manipulating AI, inspired\nby natural cognition, or machine learning via neural networks\n(Goodfellow, Bengio, and Courville 2016; Silver et al. 2018).\n\nHistorically, it is worth noting that the term \u201cAI\u201d was\nused as above ca. 1950\u20131975, then came into disrepute during the\n\u201cAI winter\u201d, ca. 1975\u20131995, and narrowed. As a\nresult, areas such as \u201cmachine learning\u201d, \u201cnatural\nlanguage processing\u201d and \u201cdata science\u201d were often\nnot labelled as \u201cAI\u201d. Since ca. 2010, the use has\nbroadened again, and at times almost all of computer science and even\nhigh-tech is lumped under \u201cAI\u201d. Now it is a name to be\nproud of, a booming industry with massive capital investment (Shoham\net al. 2018), and on the edge of hype again. As Erik Brynjolfsson\nnoted, it may allow us to\n\n\nvirtually eliminate global poverty, massively reduce disease and\nprovide better education to almost everyone on the planet. (quoted in\nAnderson, Rainie, and Luchsinger 2018)\n\n\nWhile AI can be entirely software, robots are physical machines that\nmove. Robots are subject to physical impact, typically through\n\u201csensors\u201d, and they exert physical force onto the world,\ntypically through \u201cactuators\u201d, like a gripper or a turning\nwheel. Accordingly, autonomous cars or planes are robots, and only a\nminuscule portion of robots is \u201chumanoid\u201d (human-shaped),\nlike in the movies. Some robots use AI, and some do not: Typical\nindustrial robots blindly follow completely defined scripts with\nminimal sensory input and no learning or reasoning (around 500,000\nsuch new industrial robots are installed each year (IFR 2019 [OIR])).\nIt is probably fair to say that while robotics systems cause more\nconcerns in the general public, AI systems are more likely to have a\ngreater impact on humanity. Also, AI or robotics systems for a narrow\nset of tasks are less likely to cause new issues than systems that are\nmore flexible and autonomous.\n\nRobotics and AI can thus be seen as covering two overlapping sets of\nsystems: systems that are only AI, systems that are only robotics, and\nsystems that are both. We are interested in all three; the scope of\nthis article is thus not only the intersection, but the union, of both\nsets.\n1.3 A Note on Policy\n\nPolicy is only one of the concerns of this article. There is\nsignificant public discussion about AI ethics, and there are frequent\npronouncements from politicians that the matter requires new policy,\nwhich is easier said than done: Actual technology policy is difficult\nto plan and enforce. It can take many forms, from incentives and\nfunding, infrastructure, taxation, or good-will statements, to\nregulation by various actors, and the law. Policy for AI will possibly\ncome into conflict with other aims of technology policy or general\npolicy. Governments, parliaments, associations, and industry circles\nin industrialised countries have produced reports and white papers in\nrecent years, and some have generated good-will slogans\n(\u201ctrusted/responsible/humane/human-centred/good/beneficial\nAI\u201d), but is that what is needed? For a survey, see Jobin,\nIenca, and Vayena (2019) and V. M\u00fcller\u2019s list of\n PT-AI Policy Documents and Institutions.\n\nFor people who work in ethics and policy, there might be a tendency to\noverestimate the impact and threats from a new technology, and to\nunderestimate how far current regulation can reach (e.g., for product\nliability). On the other hand, there is a tendency for businesses, the\nmilitary, and some public administrations to \u201cjust talk\u201d\nand do some \u201cethics washing\u201d in order to preserve a good\npublic image and continue as before. Actually implementing legally\nbinding regulation would challenge existing business models and\npractices. Actual policy is not just an implementation of ethical\ntheory, but subject to societal power structures\u2014and the agents\nthat do have the power will push against anything that restricts them.\nThere is thus a significant risk that regulation will remain toothless\nin the face of economical and political power.\n\nThough very little actual policy has been produced, there are some\nnotable beginnings: The latest EU policy document suggests\n\u201ctrustworthy AI\u201d should be lawful, ethical, and\ntechnically robust, and then spells this out as seven requirements:\nhuman oversight, technical robustness, privacy and data governance,\ntransparency, fairness, well-being, and accountability (AI HLEG 2019\n[OIR]). Much European research now runs under the slogan of\n\u201cresponsible research and innovation\u201d (RRI), and\n\u201ctechnology assessment\u201d has been a standard field since\nthe advent of nuclear power. Professional ethics is also a standard\nfield in information technology, and this includes issues that are\nrelevant in this article. Perhaps a \u201ccode of ethics\u201d for\nAI engineers, analogous to the codes of ethics for medical doctors, is\nan option here (V\u00e9liz 2019). What data science itself should do\nis addressed in (L. Taylor and Purtova 2019). We also expect that much\npolicy will eventually cover specific uses or technologies of AI and\nrobotics, rather than the field as a whole. A useful summary of an\nethical framework for AI is given in (European Group on Ethics in\nScience and New Technologies 2018: 13ff). On general AI policy, see\nCalo (2018) as well as Crawford and Calo (2016); Stahl, Timmermans,\nand Mittelstadt (2016); Johnson and Verdicchio (2017); and Giubilini\nand Savulescu (2018). A more political angle of technology is often\ndiscussed in the field of \u201cScience and Technology Studies\u201d\n(STS). As books like The Ethics of Invention (Jasanoff 2016)\nshow, concerns in STS are often quite similar to those in ethics\n(Jacobs et al. 2019 [OIR]). In this article, we discuss the policy for\neach type of issue separately rather than for AI or robotics in\ngeneral.\n2. Main Debates\n\nIn this section we outline the ethical issues of human use of AI and\nrobotics systems that can be more or less autonomous\u2014which means\nwe look at issues that arise with certain uses of the technologies\nwhich would not arise with others. It must be kept in mind, however,\nthat technologies will always cause some uses to be easier, and thus\nmore frequent, and hinder other uses. The design of technical\nartefacts thus has ethical relevance for their use (Houkes and Vermaas\n2010; Verbeek 2011), so beyond \u201cresponsible use\u201d, we also\nneed \u201cresponsible design\u201d in this field. The focus on use\ndoes not presuppose which ethical approaches are best suited for\ntackling these issues; they might well be virtue ethics (Vallor 2017)\nrather than consequentialist or value-based (Floridi et al. 2018).\nThis section is also neutral with respect to the question whether AI\nsystems truly have \u201cintelligence\u201d or other mental\nproperties: It would apply equally well if AI and robotics are merely\nseen as the current face of automation (cf. M\u00fcller\nforthcoming-b).\n2.1 Privacy & Surveillance\n\nThere is a general discussion about privacy and surveillance in\ninformation technology (e.g., Macnish 2017; Roessler 2017), which\nmainly concerns the access to private data and data that is personally\nidentifiable. Privacy has several well recognised aspects, e.g.,\n\u201cthe right to be let alone\u201d, information privacy, privacy\nas an aspect of personhood, control over information about oneself,\nand the right to secrecy (Bennett and Raab 2006). Privacy studies have\nhistorically focused on state surveillance by secret services but now\ninclude surveillance by other state agents, businesses, and even\nindividuals. The technology has changed significantly in the last\ndecades while regulation has been slow to respond (though there is the\nRegulation (EU) 2016/679)\u2014the result is a certain anarchy that\nis exploited by the most powerful players, sometimes in plain sight,\nsometimes in hiding.\n\nThe digital sphere has widened greatly: All data collection and\nstorage is now digital, our lives are increasingly digital, most\ndigital data is connected to a single Internet, and there is more and\nmore sensor technology in use that generates data about non-digital\naspects of our lives. AI increases both the possibilities of\nintelligent data collection and the possibilities for data analysis.\nThis applies to blanket surveillance of whole populations as well as\nto classic targeted surveillance. In addition, much of the data is\ntraded between agents, usually for a fee.\n\nAt the same time, controlling who collects which data, and who has\naccess, is much harder in the digital world than it was in the\nanalogue world of paper and telephone calls. Many new AI technologies\namplify the known issues. For example, face recognition in photos and\nvideos allows identification and thus profiling and searching for\nindividuals (Whittaker et al. 2018: 15ff). This continues using other\ntechniques for identification, e.g., \u201cdevice\nfingerprinting\u201d, which are commonplace on the Internet\n(sometimes revealed in the \u201cprivacy policy\u201d). The result\nis that \u201cIn this vast ocean of data, there is a frighteningly\ncomplete picture of us\u201d (Smolan 2016: 1:01). The result is\narguably a scandal that still has not received due public\nattention.\n\nThe data trail we leave behind is how our \u201cfree\u201d services\nare paid for\u2014but we are not told about that data collection and\nthe value of this new raw material, and we are manipulated into\nleaving ever more such data. For the \u201cbig 5\u201d companies\n(Amazon, Google/Alphabet, Microsoft, Apple, Facebook), the main\ndata-collection part of their business appears to be based on\ndeception, exploiting human weaknesses, furthering procrastination,\ngenerating addiction, and manipulation (Harris 2016 [OIR]). The\nprimary focus of social media, gaming, and most of the Internet in\nthis \u201csurveillance economy\u201d is to gain, maintain, and\ndirect attention\u2014and thus data supply. \u201cSurveillance is\nthe business model of the Internet\u201d (Schneier 2015). This\nsurveillance and attention economy is sometimes called\n\u201csurveillance capitalism\u201d (Zuboff 2019). It has caused\nmany attempts to escape from the grasp of these corporations, e.g., in\nexercises of \u201cminimalism\u201d (Newport 2019), sometimes\nthrough the open source movement, but it appears that present-day\ncitizens have lost the degree of autonomy needed to escape while fully\ncontinuing with their life and work. We have lost ownership of our\ndata, if \u201cownership\u201d is the right relation here. Arguably,\nwe have lost control of our data.\n\nThese systems will often reveal facts about us that we ourselves wish\nto suppress or are not aware of: they know more about us than we know\nourselves. Even just observing online behaviour allows insights into\nour mental states (Burr and Christianini 2019) and manipulation (see\nbelow\n section 2.2).\n This has led to calls for the protection of \u201cderived\ndata\u201d (Wachter and Mittelstadt 2019). With the last sentence of\nhis bestselling book, Homo Deus, Harari asks about the\nlong-term consequences of AI:\n\n\nWhat will happen to society, politics and daily life when\nnon-conscious but highly intelligent algorithms know us better than we\nknow ourselves? (2016: 462)\n\n\nRobotic devices have not yet played a major role in this area, except\nfor security patrolling, but this will change once they are more\ncommon outside of industry environments. Together with the\n\u201cInternet of things\u201d, the so-called \u201csmart\u201d\nsystems (phone, TV, oven, lamp, virtual assistant, home,\u2026),\n\u201csmart city\u201d (Sennett 2018), and \u201csmart\ngovernance\u201d, they are set to become part of the data-gathering\nmachinery that offers more detailed data, of different types, in real\ntime, with ever more information.\n\nPrivacy-preserving techniques that can largely conceal the identity of\npersons or groups are now a standard staple in data science; they\ninclude (relative) anonymisation , access control (plus encryption),\nand other models where computation is carried out with fully or\npartially encrypted input data (Stahl and Wright 2018); in the case of\n\u201cdifferential privacy\u201d, this is done by adding calibrated\nnoise to encrypt the output of queries (Dwork et al. 2006; Abowd\n2017). While requiring more effort and cost, such techniques can avoid\nmany of the privacy issues. Some companies have also seen better\nprivacy as a competitive advantage that can be leveraged and sold at a\nprice.\n\nOne of the major practical difficulties is to actually enforce\nregulation, both on the level of the state and on the level of the\nindividual who has a claim. They must identify the responsible legal\nentity, prove the action, perhaps prove intent, find a court that\ndeclares itself competent \u2026 and eventually get the court to\nactually enforce its decision. Well-established legal protection of\nrights such as consumer rights, product liability, and other civil\nliability or protection of intellectual property rights is often\nmissing in digital products, or hard to enforce. This means that\ncompanies with a \u201cdigital\u201d background are used to testing\ntheir products on the consumers without fear of liability while\nheavily defending their intellectual property rights. This\n\u201cInternet Libertarianism\u201d is sometimes taken to assume\nthat technical solutions will take care of societal problems by\nthemselves (Mozorov 2013).\n2.2 Manipulation of Behaviour\n\nThe ethical issues of AI in surveillance go beyond the mere\naccumulation of data and direction of attention: They include\nthe use of information to manipulate behaviour, online and\noffline, in a way that undermines autonomous rational choice. Of\ncourse, efforts to manipulate behaviour are ancient, but they may gain\na new quality when they use AI systems. Given users\u2019 intense\ninteraction with data systems and the deep knowledge about individuals\nthis provides, they are vulnerable to \u201cnudges\u201d,\nmanipulation, and deception. With sufficient prior data, algorithms\ncan be used to target individuals or small groups with just the kind\nof input that is likely to influence these particular individuals. A\n\u2019nudge\u2018 changes the environment such that it influences\nbehaviour in a predictable way that is positive for the individual,\nbut easy and cheap to avoid (Thaler & Sunstein 2008). There is a\nslippery slope from here to paternalism and manipulation.\n\nMany advertisers, marketers, and online sellers will use any legal\nmeans at their disposal to maximise profit, including exploitation of\nbehavioural biases, deception, and addiction generation (Costa and\nHalpern 2019 [OIR]). Such manipulation is the business model in much\nof the gambling and gaming industries, but it is spreading, e.g., to\nlow-cost airlines. In interface design on web pages or in games, this\nmanipulation uses what is called \u201cdark patterns\u201d (Mathur\net al. 2019). At this moment, gambling and the sale of addictive\nsubstances are highly regulated, but online manipulation and addiction\nare not\u2014even though manipulation of online behaviour is becoming\na core business model of the Internet.\n\nFurthermore, social media is now the prime location for political\npropaganda. This influence can be used to steer voting behaviour, as\nin the Facebook-Cambridge Analytica \u201cscandal\u201d (Woolley and\nHoward 2017; Bradshaw, Neudert, and Howard 2019) and\u2014if\nsuccessful\u2014it may harm the autonomy of individuals (Susser,\nRoessler, and Nissenbaum 2019).\n\nImproved AI \u201cfaking\u201d technologies make what once was\nreliable evidence into unreliable evidence\u2014this has already\nhappened to digital photos, sound recordings, and video. It will soon\nbe quite easy to create (rather than alter) \u201cdeep fake\u201d\ntext, photos, and video material with any desired content. Soon,\nsophisticated real-time interaction with persons over text, phone, or\nvideo will be faked, too. So we cannot trust digital interactions\nwhile we are at the same time increasingly dependent on such\ninteractions.\n\nOne more specific issue is that machine learning techniques in AI rely\non training with vast amounts of data. This means there will often be\na trade-off between privacy and rights to data vs. technical quality\nof the product. This influences the consequentialist evaluation of\nprivacy-violating practices.\n\nThe policy in this field has its ups and downs: Civil liberties and\nthe protection of individual rights are under intense pressure from\nbusinesses\u2019 lobbying, secret services, and other state agencies\nthat depend on surveillance. Privacy protection has diminished\nmassively compared to the pre-digital age when communication was based\non letters, analogue telephone communications, and personal\nconversation and when surveillance operated under significant legal\nconstraints.\n\nWhile the EU General Data Protection Regulation (Regulation (EU)\n2016/679) has strengthened privacy protection, the US and China prefer\ngrowth with less regulation (Thompson and Bremmer 2018), likely in the\nhope that this provides a competitive advantage. It is clear that\nstate and business actors have increased their ability to invade\nprivacy and manipulate people with the help of AI technology and will\ncontinue to do so to further their particular interests\u2014unless\nreined in by policy in the interest of general society.\n2.3 Opacity of AI Systems\n\nOpacity and bias are central issues in what is now sometimes called\n\u201cdata ethics\u201d or \u201cbig data ethics\u201d (Floridi\nand Taddeo 2016; Mittelstadt and Floridi 2016). AI systems for\nautomated decision support and \u201cpredictive analytics\u201d\nraise \u201csignificant concerns about lack of due process,\naccountability, community engagement, and auditing\u201d (Whittaker\net al. 2018: 18ff). They are part of a power structure in which\n\u201cwe are creating decision-making processes that constrain and\nlimit opportunities for human participation\u201d (Danaher 2016b:\n245). At the same time, it will often be impossible for the affected\nperson to know how the system came to this output, i.e., the system is\n\u201copaque\u201d to that person. If the system involves machine\nlearning, it will typically be opaque even to the expert, who will not\nknow how a particular pattern was identified, or even what the pattern\nis. Bias in decision systems and data sets is exacerbated by this\nopacity. So, at least in cases where there is a desire to remove bias,\nthe analysis of opacity and bias go hand in hand, and political\nresponse has to tackle both issues together.\n\nMany AI systems rely on machine learning techniques in (simulated)\nneural networks that will extract patterns from a given dataset, with\nor without \u201ccorrect\u201d solutions provided; i.e., supervised,\nsemi-supervised or unsupervised. With these techniques, the\n\u201clearning\u201d captures patterns in the data and these are\nlabelled in a way that appears useful to the decision the system\nmakes, while the programmer does not really know which\npatterns in the data the system has used. In fact, the programs are\nevolving, so when new data comes in, or new feedback is given\n(\u201cthis was correct\u201d, \u201cthis was incorrect\u201d),\nthe patterns used by the learning system change. What this means is\nthat the outcome is not transparent to the user or programmers: it is\nopaque. Furthermore, the quality of the program depends heavily on the\nquality of the data provided, following the old slogan \u201cgarbage\nin, garbage out\u201d. So, if the data already involved a bias (e.g.,\npolice data about the skin colour of suspects), then the program will\nreproduce that bias. There are proposals for a standard description of\ndatasets in a \u201cdatasheet\u201d that would make the\nidentification of such bias more feasible (Gebru et al. 2018 [OIR]).\nThere is also significant recent literature about the limitations of\nmachine learning systems that are essentially sophisticated data\nfilters (Marcus 2018 [OIR]). Some have argued that the ethical\nproblems of today are the result of technical \u201cshortcuts\u201d\nAI has taken (Cristianini forthcoming).\n\nThere are several technical activities that aim at \u201cexplainable\nAI\u201d, starting with (Van Lent, Fisher, and Mancuso 1999; Lomas et\nal. 2012) and, more recently, a DARPA programme (Gunning 2017 [OIR]).\n\nMore broadly, the demand for\n\n\na mechanism for elucidating and articulating the power structures,\nbiases, and influences that computational artefacts exercise in\nsociety (Diakopoulos 2015: 398)\n\n\nis sometimes called \u201calgorithmic accountability\nreporting\u201d. This does not mean that we expect an AI to\n\u201cexplain its reasoning\u201d\u2014doing so would require far\nmore serious moral autonomy than we currently attribute to AI systems\n(see below\n \u00a72.10).\n\nThe politician Henry Kissinger pointed out that there is a fundamental\nproblem for democratic decision-making if we rely on a system that is\nsupposedly superior to humans, but cannot explain its decisions. He\nsays we may have \u201cgenerated a potentially dominating technology\nin search of a guiding philosophy\u201d (Kissinger 2018). Danaher\n(2016b) calls this problem \u201cthe threat of algocracy\u201d\n(adopting the previous use of \u2018algocracy\u2019 from Aneesh 2002\n[OIR], 2006). In a similar vein, Cave (2019) stresses that we need a\nbroader societal move towards more \u201cdemocratic\u201d\ndecision-making to avoid AI being a force that leads to a Kafka-style\nimpenetrable suppression system in public administration and\nelsewhere. The political angle of this discussion has been stressed by\nO\u2019Neil in her influential book Weapons of Math\nDestruction (2016), and by Yeung and Lodge (2019).\n\nIn the EU, some of these issues have been taken into account with the\n(Regulation (EU) 2016/679), which foresees that consumers, when faced\nwith a decision based on data processing, will have a legal\n\u201cright to explanation\u201d\u2014how far this goes and to what\nextent it can be enforced is disputed (Goodman and Flaxman 2017;\nWachter, Mittelstadt, and Floridi 2016; Wachter, Mittelstadt, and\nRussell 2017). Zerilli et al. (2019) argue that there may be a double\nstandard here, where we demand a high level of explanation for\nmachine-based decisions despite humans sometimes not reaching that\nstandard themselves.\n2.4 Bias in Decision Systems\n\nAutomated AI decision support systems and \u201cpredictive\nanalytics\u201d operate on data and produce a decision as\n\u201coutput\u201d. This output may range from the relatively\ntrivial to the highly significant: \u201cthis restaurant matches your\npreferences\u201d, \u201cthe patient in this X-ray has completed\nbone growth\u201d, \u201capplication to credit card declined\u201d,\n\u201cdonor organ will be given to another patient\u201d,\n\u201cbail is denied\u201d, or \u201ctarget identified and\nengaged\u201d. Data analysis is often used in \u201cpredictive\nanalytics\u201d in business, healthcare, and other fields, to foresee\nfuture developments\u2014since prediction is easier, it will also\nbecome a cheaper commodity. One use of prediction is in\n\u201cpredictive policing\u201d (NIJ 2014 [OIR]), which many fear\nmight lead to an erosion of public liberties (Ferguson 2017) because\nit can take away power from the people whose behaviour is predicted.\nIt appears, however, that many of the worries about policing depend on\nfuturistic scenarios where law enforcement foresees and punishes\nplanned actions, rather than waiting until a crime has been committed\n(like in the 2002 film \u201cMinority Report\u201d). One concern is\nthat these systems might perpetuate bias that was already in the data\nused to set up the system, e.g., by increasing police patrols in an\narea and discovering more crime in that area. Actual \u201cpredictive\npolicing\u201d or \u201cintelligence led policing\u201d techniques\nmainly concern the question of where and when police forces will be\nneeded most. Also, police officers can be provided with more data,\noffering them more control and facilitating better decisions, in\nworkflow support software (e.g., \u201cArcGIS\u201d). Whether this\nis problematic depends on the appropriate level of trust in the\ntechnical quality of these systems, and on the evaluation of aims of\nthe police work itself. Perhaps a recent paper title points in the\nright direction here: \u201cAI ethics in predictive policing: From\nmodels of threat to an ethics of care\u201d (Asaro 2019).\n\nBias typically surfaces when unfair judgments are made because the\nindividual making the judgment is influenced by a characteristic that\nis actually irrelevant to the matter at hand, typically a\ndiscriminatory preconception about members of a group. So, one form of\nbias is a learned cognitive feature of a person, often not made\nexplicit. The person concerned may not be aware of having that\nbias\u2014they may even be honestly and explicitly opposed to a bias\nthey are found to have (e.g., through priming, cf. Graham and Lowery\n2004). On fairness vs. bias in machine learning, see Binns (2018).\n\nApart from the social phenomenon of learned bias, the human cognitive\nsystem is generally prone to have various kinds of \u201ccognitive\nbiases\u201d, e.g., the \u201cconfirmation bias\u201d: humans tend\nto interpret information as confirming what they already believe. This\nsecond form of bias is often said to impede performance in rational\njudgment (Kahnemann 2011)\u2014though at least some cognitive biases\ngenerate an evolutionary advantage, e.g., economical use of resources\nfor intuitive judgment. There is a question whether AI systems could\nor should have such cognitive bias.\n\nA third form of bias is present in data when it exhibits systematic\nerror, e.g., \u201cstatistical bias\u201d. Strictly, any given\ndataset will only be unbiased for a single kind of issue, so the mere\ncreation of a dataset involves the danger that it may be used for a\ndifferent kind of issue, and then turn out to be biased for that kind.\nMachine learning on the basis of such data would then not only fail to\nrecognise the bias, but codify and automate the \u201chistorical\nbias\u201d. Such historical bias was discovered in an automated\nrecruitment screening system at Amazon (discontinued early 2017) that\ndiscriminated against women\u2014presumably because the company had a\nhistory of discriminating against women in the hiring process. The\n\u201cCorrectional Offender Management Profiling for Alternative\nSanctions\u201d (COMPAS), a system to predict whether a defendant\nwould re-offend, was found to be as successful (65.2% accuracy) as a\ngroup of random humans (Dressel and Farid 2018) and to produce more\nfalse positives and less false negatives for black defendants. The\nproblem with such systems is thus bias plus humans placing excessive\ntrust in the systems. The political dimensions of such automated\nsystems in the USA are investigated in Eubanks (2018).\n\nThere are significant technical efforts to detect and remove bias from\nAI systems, but it is fair to say that these are in early stages: see\nUK Institute for Ethical AI & Machine Learning (Brownsword,\nScotford, and Yeung 2017; Yeung and Lodge 2019). It appears that\ntechnological fixes have their limits in that they need a mathematical\nnotion of fairness, which is hard to come by (Whittaker et al. 2018:\n24ff; Selbst et al. 2019), as is a formal notion of \u201crace\u201d\n(see Benthall and Haynes 2019). An institutional proposal is in (Veale\nand Binns 2017).\n2.5 Human-Robot Interaction\n\nHuman-robot interaction (HRI) is an academic fields in its own right,\nwhich now pays significant attention to ethical matters, the dynamics\nof perception from both sides, and both the different interests\npresent in and the intricacy of the social context, including\nco-working (e.g., Arnold and Scheutz 2017). Useful surveys for the\nethics of robotics include Calo, Froomkin, and Kerr (2016); Royakkers\nand van Est (2016); Tzafestas (2016); a standard collection of papers\nis Lin, Abney, and Jenkins (2017).\n\nWhile AI can be used to manipulate humans into believing and doing\nthings (see\n section 2.2),\n it can also be used to drive robots that are problematic if their\nprocesses or appearance involve deception, threaten human dignity, or\nviolate the Kantian requirement of \u201crespect for humanity\u201d.\nHumans very easily attribute mental properties to objects, and\nempathise with them, especially when the outer appearance of these\nobjects is similar to that of living beings. This can be used to\ndeceive humans (or animals) into attributing more intellectual or even\nemotional significance to robots or AI systems than they deserve. Some\nparts of humanoid robotics are problematic in this regard (e.g.,\nHiroshi Ishiguro\u2019s remote-controlled Geminoids), and there are\ncases that have been clearly deceptive for public-relations purposes\n(e.g. on the abilities of Hanson Robotics\u2019\n\u201cSophia\u201d). Of course, some fairly basic constraints of\nbusiness ethics and law apply to robots, too: product safety and\nliability, or non-deception in advertisement. It appears that these\nexisting constraints take care of many concerns that are raised. There\nare cases, however, where human-human interaction has aspects that\nappear specifically human in ways that can perhaps not be replaced by\nrobots: care, love, and sex.\n2.5.1 Example (a) Care Robots\n\nThe use of robots in health care for humans is currently at the level\nof concept studies in real environments, but it may become a usable\ntechnology in a few years, and has raised a number of concerns for a\ndystopian future of de-humanised care (A. Sharkey and N. Sharkey 2011;\nRobert Sparrow 2016). Current systems include robots that support\nhuman carers/caregivers (e.g., in lifting patients, or transporting\nmaterial), robots that enable patients to do certain things by\nthemselves (e.g., eat with a robotic arm), but also robots that are\ngiven to patients as company and comfort (e.g., the \u201cParo\u201d\nrobot seal). For an overview, see van Wynsberghe (2016);\nN\u00f8rskov (2017); Fosch-Villaronga and Albo-Canals (2019), for a\nsurvey of users Draper et al. (2014).\n\nOne reason why the issue of care has come to the fore is that people\nhave argued that we will need robots in ageing societies. This\nargument makes problematic assumptions, namely that with longer\nlifespan people will need more care, and that it will not be possible\nto attract more humans to caring professions. It may also show a bias\nabout age (Jecker forthcoming). Most importantly, it ignores the\nnature of automation, which is not simply about replacing humans, but\nabout allowing humans to work more efficiently. It is not very clear\nthat there really is an issue here since the discussion mostly focuses\non the fear of robots de-humanising care, but the actual and\nforeseeable robots in care are assistive robots for classic automation\nof technical tasks. They are thus \u201ccare robots\u201d only in a\nbehavioural sense of performing tasks in care environments, not in the\nsense that a human \u201ccares\u201d for the patients. It appears\nthat the success of \u201cbeing cared for\u201d relies on this\nintentional sense of \u201ccare\u201d, which foreseeable robots\ncannot provide. If anything, the risk of robots in care is the\nabsence of such intentional care\u2014because less human\ncarers may be needed. Interestingly, caring for something, even a\nvirtual agent, can be good for the carer themselves (Lee et al. 2019).\nA system that pretends to care would be deceptive and thus\nproblematic\u2014unless the deception is countered by sufficiently\nlarge utility gain (Coeckelbergh 2016). Some robots that pretend to\n\u201ccare\u201d on a basic level are available (Paro seal) and\nothers are in the making. Perhaps feeling cared for by a machine, to\nsome extent, is progress for come patients.\n2.5.2 Example (b) Sex Robots\n\nIt has been argued by several tech optimists that humans will likely\nbe interested in sex and companionship with robots and be comfortable\nwith the idea (Levy 2007). Given the variation of human sexual\npreferences, including sex toys and sex dolls, this seems very likely:\nThe question is whether such devices should be manufactured and\npromoted, and whether there should be limits in this touchy area. It\nseems to have moved into the mainstream of \u201crobot\nphilosophy\u201d in recent times (Sullins 2012; Danaher and McArthur\n2017; N. Sharkey et al. 2017 [OIR]; Bendel 2018; Devlin 2018).\n\nHumans have long had deep emotional attachments to objects, so perhaps\ncompanionship or even love with a predictable android is attractive,\nespecially to people who struggle with actual humans, and already\nprefer dogs, cats, birds, a computer or a tamagotchi. Danaher\n(2019b) argues against (Nyholm and Frank 2017) that these can be true\nfriendships, and is thus a valuable goal. It certainly looks like such\nfriendship might increase overall utility, even if lacking in depth.\nIn these discussions there is an issue of deception, since a robot\ncannot (at present) mean what it says, or have feelings for a human.\nIt is well known that humans are prone to attribute feelings and\nthoughts to entities that behave as if they had sentience,even to\nclearly inanimate objects that show no behaviour at all. Also, paying\nfor deception seems to be an elementary part of the traditional sex\nindustry.\n\nFinally, there are concerns that have often accompanied matters of\nsex, namely consent (Frank and Nyholm 2017), aesthetic concerns, and\nthe worry that humans may be \u201ccorrupted\u201d by certain\nexperiences. Old fashioned though this may seem, human behaviour is\ninfluenced by experience, and it is likely that pornography or sex\nrobots support the perception of other humans as mere objects of\ndesire, or even recipients of abuse, and thus ruin a deeper sexual and\nerotic experience. In this vein, the \u201cCampaign Against Sex\nRobots\u201d argues that these devices are a continuation of slavery\nand prostitution (Richardson 2016).\n2.6 Automation and Employment\n\nIt seems clear that AI and robotics will lead to significant gains in\nproductivity and thus overall wealth. The attempt to increase\nproductivity has often been a feature of the economy, though the\nemphasis on \u201cgrowth\u201d is a modern phenomenon (Harari 2016:\n240). However, productivity gains through automation typically mean\nthat fewer humans are required for the same output. This does not\nnecessarily imply a loss of overall employment, however, because\navailable wealth increases and that can increase demand sufficiently\nto counteract the productivity gain. In the long run, higher\nproductivity in industrial societies has led to more wealth overall.\nMajor labour market disruptions have occurred in the past, e.g.,\nfarming employed over 60% of the workforce in Europe and North-America\nin 1800, while by 2010 it employed ca. 5% in the EU, and even less in\nthe wealthiest countries (European Commission 2013). In the 20 years\nbetween 1950 and 1970 the number of hired agricultural workers in the\nUK was reduced by 50% (Zayed and Loft 2019). Some of these disruptions\nlead to more labour-intensive industries moving to places with lower\nlabour cost. This is an ongoing process.\n\nClassic automation replaced human muscle, whereas digital automation\nreplaces human thought or information-processing\u2014and unlike\nphysical machines, digital automation is very cheap to duplicate\n(Bostrom and Yudkowsky 2014). It may thus mean a more radical change\non the labour market. So, the main question is: will the effects be\ndifferent this time? Will the creation of new jobs and wealth keep up\nwith the destruction of jobs? And even if it is not\ndifferent, what are the transition costs, and who bears them? Do we\nneed to make societal adjustments for a fair distribution of costs and\nbenefits of digital automation?\n\nResponses to the issue of unemployment from AI have ranged from the\nalarmed (Frey and Osborne 2013; Westlake 2014) to the neutral\n(Metcalf, Keller, and Boyd 2016 [OIR]; Calo 2018; Frey 2019) to the\noptimistic (Brynjolfsson and McAfee 2016; Harari 2016; Danaher 2019a).\nIn principle, the labour market effect of automation seems to be\nfairly well understood as involving two channels:\n\n\n(i) the nature of interactions between differently skilled workers and\nnew technologies affecting labour demand and (ii) the equilibrium\neffects of technological progress through consequent changes in labour\nsupply and product markets. (Goos 2018: 362)\n\n\nWhat currently seems to happen in the labour market as a result of AI\nand robotics automation is \u201cjob polarisation\u201d or the\n\u201cdumbbell\u201d shape (Goos, Manning, and Salomons 2009): The\nhighly skilled technical jobs are in demand and highly paid, the low\nskilled service jobs are in demand and badly paid, but the\nmid-qualification jobs in factories and offices, i.e., the majority of\njobs, are under pressure and reduced because they are relatively\npredictable, and most likely to be automated (Baldwin 2019).\n\nPerhaps enormous productivity gains will allow the \u201cage of\nleisure\u201d to be realised, something (Keynes 1930) had predicted\nto occur around 2030, assuming a growth rate of 1% per annum.\nActually, we have already reached the level he anticipated for 2030,\nbut we are still working\u2014consuming more and inventing ever more\nlevels of organisation. Harari explains how this economic development\nallowed humanity to overcome hunger, disease, and war\u2014and now we\naim for immortality and eternal bliss through AI, thus his title\nHomo Deus (Harari 2016: 75).\n\nIn general terms, the issue of unemployment is an issue of how goods\nin a society should be justly distributed. A standard view is that\ndistributive justice should be rationally decided from behind a\n\u201cveil of ignorance\u201d (Rawls 1971), i.e., as if one does not\nknow what position in a society one would actually be taking (labourer\nor industrialist, etc.). Rawls thought the chosen principles would\nthen support basic liberties and a distribution that is of greatest\nbenefit to the least-advantaged members of society. It would appear\nthat the AI economy has three features that make such justice\nunlikely: First, it operates in a largely unregulated environment\nwhere responsibility is often hard to allocate. Second, it operates in\nmarkets that have a \u201cwinner takes all\u201d feature where\nmonopolies develop quickly. Third, the \u201cnew economy\u201d of\nthe digital service industries is based on intangible assets, also\ncalled \u201ccapitalism without capital\u201d (Haskel and Westlake\n2017). This means that it is difficult to control multinational\ndigital corporations that do not rely on a physical plant in a\nparticular location. These three features seem to suggest that if we\nleave the distribution of wealth to free market forces, the result\nwould be a heavily unjust distribution: And this is indeed a\ndevelopment that we can already see.\n\nOne interesting question that has not received too much attention is\nwhether the development of AI is environmentally sustainable: Like all\ncomputing systems, AI systems produce waste that is very hard to\nrecycle and they consume vast amounts of energy, especially for the\ntraining of machine learning systems (and even for the\n\u201cmining\u201d of cryptocurrency). Again, it appears that some\nactors in this space offload such costs to the general society.\n2.7 Autonomous Systems\n\nThere are several notions of autonomy in the discussion of autonomous\nsystems. A stronger notion is involved in philosophical debates where\nautonomy is the basis for responsibility and personhood (Christman\n2003 [2018]). In this context, responsibility implies autonomy, but\nnot inversely, so there can be systems that have degrees of technical\nautonomy without raising issues of responsibility. The weaker, more\ntechnical, notion of autonomy in robotics is relative and gradual: A\nsystem is said to be autonomous with respect to human control to a\ncertain degree (M\u00fcller 2012). There is a parallel here to the\nissues of bias and opacity in AI since autonomy also concerns a\npower-relation: who is in control, and who is responsible?\n\nGenerally speaking, one question is the degree to which autonomous\nrobots raise issues our present conceptual schemes must adapt to, or\nwhether they just require technical adjustments. In most\njurisdictions, there is a sophisticated system of civil and criminal\nliability to resolve such issues. Technical standards, e.g., for the\nsafe use of machinery in medical environments, will likely need to be\nadjusted. There is already a field of \u201cverifiable AI\u201d for\nsuch safety-critical systems and for \u201csecurity\napplications\u201d. Bodies like the IEEE (The Institute of Electrical\nand Electronics Engineers) and the BSI (British Standards Institution)\nhave produced \u201cstandards\u201d, particularly on more technical\nsub-problems, such as data security and transparency. Among the many\nautonomous systems on land, on water, under water, in air or space, we\ndiscuss two samples: autonomous vehicles and autonomous weapons.\n2.7.1 Example (a) Autonomous Vehicles\n\nAutonomous vehicles hold the promise to reduce the very significant\ndamage that human driving currently causes\u2014approximately 1\nmillion humans being killed per year, many more injured, the\nenvironment polluted, earth sealed with concrete and tarmac, cities\nfull of parked cars, etc. However, there seem to be questions on how\nautonomous vehicles should behave, and how responsibility and risk\nshould be distributed in the complicated system the vehicles operates\nin. (There is also significant disagreement over how long the\ndevelopment of fully autonomous, or \u201clevel 5\u201d cars (SAE\nInternational 2018) will actually take.)\n\nThere is some discussion of \u201ctrolley problems\u201d in this\ncontext. In the classic \u201ctrolley problems\u201d (Thomson 1976;\nWoollard and Howard-Snyder 2016: section 2) various dilemmas are\npresented. The simplest version is that of a trolley train on a track\nthat is heading towards five people and will kill them, unless the\ntrain is diverted onto a side track, but on that track there is one\nperson, who will be killed if the train takes that side track. The\nexample goes back to a remark in (Foot 1967: 6), who discusses a\nnumber of dilemma cases where tolerated and intended consequences of\nan action differ. \u201cTrolley problems\u201d are not supposed to\ndescribe actual ethical problems or to be solved with a\n\u201cright\u201d choice. Rather, they are thought-experiments where\nchoice is artificially constrained to a small finite number of\ndistinct one-off options and where the agent has perfect knowledge.\nThese problems are used as a theoretical tool to investigate ethical\nintuitions and theories\u2014especially the difference between\nactively doing vs. allowing something to happen, intended vs.\ntolerated consequences, and consequentialist vs. other normative\napproaches (Kamm 2016). This type of problem has reminded many of the\nproblems encountered in actual driving and in autonomous driving (Lin\n2016). It is doubtful, however, that an actual driver or autonomous\ncar will ever have to solve trolley problems (but see Keeling 2020).\nWhile autonomous car trolley problems have received a lot of media\nattention (Awad et al. 2018), they do not seem to offer anything new\nto either ethical theory or to the programming of autonomous\nvehicles.\n\nThe more common ethical problems in driving, such as speeding, risky\novertaking, not keeping a safe distance, etc. are classic problems of\npursuing personal interest vs. the common good. The vast majority of\nthese are covered by legal regulations on driving. Programming the car\nto drive \u201cby the rules\u201d rather than \u201cby the interest\nof the passengers\u201d or \u201cto achieve maximum utility\u201d\nis thus deflated to a standard problem of programming ethical machines\n(see\n section 2.9).\n There are probably additional discretionary rules of politeness and\ninteresting questions on when to break the rules (Lin 2016), but again\nthis seems to be more a case of applying standard considerations\n(rules vs. utility) to the case of autonomous vehicles.\n\nNotable policy efforts in this field include the report (German\nFederal Ministry of Transport and Digital Infrastructure 2017), which\nstresses that safety is the primary objective. Rule 10\nstates\n\n\nIn the case of automated and connected driving systems, the\naccountability that was previously the sole preserve of the individual\nshifts from the motorist to the manufacturers and operators of the\ntechnological systems and to the bodies responsible for taking\ninfrastructure, policy and legal decisions.\n\n\n(See\n section 2.10.1\n below). The resulting German and EU laws on licensing automated\ndriving are much more restrictive than their US counterparts where\n\u201ctesting on consumers\u201d is a strategy used by some\ncompanies\u2014without informed consent of the consumers or their\npossible victims.\n2.7.2 Example (b) Autonomous Weapons\n\nThe notion of automated weapons is fairly old:\n\n\nFor example, instead of fielding simple guided missiles or remotely\npiloted vehicles, we might launch completely autonomous land, sea, and\nair vehicles capable of complex, far-ranging reconnaissance and attack\nmissions. (DARPA 1983: 1)\n\n\nThis proposal was ridiculed as \u201cfantasy\u201d at the time\n(Dreyfus, Dreyfus, and Athanasiou 1986: ix), but it is now a reality,\nat least for more easily identifiable targets (missiles, planes,\nships, tanks, etc.), but not for human combatants. The main arguments\nagainst (lethal) autonomous weapon systems (AWS or LAWS), are that\nthey support extrajudicial killings, take responsibility away from\nhumans, and make wars or killings more likely\u2014for a detailed\nlist of issues see Lin, Bekey, and Abney (2008: 73\u201386).\n\nIt appears that lowering the hurdle to use such systems (autonomous\nvehicles, \u201cfire-and-forget\u201d missiles, or drones loaded\nwith explosives) and reducing the probability of being held\naccountable would increase the probability of their use. The crucial\nasymmetry where one side can kill with impunity, and thus has few\nreasons not to do so, already exists in conventional drone wars with\nremote controlled weapons (e.g., US in Pakistan). It is easy to\nimagine a small drone that searches, identifies, and kills an\nindividual human\u2014or perhaps a type of human. These are the kinds\nof cases brought forward by the Campaign to Stop Killer\nRobots and other activist groups. Some seem to be equivalent to\nsaying that autonomous weapons are indeed weapons \u2026, and\nweapons kill, but we still make them in gigantic numbers. On the\nmatter of accountability, autonomous weapons might make identification\nand prosecution of the responsible agents more difficult\u2014but\nthis is not clear, given the digital records that one can keep, at\nleast in a conventional war. The difficulty of allocating punishment\nis sometimes called the \u201cretribution gap\u201d (Danaher\n2016a).\n\nAnother question is whether using autonomous weapons in war would make\nwars worse, or make wars less bad. If robots reduce war crimes and\ncrimes in war, the answer may well be positive and has been used as an\nargument in favour of these weapons (Arkin 2009; M\u00fcller 2016a)\nbut also as an argument against them (Amoroso and Tamburrini 2018).\nArguably the main threat is not the use of such weapons in\nconventional warfare, but in asymmetric conflicts or by non-state\nagents, including criminals.\n\nIt has also been said that autonomous weapons cannot conform to\nInternational Humanitarian Law, which requires observance of the\nprinciples of distinction (between combatants and civilians),\nproportionality (of force), and military necessity (of force) in\nmilitary conflict (A. Sharkey 2019). It is true that the distinction\nbetween combatants and non-combatants is hard, but the distinction\nbetween civilian and military ships is easy\u2014so all this says is\nthat we should not construct and use such weapons if they do violate\nHumanitarian Law. Additional concerns have been raised that being\nkilled by an autonomous weapon threatens human dignity, but even the\ndefenders of a ban on these weapons seem to say that these are not\ngood arguments:\n\n\nThere are other weapons, and other technologies, that also compromise\nhuman dignity. Given this, and the ambiguities inherent in the\nconcept, it is wiser to draw on several types of objections in\narguments against AWS, and not to rely exclusively on human dignity.\n(A. Sharkey 2019)\n\n\nA lot has been made of keeping humans \u201cin the loop\u201d or\n\u201con the loop\u201d in the military guidance on\nweapons\u2014these ways of spelling out \u201cmeaningful\ncontrol\u201d are discussed in (Santoni de Sio and van den Hoven\n2018). There have been discussions about the difficulties of\nallocating responsibility for the killings of an autonomous weapon,\nand a \u201cresponsibility gap\u201d has been suggested (esp. Rob\nSparrow 2007), meaning that neither the human nor the machine may be\nresponsible. On the other hand, we do not assume that for every event\nthere is someone responsible for that event, and the real issue may\nwell be the distribution of risk (Simpson and M\u00fcller 2016). Risk\nanalysis (Hansson 2013) indicates it is crucial to identify who is\nexposed to risk, who is a potential beneficiary, and\nwho makes the decisions (Hansson 2018: 1822\u20131824).\n2.8 Machine Ethics\n\nMachine ethics is ethics for machines, for \u201cethical\nmachines\u201d, for machines as subjects, rather than for\nthe human use of machines as objects. It is often not very\nclear whether this is supposed to cover all of AI ethics or to be a\npart of it (Floridi and Saunders 2004; Moor 2006; Anderson and\nAnderson 2011; Wallach and Asaro 2017). Sometimes it looks as though\nthere is the (dubious) inference at play here that if machines act in\nethically relevant ways, then we need a machine ethics. Accordingly,\nsome use a broader notion:\n\n\nmachine ethics is concerned with ensuring that the behavior of\nmachines toward human users, and perhaps other machines as well, is\nethically acceptable. (Anderson and Anderson 2007: 15)\n\n\nThis might include mere matters of product safety, for example. Other\nauthors sound rather ambitious but use a narrower notion:\n\n\nAI reasoning should be able to take into account societal values,\nmoral and ethical considerations; weigh the respective priorities of\nvalues held by different stakeholders in various multicultural\ncontexts; explain its reasoning; and guarantee transparency. (Dignum\n2018: 1, 2)\n\n\nSome of the discussion in machine ethics makes the very substantial\nassumption that machines can, in some sense, be ethical agents\nresponsible for their actions, or \u201cautonomous moral\nagents\u201d (see van Wynsberghe and Robbins 2019). The basic idea of\nmachine ethics is now finding its way into actual robotics where the\nassumption that these machines are artificial moral agents in any\nsubstantial sense is usually not made (Winfield et al. 2019). It is\nsometimes observed that a robot that is programmed to follow ethical\nrules can very easily be modified to follow unethical rules\n(Vanderelst and Winfield 2018).\n\nThe idea that machine ethics might take the form of \u201claws\u201d\nhas famously been investigated by Isaac Asimov, who proposed\n\u201cthree laws of robotics\u201d (Asimov 1942):\n\n\nFirst Law\u2014A robot may not injure a human being or, through\ninaction, allow a human being to come to harm. Second Law\u2014A\nrobot must obey the orders given it by human beings except where such\norders would conflict with the First Law. Third Law\u2014A robot must\nprotect its own existence as long as such protection does not conflict\nwith the First or Second Laws.\n\n\nAsimov then showed in a number of stories how conflicts between these\nthree laws will make it problematic to use them despite their\nhierarchical organisation.\n\nIt is not clear that there is a consistent notion of \u201cmachine\nethics\u201d since weaker versions are in danger of reducing\n\u201chaving an ethics\u201d to notions that would not normally be\nconsidered sufficient (e.g., without \u201creflection\u201d or even\nwithout \u201caction\u201d); stronger notions that move towards\nartificial moral agents may describe a\u2014currently\u2014empty\nset.\n2.9 Artificial Moral Agents\n\nIf\n one takes machine ethics to concern moral agents, in some substantial\nsense, then these agents can be called \u201cartificial moral\nagents\u201d, having rights and responsibilities. However, the\ndiscussion about artificial entities challenges a number of common\nnotions in ethics and it can be very useful to understand these in\nabstraction from the human case (cf. Misselhorn 2020; Powers and\nGanascia forthcoming).\n\nSeveral authors use \u201cartificial moral agent\u201d in a less\ndemanding sense, borrowing from the use of \u201cagent\u201d in\nsoftware engineering in which case matters of responsibility and\nrights will not arise (Allen, Varner, and Zinser 2000). James Moor\n(2006) distinguishes four types of machine agents: ethical impact\nagents (e.g., robot jockeys), implicit ethical agents (e.g., safe\nautopilot), explicit ethical agents (e.g., using formal methods to\nestimate utility), and full ethical agents (who \u201ccan make\nexplicit ethical judgments and generally is competent to reasonably\njustify them. An average adult human is a full ethical agent\u201d.)\nSeveral ways to achieve \u201cexplicit\u201d or \u201cfull\u201d\nethical agents have been proposed, via programming it in (operational\nmorality), via \u201cdeveloping\u201d the ethics itself (functional\nmorality), and finally full-blown morality with full intelligence and\nsentience (Allen, Smit, and Wallach 2005; Moor 2006). Programmed\nagents are sometimes not considered \u201cfull\u201d agents because\nthey are \u201ccompetent without comprehension\u201d, just like the\nneurons in a brain (Dennett 2017; Hakli and M\u00e4kel\u00e4\n2019).\n\nIn some discussions, the notion of \u201cmoral patient\u201d plays a\nrole: Ethical agents have responsibilities while ethical\npatients have rights because harm to them matters. It seems\nclear that some entities are patients without being agents, e.g.,\nsimple animals that can feel pain but cannot make justified choices.\nOn the other hand, it is normally understood that all agents will also\nbe patients (e.g., in a Kantian framework). Usually, being a person is\nsupposed to be what makes an entity a responsible agent, someone who\ncan have duties and be the object of ethical concerns. Such personhood\nis typically a deep notion associated with phenomenal consciousness,\nintention and free will (Frankfurt 1971; Strawson 1998). Torrance\n(2011) suggests \u201cartificial (or machine) ethics could be defined\nas designing machines that do things that, when done by humans, are\nindicative of the possession of \u2018ethical status\u2019 in those\nhumans\u201d (2011: 116)\u2014which he takes to be \u201cethical\nproductivity and ethical receptivity\u201d (2011:\n117)\u2014his expressions for moral agents and patients.\n2.9.1 Responsibility for Robots\n\nThere is broad consensus that accountability, liability, and the rule\nof law are basic requirements that must be upheld in the face of new\ntechnologies (European Group on Ethics in Science and New Technologies\n2018, 18), but the issue in the case of robots is how this can be done\nand how responsibility can be allocated. If the robots act, will they\nthemselves be responsible, liable, or accountable for their actions?\nOr should the distribution of risk perhaps take precedence over\ndiscussions of responsibility?\n\nTraditional distribution of responsibility already occurs: A car maker\nis responsible for the technical safety of the car, a driver is\nresponsible for driving, a mechanic is responsible for proper\nmaintenance, the public authorities are responsible for the technical\nconditions of the roads, etc. In general\n\n\nThe effects of decisions or actions based on AI are often the result\nof countless interactions among many actors, including designers,\ndevelopers, users, software, and hardware.\u2026 With distributed\nagency comes distributed responsibility. (Taddeo and Floridi 2018:\n751).\n\n\nHow this distribution might occur is not a problem that is specific to\nAI, but it gains particular urgency in this context (Nyholm 2018a,\n2018b). In classical control engineering, distributed control is often\nachieved through a control hierarchy plus control loops across these\nhierarchies.\n2.9.2 Rights for Robots\n\nSome authors have indicated that it should be seriously considered\nwhether current robots must be allocated rights (Gunkel 2018a, 2018b;\nDanaher forthcoming; Turner 2019). This position seems to rely largely\non criticism of the opponents and on the empirical observation that\nrobots and other non-persons are sometimes treated as having rights.\nIn this vein, a \u201crelational turn\u201d has been proposed: If we\nrelate to robots as though they had rights, then we might be\nwell-advised not to search whether they \u201creally\u201d do have\nsuch rights (Coeckelbergh 2010, 2012, 2018). This raises the question\nhow far such anti-realism or quasi-realism can go, and what it means\nthen to say that \u201crobots have rights\u201d in a human-centred\napproach (Gerdes 2016). On the other side of the debate, Bryson has\ninsisted that robots should not enjoy rights (Bryson 2010), though she\nconsiders it a possibility (Gunkel and Bryson 2014).\n\nThere is a wholly separate issue whether robots (or other AI systems)\nshould be given the status of \u201clegal entities\u201d or\n\u201clegal persons\u201d in a sense natural persons, but also\nstates, businesses, or organisations are \u201centities\u201d,\nnamely they can have legal rights and duties. The European Parliament\nhas considered allocating such status to robots in order to deal with\ncivil liability (EU Parliament 2016; Bertolini and Aiello 2018), but\nnot criminal liability\u2014which is reserved for natural persons. It\nwould also be possible to assign only a certain subset of rights and\nduties to robots. It has been said that \u201csuch legislative action\nwould be morally unnecessary and legally troublesome\u201d because it\nwould not serve the interest of humans (Bryson, Diamantis, and Grant\n2017: 273). In environmental ethics there is a long-standing\ndiscussion about the legal rights for natural objects like trees (C.\nD. Stone 1972).\n\nIt has also been said that the reasons for developing robots with\nrights, or artificial moral patients, in the future are ethically\ndoubtful (van Wynsberghe and Robbins 2019). In the community of\n\u201cartificial consciousness\u201d researchers there is a\nsignificant concern whether it would be ethical to create such\nconsciousness since creating it would presumably imply ethical\nobligations to a sentient being, e.g., not to harm it and not to end\nits existence by switching it off\u2014some authors have called for a\n\u201cmoratorium on synthetic phenomenology\u201d (Bentley et al.\n2018: 28f).\n2.10 Singularity\n2.10.1 Singularity and Superintelligence\n\nIn some quarters, the aim of current AI is thought to be an\n\u201cartificial general intelligence\u201d (AGI), contrasted to a\ntechnical or \u201cnarrow\u201d AI. AGI is usually distinguished\nfrom traditional notions of AI as a general purpose system, and from\nSearle\u2019s notion of \u201cstrong AI\u201d:\n\n\ncomputers given the right programs can be literally said to\nunderstand and have other cognitive states. (Searle 1980:\n417)\n\n\nThe idea of singularity is that if the trajectory of\nartificial intelligence reaches up to systems that have a human level\nof intelligence, then these systems would themselves have the ability\nto develop AI systems that surpass the human level of intelligence,\ni.e., they are \u201csuperintelligent\u201d (see below). Such\nsuperintelligent AI systems would quickly self-improve or develop even\nmore intelligent systems. This sharp turn of events after reaching\nsuperintelligent AI is the \u201csingularity\u201d from which the\ndevelopment of AI is out of human control and hard to predict\n(Kurzweil 2005: 487).\n\nThe fear that \u201cthe robots we created will take over the\nworld\u201d had captured human imagination even before there were\ncomputers (e.g., Butler 1863) and is the central theme in\n\u010capek\u2019s famous play that introduced the word\n\u201crobot\u201d (\u010capek 1920). This fear was first formulated\nas a possible trajectory of existing AI into an \u201cintelligence\nexplosion\u201d by Irvin Good:\n\n\nLet an ultraintelligent machine be defined as a machine that can far\nsurpass all the intellectual activities of any man however clever.\nSince the design of machines is one of these intellectual activities,\nan ultraintelligent machine could design even better machines; there\nwould then unquestionably be an \u201cintelligence explosion\u201d,\nand the intelligence of man would be left far behind. Thus the first\nultraintelligent machine is the last invention that man need ever\nmake, provided that the machine is docile enough to tell us how to\nkeep it under control. (Good 1965: 33)\n\n\nThe optimistic argument from acceleration to singularity is spelled\nout by Kurzweil (1999, 2005, 2012) who essentially points out that\ncomputing power has been increasing exponentially, i.e., doubling ca.\nevery 2 years since 1970 in accordance with \u201cMoore\u2019s\nLaw\u201d on the number of transistors, and will continue to do so\nfor some time in the future. He predicted in (Kurzweil 1999) that by\n2010 supercomputers will reach human computation capacity, by 2030\n\u201cmind uploading\u201d will be possible, and by 2045 the\n\u201csingularity\u201d will occur. Kurzweil talks about an increase\nin computing power that can be purchased at a given cost\u2014but of\ncourse in recent years the funds available to AI companies have also\nincreased enormously: Amodei and Hernandez (2018 [OIR]) thus estimate\nthat in the years 2012\u20132018 the actual computing power available\nto train a particular AI system doubled every 3.4 months, resulting in\nan 300,000x increase\u2014not the 7x increase that doubling every two\nyears would have created.\n\nA common version of this argument (Chalmers 2010) talks about an\nincrease in \u201cintelligence\u201d of the AI system (rather than\nraw computing power), but the crucial point of\n\u201csingularity\u201d remains the one where further development of\nAI is taken over by AI systems and accelerates beyond human level.\nBostrom (2014) explains in some detail what would happen at that point\nand what the risks for humanity are. The discussion is summarised in\nEden et al. (2012); Armstrong (2014); Shanahan (2015). There are\npossible paths to superintelligence other than computing power\nincrease, e.g., the complete emulation of the human brain on a\ncomputer (Kurzweil 2012; Sandberg 2013), biological paths, or networks\nand organisations (Bostrom 2014: 22\u201351).\n\nDespite obvious weaknesses in the identification of\n\u201cintelligence\u201d with processing power, Kurzweil seems right\nthat humans tend to underestimate the power of exponential growth.\nMini-test: If you walked in steps in such a way that each step is\ndouble the previous, starting with a step of one metre, how far would\nyou get with 30 steps? (answer: almost 3 times further than the\nEarth\u2019s only permanent natural satellite.) Indeed, most progress\nin AI is readily attributable to the availability of processors that\nare faster by degrees of magnitude, larger storage, and higher\ninvestment (M\u00fcller 2018). The actual acceleration and its speeds\nare discussed in (M\u00fcller and Bostrom 2016; Bostrom, Dafoe, and\nFlynn forthcoming); Sandberg (2019) argues that progress will continue\nfor some time.\n\nThe participants in this debate are united by being technophiles in\nthe sense that they expect technology to develop rapidly and bring\nbroadly welcome changes\u2014but beyond that, they divide into those\nwho focus on benefits (e.g., Kurzweil) and those who focus on risks\n(e.g., Bostrom). Both camps sympathise with \u201ctranshuman\u201d\nviews of survival for humankind in a different physical form, e.g.,\nuploaded on a computer (Moravec 1990, 1998; Bostrom 2003a, 2003c).\nThey also consider the prospects of \u201chuman enhancement\u201d in\nvarious respects, including intelligence\u2014often called\n\u201cIA\u201d (intelligence augmentation). It may be that future AI\nwill be used for human enhancement, or will contribute further to the\ndissolution of the neatly defined human single person. Robin Hanson\nprovides detailed speculation about what will happen economically in\ncase human \u201cbrain emulation\u201d enables truly intelligent\nrobots or \u201cems\u201d (Hanson 2016).\n\nThe argument from superintelligence to risk requires the assumption\nthat superintelligence does not imply benevolence\u2014contrary to\nKantian traditions in ethics that have argued higher levels of\nrationality or intelligence would go along with a better understanding\nof what is moral and better ability to act morally (Gewirth 1978;\nChalmers 2010: 36f). Arguments for risk from superintelligence say\nthat rationality and morality are entirely independent\ndimensions\u2014this is sometimes explicitly argued for as an\n\u201corthogonality thesis\u201d (Bostrom 2012; Armstrong 2013;\nBostrom 2014: 105\u2013109).\n\nCriticism of the singularity narrative has been raised from various\nangles. Kurzweil and Bostrom seem to assume that intelligence is a\none-dimensional property and that the set of intelligent agents is\ntotally-ordered in the mathematical sense\u2014but neither discusses\nintelligence at any length in their books. Generally, it is fair to\nsay that despite some efforts, the assumptions made in the powerful\nnarrative of superintelligence and singularity have not been\ninvestigated in detail. One question is whether such a singularity\nwill ever occur\u2014it may be conceptually impossible, practically\nimpossible or may just not happen because of contingent events,\nincluding people actively preventing it. Philosophically, the\ninteresting question is whether singularity is just a\n\u201cmyth\u201d (Floridi 2016; Ganascia 2017), and not on the\ntrajectory of actual AI research. This is something that practitioners\noften assume (e.g., Brooks 2017 [OIR]). They may do so because they\nfear the public relations backlash, because they overestimate the\npractical problems, or because they have good reasons to think that\nsuperintelligence is an unlikely outcome of current AI research\n(M\u00fcller forthcoming-a). This discussion raises the question\nwhether the concern about \u201csingularity\u201d is just a\nnarrative about fictional AI based on human fears. But even if one\ndoes find negative reasons compelling and the singularity not\nlikely to occur, there is still a significant possibility that one may\nturn out to be wrong. Philosophy is not on the \u201csecure path of a\nscience\u201d (Kant 1791: B15), and maybe AI and robotics\naren\u2019t either (M\u00fcller 2020). So, it appears that discussing\nthe very high-impact risk of singularity has justification even\nif one thinks the probability of such singularity ever occurring\nis very low.\n2.10.2 Existential Risk from Superintelligence\n\nThinking about superintelligence in the long term raises the question\nwhether superintelligence may lead to the extinction of the human\nspecies, which is called an \u201cexistential risk\u201d (or XRisk):\nThe superintelligent systems may well have preferences that conflict\nwith the existence of humans on Earth, and may thus decide to end that\nexistence\u2014and given their superior intelligence, they will have\nthe power to do so (or they may happen to end it because they do not\nreally care).\n\nThinking in the long term is the crucial feature of this literature.\nWhether the singularity (or another catastrophic event) occurs in 30\nor 300 or 3000 years does not really matter (Baum et al. 2019).\nPerhaps there is even an astronomical pattern such that an intelligent\nspecies is bound to discover AI at some point, and thus bring about\nits own demise. Such a \u201cgreat filter\u201d would contribute to\nthe explanation of the \u201cFermi paradox\u201d why there is no\nsign of life in the known universe despite the high probability of it\nemerging. It would be bad news if we found out that the \u201cgreat\nfilter\u201d is ahead of us, rather than an obstacle that Earth has\nalready passed. These issues are sometimes taken more narrowly to be\nabout human extinction (Bostrom 2013), or more broadly as concerning\nany large risk for the species (Rees 2018)\u2014of which AI is only\none (H\u00e4ggstr\u00f6m 2016; Ord 2020). Bostrom also uses the\ncategory of \u201cglobal catastrophic risk\u201d for risks that are\nsufficiently high up the two dimensions of \u201cscope\u201d and\n\u201cseverity\u201d (Bostrom and \u0106irkovi\u0107 2011; Bostrom\n2013).\n\nThese discussions of risk are usually not connected to the general\nproblem of ethics under risk (e.g., Hansson 2013, 2018). The long-term\nview has its own methodological challenges but has produced a wide\ndiscussion: (Tegmark 2017) focuses on AI and human life\n\u201c3.0\u201d after singularity while Russell, Dewey, and Tegmark\n(2015) and Bostrom, Dafoe, and Flynn (forthcoming) survey longer-term\npolicy issues in ethical AI. Several collections of papers have\ninvestigated the risks of artificial general intelligence (AGI) and\nthe factors that might make this development more or less risk-laden\n(M\u00fcller 2016b; Callaghan et al. 2017; Yampolskiy 2018), including\nthe development of non-agent AI (Drexler 2019).\n2.10.3 Controlling Superintelligence?\n\nIn a narrow sense, the \u201ccontrol problem\u201d is how we humans\ncan remain in control of an AI system once it is superintelligent\n(Bostrom 2014: 127ff). In a wider sense, it is the problem of how we\ncan make sure an AI system will turn out to be positive according to\nhuman perception (Russell 2019); this is sometimes called \u201cvalue\nalignment\u201d. How easy or hard it is to control a\nsuperintelligence depends significantly on the speed of\n\u201ctake-off\u201d to a superintelligent system. This has led to\nparticular attention to systems with self-improvement, such as\nAlphaZero (Silver et al. 2018).\n\nOne aspect of this problem is that we might decide a certain feature\nis desirable, but then find out that it has unforeseen consequences\nthat are so negative that we would not desire that feature after all.\nThis is the ancient problem of King Midas who wished that all he\ntouched would turn into gold. This problem has been discussed on the\noccasion of various examples, such as the \u201cpaperclip\nmaximiser\u201d (Bostrom 2003b), or the program to optimise chess\nperformance (Omohundro 2014).\n\nDiscussions about superintelligence include speculation about\nomniscient beings, the radical changes on a \u201clatter day\u201d,\nand the promise of immortality through transcendence of our current\nbodily form\u2014so sometimes they have clear religious undertones\n(Capurro 1993; Geraci 2008, 2010; O\u2019Connell 2017: 160ff). These\nissues also pose a well-known problem of epistemology: Can we know the\nways of the omniscient (Danaher 2015)? The usual opponents have\nalready shown up: A characteristic response of an atheist is\n\n\nPeople worry that computers will get too smart and take over the\nworld, but the real problem is that they\u2019re too stupid and\nthey\u2019ve already taken over the world (Domingos 2015)\n\n\nThe new nihilists explain that a \u201ctechno-hypnosis\u201d through\ninformation technologies has now become our main method of distraction\nfrom the loss of meaning (Gertz 2018). Both opponents would thus say\nwe need an ethics for the \u201csmall\u201d problems that occur with\nactual AI and robotics\n (sections 2.1 through 2.9\n above), and that there is less need for the \u201cbig ethics\u201d\nof existential risk from AI\n (section 2.10).\n3. Closing\n\nThe singularity thus raises the problem of the concept of AI again. It\nis remarkable how imagination or \u201cvision\u201d has played a\ncentral role since the very beginning of the discipline at the\n\u201cDartmouth Summer Research Project\u201d (McCarthy et al. 1955\n[OIR]; Simon and Newell 1958). And the evaluation of this vision is\nsubject to dramatic change: In a few decades, we went from the slogans\n\u201cAI is impossible\u201d (Dreyfus 1972) and \u201cAI is just\nautomation\u201d (Lighthill 1973) to \u201cAI will solve all\nproblems\u201d (Kurzweil 1999) and \u201cAI may kill us all\u201d\n(Bostrom 2014). This created media attention and public relations\nefforts, but it also raises the problem of how much of this\n\u201cphilosophy and ethics of AI\u201d is really about AI rather\nthan about an imagined technology. As we said at the outset, AI and\nrobotics have raised fundamental questions about what we should do\nwith these systems, what the systems themselves should do, and what\nrisks they have in the long term. They also challenge the human view\nof humanity as the intelligent and dominant species on Earth. We have\nseen issues that have been raised and will have to watch technological\nand social developments closely to catch the new issues early on,\ndevelop a philosophical analysis, and learn for traditional problems\nof philosophy.\n", "bibliography": {"categories": [], "cat_ref_text": {"ref_list": ["Abowd, John M, 2017, \u201cHow Will Statistical Agencies Operate\nWhen All Data Are Private?\u201d, <em>Journal of Privacy and\nConfidentiality</em>, 7(3): 1\u201315. doi:10.29012/jpc.v7i3.404", "Allen, Colin, Iva Smit, and Wendell Wallach, 2005,\n\u201cArtificial Morality: Top-down, Bottom-up, and Hybrid\nApproaches\u201d, <em>Ethics and Information Technology</em>, 7(3):\n149\u2013155. doi:10.1007/s10676-006-0004-4", "Allen, Colin, Gary Varner, and Jason Zinser, 2000,\n\u201cProlegomena to Any Future Artificial Moral Agent\u201d,\n<em>Journal of Experimental &amp; Theoretical Artificial\nIntelligence</em>, 12(3): 251\u2013261.\ndoi:10.1080/09528130050111428", "Amoroso, Daniele and Guglielmo Tamburrini, 2018, \u201cThe\nEthical and Legal Case Against Autonomy in Weapons Systems\u201d,\n<em>Global Jurist</em>, 18(1): art. 20170012.\ndoi:10.1515/gj-2017-0012", "Anderson, Janna, Lee Rainie, and Alex Luchsinger, 2018,\n<em>Artificial Intelligence and the Future of Humans</em>, Washington,\nDC: Pew Research Center.", "Anderson, Michael and Susan Leigh Anderson, 2007, \u201cMachine\nEthics: Creating an Ethical Intelligent Agent\u201d, <em>AI\nMagazine</em>, 28(4): 15\u201326.", "\u2013\u2013\u2013 (eds.), 2011, <em>Machine Ethics</em>,\nCambridge: Cambridge University Press.\ndoi:10.1017/CBO9780511978036", "Aneesh, A., 2006, <em>Virtual Migration: The Programming of\nGlobalization</em>, Durham, NC and London: Duke University Press.", "Arkin, Ronald C., 2009, <em>Governing Lethal Behavior in\nAutonomous Robots</em>, Boca Raton, FL: CRC Press.", "Armstrong, Stuart, 2013, \u201cGeneral Purpose Intelligence:\nArguing the Orthogonality Thesis\u201d, <em>Analysis and\nMetaphysics</em>, 12: 68\u201384.", "\u2013\u2013\u2013, 2014, <em>Smarter Than Us</em>, Berkeley,\nCA: MIRI.", "Arnold, Thomas and Matthias Scheutz, 2017, \u201cBeyond Moral\nDilemmas: Exploring the Ethical Landscape in HRI\u201d, in\n<em>Proceedings of the 2017 ACM/IEEE International Conference on\nHuman-Robot Interaction\u2014HRI \u201917</em>, Vienna, Austria: ACM\nPress, 445\u2013452. doi:10.1145/2909824.3020255", "Asaro, Peter M., 2019, \u201cAI Ethics in Predictive Policing:\nFrom Models of Threat to an Ethics of Care\u201d, <em>IEEE Technology\nand Society Magazine</em>, 38(2): 40\u201353.\ndoi:10.1109/MTS.2019.2915154", "Asimov, Isaac, 1942, \u201cRunaround: A Short Story\u201d,\n<em>Astounding Science Fiction</em>, March 1942. Reprinted in\n\u201cI, Robot\u201d, New York: Gnome Press 1950, 1940ff.", "Awad, Edmond, Sohan Dsouza, Richard Kim, Jonathan Schulz, Joseph\nHenrich, Azim Shariff, Jean-Fran\u00e7ois Bonnefon, and Iyad Rahwan,\n2018, \u201cThe Moral Machine Experiment\u201d, <em>Nature</em>,\n563(7729): 59\u201364. doi:10.1038/s41586-018-0637-6", "Baldwin, Richard, 2019, <em>The Globotics Upheaval: Globalisation,\nRobotics and the Future of Work</em>, New York: Oxford University\nPress.", "Baum, Seth D., Stuart Armstrong, Timoteus Ekenstedt, Olle\nH\u00e4ggstr\u00f6m, Robin Hanson, Karin Kuhlemann, Matthijs M. Maas,\nJames D. Miller, Markus Salmela, Anders Sandberg, Kaj Sotala, Phil\nTorres, Alexey Turchin, and Roman V. Yampolskiy, 2019,\n\u201cLong-Term Trajectories of Human Civilization\u201d,\n<em>Foresight</em>, 21(1): 53\u201383.\ndoi:10.1108/FS-04-2018-0037", "Bendel, Oliver, 2018, \u201cSexroboter aus Sicht der\nMaschinenethik\u201d, in <em>Handbuch Filmtheorie</em>, Bernhard\nGro\u00df and Thomas Morsch (eds.), (Springer Reference\nGeisteswissenschaften), Wiesbaden: Springer Fachmedien Wiesbaden,\n1\u201319. doi:10.1007/978-3-658-17484-2_22-1", "Bennett, Colin J. and Charles Raab, 2006, <em>The Governance of\nPrivacy: Policy Instruments in Global Perspective</em>, second\nedition, Cambridge, MA: MIT Press.", "Benthall, Sebastian and Bruce D. Haynes, 2019, \u201cRacial\nCategories in Machine Learning\u201d, in <em>Proceedings of the\nConference on Fairness, Accountability, and Transparency - FAT*\n\u201919</em>, Atlanta, GA, USA: ACM Press, 289\u2013298.\ndoi:10.1145/3287560.3287575", "Bentley, Peter J., Miles Brundage, Olle H\u00e4ggstr\u00f6m, and\nThomas Metzinger, 2018, \u201cShould We Fear Artificial Intelligence?\nIn-Depth Analysis\u201d, European Parliamentary Research Service,\nScientific Foresight Unit (STOA), March 2018, PE 614.547, 1\u201340.\n [<a href=\"https://op.europa.eu/en/publication-detail/-/publication/f27d7e2c-88a2-11e8-ac6a-01aa75ed71a1\" target=\"other\">Bentley et al. 2018 available online</a>]", "Bertolini, Andrea and Giuseppe Aiello, 2018, \u201cRobot\nCompanions: A Legal and Ethical Analysis\u201d, <em>The Information\nSociety</em>, 34(3): 130\u2013140.\ndoi:10.1080/01972243.2018.1444249", "Binns, Reuben, 2018, \u201cFairness in Machine Learning: Lessons\nfrom Political Philosophy\u201d, <em>Proceedings of the 1st\nConference on Fairness, Accountability and Transparency</em>, in\n<em>Proceedings of Machine Learning Research</em>, 81:\n149\u2013159.", "Bostrom, Nick, 2003a, \u201cAre We Living in a Computer\nSimulation?\u201d, <em>The Philosophical Quarterly</em>, 53(211):\n243\u2013255. doi:10.1111/1467-9213.00309", "\u2013\u2013\u2013, 2003b, \u201cEthical Issues in Advanced\nArtificial Intelligence\u201d, in <em>Cognitive, Emotive and Ethical\nAspects of Decision Making in Humans and in Artificial Intelligence,\nVolume 2</em>, Iva Smit, Wendell Wallach, and G.E. Lasker (eds),\n(IIAS-147-2003), Tecumseh, ON: International Institute of Advanced\nStudies in Systems Research and Cybernetics, 12\u201317.\n [<a href=\"https://nickbostrom.com/ethics/ai.html\" target=\"other\">Botstrom 2003b revised available online</a>]", "\u2013\u2013\u2013, 2003c, \u201cTranshumanist Values\u201d,\nin <em>Ethical Issues for the Twenty-First Century</em>, Frederick\nAdams (ed.), Bowling Green, OH: Philosophical Documentation Center\nPress.", "\u2013\u2013\u2013, 2012, \u201cThe Superintelligent Will:\nMotivation and Instrumental Rationality in Advanced Artificial\nAgents\u201d, <em>Minds and Machines</em>, 22(2): 71\u201385.\ndoi:10.1007/s11023-012-9281-3", "\u2013\u2013\u2013, 2013, \u201cExistential Risk Prevention as\nGlobal Priority\u201d, <em>Global Policy</em>, 4(1): 15\u201331.\ndoi:10.1111/1758-5899.12002", "\u2013\u2013\u2013, 2014, <em>Superintelligence: Paths,\nDangers, Strategies</em>, Oxford: Oxford University Press.", "Bostrom, Nick and Milan M. \u0106irkovi\u0107 (eds.), 2011,\n<em>Global Catastrophic Risks</em>, New York: Oxford University\nPress.", "Bostrom, Nick, Allan Dafoe, and Carrick Flynn, forthcoming,\n\u201cPolicy Desiderata for Superintelligent AI: A Vector Field\nApproach (V. 4.3)\u201d, in <em>Ethics of Artificial\nIntelligence</em>, S Matthew Liao (ed.), New York: Oxford University\nPress.\n [<a href=\"https://nickbostrom.com/papers/aipolicy.pdf\" target=\"other\">Bostrom, Dafoe, and Flynn forthcoming \u2013 preprint available online</a>]", "Bostrom, Nick and Eliezer Yudkowsky, 2014, \u201cThe Ethics of\nArtificial Intelligence\u201d, in <em>The Cambridge Handbook of\nArtificial Intelligence</em>, Keith Frankish and William M. Ramsey\n(eds.), Cambridge: Cambridge University Press, 316\u2013334.\ndoi:10.1017/CBO9781139046855.020\n [<a href=\"http://intelligence.org/files/EthicsofAI.pdf\" target=\"other\">Bostrom and Yudkowsky 2014 available online</a>]", "Bradshaw, Samantha, Lisa-Maria Neudert, and Phil Howard, 2019,\n\u201cGovernment Responses to Malicious Use of Social Media\u201d,\nWorking Paper 2019.2, Oxford: Project on Computational Propaganda.\n [<a href=\"https://comprop.oii.ox.ac.uk/research/government-responses/\" target=\"other\">Bradshaw, Neudert, and Howard 2019 available online/</a>]", "Brownsword, Roger, Eloise Scotford, and Karen Yeung (eds.), 2017,\n<em>The Oxford Handbook of Law, Regulation and Technology</em>,\nOxford: Oxford University Press.\ndoi:10.1093/oxfordhb/9780199680832.001.0001", "Brynjolfsson, Erik and Andrew McAfee, 2016, <em>The Second Machine\nAge: Work, Progress, and Prosperity in a Time of Brilliant\nTechnologies</em>, New York: W. W. Norton.", "Bryson, Joanna J., 2010, \u201cRobots Should Be Slaves\u201d, in\n<em>Close Engagements with Artificial Companions: Key Social,\nPsychological, Ethical and Design Issues</em>, Yorick Wilks (ed.),\n(Natural Language Processing 8), Amsterdam: John Benjamins Publishing\nCompany, 63\u201374. doi:10.1075/nlp.8.11bry", "\u2013\u2013\u2013, 2019, \u201cThe Past Decade and Future of\nAi\u2019s Impact on Society\u201d, in <em>Towards a New\nEnlightenment: A Transcendent Decade</em>, Madrid: Turner - BVVA.\n [<a href=\"https://www.bbvaopenmind.com/en/books/towards-a-new-enlightenment-a-transcendent-decade/\" target=\"other\">Bryson 2019 available online</a>]", "Bryson, Joanna J., Mihailis E. Diamantis, and Thomas D. Grant,\n2017, \u201cOf, for, and by the People: The Legal Lacuna of Synthetic\nPersons\u201d, <em>Artificial Intelligence and Law</em>, 25(3):\n273\u2013291. doi:10.1007/s10506-017-9214-9", "Burr, Christopher and Nello Cristianini, 2019, \u201cCan Machines\nRead Our Minds?\u201d, <em>Minds and Machines</em>, 29(3):\n461\u2013494. doi:10.1007/s11023-019-09497-4", "Butler, Samuel, 1863, \u201cDarwin among the Machines: Letter to\nthe Editor\u201d, Letter in <em>The Press (Christchurch)</em>, 13\nJune 1863.\n [<a href=\"http://nzetc.victoria.ac.nz/tm/scholarly/tei-ButFir-t1-g1-t1-g1-t4-body.html\" target=\"other\">Butler 1863 available online</a>]", "Callaghan, Victor, James Miller, Roman Yampolskiy, and Stuart\nArmstrong (eds.), 2017, <em>The Technological Singularity: Managing\nthe Journey</em>, (The Frontiers Collection), Berlin, Heidelberg:\nSpringer Berlin Heidelberg. doi:10.1007/978-3-662-54033-6", "Calo, Ryan, 2018, \u201cArtificial Intelligence Policy: A Primer\nand Roadmap\u201d, <em>University of Bologna Law Review</em>, 3(2):\n180-218. doi:10.6092/ISSN.2531-6133/8670", "Calo, Ryan, A. Michael Froomkin, and Ian Kerr (eds.), 2016,\n<em>Robot Law</em>, Cheltenham: Edward Elgar.", "\u010capek, Karel, 1920, <em>R.U.R.</em>, Prague: Aventium.\nTranslated by Peter Majer and Cathy Porter, London: Methuen,\n1999.", "Capurro, Raphael, 1993, \u201cEin Grinsen Ohne Katze: Von der\nVergleichbarkeit Zwischen \u2018K\u00fcnstlicher Intelligenz\u2019\nund \u2018Getrennten Intelligenzen\u2019\u201d, <em>Zeitschrift\nf\u00fcr philosophische Forschung</em>, 47: 93\u2013102.", "Cave, Stephen, 2019, \u201cTo Save Us from a Kafkaesque Future,\nWe Must Democratise AI\u201d, <em>The Guardian</em> , 04 January\n2019.\n [<a href=\"https://www.theguardian.com/commentisfree/2019/jan/04/future-democratise-ai-artificial-intelligence-power\" target=\"other\">Cave 2019 available online</a>]", "Chalmers, David J., 2010, \u201cThe Singularity: A Philosophical\nAnalysis\u201d, <em>Journal of Consciousness Studies</em>,\n17(9\u201310): 7\u201365.\n [<a href=\"http://consc.net/papers/singularityjcs.pdf\" target=\"other\">Chalmers 2010 available online</a>]", "Christman, John, 2003 [2018], \u201cAutonomy in Moral and\nPolitical Philosophy\u201d, (Spring 2018) <em>Stanford Encyclopedia\nof Philosophy</em> (EDITION NEEDED), URL =\n &lt;<a href=\"https://plato.stanford.edu/archives/spr2018/entries/autonomy-moral/\" target=\"other\">https://plato.stanford.edu/archives/spr2018/entries/autonomy-moral/</a>&gt;", "Coeckelbergh, Mark, 2010, \u201cRobot Rights? Towards a\nSocial-Relational Justification of Moral Consideration\u201d,\n<em>Ethics and Information Technology</em>, 12(3): 209\u2013221.\ndoi:10.1007/s10676-010-9235-5", "\u2013\u2013\u2013, 2012, <em>Growing Moral Relations: Critique\nof Moral Status Ascription</em>, London: Palgrave.\ndoi:10.1057/9781137025968", "\u2013\u2013\u2013, 2016, \u201cCare Robots and the Future of\nICT-Mediated Elderly Care: A Response to Doom Scenarios\u201d, <em>AI\n&amp; Society</em>, 31(4): 455\u2013462.\ndoi:10.1007/s00146-015-0626-3", "\u2013\u2013\u2013, 2018, \u201cWhat Do We Mean by a\nRelational Ethics? Growing a Relational Approach to the Moral Standing\nof Plants, Robots and Other Non-Humans\u201d, in <em>Plant Ethics:\nConcepts and Applications</em>, Angela Kallhoff, Marcello Di Paola,\nand Maria Sch\u00f6rgenhumer (eds.), London: Routledge,\n110\u2013121.", "Crawford, Kate and Ryan Calo, 2016, \u201cThere Is a Blind Spot\nin AI Research\u201d, <em>Nature</em>, 538(7625): 311\u2013313.\ndoi:10.1038/538311a", "Cristianini, Nello, forthcoming, \u201cShortcuts to Artificial\nIntelligence\u201d, in <em>Machines We Trust</em>, Marcello Pelillo\nand Teresa Scantamburlo (eds.), Cambridge, MA: MIT Press.\n [<a href=\"https://philpapers.org/rec/CRISTA-3\" target=\"other\">Cristianini forthcoming \u2013 preprint available online</a>]", "Danaher, John, 2015, \u201cWhy AI Doomsayers Are Like Sceptical\nTheists and Why It Matters\u201d, <em>Minds and Machines</em>, 25(3):\n231\u2013246. doi:10.1007/s11023-015-9365-y", "\u2013\u2013\u2013, 2016a, \u201cRobots, Law and the\nRetribution Gap\u201d, <em>Ethics and Information Technology</em>,\n18(4): 299\u2013309. doi:10.1007/s10676-016-9403-3", "\u2013\u2013\u2013, 2016b, \u201cThe Threat of Algocracy:\nReality, Resistance and Accommodation\u201d, <em>Philosophy &amp;\nTechnology</em>, 29(3): 245\u2013268.\ndoi:10.1007/s13347-015-0211-1", "\u2013\u2013\u2013, 2019a, <em>Automation and Utopia: Human\nFlourishing in a World without Work</em>, Cambridge, MA: Harvard\nUniversity Press.", "\u2013\u2013\u2013, 2019b, \u201cThe Philosophical Case for\nRobot Friendship\u201d, <em>Journal of Posthuman Studies</em>, 3(1):\n5\u201324. doi:10.5325/jpoststud.3.1.0005", "\u2013\u2013\u2013, forthcoming, \u201cWelcoming Robots into\nthe Moral Circle: A Defence of Ethical Behaviourism\u201d,\n<em>Science and Engineering Ethics</em>, first online: 20 June 2019.\ndoi:10.1007/s11948-019-00119-x", "Danaher, John and Neil McArthur (eds.), 2017, <em>Robot Sex:\nSocial and Ethical Implications</em>, Boston, MA: MIT Press.", "DARPA, 1983, \u201cStrategic Computing. New-Generation Computing\nTechnology: A Strategic Plan for Its Development an Application to\nCritical Problems in Defense\u201d, ADA141982, 28 October 1983.\n [<a href=\"https://apps.dtic.mil/docs/citations/ADA141982\" target=\"other\">DARPA 1983 available online</a>]", "Dennett, Daniel C, 2017, <em>From Bacteria to Bach and Back: The\nEvolution of Minds</em>, New York: W.W. Norton.", "Devlin, Kate, 2018, <em>Turned On: Science, Sex and Robots</em>,\nLondon: Bloomsbury.", "Diakopoulos, Nicholas, 2015, \u201cAlgorithmic Accountability:\nJournalistic Investigation of Computational Power Structures\u201d,\n<em>Digital Journalism</em>, 3(3): 398\u2013415.\ndoi:10.1080/21670811.2014.976411", "Dignum, Virginia, 2018, \u201cEthics in Artificial Intelligence:\nIntroduction to the Special Issue\u201d, <em>Ethics and Information\nTechnology</em>, 20(1): 1\u20133. doi:10.1007/s10676-018-9450-z", "Domingos, Pedro, 2015, <em>The Master Algorithm: How the Quest for\nthe Ultimate Learning Machine Will Remake Our World</em>, London:\nAllen Lane.", "Draper, Heather, Tom Sorell, Sandra Bedaf, Dag Sverre Syrdal,\nCarolina Gutierrez-Ruiz, Alexandre Duclos, and Farshid\nAmirabdollahian, 2014, \u201cEthical Dimensions of Human-Robot\nInteractions in the Care of Older People: Insights from 21 Focus\nGroups Convened in the UK, France and the Netherlands\u201d, in\n<em>International Conference on Social Robotics 2014</em>, Michael\nBeetz, Benjamin Johnston, and Mary-Anne Williams (eds.), (Lecture\nNotes in Artificial Intelligence 8755), Cham: Springer International\nPublishing, 135\u2013145. doi:10.1007/978-3-319-11973-1_14", "Dressel, Julia and Hany Farid, 2018, \u201cThe Accuracy,\nFairness, and Limits of Predicting Recidivism\u201d, <em>Science\nAdvances</em>, 4(1): eaao5580. doi:10.1126/sciadv.aao5580", "Drexler, K. Eric, 2019, \u201cReframing Superintelligence:\nComprehensive AI Services as General Intelligence\u201d, FHI\nTechnical Report, 2019-1, 1-210.\n [<a href=\"https://www.fhi.ox.ac.uk/reframing/\" target=\"other\">Drexler 2019 available online</a>]", "Dreyfus, Hubert L., 1972, <em>What Computers Still Can\u2019t Do:\nA Critique of Artificial Reason</em>, second edition, Cambridge, MA:\nMIT Press 1992.", "Dreyfus, Hubert L., Stuart E. Dreyfus, and Tom Athanasiou, 1986,\n<em>Mind over Machine: The Power of Human Intuition and Expertise in\nthe Era of the Computer</em>, New York: Free Press.", "Dwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith,\n2006, <em>Calibrating Noise to Sensitivity in Private Data\nAnalysis</em>, Berlin, Heidelberg.", "Eden, Amnon H., James H. Moor, Johnny H. S\u00f8raker, and Eric\nSteinhart (eds.), 2012, <em>Singularity Hypotheses: A Scientific and\nPhilosophical Assessment</em>, (The Frontiers Collection), Berlin,\nHeidelberg: Springer Berlin Heidelberg.\ndoi:10.1007/978-3-642-32560-1", "Eubanks, Virginia, 2018, <em>Automating Inequality: How High-Tech\nTools Profile, Police, and Punish the Poor</em>, London: St.\nMartin\u2019s Press.", "European Commission, 2013, \u201cHow Many People Work in\nAgriculture in the European Union? An Answer Based on Eurostat Data\nSources\u201d, <em>EU Agricultural Economics Briefs</em>, 8 (July\n2013).\n [<a href=\"https://ec.europa.eu/info/sites/info/files/food-farming-fisheries/farming/documents/agri-economics-brief-08_en.pdf\" target=\"other\">Anonymous 2013 available online</a>]", "European Group on Ethics in Science and New Technologies, 2018,\n\u201cStatement on Artificial Intelligence, Robotics and\n\u2018Autonomous\u2019 Systems\u201d, 9 March 2018, European\nCommission, Directorate-General for Research and Innovation, Unit\nRTD.01.\n [<a href=\"https://op.europa.eu/en/publication-detail/-/publication/dfebe62e-4ce9-11e8-be1d-01aa75ed71a1\" target=\"other\">European Group 2018 available online </a>]", "Ferguson, Andrew Guthrie, 2017, <em>The Rise of Big Data Policing:\nSurveillance, Race, and the Future of Law Enforcement</em>, New York:\nNYU Press.", "Floridi, Luciano, 2016, \u201cShould We Be Afraid of AI? Machines\nSeem to Be Getting Smarter and Smarter and Much Better at Human Jobs,\nyet True AI Is Utterly Implausible. Why?\u201d, <em>Aeon</em>, 9 May\n2016. URL =\n &lt;<a href=\"https://aeon.co/essays/true-ai-is-both-logically-possible-and-utterly-implausible\" target=\"other\">Floridi 2016 available online</a>&gt;", "Floridi, Luciano, Josh Cowls, Monica Beltrametti, Raja Chatila,\nPatrice Chazerand, Virginia Dignum, Christoph Luetge, Robert Madelin,\nUgo Pagallo, Francesca Rossi, Burkhard Schafer, Peggy Valcke, and Effy\nVayena, 2018, \u201cAI4People\u2014An Ethical Framework for a Good\nAI Society: Opportunities, Risks, Principles, and\nRecommendations\u201d, <em>Minds and Machines</em>, 28(4):\n689\u2013707. doi:10.1007/s11023-018-9482-5", "Floridi, Luciano and Jeff W. Sanders, 2004, \u201cOn the Morality\nof Artificial Agents\u201d, <em>Minds and Machines</em>, 14(3):\n349\u2013379. doi:10.1023/B:MIND.0000035461.63578.9d", "Floridi, Luciano and Mariarosaria Taddeo, 2016, \u201cWhat Is\nData Ethics?\u201d, <em>Philosophical Transactions of the Royal\nSociety A: Mathematical, Physical and Engineering Sciences</em>,\n374(2083): 20160360. doi:10.1098/rsta.2016.0360", "Foot, Philippa, 1967, \u201cThe Problem of Abortion and the\nDoctrine of the Double Effect\u201d, <em>Oxford Review</em>, 5:\n5\u201315.", "Fosch-Villaronga, Eduard and Jordi Albo-Canals, 2019,\n\u201c\u2018I\u2019ll Take Care of You,\u2019 Said the\nRobot\u201d, <em>Paladyn, Journal of Behavioral Robotics</em>, 10(1):\n77\u201393. doi:10.1515/pjbr-2019-0006", "Frank, Lily and Sven Nyholm, 2017, \u201cRobot Sex and Consent:\nIs Consent to Sex between a Robot and a Human Conceivable, Possible,\nand Desirable?\u201d, <em>Artificial Intelligence and Law</em>,\n25(3): 305\u2013323. doi:10.1007/s10506-017-9212-y", "Frankfurt, Harry G., 1971, \u201cFreedom of the Will and the\nConcept of a Person\u201d, <em>The Journal of Philosophy</em>, 68(1):\n5\u201320.", "Frey, Carl Benedict, 2019, <em>The Technology Trap: Capital,\nLabour, and Power in the Age of Automation</em>, Princeton, NJ:\nPrinceton University Press.", "Frey, Carl Benedikt and Michael A. Osborne, 2013, \u201cThe\nFuture of Employment: How Susceptible Are Jobs to\nComputerisation?\u201d, Oxford Martin School Working Papers, 17\nSeptember 2013.\n [<a href=\"http://www.oxfordmartin.ox.ac.uk/publications/view/1314\" target=\"other\">Frey and Osborne 2013 available online</a>]", "Ganascia, Jean-Gabriel, 2017, <em>Le Mythe De La\nSingularit\u00e9</em>, Paris: \u00c9ditions du Seuil.", "EU Parliament, 2016, \u201cDraft Report with Recommendations to\nthe Commission on Civil Law Rules on Robotics (2015/2103(Inl))\u201d,\n<em>Committee on Legal Affairs</em>, 10.11.2016.\nhttps://www.europarl.europa.eu/doceo/document/A-8-2017-0005_EN.html", "EU Regulation, 2016/679, \u201cGeneral Data Protection\nRegulation: Regulation (EU) 2016/679 of the European Parliament and of\nthe Council of 27 April 2016 on the Protection of Natural Persons with\nRegard to the Processing of Personal Data and on the Free Movement of\nSuch Data, and Repealing Directive 95/46/Ec\u201d, <em>Official\nJournal of the European Union</em>, 119 (4 May 2016), 1\u201388.\n [<a href=\"http://data.europa.eu/eli/reg/2016/679/oj\" target=\"other\">Regulation (EU) 2016/679 available online</a>]\n <!-- the link is the permanent link -->", "Geraci, Robert M., 2008, \u201cApocalyptic AI: Religion and the\nPromise of Artificial Intelligence\u201d, <em>Journal of the American\nAcademy of Religion</em>, 76(1): 138\u2013166.\ndoi:10.1093/jaarel/lfm101", "\u2013\u2013\u2013, 2010, <em>Apocalyptic AI: Visions of Heaven\nin Robotics, Artificial Intelligence, and Virtual Reality</em>,\nOxford: Oxford University Press.\ndoi:10.1093/acprof:oso/9780195393026.001.0001", "Gerdes, Anne, 2016, \u201cThe Issue of Moral Consideration in\nRobot Ethics\u201d, <em>ACM SIGCAS Computers and Society</em>, 45(3):\n274\u2013279. doi:10.1145/2874239.2874278", "German Federal Ministry of Transport and Digital Infrastructure,\n2017, \u201cReport of the Ethics Commission: Automated and Connected\nDriving\u201d, June 2017, 1\u201336.\n [<a href=\"https://www.bmvi.de/SharedDocs/EN/publications/report-ethics-commission.html\" target=\"other\">GFMTDI 2017 available online</a>]", "Gertz, Nolen, 2018, <em>Nihilism and Technology</em>, London:\nRowman &amp; Littlefield.", "Gewirth, Alan, 1978, \u201cThe Golden Rule Rationalized\u201d,\n<em>Midwest Studies in Philosophy</em>, 3(1): 133\u2013147.\ndoi:10.1111/j.1475-4975.1978.tb00353.x", "Gibert, Martin, 2019, \u201c\u00c9thique Artificielle (Version\nGrand Public)\u201d, in <em>L\u2019Encyclop\u00e9die\nPhilosophique</em>, Maxime Kristanek (ed.), accessed: 16 April 2020,\nURL =\n &lt;<a href=\"https://encyclo-philo.fr/item/199\" target=\"other\">Gibert 2019 available online</a>&gt;", "Giubilini, Alberto and Julian Savulescu, 2018, \u201cThe\nArtificial Moral Advisor. The \u2018Ideal Observer\u2019 Meets\nArtificial Intelligence\u201d, <em>Philosophy &amp; Technology</em>,\n31(2): 169\u2013188. doi:10.1007/s13347-017-0285-z", "Good, Irving John, 1965, \u201cSpeculations Concerning the First\nUltraintelligent Machine\u201d, in <em>Advances in Computers 6</em>,\nFranz L. Alt and Morris Rubinoff (eds.), New York &amp; London:\nAcademic Press, 31\u201388. doi:10.1016/S0065-2458(08)60418-0", "Goodfellow, Ian, Yoshua Bengio, and Aaron Courville, 2016,\n<em>Deep Learning</em>, Cambridge, MA: MIT Press.", "Goodman, Bryce and Seth Flaxman, 2017, \u201cEuropean Union\nRegulations on Algorithmic Decision-Making and a \u2018Right to\nExplanation\u2019\u201d, <em>AI Magazine</em>, 38(3): 50\u201357.\ndoi:10.1609/aimag.v38i3.2741", "Goos, Maarten, 2018, \u201cThe Impact of Technological Progress\non Labour Markets: Policy Challenges\u201d, <em>Oxford Review of\nEconomic Policy</em>, 34(3): 362\u2013375.\ndoi:10.1093/oxrep/gry002", "Goos, Maarten, Alan Manning, and Anna Salomons, 2009, \u201cJob\nPolarization in Europe\u201d, <em>American Economic Review</em>,\n99(2): 58\u201363. doi:10.1257/aer.99.2.58", "Graham, Sandra and Brian S. Lowery, 2004, \u201cPriming\nUnconscious Racial Stereotypes about Adolescent Offenders\u201d,\n<em>Law and Human Behavior</em>, 28(5): 483\u2013504.\ndoi:10.1023/B:LAHU.0000046430.65485.1f", "Gunkel, David J., 2018a, \u201cThe Other Question: Can and Should\nRobots Have Rights?\u201d, <em>Ethics and Information\nTechnology</em>, 20(2): 87\u201399.\ndoi:10.1007/s10676-017-9442-4", "\u2013\u2013\u2013, 2018b, <em>Robot Rights</em>, Boston, MA:\nMIT Press.", "Gunkel, David J. and Joanna J. Bryson (eds.), 2014, <em>Machine\nMorality: The Machine as Moral Agent and Patient</em> special issue of\n<em>Philosophy &amp; Technology</em>, 27(1): 1\u2013142.", "H\u00e4ggstr\u00f6m, Olle, 2016, <em>Here Be Dragons: Science,\nTechnology and the Future of Humanity</em>, Oxford: Oxford University\nPress. doi:10.1093/acprof:oso/9780198723547.001.0001", "Hakli, Raul and Pekka M\u00e4kel\u00e4, 2019, \u201cMoral\nResponsibility of Robots and Hybrid Agents\u201d, <em>The\nMonist</em>, 102(2): 259\u2013275. doi:10.1093/monist/onz009", "Hanson, Robin, 2016, <em>The Age of Em: Work, Love and Life When\nRobots Rule the Earth</em>, Oxford: Oxford University Press.", "Hansson, Sven Ove, 2013, <em>The Ethics of Risk: Ethical Analysis\nin an Uncertain World</em>, New York: Palgrave Macmillan.", "\u2013\u2013\u2013, 2018, \u201cHow to Perform an Ethical Risk\nAnalysis (eRA)\u201d, <em>Risk Analysis</em>, 38(9): 1820\u20131829.\ndoi:10.1111/risa.12978", "Harari, Yuval Noah, 2016, <em>Homo Deus: A Brief History of\nTomorrow</em>, New York: Harper.", "Haskel, Jonathan and Stian Westlake, 2017, <em>Capitalism without\nCapital: The Rise of the Intangible Economy</em>, Princeton, NJ:\nPrinceton University Press.", "Houkes, Wybo and Pieter E. Vermaas, 2010, <em>Technical Functions:\nOn the Use and Design of Artefacts</em>, (Philosophy of Engineering\nand Technology 1), Dordrecht: Springer Netherlands.\ndoi:10.1007/978-90-481-3900-2", "IEEE, 2019, <em>Ethically Aligned Design: A Vision for\nPrioritizing Human Well-Being with Autonomous and Intelligent\nSystems</em> (First Version),\n &lt;<a href=\"https://standards.ieee.org/content/dam/ieee-standards/standards/web/documents/other/ead1e.pdf\" target=\"other\">IEEE 2019 available online</a>&gt;.", "Jasanoff, Sheila, 2016, <em>The Ethics of Invention: Technology\nand the Human Future</em>, New York: Norton.", "Jecker, Nancy S., forthcoming, <em>Ending Midlife Bias: New Values\nfor Old Age</em>, New York: Oxford University Press.", "Jobin, Anna, Marcello Ienca, and Effy Vayena, 2019, \u201cThe\nGlobal Landscape of AI Ethics Guidelines\u201d, <em>Nature Machine\nIntelligence</em>, 1(9): 389\u2013399.\ndoi:10.1038/s42256-019-0088-2", "Johnson, Deborah G. and Mario Verdicchio, 2017, \u201cReframing\nAI Discourse\u201d, <em>Minds and Machines</em>, 27(4):\n575\u2013590. doi:10.1007/s11023-017-9417-6", "Kahnemann, Daniel, 2011, <em>Thinking Fast and Slow</em>, London:\nMacmillan.", "Kamm, Frances Myrna, 2016, <em>The Trolley Problem Mysteries</em>,\nEric Rakowski (ed.), Oxford: Oxford University Press.\ndoi:10.1093/acprof:oso/9780190247157.001.0001", "Kant, Immanuel, 1781/1787, <em>Kritik der reinen Vernunft</em>.\nTranslated as <em>Critique of Pure Reason</em>, Norman Kemp Smith\n(trans.), London: Palgrave Macmillan, 1929.", "Keeling, Geoff, 2020, \u201cWhy Trolley Problems Matter for the\nEthics of Automated Vehicles\u201d, <em>Science and Engineering\nEthics</em>, 26(1): 293\u2013307. doi:10.1007/s11948-019-00096-1", "Keynes, John Maynard, 1930, \u201cEconomic Possibilities for Our\nGrandchildren\u201d. Reprinted in his <em>Essays in Persuasion</em>,\nNew York: Harcourt Brace, 1932, 358\u2013373.", "Kissinger, Henry A., 2018, \u201cHow the Enlightenment Ends:\nPhilosophically, Intellectually\u2014in Every Way\u2014Human Society\nIs Unprepared for the Rise of Artificial Intelligence\u201d, <em>The\nAtlantic</em>, June 2018.\n [<a href=\"https://www.theatlantic.com/magazine/archive/2018/06/henry-kissinger-ai-could-mean-the-end-of-human-history/559124/\" target=\"other\">Kissinger 2018 available online</a>]", "Kurzweil, Ray, 1999, <em>The Age of Spiritual Machines: When\nComputers Exceed Human Intelligence</em>, London: Penguin.", "\u2013\u2013\u2013, 2005, <em>The Singularity Is Near: When\nHumans Transcend Biology</em>, London: Viking.", "\u2013\u2013\u2013, 2012, <em>How to Create a Mind: The Secret\nof Human Thought Revealed</em>, New York: Viking.", "Lee, Minha, Sander Ackermans, Nena van As, Hanwen Chang, Enzo\nLucas, and Wijnand IJsselsteijn, 2019, \u201cCaring for Vincent: A\nChatbot for Self-Compassion\u201d, in <em>Proceedings of the 2019 CHI\nConference on Human Factors in Computing Systems\u2014CHI\n\u201919</em>, Glasgow, Scotland: ACM Press, 1\u201313.\ndoi:10.1145/3290605.3300932", "Levy, David, 2007, <em>Love and Sex with Robots: The Evolution of\nHuman-Robot Relationships</em>, New York: Harper &amp; Co.", "Lighthill, James, 1973, \u201cArtificial Intelligence: A General\nSurvey\u201d, <em>Artificial intelligence: A Paper Symposion</em>,\nLondon: Science Research Council.\n [<a href=\"http://www.chilton-computing.org.uk/inf/literature/reports/lighthill_report/p001.htm\" target=\"other\">Lighthill 1973 available online</a>]", "Lin, Patrick, 2016, \u201cWhy Ethics Matters for Autonomous\nCars\u201d, in <em>Autonomous Driving</em>, Markus Maurer, J.\nChristian Gerdes, Barbara Lenz, and Hermann Winner (eds.), Berlin,\nHeidelberg: Springer Berlin Heidelberg, 69\u201385.\ndoi:10.1007/978-3-662-48847-8_4", "Lin, Patrick, Keith Abney, and Ryan Jenkins (eds.), 2017,\n<em>Robot Ethics 2.0: From Autonomous Cars to Artificial\nIntelligence</em>, New York: Oxford University Press.\ndoi:10.1093/oso/9780190652951.001.0001", "Lin, Patrick, George Bekey, and Keith Abney, 2008,\n\u201cAutonomous Military Robotics: Risk, Ethics, and Design\u201d,\nONR report, California Polytechnic State University, San Luis Obispo,\n20 December 2008), 112 pp.\n [<a href=\"http://ethics.calpoly.edu/ONR_report.pdf\" target=\"other\">Lin, Bekey, and Abney 2008 available online</a>]", "Lomas, Meghann, Robert Chevalier, Ernest Vincent Cross, Robert\nChristopher Garrett, John Hoare, and Michael Kopack, 2012,\n\u201cExplaining Robot Actions\u201d, in <em>Proceedings of the\nSeventh Annual ACM/IEEE International Conference on Human-Robot\nInteraction\u2014HRI \u201912</em>, Boston, MA: ACM Press,\n187\u2013188. doi:10.1145/2157689.2157748", "Macnish, Kevin, 2017, <em>The Ethics of Surveillance: An\nIntroduction</em>, London: Routledge.", "Mathur, Arunesh, Gunes Acar, Michael J. Friedman, Elena Lucherini,\nJonathan Mayer, Marshini Chetty, and Arvind Narayanan, 2019,\n\u201cDark Patterns at Scale: Findings from a Crawl of 11K Shopping\nWebsites\u201d, <em>Proceedings of the ACM on Human-Computer\nInteraction</em>, 3(CSCW): art. 81. doi:10.1145/3359183", "Minsky, Marvin, 1985, <em>The Society of Mind</em>, New York:\nSimon &amp; Schuster.", "Misselhorn, Catrin, 2020, \u201cArtificial Systems with Moral\nCapacities? A Research Design and Its Implementation in a Geriatric\nCare System\u201d, <em>Artificial Intelligence</em>, 278: art.\n103179. doi:10.1016/j.artint.2019.103179", "Mittelstadt, Brent Daniel and Luciano Floridi, 2016, \u201cThe\nEthics of Big Data: Current and Foreseeable Issues in Biomedical\nContexts\u201d, <em>Science and Engineering Ethics</em>, 22(2):\n303\u2013341. doi:10.1007/s11948-015-9652-2", "Moor, James H., 2006, \u201cThe Nature, Importance, and\nDifficulty of Machine Ethics\u201d, <em>IEEE Intelligent\nSystems</em>, 21(4): 18\u201321. doi:10.1109/MIS.2006.80", "Moravec, Hans, 1990, <em>Mind Children</em>, Cambridge, MA:\nHarvard University Press.", "\u2013\u2013\u2013, 1998, <em>Robot: Mere Machine to\nTranscendent Mind</em>, New York: Oxford University Press.", "Mozorov, Eygeny, 2013, <em>To Save Everything, Click Here: The\nFolly of Technological Solutionism</em>, New York: Public\nAffairs.", "M\u00fcller, Vincent C., 2012, \u201cAutonomous Cognitive Systems\nin Real-World Environments: Less Control, More Flexibility and Better\nInteraction\u201d, <em>Cognitive Computation</em>, 4(3):\n212\u2013215. doi:10.1007/s12559-012-9129-4", "\u2013\u2013\u2013, 2016a, \u201cAutonomous Killer Robots Are\nProbably Good News\u201d, In <em>Drones and Responsibility: Legal,\nPhilosophical and Socio-Technical Perspectives on the Use of Remotely\nControlled Weapons</em>, Ezio Di Nucci and Filippo Santoni de Sio\n(eds.), London: Ashgate, 67\u201381.", "\u2013\u2013\u2013 (ed.), 2016b, <em>Risks of Artificial\nIntelligence</em>, London: Chapman &amp; Hall - CRC Press.\ndoi:10.1201/b19187", "\u2013\u2013\u2013, 2018, \u201cIn 30 Schritten zum Mond?\nZuk\u00fcnftiger Fortschritt in der KI\u201d,\n<em>Medienkorrespondenz</em>, 20: 5\u201315.\n [<a href=\"https://philarchive.org/archive/MLLIS\" target=\"other\">M\u00fcller 2018 available online</a>]", "\u2013\u2013\u2013, 2020, \u201cMeasuring Progress in\nRobotics: Benchmarking and the \u2018Measure-Target\nConfusion\u2019\u201d, in <em>Metrics of Sensory Motor Coordination\nand Integration in Robots and Animals</em>, Fabio Bonsignorio, Elena\nMessina, Angel P. del Pobil, and John Hallam (eds.), (Cognitive\nSystems Monographs 36), Cham: Springer International Publishing,\n169\u2013179. doi:10.1007/978-3-030-14126-4_9", "\u2013\u2013\u2013, forthcoming-a, <em>Can Machines Think?\nFundamental Problems of Artificial Intelligence</em>, New York: Oxford\nUniversity Press.", "\u2013\u2013\u2013 (ed.), forthcoming-b, <em>Oxford Handbook of\nthe Philosophy of Artificial Intelligence</em>, New York: Oxford\nUniversity Press.", "M\u00fcller, Vincent C. and Nick Bostrom, 2016, \u201cFuture\nProgress in Artificial Intelligence: A Survey of Expert\nOpinion\u201d, in <em>Fundamental Issues of Artificial\nIntelligence</em>, Vincent C. M\u00fcller (ed.), Cham: Springer\nInternational Publishing, 555\u2013572.\ndoi:10.1007/978-3-319-26485-1_33", "Newport, Cal, 2019, <em>Digital Minimalism: On Living Better with\nLess Technology</em>, London: Penguin.", "N\u00f8rskov, Marco (ed.), 2017, <em>Social Robots</em>, London:\nRoutledge.", "Nyholm, Sven, 2018a, \u201cAttributing Agency to Automated\nSystems: Reflections on Human\u2013Robot Collaborations and\nResponsibility-Loci\u201d, <em>Science and Engineering Ethics</em>,\n24(4): 1201\u20131219. doi:10.1007/s11948-017-9943-x", "\u2013\u2013\u2013, 2018b, \u201cThe Ethics of Crashes with\nSelf-Driving Cars: A Roadmap, II\u201d, <em>Philosophy Compass</em>,\n13(7): e12506. doi:10.1111/phc3.12506", "Nyholm, Sven, and Lily Frank, 2017, \u201cFrom Sex Robots to Love\nRobots: Is Mutual Love with a Robot Possible?\u201d, in Danaher and\nMcArthur 2017: 219\u2013243.", "O\u2019Connell, Mark, 2017, <em>To Be a Machine: Adventures among\nCyborgs, Utopians, Hackers, and the Futurists Solving the Modest\nProblem of Death</em>, London: Granta.", "O\u2019Neil, Cathy, 2016, <em>Weapons of Math Destruction: How\nBig Data Increases Inequality and Threatens Democracy</em>, Largo, ML:\nCrown.", "Omohundro, Steve, 2014, \u201cAutonomous Technology and the\nGreater Human Good\u201d, <em>Journal of Experimental &amp;\nTheoretical Artificial Intelligence</em>, 26(3): 303\u2013315.\ndoi:10.1080/0952813X.2014.895111", "Ord, Toby, 2020, <em>The Precipice: Existential Risk and the\nFuture of Humanity</em>, London: Bloomsbury.", "Powers, Thomas M. and Jean-Gabriel Ganascia, forthcoming,\n\u201cThe Ethics of the Ethics of AI\u201d, in <em>Oxford Handbook\nof Ethics of Artificial Intelligence</em>, Markus D. Dubber, Frank\nPasquale, and Sunnit Das (eds.), New York: Oxford.", "Rawls, John, 1971, <em>A Theory of Justice</em>, Cambridge, MA:\nBelknap Press.", "Rees, Martin, 2018, <em>On the Future: Prospects for\nHumanity</em>, Princeton: Princeton University Press.", "Richardson, Kathleen, 2016, \u201cSex Robot Matters: Slavery, the\nProstituted, and the Rights of Machines\u201d, <em>IEEE Technology\nand Society Magazine</em>, 35(2): 46\u201353.\ndoi:10.1109/MTS.2016.2554421", "Roessler, Beate, 2017, \u201cPrivacy as a Human Right\u201d,\n<em>Proceedings of the Aristotelian Society</em>, 117(2):\n187\u2013206. doi:10.1093/arisoc/aox008", "Royakkers, Lamb\u00e8r and Rinie van Est, 2016, <em>Just\nOrdinary Robots: Automation from Love to War</em>, Boca Raton, LA: CRC\nPress, Taylor &amp; Francis. doi:10.1201/b18899", "Russell, Stuart, 2019, <em>Human Compatible: Artificial\nIntelligence and the Problem of Control</em>, New York: Viking.", "Russell, Stuart, Daniel Dewey, and Max Tegmark, 2015,\n\u201cResearch Priorities for Robust and Beneficial Artificial\nIntelligence\u201d, <em>AI Magazine</em>, 36(4): 105\u2013114.\ndoi:10.1609/aimag.v36i4.2577", "SAE International, 2018, \u201cTaxonomy and Definitions for Terms\nRelated to Driving Automation Systems for on-Road Motor\nVehicles\u201d, J3016_201806, 15 June 2018.\n [<a href=\"https://www.sae.org/standards/content/j3016_201806/\" target=\"other\">SAE International 2015 available online</a>]", "Sandberg, Anders, 2013, \u201cFeasibility of Whole Brain\nEmulation\u201d, in <em>Philosophy and Theory of Artificial\nIntelligence</em>, Vincent C. M\u00fcller (ed.), (Studies in Applied\nPhilosophy, Epistemology and Rational Ethics, 5), Berlin, Heidelberg:\nSpringer Berlin Heidelberg, 251\u2013264.\ndoi:10.1007/978-3-642-31674-6_19", "\u2013\u2013\u2013, 2019, \u201cThere Is Plenty of Time at the\nBottom: The Economics, Risk and Ethics of Time Compression\u201d,\n<em>Foresight</em>, 21(1): 84\u201399.\ndoi:10.1108/FS-04-2018-0044", "Santoni de Sio, Filippo and Jeroen van den Hoven, 2018,\n\u201cMeaningful Human Control over Autonomous Systems: A\nPhilosophical Account\u201d, <em>Frontiers in Robotics and AI</em>,\n5(February): 15. doi:10.3389/frobt.2018.00015", "Schneier, Bruce, 2015, <em>Data and Goliath: The Hidden Battles to\nCollect Your Data and Control Your World</em>, New York: W. W.\nNorton.", "Searle, John R., 1980, \u201cMinds, Brains, and Programs\u201d,\n<em>Behavioral and Brain Sciences</em>, 3(3): 417\u2013424.\ndoi:10.1017/S0140525X00005756", "Selbst, Andrew D., Danah Boyd, Sorelle A. Friedler, Suresh\nVenkatasubramanian, and Janet Vertesi, 2019, \u201cFairness and\nAbstraction in Sociotechnical Systems\u201d, in <em>Proceedings of\nthe Conference on Fairness, Accountability, and\nTransparency\u2014FAT* \u201919</em>, Atlanta, GA: ACM Press,\n59\u201368. doi:10.1145/3287560.3287598", "Sennett, Richard, 2018, <em>Building and Dwelling: Ethics for the\nCity</em>, London: Allen Lane.", "Shanahan, Murray, 2015, <em>The Technological Singularity</em>,\nCambridge, MA: MIT Press.", "Sharkey, Amanda, 2019, \u201cAutonomous Weapons Systems, Killer\nRobots and Human Dignity\u201d, <em>Ethics and Information\nTechnology</em>, 21(2): 75\u201387.\ndoi:10.1007/s10676-018-9494-0", "Sharkey, Amanda and Noel Sharkey, 2011, \u201cThe Rights and\nWrongs of Robot Care\u201d, in <em>Robot Ethics: The Ethical and\nSocial Implications of Robotics</em>, Patrick Lin, Keith Abney and\nGeorge Bekey (eds.), Cambridge, MA: MIT Press, 267\u2013282.", "Shoham, Yoav, Perrault Raymond, Brynjolfsson Erik, Jack Clark,\nJames Manyika, Juan Carlos Niebles, \u2026 Zoe Bauer, 2018,\n\u201cThe AI Index 2018 Annual Report\u201d, 17 December 2018,\nStanford, CA: AI Index Steering Committee, Human-Centered AI\nInitiative, Stanford University.\n [<a href=\"https://hai.stanford.edu/ai-index/previous-reports/2018\" target=\"other\">Shoam et al. 2018 available online</a>]", "Silver, David, Thomas Hubert, Julian Schrittwieser, Ioannis\nAntonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre,\nDharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan,\nand Demis Hassabis, 2018, \u201cA General Reinforcement Learning\nAlgorithm That Masters Chess, Shogi, and Go through Self-Play\u201d,\n<em>Science</em>, 362(6419): 1140\u20131144.\ndoi:10.1126/science.aar6404", "Simon, Herbert A. and Allen Newell, 1958, \u201cHeuristic Problem\nSolving: The Next Advance in Operations Research\u201d,\n<em>Operations Research</em>, 6(1): 1\u201310.\ndoi:10.1287/opre.6.1.1", "Simpson, Thomas W. and Vincent C. M\u00fcller, 2016, \u201cJust\nWar and Robots\u2019 Killings\u201d, <em>The Philosophical\nQuarterly</em>, 66(263): 302\u2013322. doi:10.1093/pq/pqv075", "Smolan, Sandy (director), 2016, \u201cThe Human Face of Big\nData\u201d, <em>PBS Documentary,</em> 24 February 2016, 56 mins.", "Sparrow, Robert, 2007, \u201cKiller Robots\u201d, <em>Journal of\nApplied Philosophy</em>, 24(1): 62\u201377.\ndoi:10.1111/j.1468-5930.2007.00346.x", "\u2013\u2013\u2013, 2016, \u201cRobots in Aged Care: A\nDystopian Future?\u201d, <em>AI &amp; Society</em>, 31(4):\n445\u2013454. doi:10.1007/s00146-015-0625-4", "Stahl, Bernd Carsten, Job Timmermans, and Brent Daniel\nMittelstadt, 2016, \u201cThe Ethics of Computing: A Survey of the\nComputing-Oriented Literature\u201d, <em>ACM Computing Surveys</em>,\n48(4): art. 55. doi:10.1145/2871196", "Stahl, Bernd Carsten and David Wright, 2018, \u201cEthics and\nPrivacy in AI and Big Data: Implementing Responsible Research and\nInnovation\u201d, <em>IEEE Security Privacy</em>, 16(3):\n26\u201333.", "Stone, Christopher D., 1972, \u201cShould Trees Have Standing -\ntoward Legal Rights for Natural Objects\u201d, <em>Southern\nCalifornia Law Review</em>, 45: 450\u2013501.", "Stone, Peter, Rodney Brooks, Erik Brynjolfsson, Ryan Calo, Oren\nEtzioni, Greg Hager, Julia Hirschberg, Shivaram Kalyanakrishnan, Ece\nKamar, Sarit Kraus, Kevin Leyton-Brown, David Parkes, William Press,\nAnnaLee Saxenian, Julie Shah, Milind Tambe, and Astro Teller, 2016,\n\u201cArtificial Intelligence and Life in 2030\u201d, One Hundred\nYear Study on Artificial Intelligence: Report of the 2015\u20132016\nStudy Panel, Stanford University, Stanford, CA, September 2016.\n [<a href=\"https://ai100.stanford.edu/2016-report\" target=\"other\">Stone et al. 2016 available online</a>]", "Strawson, Galen, 1998, \u201cFree Will\u201d, in <em>Routledge\nEncyclopedia of Philosophy</em>, Taylor &amp; Francis.\ndoi:10.4324/9780415249126-V014-1", "Sullins, John P., 2012, \u201cRobots, Love, and Sex: The Ethics\nof Building a Love Machine\u201d, <em>IEEE Transactions on Affective\nComputing</em>, 3(4): 398\u2013409. doi:10.1109/T-AFFC.2012.31", "Susser, Daniel, Beate Roessler, and Helen Nissenbaum, 2019,\n\u201cTechnology, Autonomy, and Manipulation\u201d, <em>Internet\nPolicy Review</em>, 8(2): 30 June 2019.\n [<a href=\"https://policyreview.info/articles/analysis/technology-autonomy-and-manipulation\" target=\"other\">Susser, Roessler, and Nissenbaum 2019 available online</a>]", "Taddeo, Mariarosaria and Luciano Floridi, 2018, \u201cHow AI Can\nBe a Force for Good\u201d, <em>Science</em>, 361(6404):\n751\u2013752. doi:10.1126/science.aat5991", "Taylor, Linnet and Nadezhda Purtova, 2019, \u201cWhat Is\nResponsible and Sustainable Data Science?\u201d, Big Data &amp;\nSociety, 6(2): art. 205395171985811. doi:10.1177/2053951719858114", "Taylor, Steve, et al., 2018, \u201cResponsible AI \u2013 Key\nThemes, Concerns &amp; Recommendations for European Research and\nInnovation: Summary of Consultation with Multidisciplinary\nExperts\u201d, June.  doi:10.5281/zenodo.1303252\n [<a href=\"https://zenodo.org/record/1303253\" target=\"other\">Taylor, et al. 2018 available online</a>]", "Tegmark, Max, 2017, <em>Life 3.0: Being Human in the Age of\nArtificial Intelligence</em>, New York: Knopf.", "Thaler, Richard H and Sunstein, Cass, 2008, <em>Nudge: Improving\ndecisions about health, wealth and happiness</em>, New York:\nPenguin.", "Thompson, Nicholas and Ian Bremmer, 2018, \u201cThe AI Cold War\nThat Threatens Us All\u201d, <em>Wired</em>, 23 November 2018.\n [<a href=\"https://www.wired.com/story/ai-cold-war-china-could-doom-us-all/\" target=\"other\">Thompson and Bremmer 2018 available online</a>]", "Thomson, Judith Jarvis, 1976, \u201cKilling, Letting Die, and the\nTrolley Problem\u201d, <em>Monist</em>, 59(2): 204\u2013217.\ndoi:10.5840/monist197659224", "Torrance, Steve, 2011, \u201cMachine Ethics and the Idea of a\nMore-Than-Human Moral World\u201d, in Anderson and Anderson 2011:\n115\u2013137. doi:10.1017/CBO9780511978036.011", "Trump, Donald J, 2019, \u201cExecutive Order on Maintaining\nAmerican Leadership in Artificial Intelligence\u201d, 11 February\n2019.\n [<a href=\"https://www.federalregister.gov/documents/2019/02/14/2019-02544/maintaining-american-leadership-in-artificial-intelligence\" target=\"other\">Trump 2019 available online</a>]", "Turner, Jacob, 2019, <em>Robot Rules: Regulating Artificial\nIntelligence</em>, Berlin: Springer.\ndoi:10.1007/978-3-319-96235-1", "Tzafestas, Spyros G., 2016, <em>Roboethics: A Navigating\nOverview</em>, (Intelligent Systems, Control and Automation: Science\nand Engineering 79), Cham: Springer International Publishing.\ndoi:10.1007/978-3-319-21714-7", "Vallor, Shannon, 2017, <em>Technology and the Virtues: A\nPhilosophical Guide to a Future Worth Wanting</em>, Oxford: Oxford\nUniversity Press. doi:10.1093/acprof:oso/9780190498511.001.0001", "Van Lent, Michael, William Fisher, and Michael Mancuso, 2004,\n\u201cAn Explainable Artificial Intelligence System for Small-Unit\nTactical Behavior\u201d, in <em>Proceedings of the 16th Conference on\nInnovative Applications of Artifical Intelligence,\n(IAAI\u201904)</em>, San Jose, CA: AAAI Press, 900\u2013907.", "van Wynsberghe, Aimee, 2016, <em>Healthcare Robots: Ethics, Design\nand Implementation</em>, London: Routledge.\ndoi:10.4324/9781315586397", "van Wynsberghe, Aimee and Scott Robbins, 2019, \u201cCritiquing\nthe Reasons for Making Artificial Moral Agents\u201d, <em>Science and\nEngineering Ethics</em>, 25(3): 719\u2013735.\ndoi:10.1007/s11948-018-0030-8", "Vanderelst, Dieter and Alan Winfield, 2018, \u201cThe Dark Side\nof Ethical Robots\u201d, in <em>Proceedings of the 2018 AAAI/ACM\nConference on AI, Ethics, and Society</em>, New Orleans, LA: ACM,\n317\u2013322. doi:10.1145/3278721.3278726", "Veale, Michael and Reuben Binns, 2017, \u201cFairer Machine\nLearning in the Real World: Mitigating Discrimination without\nCollecting Sensitive Data\u201d, <em>Big Data &amp; Society</em>,\n4(2): art. 205395171774353. doi:10.1177/2053951717743530", "V\u00e9liz, Carissa, 2019, \u201cThree Things Digital Ethics\nCan Learn from Medical Ethics\u201d, <em>Nature Electronics</em>,\n2(8): 316\u2013318. doi:10.1038/s41928-019-0294-2", "Verbeek, Peter-Paul, 2011, <em>Moralizing Technology:\nUnderstanding and Designing the Morality of Things</em>, Chicago:\nUniversity of Chicago Press.", "Wachter, Sandra and Brent Daniel Mittelstadt, 2019, \u201cA Right\nto Reasonable Inferences: Re-Thinking Data Protection Law in the Age\nof Big Data and AI\u201d, <em>Columbia Business Law Review</em>,\n2019(2): 494\u2013620.", "Wachter, Sandra, Brent Mittelstadt, and Luciano Floridi, 2017,\n\u201cWhy a Right to Explanation of Automated Decision-Making Does\nNot Exist in the General Data Protection Regulation\u201d,\n<em>International Data Privacy Law</em>, 7(2): 76\u201399.\ndoi:10.1093/idpl/ipx005", "Wachter, Sandra, Brent Mittelstadt, and Chris Russell, 2018,\n\u201cCounterfactual Explanations Without Opening the Black Box:\nAutomated Decisions and the GDPR\u201d, <em>Harvard Journal of Law\n&amp; Technology</em>, 31(2): 842\u2013887.\ndoi:10.2139/ssrn.3063289", "Wallach, Wendell and Peter M. Asaro (eds.), 2017, <em>Machine\nEthics and Robot Ethics</em>, London: Routledge.", "Walsh, Toby, 2018, <em>Machines That Think: The Future of\nArtificial Intelligence</em>, Amherst, MA: Prometheus Books.", "Westlake, Stian (ed.), 2014, <em>Our Work Here Is Done: Visions of\na Robot Economy</em>, London: Nesta.\n [<a href=\"https://www.nesta.org.uk/report/our-work-here-is-done-visions-of-a-robot-economy/\" target=\"other\">Westlake 2014 available online</a>]", "Whittaker, Meredith, Kate Crawford, Roel Dobbe, Genevieve Fried,\nElizabeth Kaziunas, Varoon Mathur, \u2026 Jason Schultz, 2018,\n\u201cAI Now Report 2018\u201d, New York: AI Now Institute, New York\nUniversity.\n [<a href=\"https://ainowinstitute.org/publication/ai-now-2018-report-2\" target=\"other\">Whittaker et al. 2018 available online</a>]", "Whittlestone, Jess, Rune Nyrup, Anna Alexandrova, Kanta Dihal, and\nStephen Cave, 2019, \u201cEthical and Societal Implications of\nAlgorithms, Data, and Artificial Intelligence: A Roadmap for\nResearch\u201d, Cambridge: Nuffield Foundation, University of\nCambridge.\n [<a href=\"https://www.adalovelaceinstitute.org/nuffield-foundation-publishes-roadmap-for-ai-ethics-research/\" target=\"other\">Whittlestone 2019 available online</a>]", "Winfield, Alan, Katina Michael, Jeremy Pitt, and Vanessa Evers\n(eds.), 2019, <em>Machine Ethics: The Design and Governance of Ethical\nAI and Autonomous Systems</em>, special issue of <em>Proceedings of\nthe IEEE</em>, 107(3): 501\u2013632.", "Woollard, Fiona and Frances Howard-Snyder, 2016, \u201cDoing vs.\nAllowing Harm\u201d, <em>Stanford Encyclopedia of Philosophy</em>\n(Winter 2016 edition), Edward N. Zalta (ed.), URL =\n &lt;<a href=\"https://plato.stanford.edu/archives/win2016/entries/doing-allowing/\" target=\"other\">https://plato.stanford.edu/archives/win2016/entries/doing-allowing/</a>&gt;", "Woolley, Samuel C. and Philip N. Howard (eds.), 2017,\n<em>Computational Propaganda: Political Parties, Politicians, and\nPolitical Manipulation on Social Media</em>, Oxford: Oxford University\nPress. doi:10.1093/oso/9780190931407.001.0001", "Yampolskiy, Roman V. (ed.), 2018, <em>Artificial Intelligence\nSafety and Security</em>, Boca Raton, FL: Chapman and Hall/CRC.\ndoi:10.1201/9781351251389", "Yeung, Karen and Martin Lodge (eds.), 2019, <em>Algorithmic\nRegulation</em>, Oxford: Oxford University Press.\ndoi:10.1093/oso/9780198838494.001.0001", "Zayed, Yago and Philip Loft, 2019, \u201cAgriculture: Historical\nStatistics\u201d, <em>House of Commons Briefing Paper</em>, 3339(25\nJune 2019): 1-19.\n [<a href=\"https://commonslibrary.parliament.uk/research-briefings/sn03339/\" target=\"other\">Zayed and Loft 2019 available online</a>]", "Zerilli, John, Alistair Knott, James Maclaurin, and Colin\nGavaghan, 2019, \u201cTransparency in Algorithmic and Human\nDecision-Making: Is There a Double Standard?\u201d, <em>Philosophy\n&amp; Technology</em>, 32(4): 661\u2013683.\ndoi:10.1007/s13347-018-0330-6", "Zuboff, Shoshana, 2019, <em>The Age of Surveillance Capitalism:\nThe Fight for a Human Future at the New Frontier of Power</em>, New\nYork: Public Affairs."]}, "raw_text": "<div id=\"bibliography\">\n<h2 id=\"Bib\">Bibliography</h2>\n<p>\nNOTE: Citations in the main text annotated \u201c[OIR]\u201d may be\nfound in the\n <a href=\"#Oth\">Other Internet Resources</a>\n section below, not in the Bibliography.</p>\n<ul class=\"hanging\">\n<li>Abowd, John M, 2017, \u201cHow Will Statistical Agencies Operate\nWhen All Data Are Private?\u201d, <em>Journal of Privacy and\nConfidentiality</em>, 7(3): 1\u201315. doi:10.29012/jpc.v7i3.404</li>\n<!--  <li>AI4EU,\n2019, &ldquo;Outcomes from the Strategic Orientation Workshop\n(Deliverable 7.1)&rdquo;, (June 28, 2019).\nhttps://www.ai4eu.eu/ai4eu-project-deliverables</li> -->\n<li>Allen, Colin, Iva Smit, and Wendell Wallach, 2005,\n\u201cArtificial Morality: Top-down, Bottom-up, and Hybrid\nApproaches\u201d, <em>Ethics and Information Technology</em>, 7(3):\n149\u2013155. doi:10.1007/s10676-006-0004-4</li>\n<li>Allen, Colin, Gary Varner, and Jason Zinser, 2000,\n\u201cProlegomena to Any Future Artificial Moral Agent\u201d,\n<em>Journal of Experimental &amp; Theoretical Artificial\nIntelligence</em>, 12(3): 251\u2013261.\ndoi:10.1080/09528130050111428</li>\n<li>Amoroso, Daniele and Guglielmo Tamburrini, 2018, \u201cThe\nEthical and Legal Case Against Autonomy in Weapons Systems\u201d,\n<em>Global Jurist</em>, 18(1): art. 20170012.\ndoi:10.1515/gj-2017-0012</li>\n<li>Anderson, Janna, Lee Rainie, and Alex Luchsinger, 2018,\n<em>Artificial Intelligence and the Future of Humans</em>, Washington,\nDC: Pew Research Center.</li>\n<li>Anderson, Michael and Susan Leigh Anderson, 2007, \u201cMachine\nEthics: Creating an Ethical Intelligent Agent\u201d, <em>AI\nMagazine</em>, 28(4): 15\u201326.</li>\n<li>\u2013\u2013\u2013 (eds.), 2011, <em>Machine Ethics</em>,\nCambridge: Cambridge University Press.\ndoi:10.1017/CBO9780511978036</li>\n<li>Aneesh, A., 2006, <em>Virtual Migration: The Programming of\nGlobalization</em>, Durham, NC and London: Duke University Press.</li>\n<li>Arkin, Ronald C., 2009, <em>Governing Lethal Behavior in\nAutonomous Robots</em>, Boca Raton, FL: CRC Press.</li>\n<li>Armstrong, Stuart, 2013, \u201cGeneral Purpose Intelligence:\nArguing the Orthogonality Thesis\u201d, <em>Analysis and\nMetaphysics</em>, 12: 68\u201384.</li>\n<li>\u2013\u2013\u2013, 2014, <em>Smarter Than Us</em>, Berkeley,\nCA: MIRI.</li>\n<li>Arnold, Thomas and Matthias Scheutz, 2017, \u201cBeyond Moral\nDilemmas: Exploring the Ethical Landscape in HRI\u201d, in\n<em>Proceedings of the 2017 ACM/IEEE International Conference on\nHuman-Robot Interaction\u2014HRI \u201917</em>, Vienna, Austria: ACM\nPress, 445\u2013452. doi:10.1145/2909824.3020255</li>\n<li>Asaro, Peter M., 2019, \u201cAI Ethics in Predictive Policing:\nFrom Models of Threat to an Ethics of Care\u201d, <em>IEEE Technology\nand Society Magazine</em>, 38(2): 40\u201353.\ndoi:10.1109/MTS.2019.2915154</li>\n<li>Asimov, Isaac, 1942, \u201cRunaround: A Short Story\u201d,\n<em>Astounding Science Fiction</em>, March 1942. Reprinted in\n\u201cI, Robot\u201d, New York: Gnome Press 1950, 1940ff.</li>\n<li>Awad, Edmond, Sohan Dsouza, Richard Kim, Jonathan Schulz, Joseph\nHenrich, Azim Shariff, Jean-Fran\u00e7ois Bonnefon, and Iyad Rahwan,\n2018, \u201cThe Moral Machine Experiment\u201d, <em>Nature</em>,\n563(7729): 59\u201364. doi:10.1038/s41586-018-0637-6</li>\n<li>Baldwin, Richard, 2019, <em>The Globotics Upheaval: Globalisation,\nRobotics and the Future of Work</em>, New York: Oxford University\nPress.</li>\n<li>Baum, Seth D., Stuart Armstrong, Timoteus Ekenstedt, Olle\nH\u00e4ggstr\u00f6m, Robin Hanson, Karin Kuhlemann, Matthijs M. Maas,\nJames D. Miller, Markus Salmela, Anders Sandberg, Kaj Sotala, Phil\nTorres, Alexey Turchin, and Roman V. Yampolskiy, 2019,\n\u201cLong-Term Trajectories of Human Civilization\u201d,\n<em>Foresight</em>, 21(1): 53\u201383.\ndoi:10.1108/FS-04-2018-0037</li>\n<li>Bendel, Oliver, 2018, \u201cSexroboter aus Sicht der\nMaschinenethik\u201d, in <em>Handbuch Filmtheorie</em>, Bernhard\nGro\u00df and Thomas Morsch (eds.), (Springer Reference\nGeisteswissenschaften), Wiesbaden: Springer Fachmedien Wiesbaden,\n1\u201319. doi:10.1007/978-3-658-17484-2_22-1</li>\n<li>Bennett, Colin J. and Charles Raab, 2006, <em>The Governance of\nPrivacy: Policy Instruments in Global Perspective</em>, second\nedition, Cambridge, MA: MIT Press.</li>\n<li>Benthall, Sebastian and Bruce D. Haynes, 2019, \u201cRacial\nCategories in Machine Learning\u201d, in <em>Proceedings of the\nConference on Fairness, Accountability, and Transparency - FAT*\n\u201919</em>, Atlanta, GA, USA: ACM Press, 289\u2013298.\ndoi:10.1145/3287560.3287575</li>\n<li>Bentley, Peter J., Miles Brundage, Olle H\u00e4ggstr\u00f6m, and\nThomas Metzinger, 2018, \u201cShould We Fear Artificial Intelligence?\nIn-Depth Analysis\u201d, European Parliamentary Research Service,\nScientific Foresight Unit (STOA), March 2018, PE 614.547, 1\u201340.\n [<a href=\"https://op.europa.eu/en/publication-detail/-/publication/f27d7e2c-88a2-11e8-ac6a-01aa75ed71a1\" target=\"other\">Bentley et al. 2018 available online</a>]</li>\n<li>Bertolini, Andrea and Giuseppe Aiello, 2018, \u201cRobot\nCompanions: A Legal and Ethical Analysis\u201d, <em>The Information\nSociety</em>, 34(3): 130\u2013140.\ndoi:10.1080/01972243.2018.1444249</li>\n<li>Binns, Reuben, 2018, \u201cFairness in Machine Learning: Lessons\nfrom Political Philosophy\u201d, <em>Proceedings of the 1st\nConference on Fairness, Accountability and Transparency</em>, in\n<em>Proceedings of Machine Learning Research</em>, 81:\n149\u2013159.</li>\n<li>Bostrom, Nick, 2003a, \u201cAre We Living in a Computer\nSimulation?\u201d, <em>The Philosophical Quarterly</em>, 53(211):\n243\u2013255. doi:10.1111/1467-9213.00309</li>\n<li>\u2013\u2013\u2013, 2003b, \u201cEthical Issues in Advanced\nArtificial Intelligence\u201d, in <em>Cognitive, Emotive and Ethical\nAspects of Decision Making in Humans and in Artificial Intelligence,\nVolume 2</em>, Iva Smit, Wendell Wallach, and G.E. Lasker (eds),\n(IIAS-147-2003), Tecumseh, ON: International Institute of Advanced\nStudies in Systems Research and Cybernetics, 12\u201317.\n [<a href=\"https://nickbostrom.com/ethics/ai.html\" target=\"other\">Botstrom 2003b revised available online</a>]</li>\n<li>\u2013\u2013\u2013, 2003c, \u201cTranshumanist Values\u201d,\nin <em>Ethical Issues for the Twenty-First Century</em>, Frederick\nAdams (ed.), Bowling Green, OH: Philosophical Documentation Center\nPress.</li>\n<li>\u2013\u2013\u2013, 2012, \u201cThe Superintelligent Will:\nMotivation and Instrumental Rationality in Advanced Artificial\nAgents\u201d, <em>Minds and Machines</em>, 22(2): 71\u201385.\ndoi:10.1007/s11023-012-9281-3</li>\n<li>\u2013\u2013\u2013, 2013, \u201cExistential Risk Prevention as\nGlobal Priority\u201d, <em>Global Policy</em>, 4(1): 15\u201331.\ndoi:10.1111/1758-5899.12002</li>\n<li>\u2013\u2013\u2013, 2014, <em>Superintelligence: Paths,\nDangers, Strategies</em>, Oxford: Oxford University Press.</li>\n<li>Bostrom, Nick and Milan M. \u0106irkovi\u0107 (eds.), 2011,\n<em>Global Catastrophic Risks</em>, New York: Oxford University\nPress.</li>\n<li>Bostrom, Nick, Allan Dafoe, and Carrick Flynn, forthcoming,\n\u201cPolicy Desiderata for Superintelligent AI: A Vector Field\nApproach (V. 4.3)\u201d, in <em>Ethics of Artificial\nIntelligence</em>, S Matthew Liao (ed.), New York: Oxford University\nPress.\n [<a href=\"https://nickbostrom.com/papers/aipolicy.pdf\" target=\"other\">Bostrom, Dafoe, and Flynn forthcoming \u2013 preprint available online</a>]</li>\n<li>Bostrom, Nick and Eliezer Yudkowsky, 2014, \u201cThe Ethics of\nArtificial Intelligence\u201d, in <em>The Cambridge Handbook of\nArtificial Intelligence</em>, Keith Frankish and William M. Ramsey\n(eds.), Cambridge: Cambridge University Press, 316\u2013334.\ndoi:10.1017/CBO9781139046855.020\n [<a href=\"http://intelligence.org/files/EthicsofAI.pdf\" target=\"other\">Bostrom and Yudkowsky 2014 available online</a>]</li>\n<li>Bradshaw, Samantha, Lisa-Maria Neudert, and Phil Howard, 2019,\n\u201cGovernment Responses to Malicious Use of Social Media\u201d,\nWorking Paper 2019.2, Oxford: Project on Computational Propaganda.\n [<a href=\"https://comprop.oii.ox.ac.uk/research/government-responses/\" target=\"other\">Bradshaw, Neudert, and Howard 2019 available online/</a>]</li>\n<li>Brownsword, Roger, Eloise Scotford, and Karen Yeung (eds.), 2017,\n<em>The Oxford Handbook of Law, Regulation and Technology</em>,\nOxford: Oxford University Press.\ndoi:10.1093/oxfordhb/9780199680832.001.0001</li>\n<li>Brynjolfsson, Erik and Andrew McAfee, 2016, <em>The Second Machine\nAge: Work, Progress, and Prosperity in a Time of Brilliant\nTechnologies</em>, New York: W. W. Norton.</li>\n<li>Bryson, Joanna J., 2010, \u201cRobots Should Be Slaves\u201d, in\n<em>Close Engagements with Artificial Companions: Key Social,\nPsychological, Ethical and Design Issues</em>, Yorick Wilks (ed.),\n(Natural Language Processing 8), Amsterdam: John Benjamins Publishing\nCompany, 63\u201374. doi:10.1075/nlp.8.11bry</li>\n<li>\u2013\u2013\u2013, 2019, \u201cThe Past Decade and Future of\nAi\u2019s Impact on Society\u201d, in <em>Towards a New\nEnlightenment: A Transcendent Decade</em>, Madrid: Turner - BVVA.\n [<a href=\"https://www.bbvaopenmind.com/en/books/towards-a-new-enlightenment-a-transcendent-decade/\" target=\"other\">Bryson 2019 available online</a>]</li>\n<li>Bryson, Joanna J., Mihailis E. Diamantis, and Thomas D. Grant,\n2017, \u201cOf, for, and by the People: The Legal Lacuna of Synthetic\nPersons\u201d, <em>Artificial Intelligence and Law</em>, 25(3):\n273\u2013291. doi:10.1007/s10506-017-9214-9</li>\n<li>Burr, Christopher and Nello Cristianini, 2019, \u201cCan Machines\nRead Our Minds?\u201d, <em>Minds and Machines</em>, 29(3):\n461\u2013494. doi:10.1007/s11023-019-09497-4</li>\n<li>Butler, Samuel, 1863, \u201cDarwin among the Machines: Letter to\nthe Editor\u201d, Letter in <em>The Press (Christchurch)</em>, 13\nJune 1863.\n [<a href=\"http://nzetc.victoria.ac.nz/tm/scholarly/tei-ButFir-t1-g1-t1-g1-t4-body.html\" target=\"other\">Butler 1863 available online</a>]</li>\n<li>Callaghan, Victor, James Miller, Roman Yampolskiy, and Stuart\nArmstrong (eds.), 2017, <em>The Technological Singularity: Managing\nthe Journey</em>, (The Frontiers Collection), Berlin, Heidelberg:\nSpringer Berlin Heidelberg. doi:10.1007/978-3-662-54033-6</li>\n<li>Calo, Ryan, 2018, \u201cArtificial Intelligence Policy: A Primer\nand Roadmap\u201d, <em>University of Bologna Law Review</em>, 3(2):\n180-218. doi:10.6092/ISSN.2531-6133/8670</li>\n<li>Calo, Ryan, A. Michael Froomkin, and Ian Kerr (eds.), 2016,\n<em>Robot Law</em>, Cheltenham: Edward Elgar.</li>\n<li>\u010capek, Karel, 1920, <em>R.U.R.</em>, Prague: Aventium.\nTranslated by Peter Majer and Cathy Porter, London: Methuen,\n1999.</li>\n<li>Capurro, Raphael, 1993, \u201cEin Grinsen Ohne Katze: Von der\nVergleichbarkeit Zwischen \u2018K\u00fcnstlicher Intelligenz\u2019\nund \u2018Getrennten Intelligenzen\u2019\u201d, <em>Zeitschrift\nf\u00fcr philosophische Forschung</em>, 47: 93\u2013102.</li>\n<li>Cave, Stephen, 2019, \u201cTo Save Us from a Kafkaesque Future,\nWe Must Democratise AI\u201d, <em>The Guardian</em> , 04 January\n2019.\n [<a href=\"https://www.theguardian.com/commentisfree/2019/jan/04/future-democratise-ai-artificial-intelligence-power\" target=\"other\">Cave 2019 available online</a>]</li>\n<li>Chalmers, David J., 2010, \u201cThe Singularity: A Philosophical\nAnalysis\u201d, <em>Journal of Consciousness Studies</em>,\n17(9\u201310): 7\u201365.\n [<a href=\"http://consc.net/papers/singularityjcs.pdf\" target=\"other\">Chalmers 2010 available online</a>]</li>\n<li>Christman, John, 2003 [2018], \u201cAutonomy in Moral and\nPolitical Philosophy\u201d, (Spring 2018) <em>Stanford Encyclopedia\nof Philosophy</em> (EDITION NEEDED), URL =\n &lt;<a href=\"https://plato.stanford.edu/archives/spr2018/entries/autonomy-moral/\" target=\"other\">https://plato.stanford.edu/archives/spr2018/entries/autonomy-moral/</a>&gt;</li>\n<li>Coeckelbergh, Mark, 2010, \u201cRobot Rights? Towards a\nSocial-Relational Justification of Moral Consideration\u201d,\n<em>Ethics and Information Technology</em>, 12(3): 209\u2013221.\ndoi:10.1007/s10676-010-9235-5</li>\n<li>\u2013\u2013\u2013, 2012, <em>Growing Moral Relations: Critique\nof Moral Status Ascription</em>, London: Palgrave.\ndoi:10.1057/9781137025968</li>\n<li>\u2013\u2013\u2013, 2016, \u201cCare Robots and the Future of\nICT-Mediated Elderly Care: A Response to Doom Scenarios\u201d, <em>AI\n&amp; Society</em>, 31(4): 455\u2013462.\ndoi:10.1007/s00146-015-0626-3</li>\n<li>\u2013\u2013\u2013, 2018, \u201cWhat Do We Mean by a\nRelational Ethics? Growing a Relational Approach to the Moral Standing\nof Plants, Robots and Other Non-Humans\u201d, in <em>Plant Ethics:\nConcepts and Applications</em>, Angela Kallhoff, Marcello Di Paola,\nand Maria Sch\u00f6rgenhumer (eds.), London: Routledge,\n110\u2013121.</li>\n<li>Crawford, Kate and Ryan Calo, 2016, \u201cThere Is a Blind Spot\nin AI Research\u201d, <em>Nature</em>, 538(7625): 311\u2013313.\ndoi:10.1038/538311a</li>\n<li>Cristianini, Nello, forthcoming, \u201cShortcuts to Artificial\nIntelligence\u201d, in <em>Machines We Trust</em>, Marcello Pelillo\nand Teresa Scantamburlo (eds.), Cambridge, MA: MIT Press.\n [<a href=\"https://philpapers.org/rec/CRISTA-3\" target=\"other\">Cristianini forthcoming \u2013 preprint available online</a>]</li>\n<li>Danaher, John, 2015, \u201cWhy AI Doomsayers Are Like Sceptical\nTheists and Why It Matters\u201d, <em>Minds and Machines</em>, 25(3):\n231\u2013246. doi:10.1007/s11023-015-9365-y</li>\n<li>\u2013\u2013\u2013, 2016a, \u201cRobots, Law and the\nRetribution Gap\u201d, <em>Ethics and Information Technology</em>,\n18(4): 299\u2013309. doi:10.1007/s10676-016-9403-3</li>\n<li>\u2013\u2013\u2013, 2016b, \u201cThe Threat of Algocracy:\nReality, Resistance and Accommodation\u201d, <em>Philosophy &amp;\nTechnology</em>, 29(3): 245\u2013268.\ndoi:10.1007/s13347-015-0211-1</li>\n<li>\u2013\u2013\u2013, 2019a, <em>Automation and Utopia: Human\nFlourishing in a World without Work</em>, Cambridge, MA: Harvard\nUniversity Press.</li>\n<li>\u2013\u2013\u2013, 2019b, \u201cThe Philosophical Case for\nRobot Friendship\u201d, <em>Journal of Posthuman Studies</em>, 3(1):\n5\u201324. doi:10.5325/jpoststud.3.1.0005</li>\n<li>\u2013\u2013\u2013, forthcoming, \u201cWelcoming Robots into\nthe Moral Circle: A Defence of Ethical Behaviourism\u201d,\n<em>Science and Engineering Ethics</em>, first online: 20 June 2019.\ndoi:10.1007/s11948-019-00119-x</li>\n<li>Danaher, John and Neil McArthur (eds.), 2017, <em>Robot Sex:\nSocial and Ethical Implications</em>, Boston, MA: MIT Press.</li>\n<li>DARPA, 1983, \u201cStrategic Computing. New-Generation Computing\nTechnology: A Strategic Plan for Its Development an Application to\nCritical Problems in Defense\u201d, ADA141982, 28 October 1983.\n [<a href=\"https://apps.dtic.mil/docs/citations/ADA141982\" target=\"other\">DARPA 1983 available online</a>]</li>\n<li>Dennett, Daniel C, 2017, <em>From Bacteria to Bach and Back: The\nEvolution of Minds</em>, New York: W.W. Norton.</li>\n<li>Devlin, Kate, 2018, <em>Turned On: Science, Sex and Robots</em>,\nLondon: Bloomsbury.</li>\n<li>Diakopoulos, Nicholas, 2015, \u201cAlgorithmic Accountability:\nJournalistic Investigation of Computational Power Structures\u201d,\n<em>Digital Journalism</em>, 3(3): 398\u2013415.\ndoi:10.1080/21670811.2014.976411</li>\n<li>Dignum, Virginia, 2018, \u201cEthics in Artificial Intelligence:\nIntroduction to the Special Issue\u201d, <em>Ethics and Information\nTechnology</em>, 20(1): 1\u20133. doi:10.1007/s10676-018-9450-z</li>\n<li>Domingos, Pedro, 2015, <em>The Master Algorithm: How the Quest for\nthe Ultimate Learning Machine Will Remake Our World</em>, London:\nAllen Lane.</li>\n<li>Draper, Heather, Tom Sorell, Sandra Bedaf, Dag Sverre Syrdal,\nCarolina Gutierrez-Ruiz, Alexandre Duclos, and Farshid\nAmirabdollahian, 2014, \u201cEthical Dimensions of Human-Robot\nInteractions in the Care of Older People: Insights from 21 Focus\nGroups Convened in the UK, France and the Netherlands\u201d, in\n<em>International Conference on Social Robotics 2014</em>, Michael\nBeetz, Benjamin Johnston, and Mary-Anne Williams (eds.), (Lecture\nNotes in Artificial Intelligence 8755), Cham: Springer International\nPublishing, 135\u2013145. doi:10.1007/978-3-319-11973-1_14</li>\n<li>Dressel, Julia and Hany Farid, 2018, \u201cThe Accuracy,\nFairness, and Limits of Predicting Recidivism\u201d, <em>Science\nAdvances</em>, 4(1): eaao5580. doi:10.1126/sciadv.aao5580</li>\n<li>Drexler, K. Eric, 2019, \u201cReframing Superintelligence:\nComprehensive AI Services as General Intelligence\u201d, FHI\nTechnical Report, 2019-1, 1-210.\n [<a href=\"https://www.fhi.ox.ac.uk/reframing/\" target=\"other\">Drexler 2019 available online</a>]</li>\n<li>Dreyfus, Hubert L., 1972, <em>What Computers Still Can\u2019t Do:\nA Critique of Artificial Reason</em>, second edition, Cambridge, MA:\nMIT Press 1992.</li>\n<li>Dreyfus, Hubert L., Stuart E. Dreyfus, and Tom Athanasiou, 1986,\n<em>Mind over Machine: The Power of Human Intuition and Expertise in\nthe Era of the Computer</em>, New York: Free Press.</li>\n<li>Dwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith,\n2006, <em>Calibrating Noise to Sensitivity in Private Data\nAnalysis</em>, Berlin, Heidelberg.</li>\n<li>Eden, Amnon H., James H. Moor, Johnny H. S\u00f8raker, and Eric\nSteinhart (eds.), 2012, <em>Singularity Hypotheses: A Scientific and\nPhilosophical Assessment</em>, (The Frontiers Collection), Berlin,\nHeidelberg: Springer Berlin Heidelberg.\ndoi:10.1007/978-3-642-32560-1</li>\n<li>Eubanks, Virginia, 2018, <em>Automating Inequality: How High-Tech\nTools Profile, Police, and Punish the Poor</em>, London: St.\nMartin\u2019s Press.</li>\n<li>European Commission, 2013, \u201cHow Many People Work in\nAgriculture in the European Union? An Answer Based on Eurostat Data\nSources\u201d, <em>EU Agricultural Economics Briefs</em>, 8 (July\n2013).\n [<a href=\"https://ec.europa.eu/info/sites/info/files/food-farming-fisheries/farming/documents/agri-economics-brief-08_en.pdf\" target=\"other\">Anonymous 2013 available online</a>]</li>\n<li>European Group on Ethics in Science and New Technologies, 2018,\n\u201cStatement on Artificial Intelligence, Robotics and\n\u2018Autonomous\u2019 Systems\u201d, 9 March 2018, European\nCommission, Directorate-General for Research and Innovation, Unit\nRTD.01.\n [<a href=\"https://op.europa.eu/en/publication-detail/-/publication/dfebe62e-4ce9-11e8-be1d-01aa75ed71a1\" target=\"other\">European Group 2018 available online </a>]</li>\n<li>Ferguson, Andrew Guthrie, 2017, <em>The Rise of Big Data Policing:\nSurveillance, Race, and the Future of Law Enforcement</em>, New York:\nNYU Press.</li>\n<li>Floridi, Luciano, 2016, \u201cShould We Be Afraid of AI? Machines\nSeem to Be Getting Smarter and Smarter and Much Better at Human Jobs,\nyet True AI Is Utterly Implausible. Why?\u201d, <em>Aeon</em>, 9 May\n2016. URL =\n &lt;<a href=\"https://aeon.co/essays/true-ai-is-both-logically-possible-and-utterly-implausible\" target=\"other\">Floridi 2016 available online</a>&gt;</li>\n<li>Floridi, Luciano, Josh Cowls, Monica Beltrametti, Raja Chatila,\nPatrice Chazerand, Virginia Dignum, Christoph Luetge, Robert Madelin,\nUgo Pagallo, Francesca Rossi, Burkhard Schafer, Peggy Valcke, and Effy\nVayena, 2018, \u201cAI4People\u2014An Ethical Framework for a Good\nAI Society: Opportunities, Risks, Principles, and\nRecommendations\u201d, <em>Minds and Machines</em>, 28(4):\n689\u2013707. doi:10.1007/s11023-018-9482-5</li>\n<li>Floridi, Luciano and Jeff W. Sanders, 2004, \u201cOn the Morality\nof Artificial Agents\u201d, <em>Minds and Machines</em>, 14(3):\n349\u2013379. doi:10.1023/B:MIND.0000035461.63578.9d</li>\n<li>Floridi, Luciano and Mariarosaria Taddeo, 2016, \u201cWhat Is\nData Ethics?\u201d, <em>Philosophical Transactions of the Royal\nSociety A: Mathematical, Physical and Engineering Sciences</em>,\n374(2083): 20160360. doi:10.1098/rsta.2016.0360</li>\n<li>Foot, Philippa, 1967, \u201cThe Problem of Abortion and the\nDoctrine of the Double Effect\u201d, <em>Oxford Review</em>, 5:\n5\u201315.</li>\n<li>Fosch-Villaronga, Eduard and Jordi Albo-Canals, 2019,\n\u201c\u2018I\u2019ll Take Care of You,\u2019 Said the\nRobot\u201d, <em>Paladyn, Journal of Behavioral Robotics</em>, 10(1):\n77\u201393. doi:10.1515/pjbr-2019-0006</li>\n<li>Frank, Lily and Sven Nyholm, 2017, \u201cRobot Sex and Consent:\nIs Consent to Sex between a Robot and a Human Conceivable, Possible,\nand Desirable?\u201d, <em>Artificial Intelligence and Law</em>,\n25(3): 305\u2013323. doi:10.1007/s10506-017-9212-y</li>\n<li>Frankfurt, Harry G., 1971, \u201cFreedom of the Will and the\nConcept of a Person\u201d, <em>The Journal of Philosophy</em>, 68(1):\n5\u201320.</li>\n<li>Frey, Carl Benedict, 2019, <em>The Technology Trap: Capital,\nLabour, and Power in the Age of Automation</em>, Princeton, NJ:\nPrinceton University Press.</li>\n<li>Frey, Carl Benedikt and Michael A. Osborne, 2013, \u201cThe\nFuture of Employment: How Susceptible Are Jobs to\nComputerisation?\u201d, Oxford Martin School Working Papers, 17\nSeptember 2013.\n [<a href=\"http://www.oxfordmartin.ox.ac.uk/publications/view/1314\" target=\"other\">Frey and Osborne 2013 available online</a>]</li>\n<li>Ganascia, Jean-Gabriel, 2017, <em>Le Mythe De La\nSingularit\u00e9</em>, Paris: \u00c9ditions du Seuil.</li>\n<li>EU Parliament, 2016, \u201cDraft Report with Recommendations to\nthe Commission on Civil Law Rules on Robotics (2015/2103(Inl))\u201d,\n<em>Committee on Legal Affairs</em>, 10.11.2016.\nhttps://www.europarl.europa.eu/doceo/document/A-8-2017-0005_EN.html</li>\n<li>EU Regulation, 2016/679, \u201cGeneral Data Protection\nRegulation: Regulation (EU) 2016/679 of the European Parliament and of\nthe Council of 27 April 2016 on the Protection of Natural Persons with\nRegard to the Processing of Personal Data and on the Free Movement of\nSuch Data, and Repealing Directive 95/46/Ec\u201d, <em>Official\nJournal of the European Union</em>, 119 (4 May 2016), 1\u201388.\n [<a href=\"http://data.europa.eu/eli/reg/2016/679/oj\" target=\"other\">Regulation (EU) 2016/679 available online</a>]\n <!-- the link is the permanent link --></li>\n<li>Geraci, Robert M., 2008, \u201cApocalyptic AI: Religion and the\nPromise of Artificial Intelligence\u201d, <em>Journal of the American\nAcademy of Religion</em>, 76(1): 138\u2013166.\ndoi:10.1093/jaarel/lfm101</li>\n<li>\u2013\u2013\u2013, 2010, <em>Apocalyptic AI: Visions of Heaven\nin Robotics, Artificial Intelligence, and Virtual Reality</em>,\nOxford: Oxford University Press.\ndoi:10.1093/acprof:oso/9780195393026.001.0001</li>\n<li>Gerdes, Anne, 2016, \u201cThe Issue of Moral Consideration in\nRobot Ethics\u201d, <em>ACM SIGCAS Computers and Society</em>, 45(3):\n274\u2013279. doi:10.1145/2874239.2874278</li>\n<li>German Federal Ministry of Transport and Digital Infrastructure,\n2017, \u201cReport of the Ethics Commission: Automated and Connected\nDriving\u201d, June 2017, 1\u201336.\n [<a href=\"https://www.bmvi.de/SharedDocs/EN/publications/report-ethics-commission.html\" target=\"other\">GFMTDI 2017 available online</a>]</li>\n<li>Gertz, Nolen, 2018, <em>Nihilism and Technology</em>, London:\nRowman &amp; Littlefield.</li>\n<li>Gewirth, Alan, 1978, \u201cThe Golden Rule Rationalized\u201d,\n<em>Midwest Studies in Philosophy</em>, 3(1): 133\u2013147.\ndoi:10.1111/j.1475-4975.1978.tb00353.x</li>\n<li>Gibert, Martin, 2019, \u201c\u00c9thique Artificielle (Version\nGrand Public)\u201d, in <em>L\u2019Encyclop\u00e9die\nPhilosophique</em>, Maxime Kristanek (ed.), accessed: 16 April 2020,\nURL =\n &lt;<a href=\"https://encyclo-philo.fr/item/199\" target=\"other\">Gibert 2019 available online</a>&gt;</li>\n<li>Giubilini, Alberto and Julian Savulescu, 2018, \u201cThe\nArtificial Moral Advisor. The \u2018Ideal Observer\u2019 Meets\nArtificial Intelligence\u201d, <em>Philosophy &amp; Technology</em>,\n31(2): 169\u2013188. doi:10.1007/s13347-017-0285-z</li>\n<li>Good, Irving John, 1965, \u201cSpeculations Concerning the First\nUltraintelligent Machine\u201d, in <em>Advances in Computers 6</em>,\nFranz L. Alt and Morris Rubinoff (eds.), New York &amp; London:\nAcademic Press, 31\u201388. doi:10.1016/S0065-2458(08)60418-0</li>\n<li>Goodfellow, Ian, Yoshua Bengio, and Aaron Courville, 2016,\n<em>Deep Learning</em>, Cambridge, MA: MIT Press.</li>\n<li>Goodman, Bryce and Seth Flaxman, 2017, \u201cEuropean Union\nRegulations on Algorithmic Decision-Making and a \u2018Right to\nExplanation\u2019\u201d, <em>AI Magazine</em>, 38(3): 50\u201357.\ndoi:10.1609/aimag.v38i3.2741</li>\n<li>Goos, Maarten, 2018, \u201cThe Impact of Technological Progress\non Labour Markets: Policy Challenges\u201d, <em>Oxford Review of\nEconomic Policy</em>, 34(3): 362\u2013375.\ndoi:10.1093/oxrep/gry002</li>\n<li>Goos, Maarten, Alan Manning, and Anna Salomons, 2009, \u201cJob\nPolarization in Europe\u201d, <em>American Economic Review</em>,\n99(2): 58\u201363. doi:10.1257/aer.99.2.58</li>\n<li>Graham, Sandra and Brian S. Lowery, 2004, \u201cPriming\nUnconscious Racial Stereotypes about Adolescent Offenders\u201d,\n<em>Law and Human Behavior</em>, 28(5): 483\u2013504.\ndoi:10.1023/B:LAHU.0000046430.65485.1f</li>\n<li>Gunkel, David J., 2018a, \u201cThe Other Question: Can and Should\nRobots Have Rights?\u201d, <em>Ethics and Information\nTechnology</em>, 20(2): 87\u201399.\ndoi:10.1007/s10676-017-9442-4</li>\n<li>\u2013\u2013\u2013, 2018b, <em>Robot Rights</em>, Boston, MA:\nMIT Press.</li>\n<li>Gunkel, David J. and Joanna J. Bryson (eds.), 2014, <em>Machine\nMorality: The Machine as Moral Agent and Patient</em> special issue of\n<em>Philosophy &amp; Technology</em>, 27(1): 1\u2013142.</li>\n<li>H\u00e4ggstr\u00f6m, Olle, 2016, <em>Here Be Dragons: Science,\nTechnology and the Future of Humanity</em>, Oxford: Oxford University\nPress. doi:10.1093/acprof:oso/9780198723547.001.0001</li>\n<li>Hakli, Raul and Pekka M\u00e4kel\u00e4, 2019, \u201cMoral\nResponsibility of Robots and Hybrid Agents\u201d, <em>The\nMonist</em>, 102(2): 259\u2013275. doi:10.1093/monist/onz009</li>\n<li>Hanson, Robin, 2016, <em>The Age of Em: Work, Love and Life When\nRobots Rule the Earth</em>, Oxford: Oxford University Press.</li>\n<li>Hansson, Sven Ove, 2013, <em>The Ethics of Risk: Ethical Analysis\nin an Uncertain World</em>, New York: Palgrave Macmillan.</li>\n<li>\u2013\u2013\u2013, 2018, \u201cHow to Perform an Ethical Risk\nAnalysis (eRA)\u201d, <em>Risk Analysis</em>, 38(9): 1820\u20131829.\ndoi:10.1111/risa.12978</li>\n<li>Harari, Yuval Noah, 2016, <em>Homo Deus: A Brief History of\nTomorrow</em>, New York: Harper.</li>\n<li>Haskel, Jonathan and Stian Westlake, 2017, <em>Capitalism without\nCapital: The Rise of the Intangible Economy</em>, Princeton, NJ:\nPrinceton University Press.</li>\n<li>Houkes, Wybo and Pieter E. Vermaas, 2010, <em>Technical Functions:\nOn the Use and Design of Artefacts</em>, (Philosophy of Engineering\nand Technology 1), Dordrecht: Springer Netherlands.\ndoi:10.1007/978-90-481-3900-2</li>\n<li>IEEE, 2019, <em>Ethically Aligned Design: A Vision for\nPrioritizing Human Well-Being with Autonomous and Intelligent\nSystems</em> (First Version),\n &lt;<a href=\"https://standards.ieee.org/content/dam/ieee-standards/standards/web/documents/other/ead1e.pdf\" target=\"other\">IEEE 2019 available online</a>&gt;.</li>\n<li>Jasanoff, Sheila, 2016, <em>The Ethics of Invention: Technology\nand the Human Future</em>, New York: Norton.</li>\n<li>Jecker, Nancy S., forthcoming, <em>Ending Midlife Bias: New Values\nfor Old Age</em>, New York: Oxford University Press.</li>\n<li>Jobin, Anna, Marcello Ienca, and Effy Vayena, 2019, \u201cThe\nGlobal Landscape of AI Ethics Guidelines\u201d, <em>Nature Machine\nIntelligence</em>, 1(9): 389\u2013399.\ndoi:10.1038/s42256-019-0088-2</li>\n<li>Johnson, Deborah G. and Mario Verdicchio, 2017, \u201cReframing\nAI Discourse\u201d, <em>Minds and Machines</em>, 27(4):\n575\u2013590. doi:10.1007/s11023-017-9417-6</li>\n<li>Kahnemann, Daniel, 2011, <em>Thinking Fast and Slow</em>, London:\nMacmillan.</li>\n<li>Kamm, Frances Myrna, 2016, <em>The Trolley Problem Mysteries</em>,\nEric Rakowski (ed.), Oxford: Oxford University Press.\ndoi:10.1093/acprof:oso/9780190247157.001.0001</li>\n<li>Kant, Immanuel, 1781/1787, <em>Kritik der reinen Vernunft</em>.\nTranslated as <em>Critique of Pure Reason</em>, Norman Kemp Smith\n(trans.), London: Palgrave Macmillan, 1929.</li>\n<li>Keeling, Geoff, 2020, \u201cWhy Trolley Problems Matter for the\nEthics of Automated Vehicles\u201d, <em>Science and Engineering\nEthics</em>, 26(1): 293\u2013307. doi:10.1007/s11948-019-00096-1</li>\n<li>Keynes, John Maynard, 1930, \u201cEconomic Possibilities for Our\nGrandchildren\u201d. Reprinted in his <em>Essays in Persuasion</em>,\nNew York: Harcourt Brace, 1932, 358\u2013373.</li>\n<li>Kissinger, Henry A., 2018, \u201cHow the Enlightenment Ends:\nPhilosophically, Intellectually\u2014in Every Way\u2014Human Society\nIs Unprepared for the Rise of Artificial Intelligence\u201d, <em>The\nAtlantic</em>, June 2018.\n [<a href=\"https://www.theatlantic.com/magazine/archive/2018/06/henry-kissinger-ai-could-mean-the-end-of-human-history/559124/\" target=\"other\">Kissinger 2018 available online</a>]</li>\n<li>Kurzweil, Ray, 1999, <em>The Age of Spiritual Machines: When\nComputers Exceed Human Intelligence</em>, London: Penguin.</li>\n<li>\u2013\u2013\u2013, 2005, <em>The Singularity Is Near: When\nHumans Transcend Biology</em>, London: Viking.</li>\n<li>\u2013\u2013\u2013, 2012, <em>How to Create a Mind: The Secret\nof Human Thought Revealed</em>, New York: Viking.</li>\n<li>Lee, Minha, Sander Ackermans, Nena van As, Hanwen Chang, Enzo\nLucas, and Wijnand IJsselsteijn, 2019, \u201cCaring for Vincent: A\nChatbot for Self-Compassion\u201d, in <em>Proceedings of the 2019 CHI\nConference on Human Factors in Computing Systems\u2014CHI\n\u201919</em>, Glasgow, Scotland: ACM Press, 1\u201313.\ndoi:10.1145/3290605.3300932</li>\n<li>Levy, David, 2007, <em>Love and Sex with Robots: The Evolution of\nHuman-Robot Relationships</em>, New York: Harper &amp; Co.</li>\n<li>Lighthill, James, 1973, \u201cArtificial Intelligence: A General\nSurvey\u201d, <em>Artificial intelligence: A Paper Symposion</em>,\nLondon: Science Research Council.\n [<a href=\"http://www.chilton-computing.org.uk/inf/literature/reports/lighthill_report/p001.htm\" target=\"other\">Lighthill 1973 available online</a>]</li>\n<li>Lin, Patrick, 2016, \u201cWhy Ethics Matters for Autonomous\nCars\u201d, in <em>Autonomous Driving</em>, Markus Maurer, J.\nChristian Gerdes, Barbara Lenz, and Hermann Winner (eds.), Berlin,\nHeidelberg: Springer Berlin Heidelberg, 69\u201385.\ndoi:10.1007/978-3-662-48847-8_4</li>\n<li>Lin, Patrick, Keith Abney, and Ryan Jenkins (eds.), 2017,\n<em>Robot Ethics 2.0: From Autonomous Cars to Artificial\nIntelligence</em>, New York: Oxford University Press.\ndoi:10.1093/oso/9780190652951.001.0001</li>\n<li>Lin, Patrick, George Bekey, and Keith Abney, 2008,\n\u201cAutonomous Military Robotics: Risk, Ethics, and Design\u201d,\nONR report, California Polytechnic State University, San Luis Obispo,\n20 December 2008), 112 pp.\n [<a href=\"http://ethics.calpoly.edu/ONR_report.pdf\" target=\"other\">Lin, Bekey, and Abney 2008 available online</a>]</li>\n<li>Lomas, Meghann, Robert Chevalier, Ernest Vincent Cross, Robert\nChristopher Garrett, John Hoare, and Michael Kopack, 2012,\n\u201cExplaining Robot Actions\u201d, in <em>Proceedings of the\nSeventh Annual ACM/IEEE International Conference on Human-Robot\nInteraction\u2014HRI \u201912</em>, Boston, MA: ACM Press,\n187\u2013188. doi:10.1145/2157689.2157748</li>\n<li>Macnish, Kevin, 2017, <em>The Ethics of Surveillance: An\nIntroduction</em>, London: Routledge.</li>\n<li>Mathur, Arunesh, Gunes Acar, Michael J. Friedman, Elena Lucherini,\nJonathan Mayer, Marshini Chetty, and Arvind Narayanan, 2019,\n\u201cDark Patterns at Scale: Findings from a Crawl of 11K Shopping\nWebsites\u201d, <em>Proceedings of the ACM on Human-Computer\nInteraction</em>, 3(CSCW): art. 81. doi:10.1145/3359183</li>\n<li>Minsky, Marvin, 1985, <em>The Society of Mind</em>, New York:\nSimon &amp; Schuster.</li>\n<li>Misselhorn, Catrin, 2020, \u201cArtificial Systems with Moral\nCapacities? A Research Design and Its Implementation in a Geriatric\nCare System\u201d, <em>Artificial Intelligence</em>, 278: art.\n103179. doi:10.1016/j.artint.2019.103179</li>\n<li>Mittelstadt, Brent Daniel and Luciano Floridi, 2016, \u201cThe\nEthics of Big Data: Current and Foreseeable Issues in Biomedical\nContexts\u201d, <em>Science and Engineering Ethics</em>, 22(2):\n303\u2013341. doi:10.1007/s11948-015-9652-2</li>\n<li>Moor, James H., 2006, \u201cThe Nature, Importance, and\nDifficulty of Machine Ethics\u201d, <em>IEEE Intelligent\nSystems</em>, 21(4): 18\u201321. doi:10.1109/MIS.2006.80</li>\n<li>Moravec, Hans, 1990, <em>Mind Children</em>, Cambridge, MA:\nHarvard University Press.</li>\n<li>\u2013\u2013\u2013, 1998, <em>Robot: Mere Machine to\nTranscendent Mind</em>, New York: Oxford University Press.</li>\n<li>Mozorov, Eygeny, 2013, <em>To Save Everything, Click Here: The\nFolly of Technological Solutionism</em>, New York: Public\nAffairs.</li>\n<li>M\u00fcller, Vincent C., 2012, \u201cAutonomous Cognitive Systems\nin Real-World Environments: Less Control, More Flexibility and Better\nInteraction\u201d, <em>Cognitive Computation</em>, 4(3):\n212\u2013215. doi:10.1007/s12559-012-9129-4</li>\n<li>\u2013\u2013\u2013, 2016a, \u201cAutonomous Killer Robots Are\nProbably Good News\u201d, In <em>Drones and Responsibility: Legal,\nPhilosophical and Socio-Technical Perspectives on the Use of Remotely\nControlled Weapons</em>, Ezio Di Nucci and Filippo Santoni de Sio\n(eds.), London: Ashgate, 67\u201381.</li>\n<li>\u2013\u2013\u2013 (ed.), 2016b, <em>Risks of Artificial\nIntelligence</em>, London: Chapman &amp; Hall - CRC Press.\ndoi:10.1201/b19187</li>\n<li>\u2013\u2013\u2013, 2018, \u201cIn 30 Schritten zum Mond?\nZuk\u00fcnftiger Fortschritt in der KI\u201d,\n<em>Medienkorrespondenz</em>, 20: 5\u201315.\n [<a href=\"https://philarchive.org/archive/MLLIS\" target=\"other\">M\u00fcller 2018 available online</a>]</li>\n<li>\u2013\u2013\u2013, 2020, \u201cMeasuring Progress in\nRobotics: Benchmarking and the \u2018Measure-Target\nConfusion\u2019\u201d, in <em>Metrics of Sensory Motor Coordination\nand Integration in Robots and Animals</em>, Fabio Bonsignorio, Elena\nMessina, Angel P. del Pobil, and John Hallam (eds.), (Cognitive\nSystems Monographs 36), Cham: Springer International Publishing,\n169\u2013179. doi:10.1007/978-3-030-14126-4_9</li>\n<li>\u2013\u2013\u2013, forthcoming-a, <em>Can Machines Think?\nFundamental Problems of Artificial Intelligence</em>, New York: Oxford\nUniversity Press.</li>\n<li>\u2013\u2013\u2013 (ed.), forthcoming-b, <em>Oxford Handbook of\nthe Philosophy of Artificial Intelligence</em>, New York: Oxford\nUniversity Press.</li>\n<li>M\u00fcller, Vincent C. and Nick Bostrom, 2016, \u201cFuture\nProgress in Artificial Intelligence: A Survey of Expert\nOpinion\u201d, in <em>Fundamental Issues of Artificial\nIntelligence</em>, Vincent C. M\u00fcller (ed.), Cham: Springer\nInternational Publishing, 555\u2013572.\ndoi:10.1007/978-3-319-26485-1_33</li>\n<li>Newport, Cal, 2019, <em>Digital Minimalism: On Living Better with\nLess Technology</em>, London: Penguin.</li>\n<li>N\u00f8rskov, Marco (ed.), 2017, <em>Social Robots</em>, London:\nRoutledge.</li>\n<li>Nyholm, Sven, 2018a, \u201cAttributing Agency to Automated\nSystems: Reflections on Human\u2013Robot Collaborations and\nResponsibility-Loci\u201d, <em>Science and Engineering Ethics</em>,\n24(4): 1201\u20131219. doi:10.1007/s11948-017-9943-x</li>\n<li>\u2013\u2013\u2013, 2018b, \u201cThe Ethics of Crashes with\nSelf-Driving Cars: A Roadmap, II\u201d, <em>Philosophy Compass</em>,\n13(7): e12506. doi:10.1111/phc3.12506</li>\n<li>Nyholm, Sven, and Lily Frank, 2017, \u201cFrom Sex Robots to Love\nRobots: Is Mutual Love with a Robot Possible?\u201d, in Danaher and\nMcArthur 2017: 219\u2013243.</li>\n<li>O\u2019Connell, Mark, 2017, <em>To Be a Machine: Adventures among\nCyborgs, Utopians, Hackers, and the Futurists Solving the Modest\nProblem of Death</em>, London: Granta.</li>\n<li>O\u2019Neil, Cathy, 2016, <em>Weapons of Math Destruction: How\nBig Data Increases Inequality and Threatens Democracy</em>, Largo, ML:\nCrown.</li>\n<li>Omohundro, Steve, 2014, \u201cAutonomous Technology and the\nGreater Human Good\u201d, <em>Journal of Experimental &amp;\nTheoretical Artificial Intelligence</em>, 26(3): 303\u2013315.\ndoi:10.1080/0952813X.2014.895111</li>\n<li>Ord, Toby, 2020, <em>The Precipice: Existential Risk and the\nFuture of Humanity</em>, London: Bloomsbury.</li>\n<li>Powers, Thomas M. and Jean-Gabriel Ganascia, forthcoming,\n\u201cThe Ethics of the Ethics of AI\u201d, in <em>Oxford Handbook\nof Ethics of Artificial Intelligence</em>, Markus D. Dubber, Frank\nPasquale, and Sunnit Das (eds.), New York: Oxford.</li>\n<li>Rawls, John, 1971, <em>A Theory of Justice</em>, Cambridge, MA:\nBelknap Press.</li>\n<li>Rees, Martin, 2018, <em>On the Future: Prospects for\nHumanity</em>, Princeton: Princeton University Press.</li>\n<li>Richardson, Kathleen, 2016, \u201cSex Robot Matters: Slavery, the\nProstituted, and the Rights of Machines\u201d, <em>IEEE Technology\nand Society Magazine</em>, 35(2): 46\u201353.\ndoi:10.1109/MTS.2016.2554421</li>\n<li>Roessler, Beate, 2017, \u201cPrivacy as a Human Right\u201d,\n<em>Proceedings of the Aristotelian Society</em>, 117(2):\n187\u2013206. doi:10.1093/arisoc/aox008</li>\n<li>Royakkers, Lamb\u00e8r and Rinie van Est, 2016, <em>Just\nOrdinary Robots: Automation from Love to War</em>, Boca Raton, LA: CRC\nPress, Taylor &amp; Francis. doi:10.1201/b18899</li>\n<li>Russell, Stuart, 2019, <em>Human Compatible: Artificial\nIntelligence and the Problem of Control</em>, New York: Viking.</li>\n<li>Russell, Stuart, Daniel Dewey, and Max Tegmark, 2015,\n\u201cResearch Priorities for Robust and Beneficial Artificial\nIntelligence\u201d, <em>AI Magazine</em>, 36(4): 105\u2013114.\ndoi:10.1609/aimag.v36i4.2577</li>\n<li>SAE International, 2018, \u201cTaxonomy and Definitions for Terms\nRelated to Driving Automation Systems for on-Road Motor\nVehicles\u201d, J3016_201806, 15 June 2018.\n [<a href=\"https://www.sae.org/standards/content/j3016_201806/\" target=\"other\">SAE International 2015 available online</a>]</li>\n<li>Sandberg, Anders, 2013, \u201cFeasibility of Whole Brain\nEmulation\u201d, in <em>Philosophy and Theory of Artificial\nIntelligence</em>, Vincent C. M\u00fcller (ed.), (Studies in Applied\nPhilosophy, Epistemology and Rational Ethics, 5), Berlin, Heidelberg:\nSpringer Berlin Heidelberg, 251\u2013264.\ndoi:10.1007/978-3-642-31674-6_19</li>\n<li>\u2013\u2013\u2013, 2019, \u201cThere Is Plenty of Time at the\nBottom: The Economics, Risk and Ethics of Time Compression\u201d,\n<em>Foresight</em>, 21(1): 84\u201399.\ndoi:10.1108/FS-04-2018-0044</li>\n<li>Santoni de Sio, Filippo and Jeroen van den Hoven, 2018,\n\u201cMeaningful Human Control over Autonomous Systems: A\nPhilosophical Account\u201d, <em>Frontiers in Robotics and AI</em>,\n5(February): 15. doi:10.3389/frobt.2018.00015</li>\n<li>Schneier, Bruce, 2015, <em>Data and Goliath: The Hidden Battles to\nCollect Your Data and Control Your World</em>, New York: W. W.\nNorton.</li>\n<li>Searle, John R., 1980, \u201cMinds, Brains, and Programs\u201d,\n<em>Behavioral and Brain Sciences</em>, 3(3): 417\u2013424.\ndoi:10.1017/S0140525X00005756</li>\n<li>Selbst, Andrew D., Danah Boyd, Sorelle A. Friedler, Suresh\nVenkatasubramanian, and Janet Vertesi, 2019, \u201cFairness and\nAbstraction in Sociotechnical Systems\u201d, in <em>Proceedings of\nthe Conference on Fairness, Accountability, and\nTransparency\u2014FAT* \u201919</em>, Atlanta, GA: ACM Press,\n59\u201368. doi:10.1145/3287560.3287598</li>\n<li>Sennett, Richard, 2018, <em>Building and Dwelling: Ethics for the\nCity</em>, London: Allen Lane.</li>\n<li>Shanahan, Murray, 2015, <em>The Technological Singularity</em>,\nCambridge, MA: MIT Press.</li>\n<li>Sharkey, Amanda, 2019, \u201cAutonomous Weapons Systems, Killer\nRobots and Human Dignity\u201d, <em>Ethics and Information\nTechnology</em>, 21(2): 75\u201387.\ndoi:10.1007/s10676-018-9494-0</li>\n<li>Sharkey, Amanda and Noel Sharkey, 2011, \u201cThe Rights and\nWrongs of Robot Care\u201d, in <em>Robot Ethics: The Ethical and\nSocial Implications of Robotics</em>, Patrick Lin, Keith Abney and\nGeorge Bekey (eds.), Cambridge, MA: MIT Press, 267\u2013282.</li>\n<li>Shoham, Yoav, Perrault Raymond, Brynjolfsson Erik, Jack Clark,\nJames Manyika, Juan Carlos Niebles, \u2026 Zoe Bauer, 2018,\n\u201cThe AI Index 2018 Annual Report\u201d, 17 December 2018,\nStanford, CA: AI Index Steering Committee, Human-Centered AI\nInitiative, Stanford University.\n [<a href=\"https://hai.stanford.edu/ai-index/previous-reports/2018\" target=\"other\">Shoam et al. 2018 available online</a>]</li>\n<!--  <li>SIENNA,\n2019, &ldquo;Deliverable Report D4.4: Ethical Issues in\nArtificial Intelligence and Robotics&rdquo;, June 2019, published by the\nSIENNA project (Stakeholder-informed ethics for new technologies with\nhigh socio-economic and human rights impact), University of Twente,\npp. 1&ndash;103.\n [<a href=\"https://www.sienna-project.eu/publications/\" target=\"other\">SIENNA 2019 available online</a>]</li>\n -->\n<li>Silver, David, Thomas Hubert, Julian Schrittwieser, Ioannis\nAntonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre,\nDharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan,\nand Demis Hassabis, 2018, \u201cA General Reinforcement Learning\nAlgorithm That Masters Chess, Shogi, and Go through Self-Play\u201d,\n<em>Science</em>, 362(6419): 1140\u20131144.\ndoi:10.1126/science.aar6404</li>\n<li>Simon, Herbert A. and Allen Newell, 1958, \u201cHeuristic Problem\nSolving: The Next Advance in Operations Research\u201d,\n<em>Operations Research</em>, 6(1): 1\u201310.\ndoi:10.1287/opre.6.1.1</li>\n<li>Simpson, Thomas W. and Vincent C. M\u00fcller, 2016, \u201cJust\nWar and Robots\u2019 Killings\u201d, <em>The Philosophical\nQuarterly</em>, 66(263): 302\u2013322. doi:10.1093/pq/pqv075</li>\n<li>Smolan, Sandy (director), 2016, \u201cThe Human Face of Big\nData\u201d, <em>PBS Documentary,</em> 24 February 2016, 56 mins.</li>\n<li>Sparrow, Robert, 2007, \u201cKiller Robots\u201d, <em>Journal of\nApplied Philosophy</em>, 24(1): 62\u201377.\ndoi:10.1111/j.1468-5930.2007.00346.x</li>\n<li>\u2013\u2013\u2013, 2016, \u201cRobots in Aged Care: A\nDystopian Future?\u201d, <em>AI &amp; Society</em>, 31(4):\n445\u2013454. doi:10.1007/s00146-015-0625-4</li>\n<li>Stahl, Bernd Carsten, Job Timmermans, and Brent Daniel\nMittelstadt, 2016, \u201cThe Ethics of Computing: A Survey of the\nComputing-Oriented Literature\u201d, <em>ACM Computing Surveys</em>,\n48(4): art. 55. doi:10.1145/2871196</li>\n<li>Stahl, Bernd Carsten and David Wright, 2018, \u201cEthics and\nPrivacy in AI and Big Data: Implementing Responsible Research and\nInnovation\u201d, <em>IEEE Security Privacy</em>, 16(3):\n26\u201333.</li>\n<li>Stone, Christopher D., 1972, \u201cShould Trees Have Standing -\ntoward Legal Rights for Natural Objects\u201d, <em>Southern\nCalifornia Law Review</em>, 45: 450\u2013501.</li>\n<li>Stone, Peter, Rodney Brooks, Erik Brynjolfsson, Ryan Calo, Oren\nEtzioni, Greg Hager, Julia Hirschberg, Shivaram Kalyanakrishnan, Ece\nKamar, Sarit Kraus, Kevin Leyton-Brown, David Parkes, William Press,\nAnnaLee Saxenian, Julie Shah, Milind Tambe, and Astro Teller, 2016,\n\u201cArtificial Intelligence and Life in 2030\u201d, One Hundred\nYear Study on Artificial Intelligence: Report of the 2015\u20132016\nStudy Panel, Stanford University, Stanford, CA, September 2016.\n [<a href=\"https://ai100.stanford.edu/2016-report\" target=\"other\">Stone et al. 2016 available online</a>]</li>\n<li>Strawson, Galen, 1998, \u201cFree Will\u201d, in <em>Routledge\nEncyclopedia of Philosophy</em>, Taylor &amp; Francis.\ndoi:10.4324/9780415249126-V014-1</li>\n<li>Sullins, John P., 2012, \u201cRobots, Love, and Sex: The Ethics\nof Building a Love Machine\u201d, <em>IEEE Transactions on Affective\nComputing</em>, 3(4): 398\u2013409. doi:10.1109/T-AFFC.2012.31</li>\n<li>Susser, Daniel, Beate Roessler, and Helen Nissenbaum, 2019,\n\u201cTechnology, Autonomy, and Manipulation\u201d, <em>Internet\nPolicy Review</em>, 8(2): 30 June 2019.\n [<a href=\"https://policyreview.info/articles/analysis/technology-autonomy-and-manipulation\" target=\"other\">Susser, Roessler, and Nissenbaum 2019 available online</a>]</li>\n<li>Taddeo, Mariarosaria and Luciano Floridi, 2018, \u201cHow AI Can\nBe a Force for Good\u201d, <em>Science</em>, 361(6404):\n751\u2013752. doi:10.1126/science.aat5991</li>\n<li>Taylor, Linnet and Nadezhda Purtova, 2019, \u201cWhat Is\nResponsible and Sustainable Data Science?\u201d, Big Data &amp;\nSociety, 6(2): art. 205395171985811. doi:10.1177/2053951719858114</li>\n<li>Taylor, Steve, et al., 2018, \u201cResponsible AI \u2013 Key\nThemes, Concerns &amp; Recommendations for European Research and\nInnovation: Summary of Consultation with Multidisciplinary\nExperts\u201d, June.  doi:10.5281/zenodo.1303252\n [<a href=\"https://zenodo.org/record/1303253\" target=\"other\">Taylor, et al. 2018 available online</a>]</li>\n<li>Tegmark, Max, 2017, <em>Life 3.0: Being Human in the Age of\nArtificial Intelligence</em>, New York: Knopf.</li>\n<li>Thaler, Richard H and Sunstein, Cass, 2008, <em>Nudge: Improving\ndecisions about health, wealth and happiness</em>, New York:\nPenguin.</li>\n<li>Thompson, Nicholas and Ian Bremmer, 2018, \u201cThe AI Cold War\nThat Threatens Us All\u201d, <em>Wired</em>, 23 November 2018.\n [<a href=\"https://www.wired.com/story/ai-cold-war-china-could-doom-us-all/\" target=\"other\">Thompson and Bremmer 2018 available online</a>]</li>\n<li>Thomson, Judith Jarvis, 1976, \u201cKilling, Letting Die, and the\nTrolley Problem\u201d, <em>Monist</em>, 59(2): 204\u2013217.\ndoi:10.5840/monist197659224</li>\n<li>Torrance, Steve, 2011, \u201cMachine Ethics and the Idea of a\nMore-Than-Human Moral World\u201d, in Anderson and Anderson 2011:\n115\u2013137. doi:10.1017/CBO9780511978036.011</li>\n<li>Trump, Donald J, 2019, \u201cExecutive Order on Maintaining\nAmerican Leadership in Artificial Intelligence\u201d, 11 February\n2019.\n [<a href=\"https://www.federalregister.gov/documents/2019/02/14/2019-02544/maintaining-american-leadership-in-artificial-intelligence\" target=\"other\">Trump 2019 available online</a>]</li>\n<li>Turner, Jacob, 2019, <em>Robot Rules: Regulating Artificial\nIntelligence</em>, Berlin: Springer.\ndoi:10.1007/978-3-319-96235-1</li>\n<li>Tzafestas, Spyros G., 2016, <em>Roboethics: A Navigating\nOverview</em>, (Intelligent Systems, Control and Automation: Science\nand Engineering 79), Cham: Springer International Publishing.\ndoi:10.1007/978-3-319-21714-7</li>\n<li>Vallor, Shannon, 2017, <em>Technology and the Virtues: A\nPhilosophical Guide to a Future Worth Wanting</em>, Oxford: Oxford\nUniversity Press. doi:10.1093/acprof:oso/9780190498511.001.0001</li>\n<li>Van Lent, Michael, William Fisher, and Michael Mancuso, 2004,\n\u201cAn Explainable Artificial Intelligence System for Small-Unit\nTactical Behavior\u201d, in <em>Proceedings of the 16th Conference on\nInnovative Applications of Artifical Intelligence,\n(IAAI\u201904)</em>, San Jose, CA: AAAI Press, 900\u2013907.</li>\n<li>van Wynsberghe, Aimee, 2016, <em>Healthcare Robots: Ethics, Design\nand Implementation</em>, London: Routledge.\ndoi:10.4324/9781315586397</li>\n<li>van Wynsberghe, Aimee and Scott Robbins, 2019, \u201cCritiquing\nthe Reasons for Making Artificial Moral Agents\u201d, <em>Science and\nEngineering Ethics</em>, 25(3): 719\u2013735.\ndoi:10.1007/s11948-018-0030-8</li>\n<li>Vanderelst, Dieter and Alan Winfield, 2018, \u201cThe Dark Side\nof Ethical Robots\u201d, in <em>Proceedings of the 2018 AAAI/ACM\nConference on AI, Ethics, and Society</em>, New Orleans, LA: ACM,\n317\u2013322. doi:10.1145/3278721.3278726</li>\n<li>Veale, Michael and Reuben Binns, 2017, \u201cFairer Machine\nLearning in the Real World: Mitigating Discrimination without\nCollecting Sensitive Data\u201d, <em>Big Data &amp; Society</em>,\n4(2): art. 205395171774353. doi:10.1177/2053951717743530</li>\n<li>V\u00e9liz, Carissa, 2019, \u201cThree Things Digital Ethics\nCan Learn from Medical Ethics\u201d, <em>Nature Electronics</em>,\n2(8): 316\u2013318. doi:10.1038/s41928-019-0294-2</li>\n<li>Verbeek, Peter-Paul, 2011, <em>Moralizing Technology:\nUnderstanding and Designing the Morality of Things</em>, Chicago:\nUniversity of Chicago Press.</li>\n<li>Wachter, Sandra and Brent Daniel Mittelstadt, 2019, \u201cA Right\nto Reasonable Inferences: Re-Thinking Data Protection Law in the Age\nof Big Data and AI\u201d, <em>Columbia Business Law Review</em>,\n2019(2): 494\u2013620.</li>\n<li>Wachter, Sandra, Brent Mittelstadt, and Luciano Floridi, 2017,\n\u201cWhy a Right to Explanation of Automated Decision-Making Does\nNot Exist in the General Data Protection Regulation\u201d,\n<em>International Data Privacy Law</em>, 7(2): 76\u201399.\ndoi:10.1093/idpl/ipx005</li>\n<li>Wachter, Sandra, Brent Mittelstadt, and Chris Russell, 2018,\n\u201cCounterfactual Explanations Without Opening the Black Box:\nAutomated Decisions and the GDPR\u201d, <em>Harvard Journal of Law\n&amp; Technology</em>, 31(2): 842\u2013887.\ndoi:10.2139/ssrn.3063289</li>\n<li>Wallach, Wendell and Peter M. Asaro (eds.), 2017, <em>Machine\nEthics and Robot Ethics</em>, London: Routledge.</li>\n<li>Walsh, Toby, 2018, <em>Machines That Think: The Future of\nArtificial Intelligence</em>, Amherst, MA: Prometheus Books.</li>\n<li>Westlake, Stian (ed.), 2014, <em>Our Work Here Is Done: Visions of\na Robot Economy</em>, London: Nesta.\n [<a href=\"https://www.nesta.org.uk/report/our-work-here-is-done-visions-of-a-robot-economy/\" target=\"other\">Westlake 2014 available online</a>]</li>\n<li>Whittaker, Meredith, Kate Crawford, Roel Dobbe, Genevieve Fried,\nElizabeth Kaziunas, Varoon Mathur, \u2026 Jason Schultz, 2018,\n\u201cAI Now Report 2018\u201d, New York: AI Now Institute, New York\nUniversity.\n [<a href=\"https://ainowinstitute.org/publication/ai-now-2018-report-2\" target=\"other\">Whittaker et al. 2018 available online</a>]</li>\n<li>Whittlestone, Jess, Rune Nyrup, Anna Alexandrova, Kanta Dihal, and\nStephen Cave, 2019, \u201cEthical and Societal Implications of\nAlgorithms, Data, and Artificial Intelligence: A Roadmap for\nResearch\u201d, Cambridge: Nuffield Foundation, University of\nCambridge.\n [<a href=\"https://www.adalovelaceinstitute.org/nuffield-foundation-publishes-roadmap-for-ai-ethics-research/\" target=\"other\">Whittlestone 2019 available online</a>]</li>\n<li>Winfield, Alan, Katina Michael, Jeremy Pitt, and Vanessa Evers\n(eds.), 2019, <em>Machine Ethics: The Design and Governance of Ethical\nAI and Autonomous Systems</em>, special issue of <em>Proceedings of\nthe IEEE</em>, 107(3): 501\u2013632.</li>\n<li>Woollard, Fiona and Frances Howard-Snyder, 2016, \u201cDoing vs.\nAllowing Harm\u201d, <em>Stanford Encyclopedia of Philosophy</em>\n(Winter 2016 edition), Edward N. Zalta (ed.), URL =\n &lt;<a href=\"https://plato.stanford.edu/archives/win2016/entries/doing-allowing/\" target=\"other\">https://plato.stanford.edu/archives/win2016/entries/doing-allowing/</a>&gt;</li>\n<li>Woolley, Samuel C. and Philip N. Howard (eds.), 2017,\n<em>Computational Propaganda: Political Parties, Politicians, and\nPolitical Manipulation on Social Media</em>, Oxford: Oxford University\nPress. doi:10.1093/oso/9780190931407.001.0001</li>\n<li>Yampolskiy, Roman V. (ed.), 2018, <em>Artificial Intelligence\nSafety and Security</em>, Boca Raton, FL: Chapman and Hall/CRC.\ndoi:10.1201/9781351251389</li>\n<li>Yeung, Karen and Martin Lodge (eds.), 2019, <em>Algorithmic\nRegulation</em>, Oxford: Oxford University Press.\ndoi:10.1093/oso/9780198838494.001.0001</li>\n<li>Zayed, Yago and Philip Loft, 2019, \u201cAgriculture: Historical\nStatistics\u201d, <em>House of Commons Briefing Paper</em>, 3339(25\nJune 2019): 1-19.\n [<a href=\"https://commonslibrary.parliament.uk/research-briefings/sn03339/\" target=\"other\">Zayed and Loft 2019 available online</a>]</li>\n<li>Zerilli, John, Alistair Knott, James Maclaurin, and Colin\nGavaghan, 2019, \u201cTransparency in Algorithmic and Human\nDecision-Making: Is There a Double Standard?\u201d, <em>Philosophy\n&amp; Technology</em>, 32(4): 661\u2013683.\ndoi:10.1007/s13347-018-0330-6</li>\n<li>Zuboff, Shoshana, 2019, <em>The Age of Surveillance Capitalism:\nThe Fight for a Human Future at the New Frontier of Power</em>, New\nYork: Public Affairs.</li>\n</ul>\n</div>"}, "related_entries": {"entry_list": ["computing: and moral responsibility", "ethics: internet research", "ethics: search engines and", "information technology: and moral values", "information technology: and privacy", "manipulation, ethics of", "social networking and ethics"], "entry_link": [{"../computing-responsibility/": "computing: and moral responsibility"}, {"../ethics-internet-research/": "ethics: internet research"}, {"../ethics-search/": "ethics: search engines and"}, {"../it-moral-values/": "information technology: and moral values"}, {"../it-privacy/": "information technology: and privacy"}, {"../ethics-manipulation/": "manipulation, ethics of"}, {"../ethics-social-networking/": "social networking and ethics"}]}, "academic_tools": {"listed_text": ["<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>", "<a href=\"https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=ethics-ai\" target=\"other\">How to cite this entry</a>.", "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>", "<a href=\"https://leibniz.stanford.edu/friends/preview/ethics-ai/\" target=\"other\">Preview the PDF version of this entry</a> at the\n <a href=\"https://leibniz.stanford.edu/friends/\" target=\"other\">Friends of the SEP Society</a>.", "<img alt=\"inpho icon\" src=\"../../symbols/inpho.png\"/>", "<a href=\"https://www.inphoproject.org/entity?sep=ethics-ai&amp;redirect=True\" target=\"other\">Look up topics and thinkers related to this entry</a>\n at the Internet Philosophy Ontology Project (InPhO).", "<img alt=\"phil papers icon\" src=\"../../symbols/pp.gif\"/>", "<a href=\"https://philpapers.org/sep/ethics-ai/\" target=\"other\">Enhanced bibliography for this entry</a>\nat <a href=\"https://philpapers.org/\" target=\"other\">PhilPapers</a>, with links to its database."], "listed_links": [{"https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=ethics-ai": "How to cite this entry"}, {"https://leibniz.stanford.edu/friends/preview/ethics-ai/": "Preview the PDF version of this entry"}, {"https://leibniz.stanford.edu/friends/": "Friends of the SEP Society"}, {"https://www.inphoproject.org/entity?sep=ethics-ai&redirect=True": "Look up topics and thinkers related to this entry"}, {"https://philpapers.org/sep/ethics-ai/": "Enhanced bibliography for this entry"}, {"https://philpapers.org/": "PhilPapers"}]}, "other_internet_resources": {"listed_text": ["AI HLEG, 2019,\n \u201c<a href=\"https://ec.europa.eu/digital-single-market/en/high-level-expert-group-artificial-intelligence\" target=\"other\">High-Level Expert Group on Artificial Intelligence: Ethics Guidelines for Trustworthy AI</a>\u201d,\n <em>European Commission</em>, accessed: 9 April 2019.", "Amodei, Dario and Danny Hernandez, 2018,\n \u201c<a href=\"https://openai.com/blog/ai-and-compute/\" target=\"other\">AI and Compute</a>\u201d,\n <em>OpenAI Blog</em>, 16 July 2018.", "Aneesh, A., 2002,\n <a href=\"https://web.archive.org/web/20021231052640/http://www.ifz.tu-graz.ac.at/sumacad/02/aaneesh.pdf\" target=\"other\">Technological Modes of Governance: Beyond Private and Public Realms</a>,\n paper in the Proceedings of the 4th International Summer Academy on\nTechnology Studies, available at archive.org.", "Brooks, Rodney, 2017,\n \u201c<a href=\"https://rodneybrooks.com/the-seven-deadly-sins-of-predicting-the-future-of-ai/\" target=\"other\">The Seven Deadly Sins of Predicting the Future of AI</a>\u201d,\n on <em>Rodney Brooks: Robots, AI, and Other Stuff</em>, 7 September\n2017.", "Brundage, Miles, Shahar Avin, Jack Clark, Helen Toner, Peter\nEckersley, Ben Garfinkel, Allan Dafoe, Paul Scharre, Thomas Zeitzoff,\nBobby Filar, Hyrum Anderson, Heather Roff, Gregory C. Allen, Jacob\nSteinhardt, Carrick Flynn, Se\u00e1n \u00d3 h\u00c9igeartaigh,\nSimon Beard, Haydn Belfield, Sebastian Farquhar, Clare Lyle, et al.,\n2018,\n \u201c<a href=\"https://arxiv.org/abs/1802.07228\" target=\"other\">The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation</a>\u201d,\n unpublished manuscript, ArXiv:1802.07228 [Cs].", "Costa, Elisabeth and David Halpern, 2019,\n \u201c<a href=\"https://www.bi.team/publications/the-behavioural-science-of-online-harm-and-manipulation-and-what-to-do-about-it/\" target=\"other\">The Behavioural Science of Online Harm and Manipulation, and What to Do About It: An Exploratory Paper to Spark Ideas and Debate</a>\u201d,\n The Behavioural Insights Team Report, 1-82.", "Gebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer\nWortman Vaughan, Hanna Wallach, Hal Daume\u00e9 III, and Kate\nCrawford, 2018,\n \u201c<a href=\"https://arxiv.org/abs/1803.09010\" target=\"other\">Datasheets for Datasets</a>\u201d,\n unpublished manuscript, arxiv:1803.09010, 23 March 2018.", "Gunning, David, 2017,\n \u201c<a href=\"https://www.darpa.mil/attachments/XAIProgramUpdate.pdf\" target=\"other\">Explainable Artificial Intelligence (XAI)</a>\u201d,\n Defense Advanced Research Projects Agency (DARPA) Program.", "Harris, Tristan, 2016,\n \u201c<a href=\"https://medium.com/thrive-global/how-technology-hijacks-peoples-minds-from-a-magician-and-google-s-design-ethicist-56d62ef5edf3\" target=\"other\">How Technology Is Hijacking Your Mind\u2014from a Magician and Google Design Ethicist</a>\u201d,\n <em>Thrive Global</em>, 18 May 2016.", "International Federation of Robotics (IFR), 2019,\n <em><a href=\"https://ifr.org/free-downloads/\" target=\"other\">World Robotics 2019 Edition</a></em>.", "Jacobs, An, Lynn Tytgat, Michel Maus, Romain Meeusen, and Bram\nVanderborght (eds.), Homo Roboticus: 30 Questions and Answers on Man,\nTechnology, Science &amp; Art, 2019,\n <a href=\"https://books.google.gr/books?id=Qzo0xQEACAAJ\">Brussels: ASP</a>.", "Marcus, Gary, 2018,\n \u201c<a href=\"https://arxiv.org/abs/1801.00631\" target=\"other\">Deep Learning: A Critical Appraisal</a>\u201d,\n unpublished manuscript, 2 January 2018, arxiv:1801.00631.", "McCarthy, John, Marvin Minsky, Nathaniel Rochester, and Claude E.\nShannon, 1955,\n \u201c<a href=\"http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html\" target=\"other\">A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence</a>\u201d,\n 31 August 1955.", "Metcalf, Jacob, Emily F. Keller, and Danah Boyd, 2016,\n \u201c<a href=\"https://bdes.datasociety.net/council-output/perspectives-on-big-data-ethics-and-society/\" target=\"other\">Perspectives on Big Data, Ethics, and Society</a>\u201d,\n 23 May 2016, Council for Big Data, Ethics, and Society.", "National Institute of Justice (NIJ), 2014,\n \u201c<a href=\"https://nij.ojp.gov/topics/articles/overview-predictive-policing\" target=\"other\">Overview of Predictive Policing</a>\u201d,\n 9 June 2014.", "Searle, John R., 2015,\n \u201c<a href=\"https://www.youtube.com/watch?v=rHKwIYsPXLg\" target=\"other\">Consciousness in Artificial Intelligence</a>\u201d,\n Google\u2019s Singularity Network, Talks at Google (YouTube\nvideo).", "<a href=\"https://www.turing.ac.uk/research/interest-groups/data-ethics-group\" target=\"other\">Turing Institute (UK): Data Ethics Group</a>", "<a href=\"https://ainowinstitute.org/\" target=\"other\">AI Now</a>", "<a href=\"http://lcfi.ac.uk/\" target=\"other\">Leverhulme Centre for the Future of Intelligence</a>", "<a href=\"https://www.fhi.ox.ac.uk/\" target=\"other\">Future of Humanity Institute</a>", "<a href=\"https://futureoflife.org/\" target=\"other\">Future of Life Institute</a>", "<a href=\"http://cyberlaw.stanford.edu/\" target=\"other\">Stanford Center for Internet and Society</a>", "<a href=\"https://cyber.harvard.edu\" target=\"other\">Berkman Klein Center</a>", "<a href=\"http://www.openroboethics.org/\" target=\"other\">Open Roboethics Institute</a>", "<a href=\"https://www.pt-ai.org/\" target=\"other\">Philosophy &amp; Theory of AI</a>", "<a href=\"https://philevents.org/event/show/35634\" target=\"other\">Ethics and AI 2017</a>", "<a href=\"http://www.aies-conference.com/\" target=\"other\">AIES</a>", "<a href=\"https://conferences.law.stanford.edu/werobot/\" target=\"other\">We Robot 2018</a>", "<a href=\"http://conferences.au.dk/robo-philosophy/\" target=\"other\">Robophilosophy</a>", "<a href=\"http://www.pt-ai.org/TG-ELS/policy\" target=\"other\">EUrobotics TG \u2018robot ethics\u2019 collection of policy documents</a>", "<a href=\"https://philpapers.org/browse/ethics-of-artificial-intelligence\" target=\"other\">PhilPapers section on Ethics of Artificial Intelligence</a>", "<a href=\"https://philpapers.org/browse/robot-ethics\" target=\"other\">PhilPapers section on Robot Ethics</a>"], "listed_links": [{"https://ec.europa.eu/digital-single-market/en/high-level-expert-group-artificial-intelligence": "High-Level Expert Group on Artificial Intelligence: Ethics Guidelines for Trustworthy AI"}, {"https://openai.com/blog/ai-and-compute/": "AI and Compute"}, {"https://web.archive.org/web/20021231052640/http://www.ifz.tu-graz.ac.at/sumacad/02/aaneesh.pdf": "Technological Modes of Governance: Beyond Private and Public Realms"}, {"https://rodneybrooks.com/the-seven-deadly-sins-of-predicting-the-future-of-ai/": "The Seven Deadly Sins of Predicting the Future of AI"}, {"https://arxiv.org/abs/1802.07228": "The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation"}, {"https://www.bi.team/publications/the-behavioural-science-of-online-harm-and-manipulation-and-what-to-do-about-it/": "The Behavioural Science of Online Harm and Manipulation, and What to Do About It: An Exploratory Paper to Spark Ideas and Debate"}, {"https://arxiv.org/abs/1803.09010": "Datasheets for Datasets"}, {"https://www.darpa.mil/attachments/XAIProgramUpdate.pdf": "Explainable Artificial Intelligence (XAI)"}, {"https://medium.com/thrive-global/how-technology-hijacks-peoples-minds-from-a-magician-and-google-s-design-ethicist-56d62ef5edf3": "How Technology Is Hijacking Your Mind\u2014from a Magician and Google Design Ethicist"}, {"https://ifr.org/free-downloads/": "World Robotics 2019 Edition"}, {"https://books.google.gr/books?id=Qzo0xQEACAAJ": "Brussels: ASP"}, {"https://arxiv.org/abs/1801.00631": "Deep Learning: A Critical Appraisal"}, {"http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html": "A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence"}, {"https://bdes.datasociety.net/council-output/perspectives-on-big-data-ethics-and-society/": "Perspectives on Big Data, Ethics, and Society"}, {"https://nij.ojp.gov/topics/articles/overview-predictive-policing": "Overview of Predictive Policing"}, {"https://www.youtube.com/watch?v=rHKwIYsPXLg": "Consciousness in Artificial Intelligence"}, {"https://www.turing.ac.uk/research/interest-groups/data-ethics-group": "Turing Institute (UK): Data Ethics Group"}, {"https://ainowinstitute.org/": "AI Now"}, {"http://lcfi.ac.uk/": "Leverhulme Centre for the Future of Intelligence"}, {"https://www.fhi.ox.ac.uk/": "Future of Humanity Institute"}, {"https://futureoflife.org/": "Future of Life Institute"}, {"http://cyberlaw.stanford.edu/": "Stanford Center for Internet and Society"}, {"https://cyber.harvard.edu": "Berkman Klein Center"}, {"http://www.openroboethics.org/": "Open Roboethics Institute"}, {"https://www.pt-ai.org/": "Philosophy & Theory of AI"}, {"https://philevents.org/event/show/35634": "Ethics and AI 2017"}, {"http://www.aies-conference.com/": "AIES"}, {"https://conferences.law.stanford.edu/werobot/": "We Robot 2018"}, {"http://conferences.au.dk/robo-philosophy/": "Robophilosophy"}, {"http://www.pt-ai.org/TG-ELS/policy": "EUrobotics TG \u2018robot ethics\u2019 collection of policy documents"}, {"https://philpapers.org/browse/ethics-of-artificial-intelligence": "PhilPapers section on Ethics of Artificial Intelligence"}, {"https://philpapers.org/browse/robot-ethics": "PhilPapers section on Robot Ethics"}]}, "tokenized_text": ["1", "introduction", "11", "background", "field", "ethic", "ai", "robotics", "often", "focused", "concern", "various", "sort", "typical", "response", "new", "technology", "many", "concern", "turn", "rather", "quaint", "train", "fast", "soul", "predictably", "wrong", "suggest", "technology", "fundamentally", "change", "human", "telephone", "destroy", "personal", "communication", "writing", "destroy", "memory", "video", "cassette", "make", "going", "redundant", "broadly", "correct", "moderately", "relevant", "digital", "technology", "destroy", "industry", "make", "photographic", "film", "cassette", "tape", "vinyl", "record", "broadly", "correct", "deeply", "relevant", "car", "kill", "child", "fundamentally", "change", "landscape", "task", "article", "analyse", "issue", "deflate", "nonissue", "technology", "like", "nuclear", "power", "car", "plastic", "caused", "ethical", "political", "discussion", "significant", "policy", "effort", "control", "trajectory", "technology", "usually", "damage", "done", "addition", "ethical", "concern", "new", "technology", "challenge", "current", "norm", "conceptual", "system", "particular", "interest", "philosophy", "finally", "understood", "technology", "context", "need", "shape", "societal", "response", "including", "regulation", "law", "feature", "also", "exist", "case", "new", "ai", "robotics", "technologiesplus", "fundamental", "fear", "may", "end", "era", "human", "control", "earth", "ethic", "ai", "robotics", "seen", "significant", "press", "coverage", "recent", "year", "support", "related", "research", "also", "may", "end", "undermining", "press", "often", "talk", "issue", "discussion", "prediction", "future", "technology", "bring", "though", "already", "know", "would", "ethical", "achieve", "press", "coverage", "thus", "focus", "risk", "security", "brundage", "et", "al", "2018", "internet", "resource", "section", "hereafter", "oir", "prediction", "impact", "eg", "job", "market", "result", "discussion", "essentially", "technical", "problem", "focus", "achieve", "desired", "outcome", "current", "discussion", "policy", "industry", "also", "motivated", "image", "public", "relation", "label", "ethical", "really", "much", "new", "green", "perhaps", "used", "ethic", "washing", "problem", "qualify", "problem", "ai", "ethic", "would", "require", "readily", "know", "right", "thing", "sense", "job", "loss", "theft", "killing", "ai", "problem", "ethic", "whether", "permissible", "certain", "circumstance", "problem", "article", "focus", "genuine", "problem", "ethic", "readily", "know", "answer", "last", "caveat", "ethic", "ai", "robotics", "young", "field", "within", "applied", "ethic", "significant", "dynamic", "wellestablished", "issue", "authoritative", "overviewsthough", "promising", "outline", "european", "group", "ethic", "science", "new", "technology", "2018", "beginning", "societal", "impact", "floridi", "et", "al", "2018", "taddeo", "floridi", "2018", "s", "taylor", "et", "al", "2018", "walsh", "2018", "bryson", "2019", "gibert", "2019", "whittlestone", "et", "al", "2019", "policy", "recommendation", "ai", "hleg", "2019", "oir", "ieee", "2019", "article", "merely", "reproduce", "community", "achieved", "thus", "far", "must", "propose", "ordering", "little", "order", "exists", "12", "ai", "robotics", "notion", "artificial", "intelligence", "ai", "understood", "broadly", "kind", "artificial", "computational", "system", "show", "intelligent", "behaviour", "ie", "complex", "behaviour", "conducive", "reaching", "goal", "particular", "wish", "restrict", "intelligence", "would", "require", "intelligence", "done", "human", "minsky", "suggested", "1985", "mean", "incorporate", "range", "machine", "including", "technical", "ai", "show", "limited", "ability", "learning", "reasoning", "excel", "automation", "particular", "task", "well", "machine", "general", "ai", "aim", "create", "generally", "intelligent", "agent", "ai", "somehow", "get", "closer", "skin", "technologiesthus", "field", "philosophy", "ai", "perhaps", "project", "ai", "create", "machine", "feature", "central", "human", "see", "namely", "feeling", "thinking", "intelligent", "being", "main", "purpose", "artificially", "intelligent", "agent", "probably", "involve", "sensing", "modelling", "planning", "action", "current", "ai", "application", "also", "include", "perception", "text", "analysis", "natural", "language", "processing", "nlp", "logical", "reasoning", "gameplaying", "decision", "support", "system", "data", "analytics", "predictive", "analytics", "well", "autonomous", "vehicle", "form", "robotics", "p", "stone", "et", "al", "2016", "ai", "may", "involve", "number", "computational", "technique", "achieve", "aim", "classical", "symbolmanipulating", "ai", "inspired", "natural", "cognition", "machine", "learning", "via", "neural", "network", "goodfellow", "bengio", "courville", "2016", "silver", "et", "al", "2018", "historically", "worth", "noting", "term", "ai", "used", "ca", "19501975", "came", "disrepute", "ai", "winter", "ca", "19751995", "narrowed", "result", "area", "machine", "learning", "natural", "language", "processing", "data", "science", "often", "labelled", "ai", "since", "ca", "2010", "use", "broadened", "time", "almost", "computer", "science", "even", "hightech", "lumped", "ai", "name", "proud", "booming", "industry", "massive", "capital", "investment", "shoham", "et", "al", "2018", "edge", "hype", "erik", "brynjolfsson", "noted", "may", "allow", "u", "virtually", "eliminate", "global", "poverty", "massively", "reduce", "disease", "provide", "better", "education", "almost", "everyone", "planet", "quoted", "anderson", "rainie", "luchsinger", "2018", "ai", "entirely", "software", "robot", "physical", "machine", "move", "robot", "subject", "physical", "impact", "typically", "sensor", "exert", "physical", "force", "onto", "world", "typically", "actuator", "like", "gripper", "turning", "wheel", "accordingly", "autonomous", "car", "plane", "robot", "minuscule", "portion", "robot", "humanoid", "humanshaped", "like", "movie", "robot", "use", "ai", "typical", "industrial", "robot", "blindly", "follow", "completely", "defined", "script", "minimal", "sensory", "input", "learning", "reasoning", "around", "500000", "new", "industrial", "robot", "installed", "year", "ifr", "2019", "oir", "probably", "fair", "say", "robotics", "system", "cause", "concern", "general", "public", "ai", "system", "likely", "greater", "impact", "humanity", "also", "ai", "robotics", "system", "narrow", "set", "task", "le", "likely", "cause", "new", "issue", "system", "flexible", "autonomous", "robotics", "ai", "thus", "seen", "covering", "two", "overlapping", "set", "system", "system", "ai", "system", "robotics", "system", "interested", "three", "scope", "article", "thus", "intersection", "union", "set", "13", "note", "policy", "policy", "one", "concern", "article", "significant", "public", "discussion", "ai", "ethic", "frequent", "pronouncement", "politician", "matter", "requires", "new", "policy", "easier", "said", "done", "actual", "technology", "policy", "difficult", "plan", "enforce", "take", "many", "form", "incentive", "funding", "infrastructure", "taxation", "goodwill", "statement", "regulation", "various", "actor", "law", "policy", "ai", "possibly", "come", "conflict", "aim", "technology", "policy", "general", "policy", "government", "parliament", "association", "industry", "circle", "industrialised", "country", "produced", "report", "white", "paper", "recent", "year", "generated", "goodwill", "slogan", "trustedresponsiblehumanehumancentredgoodbeneficial", "ai", "needed", "survey", "see", "jobin", "ienca", "vayena", "2019", "v", "m\u00fcller", "list", "ptai", "policy", "document", "institution", "people", "work", "ethic", "policy", "might", "tendency", "overestimate", "impact", "threat", "new", "technology", "underestimate", "far", "current", "regulation", "reach", "eg", "product", "liability", "hand", "tendency", "business", "military", "public", "administration", "talk", "ethic", "washing", "order", "preserve", "good", "public", "image", "continue", "actually", "implementing", "legally", "binding", "regulation", "would", "challenge", "existing", "business", "model", "practice", "actual", "policy", "implementation", "ethical", "theory", "subject", "societal", "power", "structuresand", "agent", "power", "push", "anything", "restricts", "thus", "significant", "risk", "regulation", "remain", "toothless", "face", "economical", "political", "power", "though", "little", "actual", "policy", "produced", "notable", "beginning", "latest", "eu", "policy", "document", "suggests", "trustworthy", "ai", "lawful", "ethical", "technically", "robust", "spell", "seven", "requirement", "human", "oversight", "technical", "robustness", "privacy", "data", "governance", "transparency", "fairness", "wellbeing", "accountability", "ai", "hleg", "2019", "oir", "much", "european", "research", "run", "slogan", "responsible", "research", "innovation", "rri", "technology", "assessment", "standard", "field", "since", "advent", "nuclear", "power", "professional", "ethic", "also", "standard", "field", "information", "technology", "includes", "issue", "relevant", "article", "perhaps", "code", "ethic", "ai", "engineer", "analogous", "code", "ethic", "medical", "doctor", "option", "v\u00e9liz", "2019", "data", "science", "addressed", "l", "taylor", "purtova", "2019", "also", "expect", "much", "policy", "eventually", "cover", "specific", "us", "technology", "ai", "robotics", "rather", "field", "whole", "useful", "summary", "ethical", "framework", "ai", "given", "european", "group", "ethic", "science", "new", "technology", "2018", "13ff", "general", "ai", "policy", "see", "calo", "2018", "well", "crawford", "calo", "2016", "stahl", "timmermans", "mittelstadt", "2016", "johnson", "verdicchio", "2017", "giubilini", "savulescu", "2018", "political", "angle", "technology", "often", "discussed", "field", "science", "technology", "study", "sts", "book", "like", "ethic", "invention", "jasanoff", "2016", "show", "concern", "sts", "often", "quite", "similar", "ethic", "jacob", "et", "al", "2019", "oir", "article", "discus", "policy", "type", "issue", "separately", "rather", "ai", "robotics", "general", "2", "main", "debate", "section", "outline", "ethical", "issue", "human", "use", "ai", "robotics", "system", "le", "autonomouswhich", "mean", "look", "issue", "arise", "certain", "us", "technology", "would", "arise", "others", "must", "kept", "mind", "however", "technology", "always", "cause", "us", "easier", "thus", "frequent", "hinder", "us", "design", "technical", "artefact", "thus", "ethical", "relevance", "use", "houkes", "vermaas", "2010", "verbeek", "2011", "beyond", "responsible", "use", "also", "need", "responsible", "design", "field", "focus", "use", "presuppose", "ethical", "approach", "best", "suited", "tackling", "issue", "might", "well", "virtue", "ethic", "vallor", "2017", "rather", "consequentialist", "valuebased", "floridi", "et", "al", "2018", "section", "also", "neutral", "respect", "question", "whether", "ai", "system", "truly", "intelligence", "mental", "property", "would", "apply", "equally", "well", "ai", "robotics", "merely", "seen", "current", "face", "automation", "cf", "m\u00fcller", "forthcomingb", "21", "privacy", "surveillance", "general", "discussion", "privacy", "surveillance", "information", "technology", "eg", "macnish", "2017", "roessler", "2017", "mainly", "concern", "access", "private", "data", "data", "personally", "identifiable", "privacy", "several", "well", "recognised", "aspect", "eg", "right", "let", "alone", "information", "privacy", "privacy", "aspect", "personhood", "control", "information", "oneself", "right", "secrecy", "bennett", "raab", "2006", "privacy", "study", "historically", "focused", "state", "surveillance", "secret", "service", "include", "surveillance", "state", "agent", "business", "even", "individual", "technology", "changed", "significantly", "last", "decade", "regulation", "slow", "respond", "though", "regulation", "eu", "2016679", "the", "result", "certain", "anarchy", "exploited", "powerful", "player", "sometimes", "plain", "sight", "sometimes", "hiding", "digital", "sphere", "widened", "greatly", "data", "collection", "storage", "digital", "life", "increasingly", "digital", "digital", "data", "connected", "single", "internet", "sensor", "technology", "use", "generates", "data", "nondigital", "aspect", "life", "ai", "increase", "possibility", "intelligent", "data", "collection", "possibility", "data", "analysis", "applies", "blanket", "surveillance", "whole", "population", "well", "classic", "targeted", "surveillance", "addition", "much", "data", "traded", "agent", "usually", "fee", "time", "controlling", "collect", "data", "access", "much", "harder", "digital", "world", "analogue", "world", "paper", "telephone", "call", "many", "new", "ai", "technology", "amplify", "known", "issue", "example", "face", "recognition", "photo", "video", "allows", "identification", "thus", "profiling", "searching", "individual", "whittaker", "et", "al", "2018", "15ff", "continues", "using", "technique", "identification", "eg", "device", "fingerprinting", "commonplace", "internet", "sometimes", "revealed", "privacy", "policy", "result", "vast", "ocean", "data", "frighteningly", "complete", "picture", "u", "smolan", "2016", "101", "result", "arguably", "scandal", "still", "received", "due", "public", "attention", "data", "trail", "leave", "behind", "free", "service", "paid", "forbut", "told", "data", "collection", "value", "new", "raw", "material", "manipulated", "leaving", "ever", "data", "big", "5", "company", "amazon", "googlealphabet", "microsoft", "apple", "facebook", "main", "datacollection", "part", "business", "appears", "based", "deception", "exploiting", "human", "weakness", "furthering", "procrastination", "generating", "addiction", "manipulation", "harris", "2016", "oir", "primary", "focus", "social", "medium", "gaming", "internet", "surveillance", "economy", "gain", "maintain", "direct", "attentionand", "thus", "data", "supply", "surveillance", "business", "model", "internet", "schneier", "2015", "surveillance", "attention", "economy", "sometimes", "called", "surveillance", "capitalism", "zuboff", "2019", "caused", "many", "attempt", "escape", "grasp", "corporation", "eg", "exercise", "minimalism", "newport", "2019", "sometimes", "open", "source", "movement", "appears", "presentday", "citizen", "lost", "degree", "autonomy", "needed", "escape", "fully", "continuing", "life", "work", "lost", "ownership", "data", "ownership", "right", "relation", "arguably", "lost", "control", "data", "system", "often", "reveal", "fact", "u", "wish", "suppress", "aware", "know", "u", "know", "even", "observing", "online", "behaviour", "allows", "insight", "mental", "state", "burr", "christianini", "2019", "manipulation", "see", "section", "22", "led", "call", "protection", "derived", "data", "wachter", "mittelstadt", "2019", "last", "sentence", "bestselling", "book", "homo", "deus", "harari", "asks", "longterm", "consequence", "ai", "happen", "society", "politics", "daily", "life", "nonconscious", "highly", "intelligent", "algorithm", "know", "u", "better", "know", "2016", "462", "robotic", "device", "yet", "played", "major", "role", "area", "except", "security", "patrolling", "change", "common", "outside", "industry", "environment", "together", "internet", "thing", "socalled", "smart", "system", "phone", "tv", "oven", "lamp", "virtual", "assistant", "home", "smart", "city", "sennett", "2018", "smart", "governance", "set", "become", "part", "datagathering", "machinery", "offer", "detailed", "data", "different", "type", "real", "time", "ever", "information", "privacypreserving", "technique", "largely", "conceal", "identity", "person", "group", "standard", "staple", "data", "science", "include", "relative", "anonymisation", "access", "control", "plus", "encryption", "model", "computation", "carried", "fully", "partially", "encrypted", "input", "data", "stahl", "wright", "2018", "case", "differential", "privacy", "done", "adding", "calibrated", "noise", "encrypt", "output", "query", "dwork", "et", "al", "2006", "abowd", "2017", "requiring", "effort", "cost", "technique", "avoid", "many", "privacy", "issue", "company", "also", "seen", "better", "privacy", "competitive", "advantage", "leveraged", "sold", "price", "one", "major", "practical", "difficulty", "actually", "enforce", "regulation", "level", "state", "level", "individual", "claim", "must", "identify", "responsible", "legal", "entity", "prove", "action", "perhaps", "prove", "intent", "find", "court", "declares", "competent", "eventually", "get", "court", "actually", "enforce", "decision", "wellestablished", "legal", "protection", "right", "consumer", "right", "product", "liability", "civil", "liability", "protection", "intellectual", "property", "right", "often", "missing", "digital", "product", "hard", "enforce", "mean", "company", "digital", "background", "used", "testing", "product", "consumer", "without", "fear", "liability", "heavily", "defending", "intellectual", "property", "right", "internet", "libertarianism", "sometimes", "taken", "assume", "technical", "solution", "take", "care", "societal", "problem", "mozorov", "2013", "22", "manipulation", "behaviour", "ethical", "issue", "ai", "surveillance", "go", "beyond", "mere", "accumulation", "data", "direction", "attention", "include", "use", "information", "manipulate", "behaviour", "online", "offline", "way", "undermines", "autonomous", "rational", "choice", "course", "effort", "manipulate", "behaviour", "ancient", "may", "gain", "new", "quality", "use", "ai", "system", "given", "user", "intense", "interaction", "data", "system", "deep", "knowledge", "individual", "provides", "vulnerable", "nudge", "manipulation", "deception", "sufficient", "prior", "data", "algorithm", "used", "target", "individual", "small", "group", "kind", "input", "likely", "influence", "particular", "individual", "nudge", "change", "environment", "influence", "behaviour", "predictable", "way", "positive", "individual", "easy", "cheap", "avoid", "thaler", "sunstein", "2008", "slippery", "slope", "paternalism", "manipulation", "many", "advertiser", "marketer", "online", "seller", "use", "legal", "mean", "disposal", "maximise", "profit", "including", "exploitation", "behavioural", "bias", "deception", "addiction", "generation", "costa", "halpern", "2019", "oir", "manipulation", "business", "model", "much", "gambling", "gaming", "industry", "spreading", "eg", "lowcost", "airline", "interface", "design", "web", "page", "game", "manipulation", "us", "called", "dark", "pattern", "mathur", "et", "al", "2019", "moment", "gambling", "sale", "addictive", "substance", "highly", "regulated", "online", "manipulation", "addiction", "noteven", "though", "manipulation", "online", "behaviour", "becoming", "core", "business", "model", "internet", "furthermore", "social", "medium", "prime", "location", "political", "propaganda", "influence", "used", "steer", "voting", "behaviour", "facebookcambridge", "analytica", "scandal", "woolley", "howard", "2017", "bradshaw", "neudert", "howard", "2019", "andif", "successfulit", "may", "harm", "autonomy", "individual", "susser", "roessler", "nissenbaum", "2019", "improved", "ai", "faking", "technology", "make", "reliable", "evidence", "unreliable", "evidencethis", "already", "happened", "digital", "photo", "sound", "recording", "video", "soon", "quite", "easy", "create", "rather", "alter", "deep", "fake", "text", "photo", "video", "material", "desired", "content", "soon", "sophisticated", "realtime", "interaction", "person", "text", "phone", "video", "faked", "trust", "digital", "interaction", "time", "increasingly", "dependent", "interaction", "one", "specific", "issue", "machine", "learning", "technique", "ai", "rely", "training", "vast", "amount", "data", "mean", "often", "tradeoff", "privacy", "right", "data", "vs", "technical", "quality", "product", "influence", "consequentialist", "evaluation", "privacyviolating", "practice", "policy", "field", "ups", "down", "civil", "liberty", "protection", "individual", "right", "intense", "pressure", "business", "lobbying", "secret", "service", "state", "agency", "depend", "surveillance", "privacy", "protection", "diminished", "massively", "compared", "predigital", "age", "communication", "based", "letter", "analogue", "telephone", "communication", "personal", "conversation", "surveillance", "operated", "significant", "legal", "constraint", "eu", "general", "data", "protection", "regulation", "regulation", "eu", "2016679", "strengthened", "privacy", "protection", "u", "china", "prefer", "growth", "le", "regulation", "thompson", "bremmer", "2018", "likely", "hope", "provides", "competitive", "advantage", "clear", "state", "business", "actor", "increased", "ability", "invade", "privacy", "manipulate", "people", "help", "ai", "technology", "continue", "particular", "interestsunless", "reined", "policy", "interest", "general", "society", "23", "opacity", "ai", "system", "opacity", "bias", "central", "issue", "sometimes", "called", "data", "ethic", "big", "data", "ethic", "floridi", "taddeo", "2016", "mittelstadt", "floridi", "2016", "ai", "system", "automated", "decision", "support", "predictive", "analytics", "raise", "significant", "concern", "lack", "due", "process", "accountability", "community", "engagement", "auditing", "whittaker", "et", "al", "2018", "18ff", "part", "power", "structure", "creating", "decisionmaking", "process", "constrain", "limit", "opportunity", "human", "participation", "danaher", "2016b", "245", "time", "often", "impossible", "affected", "person", "know", "system", "came", "output", "ie", "system", "opaque", "person", "system", "involves", "machine", "learning", "typically", "opaque", "even", "expert", "know", "particular", "pattern", "identified", "even", "pattern", "bias", "decision", "system", "data", "set", "exacerbated", "opacity", "least", "case", "desire", "remove", "bias", "analysis", "opacity", "bias", "go", "hand", "hand", "political", "response", "tackle", "issue", "together", "many", "ai", "system", "rely", "machine", "learning", "technique", "simulated", "neural", "network", "extract", "pattern", "given", "dataset", "without", "correct", "solution", "provided", "ie", "supervised", "semisupervised", "unsupervised", "technique", "learning", "capture", "pattern", "data", "labelled", "way", "appears", "useful", "decision", "system", "make", "programmer", "really", "know", "pattern", "data", "system", "used", "fact", "program", "evolving", "new", "data", "come", "new", "feedback", "given", "correct", "incorrect", "pattern", "used", "learning", "system", "change", "mean", "outcome", "transparent", "user", "programmer", "opaque", "furthermore", "quality", "program", "depends", "heavily", "quality", "data", "provided", "following", "old", "slogan", "garbage", "garbage", "data", "already", "involved", "bias", "eg", "police", "data", "skin", "colour", "suspect", "program", "reproduce", "bias", "proposal", "standard", "description", "datasets", "datasheet", "would", "make", "identification", "bias", "feasible", "gebru", "et", "al", "2018", "oir", "also", "significant", "recent", "literature", "limitation", "machine", "learning", "system", "essentially", "sophisticated", "data", "filter", "marcus", "2018", "oir", "argued", "ethical", "problem", "today", "result", "technical", "shortcut", "ai", "taken", "cristianini", "forthcoming", "several", "technical", "activity", "aim", "explainable", "ai", "starting", "van", "lent", "fisher", "mancuso", "1999", "lomas", "et", "al", "2012", "recently", "darpa", "programme", "gunning", "2017", "oir", "broadly", "demand", "mechanism", "elucidating", "articulating", "power", "structure", "bias", "influence", "computational", "artefact", "exercise", "society", "diakopoulos", "2015", "398", "sometimes", "called", "algorithmic", "accountability", "reporting", "mean", "expect", "ai", "explain", "reasoning", "doing", "would", "require", "far", "serious", "moral", "autonomy", "currently", "attribute", "ai", "system", "see", "210", "politician", "henry", "kissinger", "pointed", "fundamental", "problem", "democratic", "decisionmaking", "rely", "system", "supposedly", "superior", "human", "explain", "decision", "say", "may", "generated", "potentially", "dominating", "technology", "search", "guiding", "philosophy", "kissinger", "2018", "danaher", "2016b", "call", "problem", "threat", "algocracy", "adopting", "previous", "use", "algocracy", "aneesh", "2002", "oir", "2006", "similar", "vein", "cave", "2019", "stress", "need", "broader", "societal", "move", "towards", "democratic", "decisionmaking", "avoid", "ai", "force", "lead", "kafkastyle", "impenetrable", "suppression", "system", "public", "administration", "elsewhere", "political", "angle", "discussion", "stressed", "neil", "influential", "book", "weapon", "math", "destruction", "2016", "yeung", "lodge", "2019", "eu", "issue", "taken", "account", "regulation", "eu", "2016679", "foresees", "consumer", "faced", "decision", "based", "data", "processing", "legal", "right", "explanation", "how", "far", "go", "extent", "enforced", "disputed", "goodman", "flaxman", "2017", "wachter", "mittelstadt", "floridi", "2016", "wachter", "mittelstadt", "russell", "2017", "zerilli", "et", "al", "2019", "argue", "may", "double", "standard", "demand", "high", "level", "explanation", "machinebased", "decision", "despite", "human", "sometimes", "reaching", "standard", "24", "bias", "decision", "system", "automated", "ai", "decision", "support", "system", "predictive", "analytics", "operate", "data", "produce", "decision", "output", "output", "may", "range", "relatively", "trivial", "highly", "significant", "restaurant", "match", "preference", "patient", "xray", "completed", "bone", "growth", "application", "credit", "card", "declined", "donor", "organ", "given", "another", "patient", "bail", "denied", "target", "identified", "engaged", "data", "analysis", "often", "used", "predictive", "analytics", "business", "healthcare", "field", "foresee", "future", "developmentssince", "prediction", "easier", "also", "become", "cheaper", "commodity", "one", "use", "prediction", "predictive", "policing", "nij", "2014", "oir", "many", "fear", "might", "lead", "erosion", "public", "liberty", "ferguson", "2017", "take", "away", "power", "people", "whose", "behaviour", "predicted", "appears", "however", "many", "worry", "policing", "depend", "futuristic", "scenario", "law", "enforcement", "foresees", "punishes", "planned", "action", "rather", "waiting", "crime", "committed", "like", "2002", "film", "minority", "report", "one", "concern", "system", "might", "perpetuate", "bias", "already", "data", "used", "set", "system", "eg", "increasing", "police", "patrol", "area", "discovering", "crime", "area", "actual", "predictive", "policing", "intelligence", "led", "policing", "technique", "mainly", "concern", "question", "police", "force", "needed", "also", "police", "officer", "provided", "data", "offering", "control", "facilitating", "better", "decision", "workflow", "support", "software", "eg", "arcgis", "whether", "problematic", "depends", "appropriate", "level", "trust", "technical", "quality", "system", "evaluation", "aim", "police", "work", "perhaps", "recent", "paper", "title", "point", "right", "direction", "ai", "ethic", "predictive", "policing", "model", "threat", "ethic", "care", "asaro", "2019", "bias", "typically", "surface", "unfair", "judgment", "made", "individual", "making", "judgment", "influenced", "characteristic", "actually", "irrelevant", "matter", "hand", "typically", "discriminatory", "preconception", "member", "group", "one", "form", "bias", "learned", "cognitive", "feature", "person", "often", "made", "explicit", "person", "concerned", "may", "aware", "biasthey", "may", "even", "honestly", "explicitly", "opposed", "bias", "found", "eg", "priming", "cf", "graham", "lowery", "2004", "fairness", "vs", "bias", "machine", "learning", "see", "binns", "2018", "apart", "social", "phenomenon", "learned", "bias", "human", "cognitive", "system", "generally", "prone", "various", "kind", "cognitive", "bias", "eg", "confirmation", "bias", "human", "tend", "interpret", "information", "confirming", "already", "believe", "second", "form", "bias", "often", "said", "impede", "performance", "rational", "judgment", "kahnemann", "2011", "though", "least", "cognitive", "bias", "generate", "evolutionary", "advantage", "eg", "economical", "use", "resource", "intuitive", "judgment", "question", "whether", "ai", "system", "could", "cognitive", "bias", "third", "form", "bias", "present", "data", "exhibit", "systematic", "error", "eg", "statistical", "bias", "strictly", "given", "dataset", "unbiased", "single", "kind", "issue", "mere", "creation", "dataset", "involves", "danger", "may", "used", "different", "kind", "issue", "turn", "biased", "kind", "machine", "learning", "basis", "data", "would", "fail", "recognise", "bias", "codify", "automate", "historical", "bias", "historical", "bias", "discovered", "automated", "recruitment", "screening", "system", "amazon", "discontinued", "early", "2017", "discriminated", "womenpresumably", "company", "history", "discriminating", "woman", "hiring", "process", "correctional", "offender", "management", "profiling", "alternative", "sanction", "compas", "system", "predict", "whether", "defendant", "would", "reoffend", "found", "successful", "652", "accuracy", "group", "random", "human", "dressel", "farid", "2018", "produce", "false", "positive", "le", "false", "negative", "black", "defendant", "problem", "system", "thus", "bias", "plus", "human", "placing", "excessive", "trust", "system", "political", "dimension", "automated", "system", "usa", "investigated", "eubanks", "2018", "significant", "technical", "effort", "detect", "remove", "bias", "ai", "system", "fair", "say", "early", "stage", "see", "uk", "institute", "ethical", "ai", "machine", "learning", "brownsword", "scotford", "yeung", "2017", "yeung", "lodge", "2019", "appears", "technological", "fix", "limit", "need", "mathematical", "notion", "fairness", "hard", "come", "whittaker", "et", "al", "2018", "24ff", "selbst", "et", "al", "2019", "formal", "notion", "race", "see", "benthall", "haynes", "2019", "institutional", "proposal", "veale", "binns", "2017", "25", "humanrobot", "interaction", "humanrobot", "interaction", "hri", "academic", "field", "right", "pay", "significant", "attention", "ethical", "matter", "dynamic", "perception", "side", "different", "interest", "present", "intricacy", "social", "context", "including", "coworking", "eg", "arnold", "scheutz", "2017", "useful", "survey", "ethic", "robotics", "include", "calo", "froomkin", "kerr", "2016", "royakkers", "van", "est", "2016", "tzafestas", "2016", "standard", "collection", "paper", "lin", "abney", "jenkins", "2017", "ai", "used", "manipulate", "human", "believing", "thing", "see", "section", "22", "also", "used", "drive", "robot", "problematic", "process", "appearance", "involve", "deception", "threaten", "human", "dignity", "violate", "kantian", "requirement", "respect", "humanity", "human", "easily", "attribute", "mental", "property", "object", "empathise", "especially", "outer", "appearance", "object", "similar", "living", "being", "used", "deceive", "human", "animal", "attributing", "intellectual", "even", "emotional", "significance", "robot", "ai", "system", "deserve", "part", "humanoid", "robotics", "problematic", "regard", "eg", "hiroshi", "ishiguro", "remotecontrolled", "geminoids", "case", "clearly", "deceptive", "publicrelations", "purpose", "eg", "ability", "hanson", "robotics", "sophia", "course", "fairly", "basic", "constraint", "business", "ethic", "law", "apply", "robot", "product", "safety", "liability", "nondeception", "advertisement", "appears", "existing", "constraint", "take", "care", "many", "concern", "raised", "case", "however", "humanhuman", "interaction", "aspect", "appear", "specifically", "human", "way", "perhaps", "replaced", "robot", "care", "love", "sex", "251", "example", "care", "robot", "use", "robot", "health", "care", "human", "currently", "level", "concept", "study", "real", "environment", "may", "become", "usable", "technology", "year", "raised", "number", "concern", "dystopian", "future", "dehumanised", "care", "a", "sharkey", "n", "sharkey", "2011", "robert", "sparrow", "2016", "current", "system", "include", "robot", "support", "human", "carerscaregivers", "eg", "lifting", "patient", "transporting", "material", "robot", "enable", "patient", "certain", "thing", "eg", "eat", "robotic", "arm", "also", "robot", "given", "patient", "company", "comfort", "eg", "paro", "robot", "seal", "overview", "see", "van", "wynsberghe", "2016", "n\u00f8rskov", "2017", "foschvillaronga", "albocanals", "2019", "survey", "user", "draper", "et", "al", "2014", "one", "reason", "issue", "care", "come", "fore", "people", "argued", "need", "robot", "ageing", "society", "argument", "make", "problematic", "assumption", "namely", "longer", "lifespan", "people", "need", "care", "possible", "attract", "human", "caring", "profession", "may", "also", "show", "bias", "age", "jecker", "forthcoming", "importantly", "ignores", "nature", "automation", "simply", "replacing", "human", "allowing", "human", "work", "efficiently", "clear", "really", "issue", "since", "discussion", "mostly", "focus", "fear", "robot", "dehumanising", "care", "actual", "foreseeable", "robot", "care", "assistive", "robot", "classic", "automation", "technical", "task", "thus", "care", "robot", "behavioural", "sense", "performing", "task", "care", "environment", "sense", "human", "care", "patient", "appears", "success", "cared", "relies", "intentional", "sense", "care", "foreseeable", "robot", "provide", "anything", "risk", "robot", "care", "absence", "intentional", "carebecause", "le", "human", "carers", "may", "needed", "interestingly", "caring", "something", "even", "virtual", "agent", "good", "carer", "lee", "et", "al", "2019", "system", "pretend", "care", "would", "deceptive", "thus", "problematicunless", "deception", "countered", "sufficiently", "large", "utility", "gain", "coeckelbergh", "2016", "robot", "pretend", "care", "basic", "level", "available", "paro", "seal", "others", "making", "perhaps", "feeling", "cared", "machine", "extent", "progress", "come", "patient", "252", "example", "b", "sex", "robot", "argued", "several", "tech", "optimist", "human", "likely", "interested", "sex", "companionship", "robot", "comfortable", "idea", "levy", "2007", "given", "variation", "human", "sexual", "preference", "including", "sex", "toy", "sex", "doll", "seems", "likely", "question", "whether", "device", "manufactured", "promoted", "whether", "limit", "touchy", "area", "seems", "moved", "mainstream", "robot", "philosophy", "recent", "time", "sullins", "2012", "danaher", "mcarthur", "2017", "n", "sharkey", "et", "al", "2017", "oir", "bendel", "2018", "devlin", "2018", "human", "long", "deep", "emotional", "attachment", "object", "perhaps", "companionship", "even", "love", "predictable", "android", "attractive", "especially", "people", "struggle", "actual", "human", "already", "prefer", "dog", "cat", "bird", "computer", "tamagotchi", "danaher", "2019b", "argues", "nyholm", "frank", "2017", "true", "friendship", "thus", "valuable", "goal", "certainly", "look", "like", "friendship", "might", "increase", "overall", "utility", "even", "lacking", "depth", "discussion", "issue", "deception", "since", "robot", "present", "mean", "say", "feeling", "human", "well", "known", "human", "prone", "attribute", "feeling", "thought", "entity", "behave", "sentience", "even", "clearly", "inanimate", "object", "show", "behaviour", "also", "paying", "deception", "seems", "elementary", "part", "traditional", "sex", "industry", "finally", "concern", "often", "accompanied", "matter", "sex", "namely", "consent", "frank", "nyholm", "2017", "aesthetic", "concern", "worry", "human", "may", "corrupted", "certain", "experience", "old", "fashioned", "though", "may", "seem", "human", "behaviour", "influenced", "experience", "likely", "pornography", "sex", "robot", "support", "perception", "human", "mere", "object", "desire", "even", "recipient", "abuse", "thus", "ruin", "deeper", "sexual", "erotic", "experience", "vein", "campaign", "sex", "robot", "argues", "device", "continuation", "slavery", "prostitution", "richardson", "2016", "26", "automation", "employment", "seems", "clear", "ai", "robotics", "lead", "significant", "gain", "productivity", "thus", "overall", "wealth", "attempt", "increase", "productivity", "often", "feature", "economy", "though", "emphasis", "growth", "modern", "phenomenon", "harari", "2016", "240", "however", "productivity", "gain", "automation", "typically", "mean", "fewer", "human", "required", "output", "necessarily", "imply", "loss", "overall", "employment", "however", "available", "wealth", "increase", "increase", "demand", "sufficiently", "counteract", "productivity", "gain", "long", "run", "higher", "productivity", "industrial", "society", "led", "wealth", "overall", "major", "labour", "market", "disruption", "occurred", "past", "eg", "farming", "employed", "60", "workforce", "europe", "northamerica", "1800", "2010", "employed", "ca", "5", "eu", "even", "le", "wealthiest", "country", "european", "commission", "2013", "20", "year", "1950", "1970", "number", "hired", "agricultural", "worker", "uk", "reduced", "50", "zayed", "loft", "2019", "disruption", "lead", "labourintensive", "industry", "moving", "place", "lower", "labour", "cost", "ongoing", "process", "classic", "automation", "replaced", "human", "muscle", "whereas", "digital", "automation", "replaces", "human", "thought", "informationprocessingand", "unlike", "physical", "machine", "digital", "automation", "cheap", "duplicate", "bostrom", "yudkowsky", "2014", "may", "thus", "mean", "radical", "change", "labour", "market", "main", "question", "effect", "different", "time", "creation", "new", "job", "wealth", "keep", "destruction", "job", "even", "different", "transition", "cost", "bear", "need", "make", "societal", "adjustment", "fair", "distribution", "cost", "benefit", "digital", "automation", "response", "issue", "unemployment", "ai", "ranged", "alarmed", "frey", "osborne", "2013", "westlake", "2014", "neutral", "metcalf", "keller", "boyd", "2016", "oir", "calo", "2018", "frey", "2019", "optimistic", "brynjolfsson", "mcafee", "2016", "harari", "2016", "danaher", "2019a", "principle", "labour", "market", "effect", "automation", "seems", "fairly", "well", "understood", "involving", "two", "channel", "nature", "interaction", "differently", "skilled", "worker", "new", "technology", "affecting", "labour", "demand", "ii", "equilibrium", "effect", "technological", "progress", "consequent", "change", "labour", "supply", "product", "market", "goo", "2018", "362", "currently", "seems", "happen", "labour", "market", "result", "ai", "robotics", "automation", "job", "polarisation", "dumbbell", "shape", "goo", "manning", "salomon", "2009", "highly", "skilled", "technical", "job", "demand", "highly", "paid", "low", "skilled", "service", "job", "demand", "badly", "paid", "midqualification", "job", "factory", "office", "ie", "majority", "job", "pressure", "reduced", "relatively", "predictable", "likely", "automated", "baldwin", "2019", "perhaps", "enormous", "productivity", "gain", "allow", "age", "leisure", "realised", "something", "keynes", "1930", "predicted", "occur", "around", "2030", "assuming", "growth", "rate", "1", "per", "annum", "actually", "already", "reached", "level", "anticipated", "2030", "still", "workingconsuming", "inventing", "ever", "level", "organisation", "harari", "explains", "economic", "development", "allowed", "humanity", "overcome", "hunger", "disease", "warand", "aim", "immortality", "eternal", "bliss", "ai", "thus", "title", "homo", "deus", "harari", "2016", "75", "general", "term", "issue", "unemployment", "issue", "good", "society", "justly", "distributed", "standard", "view", "distributive", "justice", "rationally", "decided", "behind", "veil", "ignorance", "rawls", "1971", "ie", "one", "know", "position", "society", "one", "would", "actually", "taking", "labourer", "industrialist", "etc", "rawls", "thought", "chosen", "principle", "would", "support", "basic", "liberty", "distribution", "greatest", "benefit", "leastadvantaged", "member", "society", "would", "appear", "ai", "economy", "three", "feature", "make", "justice", "unlikely", "first", "operates", "largely", "unregulated", "environment", "responsibility", "often", "hard", "allocate", "second", "operates", "market", "winner", "take", "feature", "monopoly", "develop", "quickly", "third", "new", "economy", "digital", "service", "industry", "based", "intangible", "asset", "also", "called", "capitalism", "without", "capital", "haskel", "westlake", "2017", "mean", "difficult", "control", "multinational", "digital", "corporation", "rely", "physical", "plant", "particular", "location", "three", "feature", "seem", "suggest", "leave", "distribution", "wealth", "free", "market", "force", "result", "would", "heavily", "unjust", "distribution", "indeed", "development", "already", "see", "one", "interesting", "question", "received", "much", "attention", "whether", "development", "ai", "environmentally", "sustainable", "like", "computing", "system", "ai", "system", "produce", "waste", "hard", "recycle", "consume", "vast", "amount", "energy", "especially", "training", "machine", "learning", "system", "even", "mining", "cryptocurrency", "appears", "actor", "space", "offload", "cost", "general", "society", "27", "autonomous", "system", "several", "notion", "autonomy", "discussion", "autonomous", "system", "stronger", "notion", "involved", "philosophical", "debate", "autonomy", "basis", "responsibility", "personhood", "christman", "2003", "2018", "context", "responsibility", "implies", "autonomy", "inversely", "system", "degree", "technical", "autonomy", "without", "raising", "issue", "responsibility", "weaker", "technical", "notion", "autonomy", "robotics", "relative", "gradual", "system", "said", "autonomous", "respect", "human", "control", "certain", "degree", "m\u00fcller", "2012", "parallel", "issue", "bias", "opacity", "ai", "since", "autonomy", "also", "concern", "powerrelation", "control", "responsible", "generally", "speaking", "one", "question", "degree", "autonomous", "robot", "raise", "issue", "present", "conceptual", "scheme", "must", "adapt", "whether", "require", "technical", "adjustment", "jurisdiction", "sophisticated", "system", "civil", "criminal", "liability", "resolve", "issue", "technical", "standard", "eg", "safe", "use", "machinery", "medical", "environment", "likely", "need", "adjusted", "already", "field", "verifiable", "ai", "safetycritical", "system", "security", "application", "body", "like", "ieee", "institute", "electrical", "electronics", "engineer", "bsi", "british", "standard", "institution", "produced", "standard", "particularly", "technical", "subproblems", "data", "security", "transparency", "among", "many", "autonomous", "system", "land", "water", "water", "air", "space", "discus", "two", "sample", "autonomous", "vehicle", "autonomous", "weapon", "271", "example", "autonomous", "vehicle", "autonomous", "vehicle", "hold", "promise", "reduce", "significant", "damage", "human", "driving", "currently", "causesapproximately", "1", "million", "human", "killed", "per", "year", "many", "injured", "environment", "polluted", "earth", "sealed", "concrete", "tarmac", "city", "full", "parked", "car", "etc", "however", "seem", "question", "autonomous", "vehicle", "behave", "responsibility", "risk", "distributed", "complicated", "system", "vehicle", "operates", "also", "significant", "disagreement", "long", "development", "fully", "autonomous", "level", "5", "car", "sae", "international", "2018", "actually", "take", "discussion", "trolley", "problem", "context", "classic", "trolley", "problem", "thomson", "1976", "woollard", "howardsnyder", "2016", "section", "2", "various", "dilemma", "presented", "simplest", "version", "trolley", "train", "track", "heading", "towards", "five", "people", "kill", "unless", "train", "diverted", "onto", "side", "track", "track", "one", "person", "killed", "train", "take", "side", "track", "example", "go", "back", "remark", "foot", "1967", "6", "discus", "number", "dilemma", "case", "tolerated", "intended", "consequence", "action", "differ", "trolley", "problem", "supposed", "describe", "actual", "ethical", "problem", "solved", "right", "choice", "rather", "thoughtexperiments", "choice", "artificially", "constrained", "small", "finite", "number", "distinct", "oneoff", "option", "agent", "perfect", "knowledge", "problem", "used", "theoretical", "tool", "investigate", "ethical", "intuition", "theoriesespecially", "difference", "actively", "vs", "allowing", "something", "happen", "intended", "vs", "tolerated", "consequence", "consequentialist", "vs", "normative", "approach", "kamm", "2016", "type", "problem", "reminded", "many", "problem", "encountered", "actual", "driving", "autonomous", "driving", "lin", "2016", "doubtful", "however", "actual", "driver", "autonomous", "car", "ever", "solve", "trolley", "problem", "see", "keeling", "2020", "autonomous", "car", "trolley", "problem", "received", "lot", "medium", "attention", "awad", "et", "al", "2018", "seem", "offer", "anything", "new", "either", "ethical", "theory", "programming", "autonomous", "vehicle", "common", "ethical", "problem", "driving", "speeding", "risky", "overtaking", "keeping", "safe", "distance", "etc", "classic", "problem", "pursuing", "personal", "interest", "vs", "common", "good", "vast", "majority", "covered", "legal", "regulation", "driving", "programming", "car", "drive", "rule", "rather", "interest", "passenger", "achieve", "maximum", "utility", "thus", "deflated", "standard", "problem", "programming", "ethical", "machine", "see", "section", "29", "probably", "additional", "discretionary", "rule", "politeness", "interesting", "question", "break", "rule", "lin", "2016", "seems", "case", "applying", "standard", "consideration", "rule", "vs", "utility", "case", "autonomous", "vehicle", "notable", "policy", "effort", "field", "include", "report", "german", "federal", "ministry", "transport", "digital", "infrastructure", "2017", "stress", "safety", "primary", "objective", "rule", "10", "state", "case", "automated", "connected", "driving", "system", "accountability", "previously", "sole", "preserve", "individual", "shift", "motorist", "manufacturer", "operator", "technological", "system", "body", "responsible", "taking", "infrastructure", "policy", "legal", "decision", "see", "section", "2101", "resulting", "german", "eu", "law", "licensing", "automated", "driving", "much", "restrictive", "u", "counterpart", "testing", "consumer", "strategy", "used", "companieswithout", "informed", "consent", "consumer", "possible", "victim", "272", "example", "b", "autonomous", "weapon", "notion", "automated", "weapon", "fairly", "old", "example", "instead", "fielding", "simple", "guided", "missile", "remotely", "piloted", "vehicle", "might", "launch", "completely", "autonomous", "land", "sea", "air", "vehicle", "capable", "complex", "farranging", "reconnaissance", "attack", "mission", "darpa", "1983", "1", "proposal", "ridiculed", "fantasy", "time", "dreyfus", "dreyfus", "athanasiou", "1986", "ix", "reality", "least", "easily", "identifiable", "target", "missile", "plane", "ship", "tank", "etc", "human", "combatant", "main", "argument", "lethal", "autonomous", "weapon", "system", "aws", "law", "support", "extrajudicial", "killing", "take", "responsibility", "away", "human", "make", "war", "killing", "likelyfor", "detailed", "list", "issue", "see", "lin", "bekey", "abney", "2008", "7386", "appears", "lowering", "hurdle", "use", "system", "autonomous", "vehicle", "fireandforget", "missile", "drone", "loaded", "explosive", "reducing", "probability", "held", "accountable", "would", "increase", "probability", "use", "crucial", "asymmetry", "one", "side", "kill", "impunity", "thus", "reason", "already", "exists", "conventional", "drone", "war", "remote", "controlled", "weapon", "eg", "u", "pakistan", "easy", "imagine", "small", "drone", "search", "identifies", "kill", "individual", "humanor", "perhaps", "type", "human", "kind", "case", "brought", "forward", "campaign", "stop", "killer", "robot", "activist", "group", "seem", "equivalent", "saying", "autonomous", "weapon", "indeed", "weapon", "weapon", "kill", "still", "make", "gigantic", "number", "matter", "accountability", "autonomous", "weapon", "might", "make", "identification", "prosecution", "responsible", "agent", "difficultbut", "clear", "given", "digital", "record", "one", "keep", "least", "conventional", "war", "difficulty", "allocating", "punishment", "sometimes", "called", "retribution", "gap", "danaher", "2016a", "another", "question", "whether", "using", "autonomous", "weapon", "war", "would", "make", "war", "worse", "make", "war", "le", "bad", "robot", "reduce", "war", "crime", "crime", "war", "answer", "may", "well", "positive", "used", "argument", "favour", "weapon", "arkin", "2009", "m\u00fcller", "2016a", "also", "argument", "amoroso", "tamburrini", "2018", "arguably", "main", "threat", "use", "weapon", "conventional", "warfare", "asymmetric", "conflict", "nonstate", "agent", "including", "criminal", "also", "said", "autonomous", "weapon", "conform", "international", "humanitarian", "law", "requires", "observance", "principle", "distinction", "combatant", "civilian", "proportionality", "force", "military", "necessity", "force", "military", "conflict", "a", "sharkey", "2019", "true", "distinction", "combatant", "noncombatants", "hard", "distinction", "civilian", "military", "ship", "easyso", "say", "construct", "use", "weapon", "violate", "humanitarian", "law", "additional", "concern", "raised", "killed", "autonomous", "weapon", "threatens", "human", "dignity", "even", "defender", "ban", "weapon", "seem", "say", "good", "argument", "weapon", "technology", "also", "compromise", "human", "dignity", "given", "ambiguity", "inherent", "concept", "wiser", "draw", "several", "type", "objection", "argument", "aws", "rely", "exclusively", "human", "dignity", "a", "sharkey", "2019", "lot", "made", "keeping", "human", "loop", "loop", "military", "guidance", "weaponsthese", "way", "spelling", "meaningful", "control", "discussed", "santoni", "de", "sio", "van", "den", "hoven", "2018", "discussion", "difficulty", "allocating", "responsibility", "killing", "autonomous", "weapon", "responsibility", "gap", "suggested", "esp", "rob", "sparrow", "2007", "meaning", "neither", "human", "machine", "may", "responsible", "hand", "assume", "every", "event", "someone", "responsible", "event", "real", "issue", "may", "well", "distribution", "risk", "simpson", "m\u00fcller", "2016", "risk", "analysis", "hansson", "2013", "indicates", "crucial", "identify", "exposed", "risk", "potential", "beneficiary", "make", "decision", "hansson", "2018", "18221824", "28", "machine", "ethic", "machine", "ethic", "ethic", "machine", "ethical", "machine", "machine", "subject", "rather", "human", "use", "machine", "object", "often", "clear", "whether", "supposed", "cover", "ai", "ethic", "part", "floridi", "saunders", "2004", "moor", "2006", "anderson", "anderson", "2011", "wallach", "asaro", "2017", "sometimes", "look", "though", "dubious", "inference", "play", "machine", "act", "ethically", "relevant", "way", "need", "machine", "ethic", "accordingly", "use", "broader", "notion", "machine", "ethic", "concerned", "ensuring", "behavior", "machine", "toward", "human", "user", "perhaps", "machine", "well", "ethically", "acceptable", "anderson", "anderson", "2007", "15", "might", "include", "mere", "matter", "product", "safety", "example", "author", "sound", "rather", "ambitious", "use", "narrower", "notion", "ai", "reasoning", "able", "take", "account", "societal", "value", "moral", "ethical", "consideration", "weigh", "respective", "priority", "value", "held", "different", "stakeholder", "various", "multicultural", "context", "explain", "reasoning", "guarantee", "transparency", "dignum", "2018", "1", "2", "discussion", "machine", "ethic", "make", "substantial", "assumption", "machine", "sense", "ethical", "agent", "responsible", "action", "autonomous", "moral", "agent", "see", "van", "wynsberghe", "robbins", "2019", "basic", "idea", "machine", "ethic", "finding", "way", "actual", "robotics", "assumption", "machine", "artificial", "moral", "agent", "substantial", "sense", "usually", "made", "winfield", "et", "al", "2019", "sometimes", "observed", "robot", "programmed", "follow", "ethical", "rule", "easily", "modified", "follow", "unethical", "rule", "vanderelst", "winfield", "2018", "idea", "machine", "ethic", "might", "take", "form", "law", "famously", "investigated", "isaac", "asimov", "proposed", "three", "law", "robotics", "asimov", "1942", "first", "lawa", "robot", "may", "injure", "human", "inaction", "allow", "human", "come", "harm", "second", "lawa", "robot", "must", "obey", "order", "given", "human", "being", "except", "order", "would", "conflict", "first", "law", "third", "lawa", "robot", "must", "protect", "existence", "long", "protection", "conflict", "first", "second", "law", "asimov", "showed", "number", "story", "conflict", "three", "law", "make", "problematic", "use", "despite", "hierarchical", "organisation", "clear", "consistent", "notion", "machine", "ethic", "since", "weaker", "version", "danger", "reducing", "ethic", "notion", "would", "normally", "considered", "sufficient", "eg", "without", "reflection", "even", "without", "action", "stronger", "notion", "move", "towards", "artificial", "moral", "agent", "may", "describe", "acurrentlyempty", "set", "29", "artificial", "moral", "agent", "one", "take", "machine", "ethic", "concern", "moral", "agent", "substantial", "sense", "agent", "called", "artificial", "moral", "agent", "right", "responsibility", "however", "discussion", "artificial", "entity", "challenge", "number", "common", "notion", "ethic", "useful", "understand", "abstraction", "human", "case", "cf", "misselhorn", "2020", "power", "ganascia", "forthcoming", "several", "author", "use", "artificial", "moral", "agent", "le", "demanding", "sense", "borrowing", "use", "agent", "software", "engineering", "case", "matter", "responsibility", "right", "arise", "allen", "varner", "zinser", "2000", "james", "moor", "2006", "distinguishes", "four", "type", "machine", "agent", "ethical", "impact", "agent", "eg", "robot", "jockey", "implicit", "ethical", "agent", "eg", "safe", "autopilot", "explicit", "ethical", "agent", "eg", "using", "formal", "method", "estimate", "utility", "full", "ethical", "agent", "make", "explicit", "ethical", "judgment", "generally", "competent", "reasonably", "justify", "average", "adult", "human", "full", "ethical", "agent", "several", "way", "achieve", "explicit", "full", "ethical", "agent", "proposed", "via", "programming", "operational", "morality", "via", "developing", "ethic", "functional", "morality", "finally", "fullblown", "morality", "full", "intelligence", "sentience", "allen", "smit", "wallach", "2005", "moor", "2006", "programmed", "agent", "sometimes", "considered", "full", "agent", "competent", "without", "comprehension", "like", "neuron", "brain", "dennett", "2017", "hakli", "m\u00e4kel\u00e4", "2019", "discussion", "notion", "moral", "patient", "play", "role", "ethical", "agent", "responsibility", "ethical", "patient", "right", "harm", "matter", "seems", "clear", "entity", "patient", "without", "agent", "eg", "simple", "animal", "feel", "pain", "make", "justified", "choice", "hand", "normally", "understood", "agent", "also", "patient", "eg", "kantian", "framework", "usually", "person", "supposed", "make", "entity", "responsible", "agent", "someone", "duty", "object", "ethical", "concern", "personhood", "typically", "deep", "notion", "associated", "phenomenal", "consciousness", "intention", "free", "frankfurt", "1971", "strawson", "1998", "torrance", "2011", "suggests", "artificial", "machine", "ethic", "could", "defined", "designing", "machine", "thing", "done", "human", "indicative", "possession", "ethical", "status", "human", "2011", "116", "which", "take", "ethical", "productivity", "ethical", "receptivity", "2011", "117", "his", "expression", "moral", "agent", "patient", "291", "responsibility", "robot", "broad", "consensus", "accountability", "liability", "rule", "law", "basic", "requirement", "must", "upheld", "face", "new", "technology", "european", "group", "ethic", "science", "new", "technology", "2018", "18", "issue", "case", "robot", "done", "responsibility", "allocated", "robot", "act", "responsible", "liable", "accountable", "action", "distribution", "risk", "perhaps", "take", "precedence", "discussion", "responsibility", "traditional", "distribution", "responsibility", "already", "occurs", "car", "maker", "responsible", "technical", "safety", "car", "driver", "responsible", "driving", "mechanic", "responsible", "proper", "maintenance", "public", "authority", "responsible", "technical", "condition", "road", "etc", "general", "effect", "decision", "action", "based", "ai", "often", "result", "countless", "interaction", "among", "many", "actor", "including", "designer", "developer", "user", "software", "hardware", "distributed", "agency", "come", "distributed", "responsibility", "taddeo", "floridi", "2018", "751", "distribution", "might", "occur", "problem", "specific", "ai", "gain", "particular", "urgency", "context", "nyholm", "2018a", "2018b", "classical", "control", "engineering", "distributed", "control", "often", "achieved", "control", "hierarchy", "plus", "control", "loop", "across", "hierarchy", "292", "right", "robot", "author", "indicated", "seriously", "considered", "whether", "current", "robot", "must", "allocated", "right", "gunkel", "2018a", "2018b", "danaher", "forthcoming", "turner", "2019", "position", "seems", "rely", "largely", "criticism", "opponent", "empirical", "observation", "robot", "nonpersons", "sometimes", "treated", "right", "vein", "relational", "turn", "proposed", "relate", "robot", "though", "right", "might", "welladvised", "search", "whether", "really", "right", "coeckelbergh", "2010", "2012", "2018", "raise", "question", "far", "antirealism", "quasirealism", "go", "mean", "say", "robot", "right", "humancentred", "approach", "gerdes", "2016", "side", "debate", "bryson", "insisted", "robot", "enjoy", "right", "bryson", "2010", "though", "considers", "possibility", "gunkel", "bryson", "2014", "wholly", "separate", "issue", "whether", "robot", "ai", "system", "given", "status", "legal", "entity", "legal", "person", "sense", "natural", "person", "also", "state", "business", "organisation", "entity", "namely", "legal", "right", "duty", "european", "parliament", "considered", "allocating", "status", "robot", "order", "deal", "civil", "liability", "eu", "parliament", "2016", "bertolini", "aiello", "2018", "criminal", "liabilitywhich", "reserved", "natural", "person", "would", "also", "possible", "assign", "certain", "subset", "right", "duty", "robot", "said", "legislative", "action", "would", "morally", "unnecessary", "legally", "troublesome", "would", "serve", "interest", "human", "bryson", "diamantis", "grant", "2017", "273", "environmental", "ethic", "longstanding", "discussion", "legal", "right", "natural", "object", "like", "tree", "c", "d", "stone", "1972", "also", "said", "reason", "developing", "robot", "right", "artificial", "moral", "patient", "future", "ethically", "doubtful", "van", "wynsberghe", "robbins", "2019", "community", "artificial", "consciousness", "researcher", "significant", "concern", "whether", "would", "ethical", "create", "consciousness", "since", "creating", "would", "presumably", "imply", "ethical", "obligation", "sentient", "eg", "harm", "end", "existence", "switching", "offsome", "author", "called", "moratorium", "synthetic", "phenomenology", "bentley", "et", "al", "2018", "28f", "210", "singularity", "2101", "singularity", "superintelligence", "quarter", "aim", "current", "ai", "thought", "artificial", "general", "intelligence", "agi", "contrasted", "technical", "narrow", "ai", "agi", "usually", "distinguished", "traditional", "notion", "ai", "general", "purpose", "system", "searle", "notion", "strong", "ai", "computer", "given", "right", "program", "literally", "said", "understand", "cognitive", "state", "searle", "1980", "417", "idea", "singularity", "trajectory", "artificial", "intelligence", "reach", "system", "human", "level", "intelligence", "system", "would", "ability", "develop", "ai", "system", "surpass", "human", "level", "intelligence", "ie", "superintelligent", "see", "superintelligent", "ai", "system", "would", "quickly", "selfimprove", "develop", "even", "intelligent", "system", "sharp", "turn", "event", "reaching", "superintelligent", "ai", "singularity", "development", "ai", "human", "control", "hard", "predict", "kurzweil", "2005", "487", "fear", "robot", "created", "take", "world", "captured", "human", "imagination", "even", "computer", "eg", "butler", "1863", "central", "theme", "\u010dapek", "famous", "play", "introduced", "word", "robot", "\u010dapek", "1920", "fear", "first", "formulated", "possible", "trajectory", "existing", "ai", "intelligence", "explosion", "irvin", "good", "let", "ultraintelligent", "machine", "defined", "machine", "far", "surpass", "intellectual", "activity", "man", "however", "clever", "since", "design", "machine", "one", "intellectual", "activity", "ultraintelligent", "machine", "could", "design", "even", "better", "machine", "would", "unquestionably", "intelligence", "explosion", "intelligence", "man", "would", "left", "far", "behind", "thus", "first", "ultraintelligent", "machine", "last", "invention", "man", "need", "ever", "make", "provided", "machine", "docile", "enough", "tell", "u", "keep", "control", "good", "1965", "33", "optimistic", "argument", "acceleration", "singularity", "spelled", "kurzweil", "1999", "2005", "2012", "essentially", "point", "computing", "power", "increasing", "exponentially", "ie", "doubling", "ca", "every", "2", "year", "since", "1970", "accordance", "moore", "law", "number", "transistor", "continue", "time", "future", "predicted", "kurzweil", "1999", "2010", "supercomputer", "reach", "human", "computation", "capacity", "2030", "mind", "uploading", "possible", "2045", "singularity", "occur", "kurzweil", "talk", "increase", "computing", "power", "purchased", "given", "costbut", "course", "recent", "year", "fund", "available", "ai", "company", "also", "increased", "enormously", "amodei", "hernandez", "2018", "oir", "thus", "estimate", "year", "20122018", "actual", "computing", "power", "available", "train", "particular", "ai", "system", "doubled", "every", "34", "month", "resulting", "300000x", "increasenot", "7x", "increase", "doubling", "every", "two", "year", "would", "created", "common", "version", "argument", "chalmers", "2010", "talk", "increase", "intelligence", "ai", "system", "rather", "raw", "computing", "power", "crucial", "point", "singularity", "remains", "one", "development", "ai", "taken", "ai", "system", "accelerates", "beyond", "human", "level", "bostrom", "2014", "explains", "detail", "would", "happen", "point", "risk", "humanity", "discussion", "summarised", "eden", "et", "al", "2012", "armstrong", "2014", "shanahan", "2015", "possible", "path", "superintelligence", "computing", "power", "increase", "eg", "complete", "emulation", "human", "brain", "computer", "kurzweil", "2012", "sandberg", "2013", "biological", "path", "network", "organisation", "bostrom", "2014", "2251", "despite", "obvious", "weakness", "identification", "intelligence", "processing", "power", "kurzweil", "seems", "right", "human", "tend", "underestimate", "power", "exponential", "growth", "minitest", "walked", "step", "way", "step", "double", "previous", "starting", "step", "one", "metre", "far", "would", "get", "30", "step", "answer", "almost", "3", "time", "earth", "permanent", "natural", "satellite", "indeed", "progress", "ai", "readily", "attributable", "availability", "processor", "faster", "degree", "magnitude", "larger", "storage", "higher", "investment", "m\u00fcller", "2018", "actual", "acceleration", "speed", "discussed", "m\u00fcller", "bostrom", "2016", "bostrom", "dafoe", "flynn", "forthcoming", "sandberg", "2019", "argues", "progress", "continue", "time", "participant", "debate", "united", "technophile", "sense", "expect", "technology", "develop", "rapidly", "bring", "broadly", "welcome", "changesbut", "beyond", "divide", "focus", "benefit", "eg", "kurzweil", "focus", "risk", "eg", "bostrom", "camp", "sympathise", "transhuman", "view", "survival", "humankind", "different", "physical", "form", "eg", "uploaded", "computer", "moravec", "1990", "1998", "bostrom", "2003a", "2003c", "also", "consider", "prospect", "human", "enhancement", "various", "respect", "including", "intelligenceoften", "called", "ia", "intelligence", "augmentation", "may", "future", "ai", "used", "human", "enhancement", "contribute", "dissolution", "neatly", "defined", "human", "single", "person", "robin", "hanson", "provides", "detailed", "speculation", "happen", "economically", "case", "human", "brain", "emulation", "enables", "truly", "intelligent", "robot", "em", "hanson", "2016", "argument", "superintelligence", "risk", "requires", "assumption", "superintelligence", "imply", "benevolencecontrary", "kantian", "tradition", "ethic", "argued", "higher", "level", "rationality", "intelligence", "would", "go", "along", "better", "understanding", "moral", "better", "ability", "act", "morally", "gewirth", "1978", "chalmers", "2010", "36f", "argument", "risk", "superintelligence", "say", "rationality", "morality", "entirely", "independent", "dimensionsthis", "sometimes", "explicitly", "argued", "orthogonality", "thesis", "bostrom", "2012", "armstrong", "2013", "bostrom", "2014", "105109", "criticism", "singularity", "narrative", "raised", "various", "angle", "kurzweil", "bostrom", "seem", "assume", "intelligence", "onedimensional", "property", "set", "intelligent", "agent", "totallyordered", "mathematical", "sensebut", "neither", "discus", "intelligence", "length", "book", "generally", "fair", "say", "despite", "effort", "assumption", "made", "powerful", "narrative", "superintelligence", "singularity", "investigated", "detail", "one", "question", "whether", "singularity", "ever", "occurit", "may", "conceptually", "impossible", "practically", "impossible", "may", "happen", "contingent", "event", "including", "people", "actively", "preventing", "philosophically", "interesting", "question", "whether", "singularity", "myth", "floridi", "2016", "ganascia", "2017", "trajectory", "actual", "ai", "research", "something", "practitioner", "often", "assume", "eg", "brook", "2017", "oir", "may", "fear", "public", "relation", "backlash", "overestimate", "practical", "problem", "good", "reason", "think", "superintelligence", "unlikely", "outcome", "current", "ai", "research", "m\u00fcller", "forthcominga", "discussion", "raise", "question", "whether", "concern", "singularity", "narrative", "fictional", "ai", "based", "human", "fear", "even", "one", "find", "negative", "reason", "compelling", "singularity", "likely", "occur", "still", "significant", "possibility", "one", "may", "turn", "wrong", "philosophy", "secure", "path", "science", "kant", "1791", "b15", "maybe", "ai", "robotics", "either", "m\u00fcller", "2020", "appears", "discussing", "highimpact", "risk", "singularity", "justification", "even", "one", "think", "probability", "singularity", "ever", "occurring", "low", "2102", "existential", "risk", "superintelligence", "thinking", "superintelligence", "long", "term", "raise", "question", "whether", "superintelligence", "may", "lead", "extinction", "human", "specie", "called", "existential", "risk", "xrisk", "superintelligent", "system", "may", "well", "preference", "conflict", "existence", "human", "earth", "may", "thus", "decide", "end", "existenceand", "given", "superior", "intelligence", "power", "may", "happen", "end", "really", "care", "thinking", "long", "term", "crucial", "feature", "literature", "whether", "singularity", "another", "catastrophic", "event", "occurs", "30", "300", "3000", "year", "really", "matter", "baum", "et", "al", "2019", "perhaps", "even", "astronomical", "pattern", "intelligent", "specie", "bound", "discover", "ai", "point", "thus", "bring", "demise", "great", "filter", "would", "contribute", "explanation", "fermi", "paradox", "sign", "life", "known", "universe", "despite", "high", "probability", "emerging", "would", "bad", "news", "found", "great", "filter", "ahead", "u", "rather", "obstacle", "earth", "already", "passed", "issue", "sometimes", "taken", "narrowly", "human", "extinction", "bostrom", "2013", "broadly", "concerning", "large", "risk", "specie", "rees", "2018", "of", "ai", "one", "h\u00e4ggstr\u00f6m", "2016", "ord", "2020", "bostrom", "also", "us", "category", "global", "catastrophic", "risk", "risk", "sufficiently", "high", "two", "dimension", "scope", "severity", "bostrom", "\u0107irkovi\u0107", "2011", "bostrom", "2013", "discussion", "risk", "usually", "connected", "general", "problem", "ethic", "risk", "eg", "hansson", "2013", "2018", "longterm", "view", "methodological", "challenge", "produced", "wide", "discussion", "tegmark", "2017", "focus", "ai", "human", "life", "30", "singularity", "russell", "dewey", "tegmark", "2015", "bostrom", "dafoe", "flynn", "forthcoming", "survey", "longerterm", "policy", "issue", "ethical", "ai", "several", "collection", "paper", "investigated", "risk", "artificial", "general", "intelligence", "agi", "factor", "might", "make", "development", "le", "riskladen", "m\u00fcller", "2016b", "callaghan", "et", "al", "2017", "yampolskiy", "2018", "including", "development", "nonagent", "ai", "drexler", "2019", "2103", "controlling", "superintelligence", "narrow", "sense", "control", "problem", "human", "remain", "control", "ai", "system", "superintelligent", "bostrom", "2014", "127ff", "wider", "sense", "problem", "make", "sure", "ai", "system", "turn", "positive", "according", "human", "perception", "russell", "2019", "sometimes", "called", "value", "alignment", "easy", "hard", "control", "superintelligence", "depends", "significantly", "speed", "takeoff", "superintelligent", "system", "led", "particular", "attention", "system", "selfimprovement", "alphazero", "silver", "et", "al", "2018", "one", "aspect", "problem", "might", "decide", "certain", "feature", "desirable", "find", "unforeseen", "consequence", "negative", "would", "desire", "feature", "ancient", "problem", "king", "midas", "wished", "touched", "would", "turn", "gold", "problem", "discussed", "occasion", "various", "example", "paperclip", "maximiser", "bostrom", "2003b", "program", "optimise", "chess", "performance", "omohundro", "2014", "discussion", "superintelligence", "include", "speculation", "omniscient", "being", "radical", "change", "latter", "day", "promise", "immortality", "transcendence", "current", "bodily", "formso", "sometimes", "clear", "religious", "undertone", "capurro", "1993", "geraci", "2008", "2010", "connell", "2017", "160ff", "issue", "also", "pose", "wellknown", "problem", "epistemology", "know", "way", "omniscient", "danaher", "2015", "usual", "opponent", "already", "shown", "characteristic", "response", "atheist", "people", "worry", "computer", "get", "smart", "take", "world", "real", "problem", "stupid", "already", "taken", "world", "domingo", "2015", "new", "nihilist", "explain", "technohypnosis", "information", "technology", "become", "main", "method", "distraction", "loss", "meaning", "gertz", "2018", "opponent", "would", "thus", "say", "need", "ethic", "small", "problem", "occur", "actual", "ai", "robotics", "section", "21", "29", "le", "need", "big", "ethic", "existential", "risk", "ai", "section", "210", "3", "closing", "singularity", "thus", "raise", "problem", "concept", "ai", "remarkable", "imagination", "vision", "played", "central", "role", "since", "beginning", "discipline", "dartmouth", "summer", "research", "project", "mccarthy", "et", "al", "1955", "oir", "simon", "newell", "1958", "evaluation", "vision", "subject", "dramatic", "change", "decade", "went", "slogan", "ai", "impossible", "dreyfus", "1972", "ai", "automation", "lighthill", "1973", "ai", "solve", "problem", "kurzweil", "1999", "ai", "may", "kill", "u", "bostrom", "2014", "created", "medium", "attention", "public", "relation", "effort", "also", "raise", "problem", "much", "philosophy", "ethic", "ai", "really", "ai", "rather", "imagined", "technology", "said", "outset", "ai", "robotics", "raised", "fundamental", "question", "system", "system", "risk", "long", "term", "also", "challenge", "human", "view", "humanity", "intelligent", "dominant", "specie", "earth", "seen", "issue", "raised", "watch", "technological", "social", "development", "closely", "catch", "new", "issue", "early", "develop", "philosophical", "analysis", "learn", "traditional", "problem", "philosophy"]}