{
    "url": "epistemic-game",
    "title": "Epistemic Foundations of Game Theory",
    "authorship": {
        "year": "Copyright \u00a9 2015",
        "author_text": "Eric Pacuit\n<epacuit@umd.edu>\nOlivier Roy\n<Olivier.Roy@uni-bayreuth.de>",
        "author_links": [
            {
                "http://www.philosophy.umd.edu/people/pacuit": "Eric Pacuit"
            },
            {
                "mailto:epacuit%40umd%2eedu": "epacuit@umd.edu"
            },
            {
                "http://www.philosophie1.uni-bayreuth.de/en/team/roy/": "Olivier Roy"
            },
            {
                "mailto:Olivier%2eRoy%40uni-bayreuth%2ede": "Olivier.Roy@uni-bayreuth.de"
            }
        ],
        "raw_html": "<div id=\"article-copyright\">\n<p>\n<a href=\"../../info.html#c\">Copyright \u00a9 2015</a> by\n\n<br/>\n<a href=\"http://www.philosophy.umd.edu/people/pacuit\" target=\"other\">Eric Pacuit</a>\n&lt;<a href=\"mailto:epacuit%40umd%2eedu\"><em>epacuit<abbr title=\" at \">@</abbr>umd<abbr title=\" dot \">.</abbr>edu</em></a>&gt;<br/>\n<a href=\"http://www.philosophie1.uni-bayreuth.de/en/team/roy/\" target=\"other\">Olivier Roy</a>\n&lt;<a href=\"mailto:Olivier%2eRoy%40uni-bayreuth%2ede\"><em>Olivier<abbr title=\" dot \">.</abbr>Roy<abbr title=\" at \">@</abbr>uni-bayreuth<abbr title=\" dot \">.</abbr>de</em></a>&gt;\n    </p>\n</div>"
    },
    "pubinfo": [
        "First published Fri Mar 13, 2015"
    ],
    "preamble": "\nFoundational work in game theory aims at making explicit the\nassumptions that underlie the basic concepts of the\ndiscipline. Non-cooperative game theory is the study of individual,\nrational decision making in situations of strategic interaction. This\nentry presents the epistemic foundations of non-cooperative\ngame theory (this area of research is called epistemic game\ntheory).\nEpistemic game theory views rational decision making in games as\nsomething not essentially different from rational decision making\nunder uncertainty. As in Decision Theory (Peterson 2009), to choose\nrationally in a game is to select the \u201cbest\u201d action in\nlight of one\u2019s beliefs or information. In a decision problem,\nthe decision maker\u2019s beliefs are about a passive state of\nnature, the state of which determines the consequences of her\nactions. In a game, the consequences of one\u2019s decision depend on\nthe choices of the other agents involved in the situation\n(and possibly the state of nature). Recognizing this\u2014i.e., that\none is interacting with other agents who try to choose the best course\nof action in the light of their own\nbeliefs\u2014brings higher-order information into the\npicture. The players\u2019 beliefs are no longer about a passive or\nexternal environment. They concern the choices and the\ninformation of the other players. What one expects of one\u2019s\nopponents depends on what one thinks the others expect from her, and\nwhat the others expect from a given player depends on what they think\nher expectations about them are.\nThis entry provides an overview of the issues that arise when one\ntakes this broadly decision-theoretic view on rational decision making\nin games. After some general comments about information in games, we\npresent the formal tools developed in epistemic game theory and\nepistemic logic that have been used to understand the role of\nhigher-order information in interactive decision making. We then show\nhow these tools can be used to characterize known \u201csolution\nconcepts\u201d of games in terms of rational decision making in\nspecific informational contexts. Along the way, we highlight a number\nof philosophical issues that arise in this area.\n",
    "toc": [
        {
            "#EpiVieGam": "1. The Epistemic View of Games"
        },
        {
            "#ClaGamThe": "1.1 Classical Game Theory"
        },
        {
            "#EpiGamThe": "1.2 Epistemic Game Theory"
        },
        {
            "#StaDecMak": "1.3 Stages of Decision Making"
        },
        {
            "#IncInf": "1.4 Incomplete Information"
        },
        {
            "#ImpInfPerRec": "1.5 Imperfect Information and Perfect Recall"
        },
        {
            "#MixStr": "1.6 Mixed Strategies"
        },
        {
            "#GamMod": "2. Game Models"
        },
        {
            "#GenIss": "2.1 General Issues"
        },
        {
            "#VarInfAtt": "2.1.1 Varieties of informational attitudes "
        },
        {
            "#PosWorMod": "2.1.2 Possible worlds models"
        },
        {
            "#RelMod": "2.2 Relational Models"
        },
        {
            "#AddBel": "2.2.1 Adding Beliefs"
        },
        {
            "#HarTypSpa": "2.3 Harsanyi Type Spaces"
        },
        {
            "#ComKno": "2.4 Common Knowledge"
        },
        {
            "#ChoRulChoOpt": "3. Choice Rules, or Choosing Optimally"
        },
        {
            "#MaxExpUti": "3.1 Maximization of Expected Utility"
        },
        {
            "#DomRea": "3.2 Dominance Reasoning"
        },
        {
            "#Fun": "4. Fundamentals"
        },
        {
            "#IteRemStrDomStr": "4.1 Iterated Removal of Strictly Dominated Strategies"
        },
        {
            "#Res": "4.1.1 The Result"
        },
        {
            "#PhiIss": "4.1.2 Philosophical Issues"
        },
        {
            "#BacInd": "4.2 Backward induction"
        },
        {
            "#ExtGamBasDef": "4.2.1 Extensive games: basic definitions"
        },
        {
            "#EpiChaBacInd": "4.2.2 Epistemic Characterization of Backward Induction"
        },
        {
            "#ComKnoRatWitBacInd": "4.2.3 Common Knowledge of Rationality without Backward Induction"
        },
        {
            "#ComStrBelForInd": "4.3 Common strong belief and forward induction"
        },
        {
            "#Dev": "5. Developments"
        },
        {
            "#NasEqu": "5.1 Nash Equilibrium"
        },
        {
            "#ResB": "5.1.1 The Result"
        },
        {
            "#PhiIssB": "5.1.2 Philosophical Issues"
        },
        {
            "#RemModChaNasEqu": "5.1.3 Remarks on \u201cModal\u201d Characterizations of Nash Equilibrium"
        },
        {
            "#IncAdmCauBel": "5.2 Incorporating Admissibility and \u201cCautious\u201d Beliefs"
        },
        {
            "#IncUna": "5.3 Incorporating Unawareness"
        },
        {
            "#ParSelRefGamMod": "6. A Paradox of Self-Reference in Game Models"
        },
        {
            "#ConRem": "7. Concluding Remarks"
        },
        {
            "#WhaEpiGamTheTryAcc": "7.1 What is an epistemic game theory trying to accomplish? "
        },
        {
            "#AltMaxExpUti": "7.2 Alternatives to maximizing expected utility "
        },
        {
            "#FurRea": "7.3 Further reading"
        },
        {
            "#Bib": "Bibliography"
        },
        {
            "#Aca": "Academic Tools"
        },
        {
            "#Oth": "Other Internet Resources"
        },
        {
            "#Rel": "Related Entries"
        }
    ],
    "main_text": "\n1. The Epistemic View of Games\n1.1 Classical Game Theory\nA game refers to any interactive situation involving a\ngroup of self-interested agents, or players. The defining\nfeature of a game is that the players are engaged in an\n\u201cinterdependent decision problem\u201d (Schelling\n1960). Classically, the mathematical description of a game\nincludes following components:\n\nThe players. In this entry, we only consider games with a\nfinite set of players. We use \\(\\Agt\\) to denote the set of players in a\ngame, and \\(i, j,\\ldots\\) to denote its elements.\nThe feasible options (typically called actions\nor strategies) for each player. Again, we only consider games\nwith finitely many feasible options for each player.\nThe players\u2019 preferences over possible outcome. Here\nwe represent them as von Neumann-Morgenstern utility functions \\(u_i\\)\nassigning real-valued utilities to each outcome of the game.\n\nA game can have many other structural properties. It can\nbe represented as a single-shot or multi-stage decision problem, or it\ncan include simultaneous or stochastic moves. We start with games\nin strategic form without stochastic moves, and will\nintroduce more sophisticated games as we go along in the entry. In a\nstrategic game, each player \\(i\\) can choose from a (finite) set \\(S_i\\)\nof options, also called actions or strategies. The combination of all\nthe players\u2019 choices, denoted \\({\\mathbf{s}}\\), is called\na strategy profile, or outcome of the game. We write\n\\({\\mathbf{s}}_i\\) for \\(i\\)\u2019s component in \\({\\mathbf{s}}\\), and\n\\({\\mathbf{s}}_{-i}\\) for the profile of strategies for all agents other\nthan \\(i\\). Finally, we write \\(\\Pi_{i \\in \\Agt} S_i\\) for the set of all\nstrategy profiles of a given game. Putting everything together, a\nstrategic game is a tuple \\(\\langle \\A, \\{S_i, u_i\\}_{i\\in\\A}\\rangle\\)\nwhere \\(\\A\\) is a finite set of players, for each \\(i\\in\\A\\), \\(S_i\\) is a\nfinite set of actions and \\(u_i:\\Pi_{i\\in\\A} S_i\\rightarrow\\mathbb{R}\\)\nis player \\(i\\)\u2019s utility function.\nThe game in Figure 1 is an example\nof a game in strategic form. There are two players, Ann and Bob, and\neach has to choose between two options: \\(\\Agt = \\{Ann, Bob\\}\\),\n\\(S_{Ann} = \\{u, d\\}\\) and \\(S_{Bob} = \\{l, r\\}\\). The value of \\(u_{Ann}\\)\nand \\(u_{Bob}\\), representing their respective preferences over the\npossible outcomes of the game, are displayed in the cell of the\nmatrix. If Bob chooses \\(l\\), for instance, Ann prefers the outcome she\nwould get by choosing \\(u\\) to the one she would get by choosing \\(d\\),\nbut this preference is reversed in the case Bob chooses \\(r\\). This game\nis called a \u201cpure coordination game\u201d in the literature\nbecause the players have a clear interest in coordinating their\nchoices\u2014i.e., on \\((u, l)\\) or \\((d, r)\\)\u2014but they are\nindifferent about which way they coordinate their choices.\n\n\n \u00a0 Bob \nAnn \n\n\\(l\\) \\(r\\)\n\\(u\\) 1,1  0,0 \n\\(d\\) 0,0  1,1 \n\n\n\nFigure 1: A coordination game\n\nIn a game, no single player has total control over which outcome\nwill be realized at the end of the interaction. This depends on the\ndecisions of all players. Such abstract models\nof interdependent decisions are capable of representing a\nwhole array of social situations, from strictly competitive to\ncooperative ones. See Ross (2010) for more details about classical\ngame theory and key references.\nThe central analytic tool of classical game theory are solution\nconcepts. They provide a top-down perspective specifying which\noutcomes of a game are deemed \u201crational\u201d. This can be\ngiven both a prescriptive or a predictive\nreading. Nash equilibrium is one of the most well-known solution\nconcepts, but we will encounter others below. In the game above, for\ninstance, there are two Nash equilibria in so-called \u201cpure\nstrategies.\u201d[1]\n These are the two coordination profiles: \\((u,\nl)\\) and \\((d, r)\\).\nFrom a prescriptive point of view, a solution concept is a set of\npractical recommendations\u2014i.e., recommendations about what the\nplayers should do in a game. From a predictive point of view, solution\nconcepts describe what the players will actually do in certain\ninteractive situation. Consider again the pure strategy Nash\nequilibria in the above example. Under a prescriptive interpretation,\nit singles out what players should do in the game. That is,\nAnn and Bob should either play their component of \\((u, l)\\) or \\((d,\nr)\\). Under the predictive interpretation, these profiles are the ones\nthat one would expect to observe in a actual play of that game.\nThis solution-concept-driven perspective on games faces many\nfoundational difficulties, which we do not survey here. The interested\nreader can consult Ross (2010), Bruin (2010), and Kadane & Larkey\n(1983) for a discussion.\n1.2 Epistemic Game Theory\nEpistemic game theory is a broad area of research encompassing a\nnumber of different mathematical frameworks that are used to analyze\ngames. The details of the frameworks are different, but they do share\na common perspective. In this Section, we discuss two key features of\nthis common perspective.\n\n(1)\n\nEpistemic game theory takes a broadly Bayesian perspective on\ndecision-making in strategic situations.\n\n\nThis point of view is nicely explained by Robert Stalnaker:\n\nThere is no special concept of rationality for decision making in a\nsituation where the outcomes depend on the actions of more than one\nagent. The acts of other agents are, like chance events, natural\ndisasters and acts of God, just facts about an uncertain world that\nagents have beliefs and degrees of belief about. The utilities of\nother agents are relevant to an agent only as information that,\ntogether with beliefs about the rationality of those agents, helps to\npredict their actions. (Stalnaker 1996: 136)\n\nIn other words, epistemic game theory can be seen as an attempt to\nbring back the theory of decision making in games to its\ndecision-theoretic roots.\nIn decision theory, the decision-making units are individuals with\npreferences over the possible consequences of their actions. Since the\nconsequence of a given action depend on the state of the environment,\nthe decision-maker\u2019s beliefs about the state of the environment\nare crucial to assess the rationality of a particular decision. So,\nthe formal description of a decision problem includes the possible\noutcomes and states of the environment, the decision maker\u2019s\npreferences over these outcome, and a description of the\ndecision maker\u2019s beliefs about the state of nature\n(i.e., the decision maker\u2019s doxastic state). A\ndecision-theoretic choice rule can be used to make\nrecommendations to the decision maker about what she should\nchoose (or to predict what the decision-maker will choose). A\nstandard example of a choice rule is maximization of (subjective)\nexpected utility, underlying the Bayesian view of\nrationality. It presupposes that the decision maker\u2019s\npreferences and beliefs can be represented by numerical utilities and\nprobabilities, \nrespectively.[2]\n (We postpone the formal\nrepresentation of this, and the other choice rules such as weak and\nstrict dominance, until we have presented the formal models of beliefs\nin games in Section 2.)\nFrom an epistemic point of view, the classical ingredients of a\ngame (players, actions, outcomes, and preferences) are thus not enough\nto formulate recommendations or predictions about how the players\nshould or will choose. One needs to specify the (interactive) decision\nproblem the players are in, i.e., also the beliefs players\nhave about each other\u2019s possible actions (and beliefs). In a\nterminology that is becoming increasingly popular in epistemic game\ntheory, games are played in specific contexts (Friedenberg\n& Meier 2010, Other Internet Resources), in which the players have\nspecific knowledge and/or beliefs about each other. The\nrecommendations and/or predictions that are appropriate for one\ncontext may not transfer to another, even if the underlying situation\nmay correspond to precisely the same strategic game.\n\n(2)\n\nIn epistemic game theory, uncertainty about opponents\u2019\nstrategies takes center-stage.\n\n\nThere are various types of information that a player has access to\nin a game situation. For instance, a player may have\n\nimperfect information about the play of the game (which moves have\nbeen played?);\nincomplete information about the structure of the game (what are\nthe actions/payoffs?);\nstrategic information (what will the other players do?);\nor\nhigher-order information (what are the other players\nthinking?).\n\nWhile all types of uncertainty may play a role in an epistemic\nanalysis of a game, a distinguishing feature of epistemic game theory\nis an insistence that rational decisions are assessed in terms of the\nplayers\u2019 preferences and beliefs about what their\nopponents are going to do. Again we turn to Stalnaker to summarize\nthis point of view:\n\n\u2026There are no special rules of rationality telling one what\nto do in the absence of degrees of belief [about the opponents\u2019\nchoices], except this: decide what you believe, and then maximize\nexpected utility. (Stalnaker 1996: 136)\n\nThe four types of uncertainty in games introduced above are\nconceptually important, but not necessarily exhaustive nor mutually\nexclusive. John Harsanyi, for instance, argued that all uncertainty\nabout the structure of the game, that is all possible incompleteness\nin information, can be reduced to uncertainty about the payoffs\n(Harsanyi 1967\u201368). (This was later formalized and proved by\nStuart and Hu 2002). In a similar vein, Kadane & Larkey (1982)\nargue that only strategic uncertainty is relevant for the assessment\nof decision in game situations. Contemporary epistemic game theory\ntakes the view that, although it may ultimately be reducible to\nstrategic uncertainty, making higher-order uncertainty explicit can\nclarify a great deal of what interactive or strategic rationality\nmeans.\nThe crucial difference from the classical\n\u201csolution-concept\u201d analysis of a game is that epistemic\ngame theory takes a bottom-up perspective. Once the context of the\ngame is specified, the rational outcomes are derived, given\nassumptions about how the players are making their choices and what\nthey know and believe about how the others are choosing. In the\nremainder of this section, we briefly discuss some general issues that\narise from taking an epistemic perspective on games. We postpone\ndiscussion of higher-order and strategic uncertainty until Sections 3,\n4 and 5.\n1.3 Stages of Decision Making\nIt is standard in the game theory literature to distinguish three\nstages of the decision making process: ex ante, ex\ninterim and ex post. At one extreme is the ex\nante stage where no decision has been made yet. The other extreme\nis the ex post stage where the choices of all players are\nopenly disclosed. In between these two extremes is the ex\ninterim stage where the players have made their decisions, but\nthey are still uninformed about the decisions and intentions of the\nother players.\nThese distinctions are not intended to be sharp. Rather, they\ndescribe various stages of information disclosure during the\ndecision-making process. At the ex-ante stage, little is\nknown except the structure of the game, who is taking part, and\npossibly (but not necessarily) some aspect of the agents\u2019\ncharacter. At the ex-post stage the game is basically over:\nall player have made their decision and these are now irrevocably out\nin the open. This does not mean that all uncertainty is removed as an\nagent may remain uncertain about what exactly the others were\nexpecting of her. In between these two extreme stages lies a whole\ngradation of states of information disclosure that we loosely refer to\nas \u201cthe\u201d ex-interim stage. Common to these stages\nis the fact that the agents have made a decision, although\nnot necessarily an irrevocable one.\nIn this entry, we focus on the ex interim stage of\ndecision making. This is in line with much of the literature on the\nepistemic foundations of game theory as it allows for a\nstraightforward assessment of the agents\u2019 rationality given\ntheir expectations about how their opponents will choose. Focusing on\nthe ex interim stage does raise some interesting questions\nabout possible correlations between a player\u2019s strategy\nchoice, what Stalnaker (1999) calls \u201cactive knowledge\u201d,\nand her information about the choices of others, her \u201cpassive\nknowledge\u201d (idem). The question of how a player should\nreact, that is eventually revise her decision, upon learning that she\ndid not choose \u201crationally\u201d is an interesting and\nimportant one, but we do not discuss it in the entry. Note that this\nquestion is different from the one of how agents should revise their\nbeliefs upon learning that others did not choose\nrationally. This second question is very relevant in games in which\nplayers choose sequentially, and will be addressed\nin Section 4.2.3.\n1.4 Incomplete Information\nA natural question to ask about any mathematical model of\na game situation is how does the analysis change if the players\nare uncertain about some of the parameters of the model? This\nmotivated Harsanyi\u2019s fundamental work introducing the notion of\na game-theoretic type and defining a Bayesian\ngame in Harsanyi 1967\u201368. Using these ideas, an\nextensive literature has developed that analyzes games in which\nplayers are uncertain about some aspect of the game. (Consult\nLeyton-Brown & Shoham (2008: ch. 7) for a concise summary of the\ncurrent state-of-affairs and pointers to the relevant literature.) One\ncan naturally wonder about the precise relationship between this\nliterature and the literature we survey in this entry on the epistemic\nfoundations of game theory. Indeed, the foundational literature we\ndiscuss here largely focuses on Harsanyi\u2019s approach to modeling\nhigher-order beliefs (which we discuss in Section\n2.3).\nThere are two crucial differences between the literature on\nBayesian games and the literature we discuss in this entry (cf. the\ndiscussion in Brandenburger 2010: sec. 4 and 5).\n\nIn a Bayesian game, players are uncertain about the payoffs of the\ngame, what other players believe are the correct payoffs, what other\nplayers believe that the other players believe about the payoffs, and\nso on, and this is the only source of uncertainty. That is, the\nplayers\u2019 (higher-order) beliefs about the payoffs in a game\ncompletely determine the (higher-order) beliefs about the other\naspects of the game. In particular, if a player comes to know\nthe payoffs of the other players, then that player is certain (and\ncorrect) about the possible (rational) choices of the other\nplayers.[3]\nIt is assumed that all players choose optimally given their\ninformation. That is, all players choose a strategy that maximizes\ntheir expected utility given their beliefs about the game, beliefs\nabout what other players believe about the game, and so on. This\nmeans, in particular, that players do not entertain the possibility\nthat their opponents may choose \u201cirrationally.\u201d\n\nNote that these assumptions are not inherent in the formalism that\nHarsanyi used to represent the players\u2019 beliefs in a game of\nincomplete information. Rather, they are better described as\nconventions followed by Harsanyi and subsequent researchers studying\nBayesian games.\n1.5 Imperfect Information and Perfect Recall\nIn a game with imperfect information (see Ross 2010 for a\ndiscussion), the players may not be perfectly informed about the moves\nof their opponents or the outcome of chance moves by nature. Games\nwith imperfect information can be pictured as follows:\n\n\nFigure 2\n\nThe interpretation is that the decision made at the first node\n(\\(d_0\\)) is forgotten, and so the decision maker is uncertain about\nwhether she is at node \\(d_1\\) or \\(d_2\\). See Osborne (2003: ch. 9 &\n10) for the general theory of games with imperfect information. In\nthis section, we briefly discuss a foundational issue that arises in\ngames with imperfect information.\nKuhn (1953) introduced the distinction between perfect\nand imperfect recall in games with imperfect\ninformation. Roughly, players have perfect recall provided they\nremember all of their own past moves (see Bonanno 2004; Kaneko &\nKline 1995 for general discussions of the perfect recall\nassumption). It is standard in the game theory literature to assume\nthat all players have perfect recall (i.e., they may be uncertain\nabout previous choices of their opponents or nature, but they do\nremember their own moves).\nAs we noted in Section 1.3, there are\ndifferent stages to the decision making process. Differences between\nthese stages become even more pronounced in extensive games in which\nthere is a temporal dimension to the game. There are two ways to think\nabout the decision making process in an extensive game (with imperfect\ninformation). The first is to focus on the initial \u201cplanning\nstage\u201d. That is, initially, the players settle on a strategy\nspecifying the (possibly random) move they will make at each of their\nchoice nodes (this is the players\u2019 global\nstrategy). Then, the players start making their respective moves\n(following the strategies which they have committed to without\nreconsidering their options at each choice node). Alternatively, we\ncan assume that the players make \u201clocal judgements\u201d at\neach of their choice nodes, always choosing the best option given the\ninformation that is currently available to them. A well-known theorem\nof Kuhn (1953) shows that if players have perfect recall, then a\nstrategy is globally optimal if, and only if, it is locally optimal\n(see Brandenburger 2007 for a self-contained presentation of this\nclassic result). That is, both ways of thinking about the decision\nmaking process in extensive games (with imperfect information) lead to\nthe same recommendations/predictions.\nThe assumption of perfect recall is crucial for Kuhn\u2019s\nresult. This is demonstrated by the well-known absent-minded\ndriver\u2019s problem of Piccione and Rubinstein\n(1997a). Interestingly, their example is one where a decision maker\nmay be tempted to change his strategy after the initial planning\nstage, despite getting no new information. They describe the\nexample as follows:\n\nAn individual is sitting late at night in a bar planning his\nmidnight trip home. In order to get home he has to take the highway\nand get off at the second exit. Turning at the first exit leads into a\ndisastrous area (payoff 0). Turning at the second exit yields the\nhighest reward (payoff 4). If he continues beyond the second exit, he\ncannot go back and at the end of the highway he will find a motel\nwhere he can spend the night (payoff 1). The driver is absentminded\nand is aware of this fact. At an intersection, he cannot tell whether\nit is the first or the second intersection and he cannot remember how\nmany he has passed (one can make the situation more realistic by\nreferring to the 17th intersection). While sitting at the bar, all he\ncan do is to decide whether or not to exit at an\nintersection. (Piccione & Rubinstein 1997a: 7)\n\nThe decision tree for the absent-minded driver is pictured\nbelow:\n\n\nFigure 3\n\nThis problem is interesting since it demonstrates that there is a\nconflict between what the decision maker commits to do while planning\nat the bar and what he thinks is best at the first intersection:\n\nPlanning stage:\nWhile planning his trip home at the bar,\nthe decision maker is faced with a choice between \u201cContinue;\nContinue\u201d and \u201cExit\u201d. Since he cannot distinguish\nbetween the two intersections, he cannot plan to \u201cExit\u201d at\nthe second intersection (he must plan the same behavior at both \\(X\\)\nand \\(Y\\)). Since \u201cExit\u201d will lead to the worst outcome\n(with a payoff of 0), the optimal strategy is \u201cContinue;\nContinue\u201d with a guaranteed payoff of 1.\n\nAction stage: \nWhen arriving at an intersection, the decision maker is faced with a\nlocal choice of either \u201cExit\u201d or \u201cContinue\u201d\n(possibly followed by another decision). Now the decision maker knows\nthat since he committed to the plan of choosing \u201cContinue\u201d\nat each intersection, it is possible that he is at the second\nintersection. Indeed, the decision maker concludes that he is at the\nfirst intersection with probability 1/2. But then, his expected payoff\nfor \u201cExit\u201d is 2, which is greater than the payoff\nguaranteed by following the strategy he previously committed to. Thus,\nhe chooses to \u201cExit\u201d.\n\nThis problem has been discussed by a number of different\nresearchers.[4]\n It is beyond the scope of this article to\ndiscuss the intricacies of the different analyses. An entire issue\nof Games and Economic Behavior (Volume 20, 1997) was devoted\nto the analysis of this problem. For a representative sampling of the\napproaches to this problem, see Kline (2002); Aumann, Hart, &\nPerry (1997); Board (2003); Halpern (1997); Piccione & Rubinstein\n(1997b).\n1.6 Mixed Strategies\nMixed strategies play an important role in many game-theoretic\nanalyses. Let \\(\\Delta(X)\\) denote the set of probability measures over\nthe finite[5]\n set \\(X\\). A mixed strategy\nfor player \\(i\\), is an element \\(m_i\\in \\Delta(S_i)\\). If\n\\(m_i\\in\\Delta(S_i)\\) assigns probability 1 to an element \\(s_i\\in S_i\\),\nthen \\(m_i\\) is called a pure strategy (in such a case,\nI write \\(s_i\\) for \\(m_i\\)). Mixed strategies are incorporated into a\ngame-theoretic analysis as follows. Suppose that \\(G=\\langle N, \\{S_i,\nu_i\\}_{i\\in N}\\rangle\\) is a finite strategic game. The mixed\nextension of \\(G\\) is the strategic game in which the strategies\nfor player \\(i\\) are the mixed strategies in \\(G\\) (i.e., \\(\\Delta(S_i)\\))\nand the utility for player \\(i\\) (denoted \\(U_i\\)) of the joint mixed\nstrategy \\(m\\in \\Pi_{i\\in N} \\Delta(S_i)\\) is calculated in the obvious\nway (let \\(m(s)=m_1(s_1)\\cdot m_2(s_2)\\cdots m_n(s_n)\\) for \\(s\\in\n\\Pi_{i\\in N} S_i\\)):\n\n\\[U_i(m)=\\sum_{s\\in\\Pi_{i\\in N}S_i} m(s)\\cdot u_i(s)\\]\n\nThus, the solution space of a mixed extension of the game \\(G\\) is\nthe set \\(\\Pi_{i\\in N} \\Delta(S_i)\\).\nDespite their prominence in game theory, the interpretation of\nmixed strategies is controversial, as Ariel Rubinstein notes:\nWe are reluctant to believe that our decisions are\nmade at random. We prefer to be able to point to a reason for each\naction we take. Outside of Las Vegas we do not spin\nroulettes. (Rubinstein 1991: 913).\nIn epistemic game theory, it is more natural to\n work with an alternative interpretation of mixed strategies: A mixed\n strategy for player \\(i\\) is a representation of the beliefs\n of \\(i\\)\u2019s opponent(s) about what she will do. This is nicely\n explained in Robert Aumann\u2019s influential paper \n (Aumann 1987\u2014see, especially, Section 6 of this paper \n  for a discussion of this interpretation of mixed strategies):\n\nAn important feature of our approach is that it does not require\nexplicit randomization on the part of the players. Each player always\nchooses a definite pure strategy, with no attempt to randomize; the\nprobabilistic nature of the strategies reflects the uncertainties of\nother players about his choice. (Aumann 1987: 3)\n\n2. Game Models\nA model of a game is a structure that represents the\ninformational context of a given play of the game. The states, or\npossible worlds, in a game model describe a possible play of the\ngame and the specific information that influenced the\nplayers\u2019 choices (which may be different at each state). This\nincludes each player\u2019s \u201cknowledge\u201d of her own choice\nand opinions about the choices and \u201cbeliefs\u201d of her\nopponents. A key challenge when constructing a model of a game is how\nto represent the different informational attitudes of the\nplayers. Researchers interested in the foundation of decision theory,\nepistemic and doxastic logic and, more recently, formal\nepistemology have developed many different formal models that can\ndescribe the many varieties of informational attitudes important for\nassessing the choice of a rational agent in a decision- or\ngame-theoretic situation.\nAfter discussing some general issues that arise when describing the\ninformational context of a game, we introduce the two main types of\nmodels that have been used to describe the players\u2019 beliefs (and\nother informational attitudes) in a game situation: type\nspaces (Harsanyi 1967\u201368; Siniscalchi 2008) and the\nso-called Aumann- or Kripke-structures (Aumann\n1999a; Fagin, Halpern, Moses, & Vardi 1995). Although these two\napproaches have much in common, there are some important differences\nwhich we highlight below. A second, more fundamental, distinction\nfound in the literature is between \u201cquantitative\u201d\nstructures, representing \u201cgraded\u201d attitudes (typically via\nprobability distributions), and \u201cqualitative\u201d structures\nrepresenting \u201call out\u201d attitudes. Kripke structures are\noften associated with the former, and type spaces with the latter, but\nthis is not a strict classification.\n2.1 General Issues\n2.1.1 Varieties of informational attitudes \nInformational contexts of games can include various forms of\nattitudes, from the classical knowledge and belief to robust\n(Stalnaker 1994) and strong (Battigalli & Siniscalchi 2002)\nbelief, each echoing in different notions of game-theoretical\nrationality. It is beyond the scope of this article to survey the\ndetails of this vast literature (cf. the next section for some\ndiscussion of this literature). Rather, we will introduce a general\ndistinction between hard and soft attitudes,\ndistinction mainly developed in dynamic epistemic logic (van Benthem\n2011), which proves useful in understanding the various philosophical\nissues raised by epistemic game theory.\nWe call hard information, information that\nis veridical, fully introspective and not\nrevisable. This notion is intended to capture what the agents are\nfully and correctly certain of in a given interactive situation. At\nthe ex interim stage, for instance, the players have hard\ninformation about their own choice. They \u201cknow\u201d\nwhich strategy they have chosen, they know that they know this, and no\nnew incoming information could make them change their opinion on\nthis. As this phrasing suggests, the term knowledge is often\nused, in absence of better terminology, to describe this very strong\ntype of informational attitude. Epistemic logicians and game theorist\nare well aware of the possible discrepancies between such hard\n\u201cknowledge\u201d and our intuitive or even philosophical\nunderstanding of this notion. In the present context is it sufficient\nto observe that hard information shares some of the\ncharacteristics that have been attributed to knowledge in the\nepistemological literature, for instance truthfulness. Furthermore,\nhard information might come closer to what has been called\n\u201cimplicit knowledge\u201d (see Section\n5.3 below). In any case, it seems philosophically more\nconstructive to keep an eye on where the purported counter-intuitive\nproperties of hard information come into play in epistemic game\ntheory, rather than reject this notion as wrong or flawed at the\nupshot.\nSoft information is, roughly speaking, anything that is\nnot \u201chard\u201d: it is not necessarily veridical, not\nnecessarily fully introspective and/or highly revisable in the\npresence of new information. As such, it comes much closer\nto beliefs. Once again, philosophical carefulness is in order\nhere. The whole range of informational attitudes that is labeled as\n\u201cbeliefs\u201d indeed falls into the category of attitudes that\ncan be described as \u201cregarding something as true\u201d\n(Schwitzgebel 2010), among which beliefs, in the philosophical sense,\nseem to form a proper sub-category.\n2.1.2 Possible worlds models\nThe models introduced below describe the players\u2019 hard and\nsoft information in interactive situations. They differ in their\nrepresentation of a state of the world, but they can all be broadly\ndescribed as \u201cpossible worlds models\u201d familiar in much of\nthe philosophical logic literature. The starting point is a non-empty\n(finite or infinite) set \\(S\\) of states of nature describing\nthe exogenous parameters (i.e., facts about the physical\nworld) that do not depend on the agents\u2019 uncertainties. Unless\notherwise specified, \\(S\\) is the set of possible outcomes of the games,\nthe set of all strategy \nprofiles.[6]\n Each player is assumed to\nentertain a number of possibilities, called possible\nworlds or simply (epistemic) states. These\n\u201cpossibilities\u201d are intended to represent a possible way a\ngame situation may evolve. So each possibility will be associated with\na unique state of nature (i.e., there is a function from\npossible worlds to states of nature, but this function need not be\n1\u20131 or even onto). It is crucial for the analysis of rationality\nin games that there may be different possible worlds\nassociated with the same state of nature. Such possible worlds are\nimportant because they open the door to representing different state\nof information. Such state-based modeling naturally yields\na propositional view of the agents\u2019 informational\nattitudes. Agents will have beliefs/knowledge\nabout propositions, which are also called events in\nthe game-theory literature, and are represented as sets of possible\nworlds. These basic modeling choices are not uncontroversial, but such\nissues are not our concern in this entry.\n2.2 Relational Models\nWe start with the models that are familiar to philosophical\nlogicians (van Benthem 2010) and computer scientists (Fagin et\nal. 1995). These models were introduced to game theory by Robert\nAumann (1976) in his seminal paper Agreeing to Disagree (see\nVanderschraaf & Sillari 2009, Section 2.3, for a discussion of\nthis result). First, some terminology: Given a set \\(W\\) of states, or\npossible worlds, let us call any subset \\(E\\subseteq W\\)\nan event or proposition. Given events \\(E\\subseteq W\\)\nand \\(F\\subseteq W\\), we use standard set-theoretic notation for\nintersection (\\(E\\cap F\\), read \u201c\\(E\\) and \\(F\\)\u201d), union\n(\\(E\\cup F\\), read \u201c\\(E\\) or \\(F\\)\u201d) and (relative) complement\n(\\(-{E}\\), read \u201cnot \\(E\\)\u201d). We say that an event \\(E\\subseteq\nW\\) occurs at state \\(w\\) if \\(w\\in E\\). This terminology will be crucial\nfor studying the following models.\n\n\nDefinition 2.1 (Epistemic Model)\n Suppose that \\(G\\) is a strategic game, \\(S\\) is the set of strategy\nprofiles of \\(G\\), and \\(\\A\\) is the set of players. An epistemic\nmodel based on \\(S\\) and \\(\\Agt\\) is a triple \\(\\langle\nW,\\{\\Pi_i\\}_{i\\in\\Agt},\\sigma\\rangle\\), where \\(W\\) is a nonempty set,\nfor each \\(i\\in\\Agt\\), \\(\\Pi_i\\) is a\npartition[7]\n over \\(W\\) and \\(\\sigma:W\\rightarrow S\\).\n\n\nEpistemic models represent the informational context of a given\ngame in terms of possible configurations of states of the game and the\nhard information that the agents have about them. The function\n\\(\\sigma\\) assigns to each possible world a unique state of the game in\nwhich every ground fact is either true or false. If \\(\\sigma(w) =\n\\sigma(w')\\) then the two worlds \\(w,w'\\) will agree on all the ground\nfacts (i.e., what actions the players will choose) but, crucially, the\nagents may have different information in them. So, elements of \\(W\\)\nare richer, than the elements of \\(S\\) (more on this\nbelow).\nGiven a state \\(w\\in W\\), the cell \\(\\Pi_i(w)\\) is called agent\n\\(i\\)\u2019s information set. Following standard terminology,\nif \\(\\Pi_i(w)\\subseteq E\\), we say the agent \\(i\\) knows the\nevent \\(E\\) at state \\(w\\). Given an event \\(E\\), the event that agent \\(i\\)\nknows \\(E\\) is denoted \\(K_i(E)\\). Formally, we define for each agent \\(i\\)\na knowledge function assigning to every event \\(E\\) the event that the\nagent \\(i\\) knows \\(E\\):\n\nDefinition 2.2 (Knowledge Function) \nLet \\(\\M=\\langle W,\\{\\Pi_i\\}_{i\\in\\A},\\sigma\\rangle\\) be an epistemic\nmodel. The knowledge function for agent \\(i\\) based on\n\\(\\M\\) is \\(K_i:\\pow(W)\\rightarrow\\pow(W)\\) with:\n\n\\[K_i(E)=\\{w \\mid \\Pi_i(w)\\subseteq E\\}\\]\n\nwhere for any set \\(X\\), \\(\\pow(X)\\) is the powerset of \\(X\\).\n\n\nRemark 2.3\nIt is often convenient to\nwork with equivalence relations rather than partitions. In\nthis case, an epistemic model based on \\(S\\) and \\(\\Agt\\) can also be\ndefined as a triple \\(\\langle W,\\{\\sim_i\\}_{i\\in\\Agt},\\sigma \\rangle\\)\nwhere \\(W\\) and \\(\\sigma\\) are as above and for each \\(i\\in\\Agt\\),\n\\(\\sim_i\\subseteq W\\times W\\) is reflexive, transitive and\nsymmetric. Given such a model \\(\\langle W,\n\\{\\sim_i\\}_{i\\in\\Agt},\\sigma\\rangle\\), we write \n\n\\[[w]_i=\\{v\\in W \\mid w\\sim_i v\\}\\] \n\nfor the equivalence class of \\(w\\). Since there is a\n1\u20131 correspondence between equivalence relations and\npartitions,[8]\n we will abuse notation and use \\(\\sim_i\\) and\n\\(\\Pi_i\\) interchangeably.\n\nApplying the above remark, an alternative definition of \\(K_i(E)\\) is\nthat \\(E\\) is true in all the states the agent \\(i\\) considers possible\n(according to \\(i\\)\u2019s hard information). That is, \\(K_i(E)=\\{w\\mid\n[w]_i\\subseteq E\\}\\).\n\nPartitions or equivalence relations are intended to represent the\nagents\u2019 hard information at each state. It is\nwell-known that the knowledge operator satisfies the properties of the\nepistemic logic \\(\\mathbf{S5}\\) (see Hendricks & Symons 2009 for a\ndiscussion). We do not discuss this and related issues here and\ninstead focus on how these models can be used to provide the\ninformational context of a game.\nAn Example.  Consider the following coordination\n game between Ann (player 1) and Bob (player 2). As is well-known,\n there are two pure-strategy Nash equilibrium (\\((u,l)\\) and\n \\((d,r)\\)).\n\n\n \u00a0 Bob \nAnn \n\nl r\nu 3,3  0,0 \nd 0,0  1,1 \n\n\n\nFigure 4: A strategic coordination\ngame between Ann and Bob\n\nThe utilities of the players are not important for us at this\nstage. To construct an epistemic model for this game, we need first to\nspecify what are the states of nature we will consider. For\nsimplicity, take them to be the set of strategy profiles\n\\(S=\\{(u,l),(d,l),(u,r),(d,l)\\}\\). The set of agents is of course\n\\(\\Agt=\\{A,B\\}\\). What will be the set of states \\(W\\)? We start by\nassuming \\(W=S\\), so there is exactly one possible world corresponding\nto each state of nature. This needs not be so, but here this will help\nto illustrate our point.\nThere are many different partitions for Ann and Bob that we can use\nto complete the description of this simple epistemic model. Not all of\nthe partitions are appropriate for analyzing the ex interim\nstage of the decision-making process, though. For example, suppose\n\\(\\Pi_{A}=\\Pi_{B}=\\{W\\}\\) and consider the event \\(U=\\{(u,l),(u,r)\\}\\)\nrepresenting the situation where Ann chooses \\(u\\). Notice that\n\\(K_A(U)=\\emptyset\\) since for all \\(w\\in W\\), \\(\\Pi_A(w)\\not\\subseteq U\\),\nso there is no state where Ann knows that she chooses\n\\(u\\). This means that this model is appropriate for reasoning about\nthe ex ante stage rather than the ex interim\nstage. This is easily fixed with an additional technical assumption:\nSuppose \\(S\\) is a set of strategy profiles for some (strategic or\nextensive) game with players \\(\\A=\\{1,\\ldots,n\\}\\).\nA model \\(\\M=\\langle W, \\{\\Pi_i\\}_{i\\in \\A},\\sigma \\rangle\\) is said\nto be an ex interim epistemic model if for\nall \\(i\\in\\Agt\\) and \\(w,v\\in W\\), if \\(v\\in\\Pi_i(w)\\) then\n\\(\\sigma_i(w)=\\sigma_i(v)\\)\nwhere \\(\\sigma_i(w)\\) is the \\(i\\)th component of the\nstrategy profile \\(s\\in S\\) assigned to \\(w\\) by \\(\\sigma\\). An example of\nan ex interim epistemic model with states \\(W\\) is:\n\n\\(\\Pi_A=\\{\\{(u,l),(u,r)\\},\\{(d,l),(d,r)\\}\\}\\) and\n\\(\\Pi_B=\\{\\{(u,l),(d,l)\\},\\{(u,r),(d,r)\\}\\}\\).\n\nNote that this simply reinterprets the game matrix\nin Figure 1 as an epistemic model\nwhere the rows are Ann\u2019s information sets and the columns are\nBob\u2019s information sets. Unless otherwise stated, we will always\nassume that our epistemic models are ex interim. The class\nof ex interim epistemic models is very rich with models\ndescribing the (hard) information the agents have about their own\nchoices, the (possible) choices of the other players and\nhigher-order (hard) information (e.g., \u201cAnn knows that Bob knows\nthat\u2026\u201d) about these decisions.\nWe now look at the epistemic model described above in more\ndetail. We will often use the following diagrammatic representation of\nthe model to ease exposition. States are represented by nodes in a\ngraph where there is a (undirected) edge between states \\(w_i\\) and\n\\(w_j\\) when \\(w_i\\) and \\(w_j\\) are in the same partition cell. We use a\nsolid line labeled with \\(A\\) for Ann\u2019s partition and a dashed\nline labeled with \\(B\\) for Bob\u2019s partition (reflexive edges are\nnot represented for simplicity). The event \\(U=\\{w_1,w_3\\}\\)\nrepresenting the proposition \u201cAnn decided to choose option\n\\(u\\)\u201d is the shaded gray region:\n\n\nFigure 5\n\nNotice that the following events are true at all states:\n\n\\(- K_B(U)\\): \u201cBob does not know that Ann decided to choose\n\\(u\\)\u201d\n\\(K_B(K_A(U)\\vee K_A(-U))\\): \u201cBob knows that Ann knows whether\nshe has decided to choose \\(u\\)\u201d\n\\(K_A(-K_B(U))\\): \u201cAnn knows that Bob does not know that she\nhas decided to choose \\(u\\)\u201d\n\nIn particular, these events are true at state \\(w_1\\) where Ann has\ndecided to choose \\(u\\) (i.e., \\(w_1\\in U\\)). The first event makes sense\ngiven the assumptions about the available information at the ex\ninterim stage: each player knows their own choice but not the\nother players\u2019 choices. The second event is a concrete example\nof another assumption about the available information: Bob has the\ninformation that Ann has, in fact, made some choice. But what\nwarrants Ann to conclude that Bob does not know she has chosen \\(u\\)\n(the third event)? This is a much more significant statement about\nwhat Ann knows about what Bob expects her to do. Indeed, in certain\ncontexts, Ann may have very good reasons to think it is possible that\nBob actually knows she will choose \\(u\\). We can find an ex\ninterim epistemic model where this event (\\(-K_A(-K_B(U))\\)) is\ntrue at \\(w_1\\), but this requires adding a new possible world:\n\n\nFigure 6\n\nNotice that since \\(\\Pi_B(w')=\\{\\{w'\\}\\}\\subseteq U\\) we have \\(w'\\in\nK_B(U)\\). That is, Bob knows that Ann chooses \\(u\\) at state\n\\(w'\\). Finally, a simple calculation shows that \\(w_1\\in -K_A(-K_B(U))\\),\nas desired. Of course, we can question the other substantive\nassumptions built-in to this model (e.g., at \\(w_1\\), Bob knows\nthat Ann does not know he will choose \\(L\\)) and continue modifying the\nmodel. This raises a number of interesting conceptual and technical\nissues which we discuss in Section 7.\n2.2.1 Adding Beliefs\nSo far we have looked at relational models of hard information. A\nsmall modification of these models allows us to model a softer\ninformational attitude. Indeed, by simply replacing the assumption of\nreflexivity of the relation \\(\\sim_i\\) with seriality (for each state\n\\(w\\) there is a state \\(v\\) such that \\(w\\sim_i v\\)), but keeping the other\naspects of the model the same, we can capture what epistemic logicians\nhave called \u201cbeliefs\u201d. Formally,\na doxastic model is a tuple \\(\\langle W,\n\\{R_i\\}_{i\\in\\Agt},V\\rangle\\) where \\(W\\) is a nonempty set of states,\n\\(R_i\\) is a transitive, Euclidean and serial relation on \\(W\\) and \\(V\\) is\na valuation function (cf. Definition 2.1). This\nnotion of belief is very close to the above hard informational\nattitude and, in fact, shares all the properties of \\(K_i\\) listed above\nexcept Veracity (this is replaced with a weaker assumption\nthat agents are \u201cconsistent\u201d and so cannot believe\ncontradictions). This points to a logical analysis of both\ninformational attitudes with various \u201cbridge principles\u201d\nrelating knowledge and belief (such as knowing something implies\nbelieving it or if an agent believes \\(\\phi\\) then the agent knows that\nhe believes it). However, we do not discuss this line of research here\nsince these models are not our preferred ways of representing the\nagents\u2019 soft information (see, for example, Halpern 1991 and\nStalnaker 2006).\nPlausibility Orderings\nA key aspect of beliefs which is not yet represented in the above\nmodels is that they are revisable in the presence of new\ninformation. While there is an extensive literature on the theory of\nbelief revision in the \u201cAGM\u201d style (Alchourr\u00f3n,\nG\u00e4rdenfors, & Makinson 1985), we focus on how to extend an\nepistemic model with a representation of softer, revisable\ninformational attitudes. The standard approach is to include\na plausibility ordering for each agent: a preorder (reflexive\nand transitive) denoted \\(\\preceq_i\\,\\subseteq W\\times W\\). If\n\\(w\\preceq_i v\\) we say \u201cplayer \\(i\\) considers \\(w\\) at least as\nplausible as \\(v\\).\u201d For an event \\(X\\subseteq W\\), let\n\n\\[\nMin_{\\preceq_i}(X)=\\{v\\in W\\ |\\ v\\preceq_i w \\text{ for all \\(w\\in X\\) }\\}\n\\]\n\ndenote the set of minimal elements of \\(X\\) according to\n\\(\\preceq_i\\). Thus while the \\(\\sim_i\\) partitions the set of possible\nworlds according to the agents\u2019 hard information, the\nplausibility ordering \\(\\preceq_i\\) represents which of the possible\nworlds the agent considers more likely (i.e., it represents the\nplayers soft information).\n\nDefinition 2.4 (Epistemic-Plausibility Models)\nSuppose\nthat \\(G\\) is a strategic game, \\(S\\) is the set of strategy profiles of\n\\(G\\), and \\(\\A\\) is the set of players. An epistemic-plausibility\nmodel is a tuple \\(\\langle W,\\{\\Pi_i\\}_{i\\in \\Agt},\n\\{\\preceq_i\\}_{i\\in \\Agt},\\sigma\\rangle\\) where \\(\\kripkemodel\\) is an\nepistemic model, \\(\\sigma:W\\rightarrow S\\) and for each \\(i\\in\\Agt\\),\n\\(\\preceq_i\\) is a \nwell-founded,[9]\n reflexive and transitive\nrelation on \\(W\\) satisfying the following properties, for all \\(w,v\\in\nW\\)\n\nplausibility implies possibility: if \\(w\\preceq_i v\\) then\n\\(v\\in \\Pi_i(w)\\).\nlocally-connected: if \\(v\\in \\Pi_i(w)\\) then either\n\\(w\\preceq_i v\\) or \\(v\\preceq_i w\\).\n\n\n\n Remark 2.5\n Note that if \\(v\\not\\in\\Pi_i(w)\\) then\n\\(w\\not\\in \\Pi_i(v)\\). Hence, by property 1, \\(w\\not\\preceq_i v\\) and\n\\(v\\not\\preceq_i w\\). Thus, we have the following equivalence: \\(v\\in\n\\Pi_i(w)\\) iff \\(w\\preceq_i v\\) or \\(v\\preceq_i w\\).\n\nLocal connectedness implies that \\(\\preceq_i\\) totally orders\n\\(\\Pi_i(w)\\) and well-foundedness implies that\n\\(Min_{\\preceq_i}(\\Pi_i(w))\\) is nonempty. This richer model allows us\nto formally define a variety of (soft) informational attitudes. We\nfirst need some additional notation: the plausibility relation\n\\(\\preceq_i\\) can be lifted to subsets of \\(W\\) as\nfollows[10]\n\n\\[\nX\\preceq_i Y\\text{ iff \\(x\\preceq_i y\\) for all \\(x\\in X\\) and \\(y\\in Y\\) }\n\\]\n\nSuppose \\(\\M=\\plausmodel\\) is an epistemic-plausibility model,\nconsider the following operators (formally, each is a function from\n\\(\\pow(W)\\) to \\(\\pow(W)\\) similar to the knowledge operator defined\nabove):\n\nBelief: \\(B_i(E)=\\{w \\mid\nMin_{\\preceq_i}(\\Pi_i(w))\\subseteq E\\}\\) This is the usual notion\nof belief which satisfies the standard properties discussed above\n(e.g., consistency, positive and negative introspection).\nRobust Belief: \\(B_i^r(E)=\\{w \\mid v\\in E, \\mbox{ for all }\nv \\mbox{ with } w \\preceq_i v\\}\\)  So, \\(E\\) is robustly believed\nif it is true in all worlds more plausible then the current\nworld. This stronger notion of belief has also been\ncalled certainty by some authors (cf. Shoham &\nLeyton-Brown 2008: sec. 13.7).\nStrong Belief: \n\n\\[\nB_i^s(E)=\\{w \\mid E \\cap \\Pi_i (w) \\neq \\emptyset \\text{ and }\nE \\cap \\Pi_i (w) \\preceq_{i} - E \\cap \\Pi_i(w)\\}\n\\] \n\nSo, \\(E\\) is strongly believed provided it is epistemically possible and\nagent \\(i\\) considers any state in \\(E\\) more plausible\nthan any state in the complement of \\(E\\).\n\nIt is not hard to see that if agent \\(i\\) knows that \\(E\\) then \\(i\\)\n(robustly, strongly) believes that \\(E\\). However, much more can be said\nabout the logical relationship between these different notions. (The\nlogic of these notions has been extensively studied by Alexandru\nBaltag and Sonja Smets in a series of articles, see Baltag & Smets\n2009 in Other Internet Resources for references.)\nAs noted above, a crucial feature of these informational attitudes\nis that they may be defeated by appropriate evidence. In fact, we can\ncharacterize these attitudes in terms of the type of evidence which\ncan prompt the agent to adjust her beliefs. To make this precise, we\nintroduce the notion of a conditional belief: suppose\n\\(\\M=\\plausmodel\\) is an epistemic-plausibility model and \\(E\\) and \\(F\\)\nare events, then the conditional belief operator is\ndefined as follows:\n\\[\nB_i^F(E)=\\{w \\mid Min_{\\preceq_i}(F\\cap\\Pi_i(w))\\subseteq E\\}\n\\]\n\nSo, \u2018\\(B_i^F\\)\u2019 encodes what agent \\(i\\) will believe upon\nreceiving (possibly misleading) evidence that \\(F\\)\nis true.\nWe conclude this section with an example to illustrate the above\nconcepts. Recall again the coordination game\nof Figure 4: there are two actions for\nplayer 1 (Ann), \\(u\\) and \\(d\\), and two actions for player 2 (Bob), \\(r\\)\nand \\(l\\). Again, the preferences (or utilities) of the players are not\nimportant at this stage since we are only interested in describing the\nplayers\u2019 information. The following epistemic-plausibility model\nis a possible description of the players\u2019 informational\nattitudes that can be associated with this game. The solid lines\nrepresent player 1\u2019s informational attitudes and the dashed line\nrepresents player 2\u2019s. The arrows correspond to the players\nplausibility orderings with an \\(i\\)-arrow from \\(w\\) to \\(v\\) meaning\n\\(v\\preceq_i w\\) (we do not draw all the arrows: each plausibility\nordering can be completed by filling in arrows that result from\nreflexivity and transitivity). The different regions represent the\nplayers\u2019 hard information.\n\n\nFigure 7\n\nSuppose that the actual state of play is \\(w_4\\). So, player 1 (Ann)\nchooses \\(u\\) and player 2 (Bob) chooses \\(r\\). Further, suppose that \\(L=\n\\{w_1,w_2,w_5\\}\\) is the event where where player 2 chooses \\(l\\)\n(similarly for \\(U\\), \\(D\\), and \\(R\\))\n\n\\(B_1(L)\\): \u201cplayer 1 believes that player 2 is choosing\n\\(L\\)\u201d\n\\(B_1(B_2(U))\\): \u201cplayer 1 believes that player 2 believes that\nplayer 1 chooses \\(u\\)\u201d\n\\(B_1^{R} (- B_2(U))\\): \u201cgiven that player 2 chooses \\(r\\),\nplayer 1 believes that player 2 does not believe she is choosing\n\\(u\\)\u201d\n\nThis last formula is interesting because it\n\u201cpre-encodes\u201d what player 1 would believe upon learning\nthat player 2 is choosing \\(R\\). Note that upon receiving\nthis true information, player 1 drops her belief that player\n2 believes she is choosing \\(u\\). The situation can be even more\ninteresting if there are statements in the language that reveal\nonly partial information about the player strategy\nchoices. Suppose that \\(E\\) is the event \\(\\{w_4,w_6\\}\\). Now \\(E\\) is true\nat \\(w_4\\) and player 2 believes that player 1 chooses \\(d\\)\ngiven that \\(E\\) is true (i.e., \\(w_4\\in B_2^E(D)\\)). So, player 1 can\n\u201cbluff\u201d by revealing the true (though partial) information\n\\(E\\).\nProbabilities\nThe above models use a \u201ccrisp\u201d notion of uncertainty,\ni.e., for each agent and state \\(w\\), any other state \\(v\\in W\\) either\nis or is not possible at/more plausible than \\(w\\). However, there is an\nextensive body of literature focused on graded,\nor quantitative, models of uncertainty (Huber 2009; Halpern\n2003). For instance, in the Game Theory literature it is standard to\nrepresent the players\u2019 beliefs by probabilities (Aumann\n1999b; Harsanyi 1967\u201368). The idea is simple: replace the\nplausibility orderings with probability distributions:\n\nDefinition 2.6 (Epistemic-Probability Model)\nSuppose that \\(G\\) is a strategic game, \\(S\\) is the set of\nstrategy profiles of \\(G\\), and \\(\\A\\) is the set of\nplayers. An epistemic-probabilistic model is a tuple\n\n\\[\n\\M=\\langle W,\\{\\sim_i\\}_{i\\in\\Agt},\\{P_i\\}_{i\\in\\Agt},\\sigma\\rangle\n\\] \n\nwhere \\(\\kripkemodel\\) is an epistemic model and\n\n\\begin{align}\n& P_i:W\\rightarrow \\Delta(W) \\\\\n& \\Delta(W)=\\{p:W\\rightarrow [0,1] \\mid p \\text{ is a probability measure}\\}\n\\end{align}\n\nassigns to each state a probability measure over\n\\(W\\). Write \\(p_i^w\\) for the \\(i\\)\u2019s probability measure at state\n\\(w\\). We make two natural assumptions\n(cf. Definition 2.4):\n\nFor all \\(v\\in W\\), if \\(p_i^w(v)>0\\) then \\(p_i^w=p_i^v\\);\nand\nFor all \\(v\\not\\in\\Pi_i(w)\\), \\(p_i^w(v)=0\\).\n\n\nProperty 1 says that if \\(i\\) assigns a non-zero probability to state\n\\(v\\) at state \\(w\\) then the agent uses the same probability measure at\nboth states. This means that the players \u201cknow\u201d their own\nprobability measures. The second property implies that players must\nassign a probability of zero to all states outside the current (hard)\ninformation cell. These models provide a very precise description of\nthe players\u2019 hard and soft informational attitudes. However,\nnote that writing down a model requires us to specify a different\nprobability measure for each partition cell which can be quite\ncumbersome. Fortunately, the properties in the above definition imply\nthat, for each agent, we can view the agent\u2019s probability\nmeasures as arising from one probability measure through\nconditionalization. Formally, for each \\(i\\in\\Agt\\), agent\n\\(i\\)\u2019s (subjective) prior probability is any\nelement of \\(p_i\\in\\Delta(W)\\). Then, in order to define an\nepistemic-probability model we need only give for each agent\n\\(i\\in\\Agt\\), (1) a prior probability \\(p_i\\in\\Delta(W)\\) and (2) a\npartition \\(\\Pi_i\\) on \\(W\\) such that for each \\(w\\in W\\),\n\\(p_i(\\Pi_i(w))>0\\). The probability measures for each \\(i\\in\\Agt\\) are\nthen defined by:\n\n\\[\nP_i(w)= p_i(\\cdot \\mid \\Pi_i(w)) = \\frac{p_i(\\cdot\\cap \\Pi_i(w))}{p_i(\\Pi_i(w))}\n\\]\n\nOf course, the side condition that for each \\(w\\in W\\),\n\\(p_i(\\Pi_i(w))>0\\) is important since we cannot divide by\nzero\u2014this will be discussed in more detail in later\nsections. Indeed, (assuming \\(W\\) is \nfinite[11]) given any\nepistemic-plausibility model we can find, for each agent, a prior\n(possibly different ones for different agents) that generates the\nmodel as described above. This is not only a technical observation: it\nmeans that we are assuming that the players\u2019 beliefs about the\noutcome of the situation are fixed ex ante with the ex\ninterim beliefs being derived through conditionalization on the\nagent\u2019s hard information. (See Morris 1995 for an\nextensive discussion of the situation when there is a common\nprior.) We will return to these key assumptions throughout the\ntext.\nAs above we can define belief operators, this time specifying the\nprecise degree to which an agent believes an event:\n\nProbabilistic belief: \\(B_i^r(E)=\\{w \\mid\np_i^w(E)=r\\}\\) Here, \\(r\\) can be any real number in the unit\ninterval; however, it is often enough to restrict attention to the\nrational numbers in the unit interval.\nFull belief: \\(B_i(E)=B_i^1(E)=\\{w \\mid p_i^w(E)=1\\}\\)\nSo, full belief is defined to belief with probability one. This is a\nstandard assumption in this literature despite a number of well-known\nconceptual difficulties (see Huber 2009 for an extensive discussion of\nthis and related issues). It is sometimes useful to work with the\nfollowing alternative characterization of full-belief (giving it a\nmore \u201cmodal\u201d flavor): Agent \\(i\\) believes \\(E\\) at state \\(w\\)\nprovided all the states that \\(i\\) assigns positive probability to at\n\\(w\\) are in \\(E\\). Formally,\n\\[\nB_i(E)=\\{w \\mid \\text{for all \\(v\\), if \\(p_i^w(v)>0\\) then \\(v\\in E\\)}\\}\n\\]\n\nThese models have also been subjected to sophisticated logical\nanalyses (Fagin, Halpern, & Megiddo 1990; Heifetz & Mongin\n2001) complementing the logical frameworks discussed above (cf. Baltag\n& Smets 2006).\nWe conclude this section with an example of an\nepistemic-probability model. Recall again the coordination game\nof Figure 4: there are two actions for\nplayer 1 (Ann), \\(u\\) and \\(d\\), and two actions for player 2 (Bob), \\(r\\)\nand \\(l\\). The preferences (or utilities) of the players are not\nimportant at this stage since we are only interested in describing the\nplayers\u2019 information.\n\n\nFigure 8\n\nThe solid lines are Ann\u2019s information partition and the\ndashed lines are Bob\u2019s information partition. We further assume\nthere is a common prior \\(p_0\\) with the probabilities assigned to each\nstate written to the right of the state. Let \\(E=\\{w_2,w_5,w_6\\}\\) be an\nevent. Then, we have\n\n\\(B_1^{\\frac{1}{2}}(E)=\\{w \\mid p_0(E \\mid\n\\Pi_1(w))=\\frac{p_0(E\\cap\\Pi_1(w))}{p_0(\\Pi_1(w))}=\\frac{1}{2}\\}=\\{w_1,w_2,w_3\\}\\):\n\u201cAnn assigns probability 1/2 to the event \\(E\\) given her\ninformation cell \\(\\Pi_1(w_1)\\).\n\\(B_2(E)=B_2^1(E)=\\{w_2,w_5,w_3,w_6\\}\\). In particular, note that at\n\\(w_6\\), the agent believes (with probability 1) that \\(E\\) is true, but\ndoes not know that \\(E\\) is true as \\(\\Pi_2(w_6)\\not\\subseteq\nE\\). So, there is a distinction between states the agent considers\npossible (given their \u201chard information\u201d) and states to\nwhich players assign a non-zero probability.\nLet \\(U=\\{w_1,w_2,w_3\\}\\) be the event that Ann plays \\(u\\) and\n\\(L=\\{w_1,w_4\\}\\) the event that Bob plays \\(l\\). Then, we have\n\n\\(K_1(U)=U\\) and \\(K_2(L)=L\\): Both Ann and Bob know that strategy they\nhave chosen;\n\\(B_1^{\\frac{1}{2}}(L)=U\\): At all states where Ann plays \\(u\\), Ann\nbelieves that Bob plays \\(L\\) with probability 1/2; and\n\\(B_1(B_2^{\\frac{1}{2}}(U))=\\{w_1,w_2,w_3\\}=U\\): At all states where\nAnn plays \\(u\\), she believes that Bob believes with probability 1/2\nthat she is playing \\(u\\).\n\n\n2.3 Harsanyi Type Spaces\nAn alternative approach to modeling beliefs was initiated by\nHarsanyi in his seminal paper (Harsanyi 1967\u201368). Rather than\n\u201cpossible worlds\u201d, Harsanyi takes the notion of the\nplayers\u2019 type as primitive. Formally, the players are\nassigned a nonempty set of types. Typically, players are assumed\nto know their own type but not the types of the other\nplayers. As we will see, each type can be associated with a specific\nhierarchy of belief\n\n  Definition 2.7 (Qualitative Type Space)\n A Qualitative type space for a\n(nonempty) set of states of nature \\(S\\) and agents \\(\\Agt\\) is a tuple\n\\(\\langle \\{T_i\\}_{i\\in\\A}, \\{\\lambda_i\\}_{i\\in\\Agt},S\\rangle\\) where\nfor each \\(i\\in\\Agt\\), \\(T_i\\) is a nonempty set and\n\\[\n \\lambda_i:T_i\\rightarrow \\pow(\\bigtimes_{j\\ne i} T_j\\times S). \n\\]\n\nSo, each type \\(t\\in T_i\\) is associated with a set of tuples\nconsisting of types of the other players and a state of nature. For\nsimplicity, suppose there are only two players, Ann and\nBob. Intuitively, \\((t',o')\\in\\lambda_{Ann}(t)\\) means that Ann\u2019s\ntype \\(t\\) considers it possible that the outcome is \\(o'\\) and\nBob is of type \\(t'\\). Since the players\u2019 uncertainty is directed\nat the choices and types of the other players, the\ninformational attitude captured by these models will certainly not\nsatisfy the Truth axiom. In fact, qualitative type spaces can be\nviewed as simply a \u201cre-packaging\u201d of the relational models\ndiscussed above (cf. Zvesper 2010 for a discussion).\nConsider again the running example of the coordination game between\nAnn and Bob (pictured in Figure 1). In\nthis case, the set of states of nature is\n\\(S=\\{(u,l),(d,l),(u,r),(d,r)\\}\\). In this context, it is natural to\nmodify the definition of the type functions \\(\\lambda_i\\) to account for\nthe fact that the players are only uncertain about the other\nplayers\u2019 choices: let \\(S_A=\\{u,d\\}\\) and \\(S_B=\\{l,r\\}\\) and\nsuppose \\(T_A\\) and \\(T_B\\) are nonempty sets of types. Define \\(\\lambda_A\\)\nand \\(\\lambda_B\\) as follows:\n\\[\n\\lambda_A:T_A\\rightarrow \\pow(T_B\\times S_B)\\hspace{.5in}\\lambda_B:T_B\\rightarrow\\pow(T_A\\times S_A)\n\\]\n\nSuppose that there are two types for each player:\n\\(T_A=\\{t^A_1,t^A_2\\}\\) and \\(T_B=\\{t^B_1,t^B_2\\}\\). A convenient way to\ndescribe the maps \\(\\lambda_A\\) and \\(\\lambda_B\\) is:\n\n\n\\(\\lambda_A (T^{A}_{1})\\)\n\n\nl r\n\\(t_1^B\\) 1  0 \n\\(t_2^B\\) 1  0 \n\n\u00a0\u00a0\u00a0\u00a0\u00a0\n\\(\\lambda_A (T^{A}_{2})\\)\n\n\nl r\n\\(t_1^B\\) 0  0 \n\\(t_2^B\\) 1  0 \n\n\\(\\lambda_B (T^{B}_{1})\\)\n\n\nu d\n\\(t_1^A\\) 1  0 \n\\(t_2^A\\) 0  0 \n\n\u00a0\n\\(\\lambda_B (T^{B}_{2})\\)\n\n\nu d\n\\(t_1^A\\) 0  0 \n\\(t_2^A\\) 0  1 \n\n\nFigure 9\n\nwhere a 1 in the \\((t',s)\\) entry of the above matrices corresponds\nto assuming \\((t',s)\\in\\lambda_i(t)\\) (\\(i=A,B\\)). What does it mean\nfor Ann (Bob) to believe an event \\(E\\) in a type structure?\nWe start with some intuitive observations about the above type\nstructure:\n\nRegardless of what type we assign to Ann, she believes that Bob\nwill choose \\(l\\) since in both matrices, \\(\\lambda_A(t_1^A)\\) and\n\\(\\lambda_A(t_2^A)\\), the only places where a 1 appears is under the \\(l\\)\ncolumn. So, fixing a type for Ann, in all of the situations Ann\nconsiders possible it is true that Bob chooses \\(l\\).\nIf Ann is assigned the type \\(t_1^A\\), then she considers it possible\nthat Bob believes she will choose \\(u\\). Notice that type \\(t_1^A\\) has a\n1 in the row labeled \\(t^B_1\\), so she considers it possible that Bob is\nof type \\(t^B_1\\), and type \\(t^B_1\\) believes that Ann chooses \\(u\\) (the\nonly places where 1 appears is under the \\(u\\) column).\nIf Ann is assigned the type \\(t_2^A\\), then Ann believes that Bob\nbelieves that Ann believes that Bob will choose \\(l\\). Note that type\n\\(t_2^A\\) \u201cbelieves\u201d that Bob will choose \\(l\\) and\nfurthermore \\(t_2^A\\) believes that Bob is of type \\(t^B_2\\) who in turn\nbelieves that Ann is of type \\(t^A_2\\).\n\nWe can formalize the above informal observations using the\nfollowing notions: Fix a qualitative type space \\(\\langle\n\\{T_i\\}_{i\\in\\A}, \\{\\lambda_i\\}_{i\\in\\Agt},S\\rangle\\) for a (nonempty)\nset of states of nature \\(S\\) and agents \\(\\Agt\\).\n\nA (global) state, or possible\nworld is a tuple \\((t_1,t_2,\\ldots,t_n,s)\\) where \\(t_i\\in T_i\\)\nfor each \\(i=1,\\ldots,n\\) and \\(s\\in S\\). If \\(S=\\bigtimes S_i\\) is the set\nof strategy profiles for some game, then we write a possible world as:\n\\((t_1,s_1,t_2,s_2,\\ldots,t_n,s_n)\\) where \\(s_i\\in S_i\\) for each\n\\(i=1,\\ldots,n\\).\nType spaces describe the players beliefs about the other\nplayers\u2019 choices, so the notion of an event needs to be\nrelativized to an agent. An event for agent \\(i\\) is a\nsubset of \\(\\bigtimes_{j\\ne i}T_j\\times S\\). Again if \\(S\\) is a set of\nstrategy profiles (so \\(S=\\bigtimes S_i\\)), then an event for agent \\(i\\)\nis a subset of \\(\\bigtimes_{j\\ne i} (T_j\\times S_j)\\).\nSuppose that \\(E\\) is an event for agent \\(i\\), then we say that\nagent \\(i\\) believes \\(E\\) at \\((t_1,t_2,\\ldots,t_n,s)\\)\nprovided \\(\\lambda(t_1,s)\\subseteq E\\).\n\nIn the specific example above, an event for Ann is a set\n\\(E\\subseteq T_B\\times S_B\\) and we can define the set of pairs\n\\((t^A,s^A)\\) that believe this event: \n\\[\nB_A(E)=\\{(t^A,s^A) \\mid \\lambda_A(t^A,s^A)\\subseteq E\\}\n\\]\n\nsimilarly for Bob. Note that the event \\(B_A(E)\\) is an event for Bob\nand vice versa. A small change to the above definition of a\ntype space (Definition 2.7) allows us to\nrepresent probabilistic beliefs (we give the full definition\nhere for future reference):\n\nDefinition 2.8 (Type Space)\nA type space for a (nonempty) set of states of\nnature \\(S\\) and agents \\(\\Agt\\) is a tuple \\(\\langle \\{T_i\\}_{i\\in\\A},\n\\{\\lambda_i\\}_{i\\in\\Agt},S\\rangle\\) where for each \\(i\\in\\Agt\\), \\(T_i\\) is\na nonempty set and\n\\[\n\\lambda_i:T_i\\rightarrow \\Delta(\\bigtimes_{j\\ne i} T_j\\times S).\n\\] \n\nwhere \\(\\Delta(\\bigtimes_{j\\ne i} T_j\\times S)\\) is the set of\nprobability measures on \\(\\bigtimes_{j\\ne i} T_j\\times S\\).\n\nTypes and their associated image under \\(\\lambda_i\\) encode the\nplayers\u2019 (probabilistic) information about the others\u2019\ninformation. Indeed, each type is associated with a hierarchy of\nbelief. More formally, recall that an event \\(E\\) for a type \\(t_i\\) is a\nset of pairs \\((\\sigma_{-j}, t_{-j})\\), i.e., a set of strategy choices\nand types for all the other players. Given an event \\(E\\) for player\n\\(i\\), let \\(\\lambda_i(t_i)(E)\\) denote the sum of the probabilities that\n\\(\\lambda_i(t_i)\\) assigns to the elements of \\(E\\). The type \\(t_i\\) of\nplayer \\(i\\) is said to (all-out) believe the event \\(E\\)\nwhenever \\(\\lambda_i(t_i)(E) = 1\\). Conditional beliefs are computed in\nthe standard way: type \\(t_i\\) believes that \\(E\\) given \\(F\\) whenever:\n\\[\n\\frac{\\lambda_i(t_i)(E \\cap F)}{\\lambda_i(t_i)(F)} = 1\n\\]\n\n\nA state in a type structure is a tuple \\((\\sigma, t)\\) where\n\\(\\sigma\\) is a strategy profile and \\(t\\) is \u201ctype profile\u201d,\na tuple of types, one for each player. Let \\(B_i(E) = \\{(\\sigma_{-j},\nt_{-j}) : t_i \\text{ believes that } E \\}\\) be the event (for \\(j\\)) that\n\\(i\\) believes that \\(E\\). Then agent \\(j\\) believes that \\(i\\) believes that\n\\(E\\) when \\(\\lambda_j(t_j)(B_i(E)) = 1\\). We can continue in this manner\ncomputing any (finite) level of such higher-order information.\nExample\n\nReturning again to our running example game where player 1 (Ann) has\ntwo available actions \\(\\{u,d\\}\\) and player 2 (Bob) has two available\nactions \\(\\{l,r\\}\\). The following type space describes the\nplayers\u2019 information: there is one type for Ann (\\(t_1\\)) and two\nfor Bob (\\(t_2,t_2'\\)) with the corresponding probability measures given\nbelow:\n\n\n\\(\\lambda_1(t_1)\\) \n\nl r\n\\(t_2\\) 0.5  0 \n\\(t^\\prime_1\\) 0.4  0.1 \n\n\n\nFigure 10: Ann's beliefs about\nBob\n\n\n\n\\(\\lambda_2(t_2)\\) \n\nu d\n\\(t_1\\) 1  0 \n\n\u00a0\u00a0\u00a0\u00a0\n\\(\\lambda_2(t^\\prime_2)\\) \n\nu d\n\\(t_1\\) 0.75  0.25 \n\n\n\nFigure 11: Bob's belief about Ann\n\nIn this example, since there is only one type for Ann, both of\nBob\u2019s types are certain about Ann\u2019s beliefs. If\nBob is of type \\(t_2\\) then he is certain Ann is choosing \\(u\\) while if\nhe is of type \\(t_2'\\) he thinks there is a 75% chance she plays\n\\(u\\). Ann assigns equal probability (\\(0.5\\)) to Bob\u2019s types; and\nso, she believes it is equally likely that Bob is certain she plays\n\\(u\\) as Bob thinking there is a 75% chance she plays \\(u\\). The above\ntype space is a very compact description of the players\u2019\ninformational attitudes. An epistemic-probabilistic model can describe\nthe same situation (here \\(p_i\\) for \\(i=1,2\\) is player \\(i\\)\u2019s prior\nprobability):\n\n\nFigure 12\n\nSome simple (but instructive!) calculations can convince us that\nthese two models represent the same situation. The more interesting\nquestion is how do these probabilistic models relate to the\nepistemic-doxastic models of Definition\n2.4. Here the situation is more complex. On the one hand,\nprobabilistic models with a graded notion of belief which is much more\nfine-grained than the \u201call-out\u201d notion of belief discussed\nin the context of epistemic-doxastic models. On the other hand, in an\nepistemic-doxastic model, conditional believes are defined\nfor all events. In the above models, they are only defined\nfor events that are assigned nonzero probabilities. In other words,\nepistemic-probabilistic models do not describe what a player may\nbelieve upon learning something \u201csurprising\u201d (i.e.,\nsomething currently assigned probability zero).\nA number of extensions to basic probability theory have been\ndiscussed in the literature that address precisely this problem. We do\nnot go into details here about these approaches (a nice summary and\ndetailed comparison between different approaches can be found in\nHalpern (2010) and instead sketch the main ideas. The first approach\nis to use so-called Popper functions which\ntake conditional probability measures as primitive. That is,\nfor each non-empty event \\(E\\), there is a probability measure\n\\(p_E(\\cdot)\\) satisfying the usual Kolmogrov axioms (relativized to\n\\(E\\), so for example \\(p_E(E)=1\\)). A second approach assigns to each\nagent a finite sequence of probability measures \\((p_1,p_2,\\ldots,p_n)\\)\ncalled a lexicographic probability system. The idea is that\nto condition on \\(F\\), first find the first probability measure not\nassigning zero to \\(F\\) and use that measure to condition on\n\\(F\\). Roughly, one can see each of the probability measures in a\nlexicographic probability system as corresponding to a level of a\nplausibility ordering. We will return to these notions\nin Section 5.2.\n2.4 Common Knowledge\nStates in a game model not only represent the player\u2019s\nbeliefs about what their opponents will do, but also\ntheir higher-order beliefs about what their opponents are\nthinking. This means that outcomes identified as\n\u201crational\u201d in a particular informational context will\ndepend, in part, on these higher-order beliefs. Both game\ntheorists and logicians have extensively discussed different notions\nof knowledge and belief for a group, such as common knowledge and\nbelief. In this section, we briefly recount the standard definition of\ncommon knowledge. For more information and pointers to the relevant\nliterature, see Vanderschraaf & Sillari (2009) and Fagin et al.,\n(1995: ch. 6).\nConsider the statement \u201ceveryone in group \\(I\\) knows that\n\\(E\\)\u201d. This is formally defined as follows:\n\\[\nK_I(E)\\ \\ :=\\ \\ \\bigcap_{i\\in I}K_i(E)\n\\]\n\nwhere \\(I\\) is any nonempty set of players. If \\(E\\) is common\nknowledge for the group \\(I\\), then not only does everyone in the group\nknow that \\(E\\) is true, but this fact is completely transparent to all\nmembers of the group. We first define \\(K_I^n(E)\\) for each \\(n\\ge 0\\) by\ninduction:\n\\[\n K_I^0(E)=E \\qquad{\\text{and for \\(n\\ge 1\\),}}\\quad \n K_I^n(E)=K_I(K^{n-1}_I(E)\n\\]\n\nThen, following Aumann (1976), common knowledge of\n\\(E\\) is defined as the following infinite conjunction:\n\n\\[\nC_I(E)=\\bigcap_{n\\ge 0}K_I^n(E)\n\\]\n\nUnpacking the definitions, we have \n\n\\[\nC_I(E)=E\\cap K_I\\phi(E) \\cap K_I(K_I(E)) \\cap K_I(K_I(K_I(E)))\\cap \\cdots\n\\]\n\nThe approach to defining common knowledge outlined above can be\nviewed as a recipe for defining common (robust/strong) belief (simply\nreplace the knowledge operators \\(K_i\\) with the appropriate belief\noperator). See Bonanno (1996) and Lismont & Mongin (1994, 2003)\nfor more information about the logic of common belief. Although we do\nnot discuss it in this entry, a probabilistic variant of common belief\nwas introduced by Monderer & Samet (1989).\n3. Choice Rules, or Choosing Optimally\nThere are many philosophical issues that arise in decision theory,\nbut that is not our concern here. See Joyce 2004 and reference therein\nfor discussions of the main philosophical issues. This section\nprovides enough background on decision theory to understand the key\nresults of epistemic game theory presented in the remainder of this\nentry.\nDecision rules or choice rules determine what\neach individual player will, or should do, given her preferences and\nher information in a given context. In the epistemic game theory\nliterature the most commonly used choice rules are:\n(strict) dominance, maximization of expected utility\nand admissibility (also known as weak dominance). One can do\nepistemic analysis of games using alternative choice rules, e.g.,\nminmax regret (Halpern & Pass 2011). In this entry, we focus only\non the most common ones.\nDecision theorists distinguish between choice\nunder uncertainty and choice under risk. In the\nlatter case, the decision maker has probabilistic information about\nthe possible states of the world. In the former case, there is no such\ninformation. There is an extensive literature concerning decision\nmaking in both types of situations (see Peterson 2009 for a discussion\nand pointers to the relevant literature). In the setting of epistemic\ngame theory, the appropriate notion of a \u201crational choice\u201d\ndepends on the type of game model used to describe the informational\ncontext of the game. So, in general, \u201crationality\u201d should\nbe read as following a given choice rule. The general approach is to\nstart with a definition of an irrational choice (for\ninstance, one that is strictly dominated given one\u2019s\nbeliefs), and then define rationality as not being irrational. Some\nauthors have recently looked at the consequences of lifting this\nsimplifying assumption (cf., the tripartite notion of\na categorization in Cubitt & Sugden (2011) and Pacuit\n& Roy (2011)), but the presentation of this goes beyond the scope\nof this entry.\nFinally, when the underlying notion of rationality\ngoes beyond maximization of expected utility, some authors\nhave reserved the word \u201coptimal\u201d to qualify decisions that\nmeet the latter requirement, but not necessarily the full requirements\nof rationality. See the remarks in Section\n5.2 for more on this.\n3.1 Maximization of Expected Utility\nMaximization of expected utility is the most well-known choice rule\nin decision theory. Given an agent\u2019s preferences (represented as\nutility functions) and beliefs (represented as subjective probability\nmeasures), the expected utility of an action, or option, is the sum of\nthe utilities of the outcomes of the action weighted by the\nprobability that they will occur (according to the agent\u2019s\nbeliefs). The recommendation is to choose the action that maximizes\nthis weighted average. This idea underlies the Bayesian view\non practical rationality, and can be straightforwardly defined in type\nspaces.[12]\n We start by defining expected utility for a\nplayer in a game.\nExpected utility\n Suppose that \\(G=\\langle N, \\{S_i, u_i\\}_{i\\in N}\\rangle\\) is a\nstrategic game. A conjecture for player \\(i\\) is a\nprobability on the set \\(S_{-i}\\) of strategy profiles of \\(i\\)\u2019s\nopponents. That is, a conjecture for player \\(i\\) is an element of\n\\(\\Delta(S_{-i})\\), the set of probability measures over\n\\(S_{-i}\\). The expected utility of \\(s_i\\in S_i\\) with\nrespect to a conjecture \\(p\\in \\Delta(S_{-i})\\) is defined as follows:\n\\[\n EU(s_i,p)\\ :=\\ \\sum_{s_{-i}\\in S_{-i}} p(s_{-i})u(s_i, s_{-i})\n\\]\n\nA strategy \\(s_i\\in S_i\\) maximizes expected utility\nfor player \\(i\\) with respect to \\(p\\in \\Delta(S_{-i})\\) provided for all\n\\(s_i'\\in S_i\\), \\(EU(s_i,p)\\ge EU(s_i',p)\\). In such a case, we also say\n\\(s_i\\) is a best response to \\(p\\) in game \\(G\\).\nWe now can define an event in a type space or epistemic-probability\nmodel where all players \u201cchoose rationally\u201d, in the sense\nthat their choices maximize expected utility with respect to their\nbeliefs.\nExpected utility in type spaces\n Let \\(G=\\langle N, \\{S_i, u_i\\}_{i\\in N}\\rangle\\) be a strategic\ngame and \\(\\T=\\langle \\{T_i\\}_{i\\in\\Agt},\n\\{\\lambda_i\\}_{i\\in\\Agt},S\\rangle\\) a type space for \\(G\\). Recall that\neach \\(t_i\\) is associated with a probability measure \\(\\lambda(t_i)\\in\n\\Delta(S_{-i}\\times T_{-i})\\). Then, for each \\(t_i\\in T_i\\), we can\ndefine a probability measure \\(p_{t_i}\\in \\Delta(S_{-i})\\) as follows:\n\\[\n p_{t_i}(s_{-i})= \\sum_{t_{-i}\\in T_{-i}}\\lambda_i(t_i)(s_{-i},t_{-i})\n\\]\n\nThe set of states (pairs of strategy profiles and type profiles)\nwhere player \\(i\\) chooses rationally is then defined as:\n\\[\n\\mathsf{Rat_i}\\ :=\\ \\{(s_i,t_i) \\mid s_i \\text{ is a best response to } p_{t_i}\\}\n\\]\n\nThe event that all players are rational is\n\\[\n\\mathsf{Rat}=\\{(s,t) \\mid \\mbox{ for all } i, (s_i,t_i)\\in \\mathsf{Rat}_i\\}.\n\\]\n\nNotice that here types, as opposed to players, maximize\nexpected utility. This is because in type structure, beliefs are\nassociated to types (see Section 2.3\nabove). The reader acquainted with decision theory will recognize that\nthis is just the standard notion of maximization of expected utility,\nwhere the space of uncertainty of each player, i.e., the possible\n\u201cstates of the world\u201d on which the consequences of her\naction depend, is the possible combinations of types and strategy\nchoices of the other players.\nTo illustrate the above definitions, consider the game\nin Figure 4 and the type space\nin Figure 11. The following calculations show that\n\\((u, t_1)\\in \\mathsf{Rat}_1\\) (\\(u\\) is the best response for player \\(1\\)\ngiven her beliefs defined by \\(t_1\\)):\n\n\\begin{align*} \nEU(u,p_{t_1}) &= p_{t_1}(l)u_1(u,l) + p_{t_1}(r)u_1(u,r)\\\\ \n & = [\\lambda_1(t_1)(l,t_2) + \\lambda_1(t_1)(l,t_2')]\\cdot u_1(u,l) \\\\ \n &\\quad\\quad\\quad + [\\lambda_1(t_1)(r,t_2) + \\lambda_1(t_1)(r,t_2')] \\cdot u_1(u,r) \\\\\n &= (0.5 +0.4) \\cdot 3 + (0 + 0.1)\\cdot 0 \\\\ \n &= 2.7\n\\end{align*}\n\n\\begin{align*} \nEU(d,p_{t_1}) &= p_{t_1}(l)u_1(d,l) + p_{t_1}(r)u_1(d,r)\\\\ \n & = [\\lambda_1(t_1)(l,t_2) + \\lambda_1(t_1)(l,t_2')]\\cdot u_1(d,l) \\\\ \n &\\quad\\quad + [\\lambda_1(t_1)(r,t_2) + \\lambda_1(t_1)(r,t_2')] \\cdot u_1(d,r) \\\\\n &= (0.5 +0.4) \\cdot 0 + (0 + 0.1)\\cdot 1 \\\\ \n &= 0.1\n\\end{align*}\n\nA similar calculation shows that \\((l, t_2)\\in \\mathsf{Rat}_2\\).\nExpected utility in epistemic-probability models\n The definition of a rationality event is similar in an\nepistemic-probability model. For completeness, we give the formal\ndetails. Suppose that \n\\[\n G=\\langle N, \\{S_i, u_i\\}_{i\\in N}\\rangle\n\\] \n\nis a strategic game and \n\\[\n\\M=\\langle W,\\{\\sim_i\\}_{i\\in\\Agt},\\{p_i\\}_{i\\in\\Agt},\\sigma\\rangle\n\\]\n\n is an\nepistemic probability models with each \\(p_i\\) a prior probability\nmeasure over \\(W\\). Each state \\(w\\in W\\), let \n\n\\[E_{s_-i}=\\{w\\in W \\mid (\\sigma(w))_{-i}=s_{-i}\\}.\\] \n\nThen, for each state \\(w\\in W\\), we define\na measure \\(p_w\\in \\Delta(S_{-i})\\) as follows:\n\n\\[p_w(s_{-i})=p(E_{s_{-i}} \\mid \\Pi_i(w))\\] \n\nAs above, \n\n\\[\\mathsf{Rat}_i :=\\{w \\mid \\sigma_i(w) \\mbox{ is a best response to } p_w\\}\\] \n\nand\n\n\\[\\mathsf{Rat}:=\\bigcap_{i\\in \\Agt} \\mathsf{Rat}_i.\\]\n\n3.2 Dominance Reasoning\nWhen a game model does not describe the players\u2019\nprobabilistic beliefs, we are in a situation of choice\nunder uncertainty. The standard notion of \u201crational\nchoice\u201d in this setting is based on dominance reasoning\n(Finetti 1974). The two standard notions of dominance are:\n\n\nDefinition 3.1 (Strict Dominance)\n  \nSuppose that \\(G=\\langle N, \\{S_i, u_i\\}_{i\\in N}\\rangle\\) is a\nstrategic game and \\(X\\subseteq S_{-i}\\). Let \\(m_i, m_i'\\in \\Delta(S_i)\\)\nbe two mixed strategies for player \\(i\\). The strategy\n\\(m_i\\) strictly dominates \\(m_i'\\) with respect to \\(X\\)\nprovided \n\n\\[\\text{for all }s_{-i}\\in X, U_i(m_i,s_{-i}) > U_i(m_i',s_{-i}).\n\\] \n\nWe say \\(m_i\\) is strictly dominated provided there\nis some \\(m_i'\\in \\Delta(S_i)\\) that strictly dominates \\(m_i\\).\n\nA strategy \\(m_i\\in \\Delta(S_i)\\) strictly dominates \\(m_i'\\in\n\\Delta(S_i)\\) provided \\(m_i\\) is better than \\(m_i'\\) (i.e., gives higher\npayoff to player \\(i\\)) no matter what the other players\ndo. There is also a weaker notion:\n\nDefinition 3.2 (Weak Dominance)\nSuppose that \\(G=\\langle N, \\{S_i, u_i\\}_{i\\in N}\\rangle\\) is a\nstrategic game and \\(X\\subseteq S_{-i}\\). Let \\(m_i, m_i'\\in \\Delta(S_i)\\)\nbe two mixed strategies for player \\(i\\). The strategy\n\\(m_i\\) weakly dominates \\(m_i'\\) with respect to \\(X\\)\nprovided\nfor all \\(s_{-i}\\in X\\), \\(U_i(m_i,s_{-i})\\ge U_i(m_i',s_{-i})\\)\nand\nthere is some \\(s_{-i}\\in X\\) such that \\(U_i(m_i,s_{-i})>\nU_i(m_i',s_{-i})\\).\nWe say \\(m_i\\) is weakly dominated provided there is\nsome \\(m_i'\\in \\Delta(S_i)\\) that weakly dominates \\(m_i\\).\n\nSo, a mixed strategy \\(m_i\\) weakly dominates another strategy \\(m_i'\\)\nprovided \\(m_i\\) is at least as good as \\(m_i'\\) no matter what the other\nplayers do and there is at least one situation in which \\(m_i\\)\nis strictly better than \\(m_i'\\).\nBefore we make use of these choice rules, we need to address two\npotentially confusing issues about these definitions.\n\nThe definitions of strict and weak dominance are given in terms of\nmixed strategies even though we are assuming that players only\nselect pure strategies. That is, we are not considering\nsituations in which players explicitly randomize. In particular,\nrecall that only pure strategies are associated with states in a game\nmodel. Nonetheless, it is important to define strict/weak dominance in\nterms of mixed strategies because there are games in which a pure\nstrategy is strictly (weakly) dominated by a mixed strategy, but not\nby any of the other pure strategies.\nEven though it is important to consider situations in which a\nplayer\u2019s pure strategy is strictly/weakly dominated by a mixed\nstrategy, we do not extend the above definitions to probabilities over\nthe opponents\u2019 strategies. That is, we do not replace the above\ndefinition with\n\\(m_i\\) is strictly \\(p\\)-dominates \\(m_i'\\) with\nrespect to \\(X\\subseteq \\Delta(S_{-i})\\), provided for all \\(q\\in X\\),\n\\(U_i(m_i,q)>U_i(m_i',q)\\).\nThis is because both definitions are\n equivalent. Obviously, \\(p\\)-strict dominance implies strict\n dominance. To see the converse, suppose that \\(m_i'\\) is dominated by\n \\(m_i\\) with respect to \\(X\\subseteq S_{-i}\\). We show that for all \\(q\\in\n \\Delta(X)\\), \\(U_i(m_i,q) > U_i(m_i',q)\\) (and so \\(m_i'\\) is\n \\(p\\)-strictly dominated by \\(m_i\\) with respect to \\(X\\)). Suppose that\n \\(q\\in \\Delta(X)\\). Then,\n\n\\[U_i(m_i, q)=\\kern-8pt \\sum_{s_{-i}\\in S_{-i}} q(s_{-i}) U_i(m_i,\ns_{-i}) >\\kern-8pt \\sum_{s_{-i}\\in S_{-i}} q(s_{-i}) U_i(m_i',\ns_{-i})=U_i(m_i',q).\\]\n\n\nThe parameter \\(X\\) in the above definitions is intended to represent\nthe set of strategy profiles that the player \\(i\\) take to be\n\u201clive possibilities\u201d. Each state in an epistemic\n(-plausibility) model is associated with a such a set of strategy\nprofiles. Given a possible world \\(w\\) in a game model, let \\(S_{-i}(w)\\)\ndenote the set of states that player \\(i\\) \u201cthinks\u201d are\npossible. The precise definition depends on the type of game\nmodel:\n\nEpistemic models\nSuppose that \n\\[G=\\langle N, \\{S_i, u_i\\}_{i\\in N}\\rangle\\]\n\n is a strategic game and \n\n\\[\\M=\\langle W, \\{\\Pi_i\\}_{i\\in N},\\sigma\\rangle\\]\n\nis an epistemic model of \\(G\\). For each player \\(i\\) and \\(w\\in W\\),\ndefine the set \\(S_{-i}(w)\\) as follows:\n\n\\[S_{-i}(w)= \\{\\sigma_{-i}(v) \\mid v\\in\\Pi_i(w)\\}\\]\n\n\n\nEpistemic-Plausibility Models\nSuppose that\n\n\\[G=\\langle N, \\{S_i, u_i\\}_{i\\in N}\\rangle\\] \n\nis a strategic game and\n\n\\[\\M=\\langle W, \\{\\sim_i\\}_{i\\in N},\\{\\preceq_i\\}_{i\\in N}, \\sigma\\rangle\\] \n\nis an epistemic-plausibility model of \\(G\\). For each player \\(i\\) and \\(w\\in W\\),\ndefine the set \\(S_{-i}(w)\\) as follows: \n\n\\[S_{-i}(w)= \\{\\sigma_{-i}(v) \\mid v\\in Min_{\\preceq_i}([w]_i)\\}\\]\n\nIn either case, we say that a choice at state \\(w\\) is sd-rational\nfor player \\(i\\) at state \\(w\\) provided it is not strictly dominated with\nrespect to \\(S_{-i}(w)\\). The event in which \\(i\\) chooses rationality is\nthen defined as\n\n\\[\\mathsf{Rat_i^{sd}}\\ :=\\ \\{w \\mid \\text{\\(\\sigma_i(w)\\) is not strictly dominated with respect to\n\\(S_{-i}(w)\\)}\\}.\n\\] \n\n\nIn addition, we have \\(\\mathsf{Rat^{sd}}\\ :=\\ \\bigcap_{i\\in N}\\mathsf{Rat_i^{sd}}\\). \nSimilarly, we can define the set\nof states in which player \\(i\\) is playing a strategy that is not weakly\ndominated, denoted \\(\\mathsf{Rat_i^{wd}}\\) and \\(\\mathsf{Rat^{wd}}\\) using\nweak dominance.\nKnowledge of one\u2019s own action, the trademark\nof ex-interim situations, plays an important role in the\nabove definitions. It enforces that \\(\\sigma_i(w') = \\sigma_i(w)\\)\nwhenever \\(w'\\in \\Pi_i(w)\\). This means that player \\(i\\)\u2019s\nrationality is assessed on the basis of the result of\nher current choice according to different combinations of\nactions of the other players.\nAn important special case is when the players consider all\nof their opponents\u2019 strategies possible. It should be clear that\na rational player will never choose a strategy that is\nstrictly dominated with respect to \\(S_{-i}\\). That is, if \\(s_i\\) is\nstrictly dominated with respect that \\(S_{-i}\\), then there is no\ninformational context in which it is rational for player \\(i\\) to choose\n\\(s_i\\). This can be made more precise using the following well-known\nLemma.\n\n Lemma 3.1\nSuppose that \\(G=\\langle N, \\{S_i, u_i\\}_{i\\in N}\\rangle\\) is a\nstrategic game. A strategy \\(s_i\\in S_i\\) is strictly dominated\n(possibly by a mixed strategy) with respect to \\(X\\subseteq S_{-i}\\) iff\nthere is no probability measure \\(p\\in \\Delta(X)\\) such that \\(s_i\\) is a\nbest response with respect to \\(p\\).\n\nThe proof of this Lemma is given in\nthe supplement, Section 1.\nThe general conclusion is that no dominated strategy can maximize\nexpected utility at a given state; and, conversely, if there is a\nstrategy that is not a best in a specific context, then it is not\nstrictly dominated.\nSimilar facts hold about weak dominance, though the\nsituation is more subtle. The crucial observation is that there is a\ncharacterization of weak dominance in terms of best response to\ncertain types of probability measures. A probability measure \\(p\\in\n\\Delta(X)\\) is said to have full support (with respect\nto \\(X\\)) if \\(p\\) assigns positive probability to every element of \\(X\\)\n(formally, \\(supp(p)=\\{x\\in X \\mid p(x)>0\\}=X\\)). Let\n\\(\\Delta^{>0}(X)\\) be the set of full support probability measures on\n\\(X\\). A full support probability on \\(S_{-i}\\) means that player \\(i\\) does\nnot completely rule out (in the sense, that she assigns zero\nprobability to) any strategy profiles of her opponents. The following\nanalogue of Lemma 3.1 is also well-known:\n\nLemma 3.2\nSuppose that \\(G=\\langle N, \\{S_i, u_i\\}_{i\\in\nN}\\rangle\\) is a strategic game. A strategy \\(s_i\\in S_i\\) is weakly\ndominated (possibly by a mixed strategy) with respect to \\(X\\subseteq\nS_{-i}\\) iff there is no full support probability measure \\(p\\in\n\\Delta^{>0}(X)\\) such that \\(s_i\\) is a best response with respect\nto \\(p\\).\n\nThe proof of this Lemma is more involved. See Bernheim (1984:\nAppendix A) for a proof. In order for a strategy \\(s_i\\) to not be\nstrictly dominated, it is sufficient for \\(s_i\\) to be a best response\nto a belief, whatever that belief is, about the opponents\u2019\nchoices. Admissibility requires something more: the strategy must be a\nbest response to a belief that does not explicitly rule-out any of the\nopponents\u2019 choices. Comparing these two Lemmas, we see that\nstrict dominance implies weak dominance, but not necessarily vice\nversa. A strategy might not be a best response to any\nfull-support probability measure while being a best response to some\nparticular beliefs, those assigning probability one to a state where\nthe player is indifferent between the outcome of its present action\nand the potentially inadmissible one.\nThere is another, crucial, difference between weak and strict\ndominance. The following observation is immediate from the definition\nof strict dominance:\n\nObservation 3.3\nIf \\(s_i\\) is strictly dominated with respect to \\(X\\) and\n\\(X'\\subseteq X\\), then \\(s_i\\) is strictly dominated with respect to\n\\(X'\\).\n\nIf a strategy is strictly dominated, it remains so if the player\ngets more information about what her opponents (might) do. Thus, if a\nstrategy \\(s_i\\) is strictly dominated in a game \\(G\\) with respect to\nthe entire set of her opponents\u2019 strategies \\(S_{-i}\\),\nthen it will never be rational (according to the above definitions) in\nany epistemic (-plausibility) model for \\(G\\). I.e., there are no\nbeliefs player \\(i\\) can have that makes \\(s_i\\) rational. The same\nobservation does not hold for weak dominance. The existential part of\nthe definition of weak dominance means that the analogue\nof Observation 3.3 does not hold for weak\ndominance: if \\(s_i\\) is weakly dominated with respect to \\(X\\) then it\nneed not be the case that \\(s_i\\) is weakly dominated with respect to\nsome \\(X'\\subseteq X\\).\n4. Fundamentals\nThe epistemic approach to game theory focuses on the choices\nof individual decision makers in specific informational\ncontexts, assessed on the basis of decision-theoretic choice\nrules. This is a bottom-up, as opposed to the classical top-down,\napproach. Early work in this paradigm include Bernheim (1984) and\nPearce\u2019s (1984) notion of rationalizability and\nAumann\u2019s derivation of correlated equilibrium from the\nminimal assumption that the players are \u201cBayesian\nrational\u201d (Aumann 1987).\nAn important line of research in epistemic game theory asks under\nwhat epistemic conditions will players follow the\nrecommendations of particular solution concept? Providing such\nconditions is known as an epistemic characterization of a\nsolution concept.\nIn this section, we present two fundamental epistemic\ncharacterization results. The first is a characterization of iterated\nremoval of strictly dominated strategies (henceforth ISDS), and the\nsecond is a characterization of backward induction. These epistemic\ncharacterization results are historically important. They mark the\nbeginning of epistemic game theory as we know it today. Furthermore,\nthey are also conceptually important. The developments in later\nsections build on the ideas presented in this section.\n4.1 Iterated Removal of Strictly Dominated Strategies\nThe central result of epistemic game theory is that\n\u201crationality and common belief in rationality implies\niterated elimination of strictly dominated strategies.\u201d This\nresult is already covered in Vanderschraaf & Sillari (2009). For\nthat reason, instead of focusing on the formal details, the emphasis\nhere will be on its significance for the epistemic foundations of game\ntheory. One important message is that the result highlights the\nimportance of higher-order information.\n4.1.1 The Result\nIterated elimination of strictly dominated strategies\n(ISDS) is a solution concept that runs as follows. First, remove from\nthe original game any strategy that is strictly dominated for player\n\\(i\\) (with respect to all of the opponents\u2019 strategy\nprofiles). After having removed the strictly dominated strategies in\nthe original game, look at the resulting sub-game, remove the\nstrategies which have become strictly dominated there, and repeat this\nprocess until the elimination does not remove any strategies. The\nprofiles that survive this process are said to be iteratively\nnon-dominated.\nFor example, consider the following strategic game:\n\n\n \u00a0 Bob \nAnn \n\nl c r\nt 3,3  1,1 0,0\nm 1,1  3,3 1,0\nb 0,4  0,0 4,0\n\n\n\nFigure 13\n\nNote that \\(r\\) is strictly dominated for player \\(2\\) with respect to\n\\(\\{t, m, b\\}\\). Once \\(r\\) is removed from the game, we have \\(b\\) is\nstrictly dominated for player 1 with respect to \\(\\{l, c\\}\\). Thus,\n\\(\\{(t,l), (t,c), (m,l), (m,c)\\}\\) are iteratively undominated. That is,\niteratively removing strictly dominated strategies generates the\nfollowing sequence of games:\n\n\n\n\nl c r\nt 3,3  1,1 0,0\nm 1,1  3,3 1,0\nb 0,4  0,0 4,0\n\n\\(\\ \\rightarrowtail\\ \\)\n\n\nl c \nt 3,3  1,1 \nm 1,1  3,3 \nb 0,4  0,0 \n\n\\(\\ \\rightarrowtail\\ \\)\n\n\nl c \nt 3,3  1,1 \nm 1,1  3,3 \n\nFigure 14\n\nFor arbitrary large (finite) strategic games, if all players\nare rational and there is common belief that all\nplayers are rational, then they will choose a strategy that\nis iteratively non-dominated. The result is credited to Bernheim\n(1984) and Pearce (1984). See Spohn (1982) for an early version, and\nBrandenburger & Dekel (1987) for the relation with correlated\nequilibrium.\nBefore stating the formal result, we illustrate the result with an\nexample. We start by describing an \u201cinformational context\u201d\nof the above game. To that end, define a type space \\(\\T=\\langle \\{T_1,\nT_2\\}, \\{\\lambda_1,\\lambda_2\\}, S\\rangle\\), where \\(S\\) is the strategy\nprofiles in the above game, there are two types for player 1\n\\((T_1=\\{t_1, t_2\\})\\) and three types for player 2 \\((T_2=\\{s_1, s_2,\ns_3\\})\\). The type functions \\(\\lambda_i\\) are defined as follows:\n\n\n\n\\(\\lambda_1(t_1)\\)\n\nl c r\n\\(s_1\\) 0.5  0.5 0\n\\(s_2\\) 0  0 0\n\\(s_3\\) 0  0 0\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\\(\\lambda_1(t_2)\\)\n\nl c r\n\\(s_1\\) 0  0.5 0\n\\(s_2\\) 0  0 0.5\n\\(s_3\\) 0  0 0\n\n\n\n\n\n\\(\\lambda_2(s_1)\\)\n\nt m b\n\\(t_1\\) 0.5  0.5 0\n\\(t_2\\) 0  0 0\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\\(\\lambda_2(s_2)\\)\n\nt m b\n\\(t_1\\) 0.25  0.25 0\n\\(t_2\\) 0.25  0.25 0\n\n\n\n\n\n\\(\\lambda_2(s_3)\\)\n\nt m b\n\\(t_1\\) 0.5  0 0\n\\(t_2\\) 0  0 0.5\n\n\n\nFigure 15\n\nWe then consider the pairs \\((s,t)\\) where \\(s\\in S_i\\) and \\(t\\in T_i\\)\nand identify  all the rational pairs (i.e., where \\(s\\) is a best\nresponse to \\(\\lambda_i(t)\\), see the previous section for a\ndiscussion):\n\n\\(\\mathsf{Rat_1}=\\{(t, t_1), (m, t_1), (b,t_2)\\}\\)\n\\(\\mathsf{Rat_2}=\\{(l, s_1), (c,s_1), (l, s_2), (c, s_2), (l, s_3)\n\\}\\)\n\nThe next step is to identify the types that believe that\nthe other players are rational. In this context, belief\nmeans probability 1. For the type \\(t_1\\), we have\n\\(\\lambda_1(t_1)(\\mathsf{Rat}_2)=1\\); however,\n\n\\[\\lambda_1(t_2)(s_2,r)=0.5>0,\\] \n\nbut \\((r,s_2)\\not\\in \\mathsf{Rat_2}\\), so \\(t_2\\) does not\nbelieve that player \\(2\\) is rational. This can be turned into an\niterative process as follows: Let \\(R_i^1=\\mathsf{Rat_i}\\). We first\nneed some notation. Suppose that for each \\(i\\), \\(R_i^n\\) has been\ndefined. Then, define \\(R_{-i}^n\\) as follows: \n\n\\[R_{-i}^n=\\{(s,t) \\mid \\text{\\(s\\in S_{-i}\\), \\(t\\in T_{-j}\\), and for each \\(j\\ne i\\),\n\\((s_j,t_j)\\in R_j^n\\)}\\}.\n\\] \n\n\nFor each \\(n>1\\), define \\(R_i^n\\) inductively as follows:\n\n\\[R_i^{n+1}=\\{(s,t) \\mid (s,t)\\in R_i^n \n\\text{ and \\(\\lambda_i(t)\\) assigns probability 1 to \\(R_{-i}^n\\)}\\}\n\\]\n\nThus, we have \\(R_1^2=\\{(t, t_1), (m, t_1)\\}\\). Note that \\(s_2\\)\nassigns non-zero probability to the pair \\((m,t_2)\\) which is not in\n\\(R_1^1\\), so \\(s_2\\) does not believe that \\(1\\) is rational. Thus, we have\n\\(R_2^2=\\{(l,s_1), (c,s_1),(l,s_3)\\}\\). Continuing with this process, we\nhave \\(R_1^2=R_1^3\\). However, \\(s_3\\) assigns non-zero probability to\n\\((b,t_2)\\) which is not in \\(R_1^2\\), so \\(R_2^3=\\{(l, s_1),\n(c,s_1)\\}\\). Putting everything together, we have \n\n\\[\\bigcap_{n\\ge 1}R^n_1\\ \\times \\ \\bigcap_{n\\ge 1}R^n_2=\\{(t, t_1), (m, t_1)\\}\\times\n\\{(l, s_1), (c, s_1)\\}.\n\\] \n\n\nThus, all the profiles that survive\niteratively removing strictly dominated strategies \n\\((\\{(t,l), (m,l), (t,c), (m,c)\\})\\) are consistent with \nstates where the players are rational and commonly believe they are\nrational.\nNote that, the above process need not generate all\nstrategies that survive iteratively removing strictly dominated\nstrategies. For example, consider a type space with a single type for\nplayer 1 assigning probability 1 to the single type of player 2 and\n\\(l\\), and the single type for player 2 assigning probability 1 to\nthe single type for player 1 and \\(u\\). Then, \\((u,l)\\) is the only\nstrategy profile in this model and obviously rationality and common\nbelief of rationality is satisfied. However, for any type space, if a\nstrategy profile is consistent with rationality and common belief of\nrationality, then it must be a strategy that is in the set of\nstrategies that survive iteratively removing strictly dominated\nstrategies.\n\nTheorem 4.1\nSuppose that \\(G\\) is a strategic game and \\(\\T\\) is any type space\nfor \\(G\\). If \\((s,t)\\) is a state in \\(\\T\\) in which all the players are\nrational and there is common belief of rationality\u2014formally,\nfor each \\(i\\),\n\n\\[(s_i, t_i)\\in \\bigcap_{n\\ge 1} R_i^n\\]\n\n\u2014then \\(s\\) is a strategy profile that survives iteratively\nremoval of strictly dominated strategies.\n\nThis result establishes sufficient conditions for ISDS. It\nhas also a converse direction: given any strategy profile that\nsurvives iterated elimination of strictly dominated strategies, there\nis a model in which this profile is played where all players are\nrational and this is common knowledge. In other words, one can\nalways view or interpret the choice of a strategy\nprofile that would survive the iterative elimination procedure as one\nthat results from common knowledge of rationality. Of course, this\nform of the converse is not particularly interesting as we can always\ndefine a type space where all the players assign probability 1 to the\ngiven strategy profile (and everyone playing their requisite\nstrategy). Much more interesting is the question whether\nthe entire set of strategy profiles that survive iteratively\nremoval of strictly dominated strategies is consistent with\nrationality and common belief in rationality. This is covered by the\nfollowing theorem of Brandenburger & Dekel (1987) (cf. also Tan\n& Werlang 1988):\n\nTheorem 4.2\nFor any game \\(G\\), there is a type structure for that game in\nwhich the strategy profiles consistent with rationality and common\nbelief in rationality is the set of strategies that survive\niterative removal of strictly dominated strategies.\n\nSee Friedenberg & Keisler (2010) for the strongest versions of\nthe above results. Analogues of the above results have been proven\nusing different game models (e.g., epistemic models,\nepistemic-plausibility models, etc.). For example, see Apt &\nZvesper (2010) proofs of corresponding theorems using Kripke\nmodels.\n4.1.2 Philosophical Issues\nMany authors have pointed out the strength of the common belief\nassumption in the results of the previous section (see, e.g., Gintis\n2009; Bruin 2010). It requires that the players not only believe that\nthe others are not choosing an irrational strategy, but also to\nbelieve that everybody believes that nobody is choosing an irrational\nstrategy, and everyone believes that everyone believes that everyone\nbelieves that nobody is choosing an irrational strategy, and so on. It\nshould be noted, however, that this unbounded character is there only\nto ensure that the result holds for arbitrary finite\ngames. For a particular game and a model for it, a finite iteration\nof \u201ceverybody believes that\u201d suffices to ensure a play\nthat survives the iterative elimination procedure.\nA possible reply to the criticism of the infinitary nature of the\ncommon belief assumption is that the result should be seen as the\nanalysis of a benchmark case, rather than a description of\ngenuine game playing situations or a prescription for what rational\nplayers should do (Aumann 2010). Indeed, common\nknowledge/belief of rationality has long been used as an informal\nexplanation of the idealizations underlying classical game-theoretical\nanalyses (Myerson 1991). The results above show that, once formalized,\nthis assumption does indeed lead to a classical solution concept,\nalthough, interestingly, not the well-known Nash equilibrium,\nas is often informally claimed in early game-theoretic\nliterature. Epistemic conditions for Nash equilibrium are presented\nin Section 5.1.\nThe main message to take away from the results in the previous\nsection is: Strategic reasoning in games involves higher-order\ninformation. This means that, in particular,\n\u201cBayesian rationality\u201d alone\u2014i.e., maximization\nof expected utility\u2014is not sufficient to ensure a strategy\nprofile is played that is iteratively undominated, in the general\ncase.\nIn general, first-order belief of rationality will not do\neither. Exactly how many levels of beliefs is needed to guarantee\n\u201crational play\u201d in game situations is still the subject of\nmuch debate (Kets 2014; Colman 2003; de Weerd, Verbrugge, &\nVerheij 2013; Rubinstein 1989). There are two further issues we need\nto address.\nFirst of all, how can agents arrive at a context where rationality\nis commonly believed? The above results do not answer that\nquestion. This has been the subject of recent work in Dynamic\nEpistemic Logic (van Benthem 2003). In this literature, this question\nis answered by showing that the agents can eliminate all higher-order\nuncertainty regarding each others\u2019 rationality, and thus ensure\nthat no strategy is played that would not survive the iterated\nelimination procedure, by repeatedly and publicly\n\u201cannouncing\u201d that they are not irrational. In\nother words, iterated public announcement of rationality makes the\nplayers\u2019 expectations converge towards sufficient epistemic\nconditions to play iteratively non-dominated strategies. For more on\nthis dynamic view on solution epistemic characterization see van\nBenthem (2003); Pacuit & Roy (2011); van Benthem & Gheerbrant\n(2010); and van Benthem, Pacuit, & Roy (2011).\nSecond of all, when there are more than two players, the above\nresults only hold if players can believe that the choices of their\nopponents are correlated (Bradenburger & Dekel 1987;\nBrandenburger & Friedenberg 2008). The following example from\nBrandenburger & Friedenberg (2008) illustrates this\npoint. Consider the following three person game where Ann\u2019s\nstrategies are \\(S_A=\\{u,d\\}\\), Bob\u2019s strategies are \\(S_B=\\{l,r\\}\\)\nand Charles\u2019 strategies are \\(S_C=\\{x,y,z\\}\\) and their respective \npreferences for each outcome are given in the corresponding cell:\n\n\n\n\nl r\nu 1,1,3  1,0,3 \nd 0,1,0  0,0,0 \n\n\nl r\nu 1,1,2  1,0,0 \nd 0,1,0  1,1,2 \n\n\nl r\nu 1,1,0  1,0,0 \nd 0,1,3  0,0,3 \n\n\n\nx \ny \nz \n\nFigure 16\n\nNote that \\(y\\) is not strictly dominated for Charles. It is easy to\nfind a probability measure \\(p\\in\\Delta(S_A\\times S_B)\\) such that \\(y\\)\nis a best response to \\(p\\). Suppose that\n\\(p(u,l)=p(d,r)=\\frac{1}{2}\\). Then, \\(EU(x,p)=EU(z,p)=1.5\\) while\n\\(EU(y,p)=2\\). However, there is no probability measure \\(p\\in\n\\Delta(S_A\\times S_B)\\) such that \\(y\\) is a best response to \\(p\\) and\n\\(p(u,l)=p(u)\\cdot p(l)\\) (i.e., Charles believes that Ann and\nBob\u2019s choices are independent). To see this, suppose that \\(a\\) is\nthe probability assigned to \\(u\\) and \\(b\\) is the probability assigned to\n\\(l\\). Then, we have:\n\nThe expected utility of \\(y\\) is\n\\[2ab + 2(1-a)(1-b);\\]\n\nThe expected utility of \\(x\\) is\n\\[3ab + 3a(1-b)=3a(b+(1-b))=3a;\\]\n\nand\nThe expected utility of \\(z\\) is\n\n\\begin{align*}\n 3(1-a)b+3(1-a)(1-b) &= 3(1-a)(b+(1-b))\\\\ \n  & =3(1-a).\n\\end{align*}\n\n\nThere are three cases:\n\nSuppose that \\(a=1-a\\) (i.e., \\(a=1/2\\)). Then,\n\\begin{align*}\n 2ab + 2(1-a)(1-b)&=2ab+2a(1-b)\\\\\n  &=2a(b+(1-b))\\\\\n  &=2a<3a.\n\\end{align*}\n\nHence, \\(y\\) is not a best response.\nSuppose that \\(a>1-a\\). Then, \n\\[2ab + 2(1-a)(1-b)<2ab+2a(1-b)=2a<3a.\\]\n\nHence, \\(y\\) is not a best response.\nSuppose that \\(1-a>a\\). Then, \n\\begin{align*} \n2ab + 2(1-a)(1-b) &<2(1-a)b+2(1-a)(1-b) \\\\ \n & =2(1-a) \\\\\n & < 3(1-a). \n\\end{align*}\n\nHence, \\(y\\) is not a best response.\n\nIn all of the cases, \\(y\\) is not a best response.\n4.2 Backward induction\nThe second fundamental result analyzes the consequences of\nrationality and common belief/knowledge of rationality\nin extensive games (i.e., trees instead of matrices). Here,\nthe most well-known solution concept is the so-called subgame\nperfect equilibrium, also known as backward induction in\ngames of perfect information. The epistemic characterization of this\nsolution concept is in terms of \u201csubstantive rationality\u201d\nand common belief that all players are substantively rational\n(cf. also Vanderschraaf & Sillari 2009: sec. 2.8). The main point\nthat we highlight in this section, which is by now widely acknowledged\nin the literature, is:\n Belief revision policies play a key role\nin the epistemic analysis of extensive games\n\nThe most well-known illustration of this is through the comparison\nof two apparently contradictory results regarding the consequences of\nassuming rationality and common knowledge of rationality in extensive\ngames. Aumann (1995) showed that this epistemic condition implies that\nthe players will play according to the backward induction solution\nwhile Stalnaker (1998) argued that this is not necessarily true. The\ncrucial difference between these two results is the way in which they\nmodel the players\u2019 belief change upon (hypothetically) learning\nthat an opponent has deviated from the backward induction path.\n4.2.1 Extensive games: basic definitions\nExtensive games make explicit the sequential structure of choices\nin a game situation. In this section, we focus on games of perfect\ninformation in which there is no uncertainty about earlier\nchoices in the game. These games are represented by tree-like\nstructures:\n\n\nDefinition 4.3 (Perfect Information Extensive Game)\nAn extensive game is a tuple \\(\\langle N, T, Act, \\tau, \\{ u_i \\}_{i\\in N}\\rangle\\), where\n\n\n\\(N\\) is a finite set of players;\n\\(T\\) is a tree describing the temporal structure of the game\nsituation: Formally, \\(T\\) consists of a set of nodes and an immediate\nsuccessor relation \\(\\rightarrowtail\\). Let \\(Z\\) denote the set of\nterminal nodes (i.e., nodes without any successors) and \\(V\\) the\nremaining nodes (called decision nodes). Let \\(v_0\\) denote the initial\nnode (i.e., the root of the tree). The edges at a\ndecision node \\(v\\in V\\) are each labeled with actions\nfrom a set \\(Act\\). Let \\(Act(v)\\) denote the set of actions available at\n\\(v\\). Let \\(\\rightsquigarrow\\) be the transitive closure of\n\\(\\rightarrowtail\\).\n\\(\\tau\\) is a turn function assigning a player to each node \\(v\\in V\\)\n(for a player \\(i\\in N\\), let \\(V_i=\\{v\\in V \\mid \\tau(v)=i\\}\\)).\n\\(u_i: Z\\rightarrow \\mathbb{R}\\) is the utility function for player\n\\(i\\) assigning real numbers to outcome nodes.\n\n\nA strategy is a term of art in extensive games. It\ndenotes a plan for every eventuality, which tells an agent what to do\nat all histories she is to play, even those which are excluded by the\nstrategy itself.\n\nDefinition 4.4 (Strategies)\nA strategy for player \\(i\\) is a function \\(s_i:V_i\n\\rightarrow Act\\) where for all \\(v\\in V_i\\), \\(s_i(v)\\in Act(v)\\). A\nstrategy profile, denoted \\({\\mathbf{s}}\\), is an element of \\(\\Pi_{i\\in\nN} S_i\\). Given a strategy profile \\({\\mathbf{s}}\\), let \\({\\mathbf{s}}_i\\)\nbe player \\(i\\)\u2019s component of \\({\\mathbf{s}}\\) and\n\\({\\mathbf{s}}_{-i}\\) the sequence of strategies form \\({\\mathbf{s}}\\) for\nall players except \\(i\\).\n\nEach strategy profile \\({\\mathbf{s}}\\) generates a path through an\nextensive game, where a path is a maximal sequence of nodes from the\nextensive game ordered by the immediate successor relations\n\\(\\rightarrowtail\\). We say that \\(v\\) is reached by a\nstrategy profile \\({\\mathbf{s}}\\) is \\(v\\) is on the path generated by\n\\({\\mathbf{s}}\\). Suppose that \\(v\\) is any node in an extensive game. Let\n\\(out(v,{\\mathbf{s}})\\) be the terminal node that is reached if,\nstarting at node \\(v\\), all the players move according to their\nrespective strategies in the profile \\({\\mathbf{s}}\\). Given a decision\nnode \\(v\\in V_i\\) for player \\(i\\), a strategy \\(s_i\\) for player \\(i\\), and a\nset \\(X\\subseteq S_{-i}\\) of strategy profiles of the opponents of \\(i\\),\nlet \\(Out_i(v,s_i, X)=\\{out(v, (s_i, s_{-i})) \\mid s_{-i}\\in X\\}\\). That\nis, \\(Out_i(v, s_i, X)\\) is the set of terminal nodes that may be\nreached if, starting at node \\(v\\), player \\(i\\) uses strategy \\(s_i\\) and\n\\(i\\)\u2019s opponents use strategy profiles from \\(X\\).\nThe following example of a perfect information extensive game will\nbe used to illustrate these concepts. The game is an instance of the\nwell-known centipede game, which has played an important role\nin the epistemic game theory literature on extensive games.\n\n\nFigure 17: An extensive game\n\nThe decision nodes for \\(A\\) and \\(B\\) respectively are \\(V_A=\\{v_1,\nv_3\\}\\) and \\(V_B=\\{v_2\\}\\); and the outcome nodes are \\(O=\\{o_1, o_2,\no_3, o_4\\}\\). The labels of the edges in the above tree are the actions\navailable to each player. For instance, \\(Act(v_1)=\\{O_1, I_1\\}\\). There\nare four strategies for \\(A\\) and two strategies for \\(B\\). To simplify\nnotation, we denote the players\u2019 strategies by the sequence of\nchoices at each of their decision nodes. For example, \\(A\\)\u2019s\nstrategy \\(s_A^1\\) defined as \\(s_A^1(v_1)=O_1\\) and \\(s_A^1(v_3)=O_3\\) is\ndenoted by the sequence \\(O_1O_3\\). Thus, \\(A\\)\u2019s strategies are:\n\\(s_A^1=O_1O_3\\), \\(s_A^2=O_1I_3\\), \\(s_A^3=I_1O_3\\) and\n\\(s_A^4=I_1I_3\\). Note that \\(A\\)\u2019s strategy \\(s_A^2\\) specifies a\nmove at \\(v_3\\), even though the earlier move at \\(v_1\\), \\(O_1\\), means\nthat \\(A\\) will not be given a chance to move at \\(v_3\\). Similarly,\nBob\u2019s strategies will be denoted by \\(s_B^1=O_2\\) and \\(s_B^2=I_2\\),\ngiving the actions chosen by \\(B\\) at his decision node. Then, for\nexample, \\(out(v_2,(s_A^2, s_B^2))=o_4\\). Finally, if \\(X=\\{s_A^1,\ns_A^4\\}\\), then \\(Out_B(v_2,s_B^2, X)=\\{o_3, o_4\\}\\).\n4.2.2 Epistemic Characterization of Backward Induction\nThere are a variety of ways to describe the players\u2019\nknowledge and beliefs in an extensive game. The game models vary\naccording to which epistemic attitudes are represented (e.g.,\nknowledge and/or various notions of beliefs) and precisely how the\nplayers\u2019 disposition to revise their beliefs during a play of\nthe game is represented. Consult Battigalli, Di Tillio, & Samet\n(2013); Baltag, Smets, & Zvesper (2009); and Battigalli &\nSiniscalchi (2002) for a sampling of the different types of models\nfound in the literature.\nOne of the simplest approaches is to use the epistemic models\nintroduced in Section 2.2 (cf.  Aumann 1995;\nHalpern 2001b). An epistemic model of an extensive game \\(G=\\langle N,\nT, Act, \\tau, \\{ u_i \\}_{i \\in N}\\rangle\\) is a tuple \\(\\langle W,\n\\{\\Pi_i\\}_{i\\in N}, \\sigma\\rangle \\) where \\(W\\) is a nonempty set of\nstates; for each \\(i\\in N\\), \\(\\Pi_i\\) is a partition on \\(W\\); and\n\\(\\sigma:W\\rightarrow \\Pi_{i\\in N} S_i\\) is a function assigning to each\nstate \\(w\\), a strategy profile from \\(G\\). If \\(\\sigma(w)={\\mathbf{s}}\\),\nthen we write \\(\\sigma_i(w)\\) for \\({\\mathbf{s}}_i\\) and \\(\\sigma_{-i}(w)\\)\nfor \\({\\mathbf{s}}_{-i}\\). As usual, we assume that players know their\nown strategies: for all \\(w\\in W\\), if \\(w'\\in \\Pi_i(w)\\), then\n\\(\\sigma_i(w)=\\sigma_i(w')\\).\nThe rationality of a strategy at a decision node depends both on\nwhat actions the strategy prescribes at all future decision\nnodes and what the players know about the strategies that\ntheir opponents are following. Let \\(S_{-i}(w)=\\{\\sigma_{-i}(w') \\mid\nw'\\in \\Pi_i(w)\\}\\) be the set of strategy profiles of player\n\\(i\\)\u2019s opponents that \\(i\\) thinks are possible at state \\(w\\). Then,\n\\(Out_i(v, s_i, S_{-i}(w))\\) is the set of outcomes that player \\(i\\)\nthinks are possible starting at node \\(v\\) if she follows strategy\n\\(s_i\\).\n\nDefinition 4.5 (Rationality at a decision node)\nPlayer \\(i\\) is rational at node \\(v\\in V_i\\) in state\n\\(w\\) provided, for all strategies \\(s_i\\) such that \\(s_i\\ne\n\\sigma_i(w)\\), there is an \\(o'\\in Out_i(v, s_i, S_{-i}(w))\\) and \\(o\\in\nOut_i(v, \\sigma_i(w), S_{-i}(w))\\) such that \\(u_i(o)\\ge\nu_i(o')\\).\n\nSo, a player \\(i\\) is rational at a decision node \\(v\\in V_i\\) in state\n\\(w\\) provided that \\(i\\) does not know that there is an alternative\nstrategy that would give her a higher payoff.\n\nDefinition 4.6 (Substantive rationality)\nPlayer \\(i\\) is substantively rational at state \\(w\\)\nprovided for all decision nodes \\(v\\in V_i\\), \\(i\\) is rational at \\(v\\) in\nstate \\(w\\).\n\nWe can define the event that player \\(i\\) is substantively rational\nis the standard way: \\(\\mathsf{Rat}_i=\\{w \\mid \\mbox{player } i\\) is\nsubstantively rational at state \\(w\\}\\); and so, the event that all\nplayers are substantively rational is \\(\\mathsf{Rat}=\\bigcap_{i\\in N}\n\\mathsf{Rat}_i\\).\nThis notion of rationality at a decision node \\(v\\) is\nforward-looking in the sense that it only takes account of the\npossibilities that can arise from that point on in the\ngame. It does not take account of the previous moves leading to\n\\(v\\)\u2014i.e., which choices have or could have lead to \\(v\\). We shall\nreturn to this in the discussion below.\nAn important consequence of this is that the rationality of choices\nat nodes that are only followed by terminal nodes are independent of\nthe relevant player\u2019s knowledge. Call a node\n\\(v\\) pre-terminal if all of \\(v\\)\u2019s immediate\nsuccessors are terminal nodes. At such nodes, it does not matter what\nstrategies the player thinks are possible: If \\(v\\) is a pre-terminal\nnode and player \\(i\\) is moving at \\(v\\), then for all states in \\(w\\) in an\nepistemic model of the game and for all strategies \\(s_i\\in S_i\\),\n\\(Out_i(v, s_i, S_{-i}(w))=\\{s_i(v)\\}\\). This means, for example, that\nfor any state \\(w\\) in an epistemic model for the extensive game\nin Figure 17, the only strategies that are\nrational at node \\(v_3\\) in \\(w\\) are those that prescribe that \\(A\\)\nchooses \\(O_3\\) at node \\(v_3\\). Therefore, if \\(w\\in \\mathsf{Rat}_A\\), then\n\\(\\sigma_A(w)(v_3)=O_3\\). Whatever \\(A\\) knows, or rather knew about what\n\\(B\\) would do, if the game reaches the node \\(v_3\\), then the only\nrational choice for \\(A\\) is \\(O_3\\).\nInformation about the rationality of players at pre-terminal nodes\nis very important for players choosing earlier in the game. Returning\nto the game in Figure 17, if \\(B\\) knows that \\(A\\)\nis substantively rational at a state \\(w\\) in an epistemic model of the\ngame, then \\(\\Pi_B(w)\\subseteq \\mathsf{Rat}_A\\). Given the above\nargument, this means that if \\(w'\\in \\Pi_B(w)\\), then\n\\(\\sigma_A(w')(v_3)=O_3\\). Thus, we have for any state \\(w\\) in an\nepistemic model of the game, \n\n\\[Out_B(v_2, I_2, S_{-i}(w))=\\{o_3\\};\\]\n\nand, of course, \n\n\\[Out_B(v_2, O_2, S_{-i}(w))=\\{o_2\\}.\\] \n\nBut then,\n\\((O_2)\\) is the only strategy that is rational for \\(B\\) at \\(v_2\\) in any\nstate \\(w\\) (this follows since \\(u_B(o_2)=2\\ge 1=u_B(o_3)\\)). This means\nthat if \\(w\\in \\mathsf{Rat}_B\\) and \\(\\Pi_B(w)\\subseteq \\mathsf{Rat}_A\\),\nthen \\(\\sigma_B(w)(v_2)=O_2\\). Finally, if \\(A\\) knows that \\(B\\) knows that\n\\(A\\) is substantively rational, then\n\n\\[\\Pi_A(w)\\subseteq K_B\\mathsf{Rat}_A=\\{w' \\mid \\Pi_B(w')\\subseteq \\mathsf{Rat}_A\\}.\\] \n\nA similar argument shows that if \\(w\\in \\mathsf{Rat}_A\\) and \\(w\\in\nK_A(K_B(\\mathsf{Rat}_A))\\), then \\(\\sigma_A(w)(v_1)=O_1\\).\nThe strategy profile \\((O_1O_3, O_2)\\) is the unique\npure-strategy sub-game perfect equilibrium (Selten 1975) of\nthe game in Figure 17. Furthermore, the\nreasoning that we went through in the previous paragraphs is very\nclose to backward induction algorithm. This algorithm can be\nused to calculate the sub-game perfect equilibrium in any perfect\ninformation game in which all players receive unique payoffs at each\noutcome.[13]\n The algorithm runs as follows:\n\nBI Algorithm\nAt terminal nodes, players already have the\nnodes marked with their utilities. At a non-terminal node \\(v\\), once\nall immediate successors are marked, the node is marked as follows:\nfind the immediate successor \\(d\\) that has the highest utility for\nplayer \\(\\tau(v)\\) (the players whose turn it is to move at \\(v\\)). Copy\nthe utilities from \\(d\\) onto \\(v\\).\n\nGiven a marked game tree, the unique path that leads from the root\n\\(v_0\\) of the game tree to the outcome with the utilities that match\nthe utilities assigned to \\(v_0\\) is called the backward\ninduction path. In fact, the markings on each and every node\n(even nodes not on the backward induction path) defines a unique path\nthrough the game tree. These paths can be used to define strategies\nfor each player: At each decision node \\(v\\), choose the action that is\nconsistent with the path from \\(v\\). Let \\(BI\\) denote the\nresulting backward induction profile (where each\nplayer is following the strategy given by the backward induction\nalgorithm).\nAumann (1995) showed that the above reasoning can be carried out\nfor any extensive game of perfect information.\n\nTheorem 4.7 (Aumann 1995)\nSuppose that \\(G\\) is an extensive game of perfect information and\n\\({\\mathbf{s}}\\) is a strategy profile for \\(G\\). The following are\nequivalent:\n\nThere is a state \\(w\\) in an epistemic model of \\(G\\) such that\n\\(\\sigma(w) = {\\mathbf{s}}\\) and \\(w \\in C_N(\\mathsf{Rat})\\) (there is\ncommon knowledge that all players are substantively\nrational).\n\\({\\mathbf{s}}\\) is a sub-game perfect equilibrium of \\(G\\).\n\n\nThis result has been extensively discussed. The standard ground of\ncontention is that common knowledge of rationality used in this\nargument seems self-defeating, at least intuitively. Recall\nthat we asked what would \\(B\\) do at node \\(v_2\\) under common knowledge\nof rationality, and we concluded that he would choose \\(O_2\\). But, if\nthe game ever reaches that state, then, by the theorem above, \\(B\\) has\nto conclude that either \\(A\\) is not rational, or that she does not know\nthat he is. Both violate common knowledge of rationality. Is there a\ncontradiction here? This entry will not survey the extensive\nliterature on this question. The reader can consult the references in\nBruin 2010. Our point here is rather that how one looks at this\npotential paradox hinges on the way the players will revise their\nbeliefs in \u201cfuture\u201d rationality in the light of observing\na move that would be \u201cirrational\u201d under common knowledge\nof rationality.\n4.2.3 Common Knowledge of Rationality without Backward Induction\nStalnaker (1996, 1998) offers a different perspective on backward\ninduction. The difference with Aumann\u2019s analysis is best\nillustrated with the following example:\n\n\nFigure 18: An extensive game\n\nIn the above game the backward induction profile is \\((I_1I_3, I_2)\\)\nleading to the outcome \\(o_4\\) with both players receiving a payoff of\n\\(3\\). Consider an epistemic model with a single state \\(w\\) where\n\\(\\sigma(w)=(O_1I_3,O_2)\\). This is not the backward induction profile,\nand so, by Aumann\u2019s Theorem (Theorem\n4.7) it cannot be common knowledge among \\(A\\) and \\(B\\) at state\n\\(w\\) that both \\(A\\) and \\(B\\) are substantively rational.\nRecall that a strategy for a player \\(i\\) specifies choices\nat all decision nodes for \\(i\\), even those nodes that are\nimpossible to reach given earlier moves prescribed by the\nstrategy. Thus, strategies include \u201ccounterfactual\u201d\ninformation about what players would do if they were given a chance to\nmove at each of their decision nodes. In the single state epistemic\nmodel, \\(B\\) knows that \\(A\\) is following the strategy \\(O_1I_3\\). This\nmeans that \\(B\\) knows two things about \\(A\\)\u2019s choice behavior in\nthe game. The first is that \\(A\\) is choosing \\(O_1\\) initially. The\nsecond is that if \\(A\\) where given the opportunity to choose at \\(v_3\\),\nthen she would choose \\(I_3\\). Now, given \\(B\\)\u2019s knowledge about\nwhat \\(A\\) is doing, there is a sense in which whatever \\(B\\) would\nchoose at \\(v_2\\), his choice is rational. This follows trivially\nsince \\(A\\)\u2019s initial choice prescribed by her strategy at \\(w\\)\nmakes it impossible for \\(B\\) to move. Say that a player \\(i\\)\nis materially rational at a state \\(w\\) in an epistemic\nmodel of a game if \\(i\\) is rational at all decision nodes \\(v\\in V_i\\) in\nstate \\(w\\) that are reachable according to the strategy profile\n\\(\\sigma(w)\\). We have seen that \\(B\\) is trivially materially\nrational. Furthermore, \\(A\\) is materially rational since she knows that\n\\(B\\) is choosing \\(O_2\\) (i.e., \\(S_{-A}(w)=\\{O_2\\}\\)). Thus, \\(Out_A(v_1,\nO_1, S_{-i}(w))=\\{o_1\\}\\) and \\(Out_A(v_1, O_I, S_{-i}(w))=\\{o_2\\}\\); and\nso, \\(A\\)\u2019s choice of \\(O_1\\) at \\(v_1\\) makes her materially rational\nat \\(w\\). The main point of contention between Aumann and Stalnaker\nboils down to whether the single state epistemic model includes enough\ninformation about what exactly \\(B\\) thinks about \\(A\\)\u2019s choice at\n\\(v_3\\) when assessing the rationality of\n\\(B\\)\u2019s hypothetical choice of \\(O_2\\) at \\(v_2\\).\nAccording to Aumann, \\(B\\) is not substantively rational: Since\n\\(S_{-B}(w)=\\{O_1I_3\\}\\), we have\n\n\\[Out_B(v_2, O_2, S_{-B}(w))=\\{o_2\\}\\]\n\nand \n\n\\[Out_B(v_2, I_2, S_{-B}(w))=\\{o_4\\};\\] \n\nand so, \\(B\\) is not\nrational at \\(v_2\\) in \\(w\\) (note that\n\\(u_B(o_4)=3>1=u_B(o_2)\\)). Stalnaker suggests that the players\nshould be endowed with a belief revision policy that\ndescribes which informational state they would revert to in case they\nwere to observe moves that are inconsistent with what they know about\ntheir opponents\u2019 strategies. If \\(B\\) does learn that he can in\nfact move, then he has learned something about \\(A\\)\u2019s\nstrategy. In particular, he now knows that she cannot be following any\nstrategy that prescribes that she chooses \\(O_1\\) at \\(v_1\\) (so, in\nparticular, she cannot be following the strategy \\(O_1I_3\\).) Suppose\nthat \\(B\\) is disposed to react to surprising information about\n\\(A\\)\u2019s choice of strategy as follows: Upon learning that \\(A\\) is\nnot following a strategy in which she chooses \\(O_1\\) at \\(v_1\\), he\nconcludes that she is following strategy \\(I_1O_3\\). That is,\n\\(B\\)\u2019s \u201cbelief revision policy\u201d can be summarized as\nfollows: If \\(A\\) makes one \u201cirrational move\u201d, then she will\nmake another one. Stalnaker explains the apparent tension between this\nbelief revision policy and his knowledge that if \\(A\\) where given the\nopportunity to choose at \\(v_3\\), then she would choose \\(I_3\\) as\nfollows:\n\nTo think there is something incoherent about this combination of\nbeliefs and belief revision policy is to confuse epistemic with causal\ncounterfactuals\u2014it would be like thinking that because I believe\nthat if Shakespeare hadn\u2019t written Hamlet, it would have never\nbeen written by anyone, I must therefore be disposed to conclude that\nHamlet was never written, were I to learn that Shakespeare was in fact\nnot its author. (Stalnaker 1996: 152)\n\nThen, with respect to \\(B\\)\u2019s appropriately updated knowledge\nabout \\(A\\)\u2019s choice at \\(v_3\\) (according to his specified belief\nrevision policy), his strategy \\(O_2\\) is in fact rational. According to\nStalnaker, the rationality of a choice at a node \\(v\\) should be\nevaluated in the (counterfactual) epistemic state the player would\nbe in if that node was reached. Assuming \\(A\\) knows that \\(B\\) is\nusing the belief revision policy described above, then \\(A\\) knows that\n\\(B\\) is substantively rational in Stalnaker\u2019s sense. If the model\nincludes explicit information about the players\u2019 belief revision\npolicy, then there can be common knowledge of substantive rationality\n(in Stalnaker\u2019s sense) yet the players\u2019 choices do not\nconform to the backward induction profile.\n4.3 Common strong belief and forward induction\nIn the previous section, we assumed that the players interpret an\nopponent\u2019s deviation from expected play in an extensive game\n(e.g., deviation from the backward induction path) as an indication\nthat that player will choose \u201cirrationally\u201d at future\ndecision nodes. However, this is just one example of a belief revision\npolicy. It is not suggested that this is the belief revision policy\nthat players should adopt. Stalnaker\u2019s central claim is\nthat models of extensive games should include a component that\ndescribes the players\u2019 disposition to change their beliefs\nduring a play of the game, which may vary from model to model or even\namong the players in a single model:\n\nFaced with surprising behavior in the course of a game, the players\nmust decide what then to believe. Their strategies will be based on\nhow their beliefs would be revised, which will in turn be based on\ntheir epistemic priorities\u2014whether an unexpected action should\nbe regarded as an isolated mistake that is thereby epistemically\nindependent of beliefs about subsequent actions, or whether it\nreveals, intentionally or inadvertently, something about the\nplayer\u2019s expectations, and so about the way she is likely to\nbehave in the future. The players must decide, but the theorists\nshould not\u2014at least they should not try to generalize about\nepistemic priorities that are meant to apply to any rational agent in\nall situations. (Stalnaker 1998: 54)\n\nOne belief revision policy that has been extensively discussed in\nthe epistemic game theory literature is the rationalizability\nprinciple. Battigalli (1997) describes this belief revision\npolicy as follows:\n\nRationalizability Principle\nA player should always try to interpret her information about the\nbehavior of her opponents assuming that they are not implementing\n\u2018irrational\u2019 strategies.\n\nThis belief revision policy is closely related to\nso-called forward induction reasoning. To illustrate,\nconsider the following imperfect information game:\n\n\nFigure 19\n\nIn the above game, \\(A\\) can either exit the game initially (by\nchoosing \\(e\\)) for a guaranteed payoff of \\(2\\) or decide to play a game\nof imperfect information with \\(B\\). Notice that \\(r_1\\) is strictly\ndominated by \\(e\\): No matter what \\(B\\) chooses at \\(v_3\\), \\(A\\) is better\noff choosing \\(e\\). This means that if \\(A\\) is following a rational\nstrategy, then she will not choose \\(r_1\\) at \\(v_1\\). According to the\nrationalizability principle, \\(B\\) is disposed to believe that \\(A\\) did\nnot choose \\(r_1\\) if he is given a chance to move. Thus, assuming that\n\\(B\\) knows the structure of the game and revises his beliefs according\nto the rationalizability principle, his only rational strategy is to\nchoose \\(l_2\\) at his informational cell (consisting of \\(\\{v_2,\nv_3\\})\\). If \\(A\\) can anticipate this reasoning, then her only rational\nstrategy is to choose \\(e\\) at \\(v_1\\). This is the forward induction\noutcome of the above game.\nBattigalli & Siniscalchi (2002) develop an epistemic analysis\nof forward induction reasoning in extensive games (cf. also, Stalnaker\n1998: sec. 6). They build on an idea of Stalnaker (1998, 1996) to\ncharacterize forward induction solution concepts in terms of\ncommon strong belief in rationality. We discussed the\ndefinition of \u201cstrong belief\u201d in Section\n2.4. The mathematical representation of beliefs in Battigalli\n& Siniscalchi (2002) is different, although the underlying idea is\nthe same. A player strongly believes an event \\(E\\) provided she\nbelieves \\(E\\) is true at the beginning of the game (in the sense that\nshe assigns probability 1 to \\(E\\)) and continues to believe \\(E\\) as long\nas it is not falsified by the evidence. The evidence\navailable to a player in an extensive game consists of the\nobservations of the previous moves that are consistent with the\nstructure of the game tree\u2014i.e., the paths through a game\ntree. A complete discussion of this approach is beyond the scope of\nthe entry. Consult Battigalli & Siniscalchi (2002); Baltag et\nal. (2009); Battigalli & Friedenberg (2012); Bonanno (2013); Perea\n(2012, 2014); and van Benthem & Gheerbrant (2010) for a discussion\nof this approach and alternative epistemic analyses of backward and\nforward induction.\n5. Developments\nIn this section, we present a number of results that build on the\nmethodology presented in the previous section. We discuss the\ncharacterization of the Nash equilibrium, incorporate considerations\nof weak dominance into the players\u2019 reasoning and allow the\nplayers to be unaware, as opposed to uncertain,\nabout some aspects of the game.\n5.1 Nash Equilibrium\n5.1.1 The Result\nIterated elimination of strictly dominated strategies is a very\nintuitive concept, but for many games it does not tell anything about\nwhat the players will or should choose. In coordination games\n(Figure 1 above) for instance, all\nprofiles, can be played under rationality and common belief of\nrationality.\nLooking again at Figure 1, one can\nask what would happen if Bob knew (that is had correct\nbeliefs about) Ann\u2019s strategy choice? Intuitively, it is quite\nclear that his rational choice is to coordinate with her. If\nhe knows that she plays \\(t\\), for instance, then playing \\(l\\)\nis clearly the only rational choice for him, and similarly, if he\nknows that she plays \\(b\\), then \\(r\\) is the only rational choice. The\nsituation is symmetric for Ann. For instance, if she knows that Bob\nplays \\(l\\), then her only rational choice is to choose \\(t\\). More\nformally, the only states where Ann is rational and her\ntype knows (i.e., is correct and assigns probability 1 to)\nBob\u2019s strategy choice and where Bob is also rational and his\ntype knows Ann\u2019s strategy choices are states where they\nplay either \\((t,l)\\) or \\((b,r)\\), the pure-strategy Nash equilibria of\nthe game.\nA Nash equilibrium is a profile where no player has an\nincentive to unilaterally deviate from his strategy choice. In other\nwords, a Nash equilibrium is a combination of (possibly mixed)\nstrategies such that they all play their best response given the\nstrategy choices of the others. Again, \\((t,l)\\) and \\((b,r)\\) are the\nonly pure-strategy equilibria of the above coordination game. Nash\nequilibrium, and its numerous refinements, is arguably the game\ntheoretical solution concept that has been most used in game theory\n(Aumann & Hart 1994) and philosophy (e.g., famously in Lewis\n1969).\nThe seminal result of Aumann & Brandenburger 1995 provides an\nepistemic characterization of the Nash equilibrium in terms\nof mutual knowledge of strategy choices (and the structure of\nthe game). See, also, Spohn (1982) for an early statement. Before\nstating the theorem, we discuss an example from Aumann &\nBrandenburger (1995) that illustrates the key ideas. Consider the\nfollowing coordination game:\n\n\n \u00a0 B \nA \n\nl r\nu 2,2  0,0 \nd 0,0  1,1 \n\n\n\nFigure 20\n\nThe two pure-strategy Nash equilibria are \\((u,l)\\) and \\((d,r)\\)\n(there is also a mixed-strategy equilibrium). As usual, we fix an\ninformational context for this game. Let \\(\\T\\) be a type space for the\ngame with three types for each player \\(T_A=\\{a_1,a_2, a_3\\}\\) and\n\\(T_B=\\{b_1,b_2,b_3\\}\\) with the following type functions:\n\n\n\n\nl r\n\\(b_1\\) 0.5  0.5 \n\\(b_2\\) 0  0 \n\\(b_3\\) 0  0 \n\n\u00a0\u00a0\u00a0\u00a0\u00a0\n\nl r\n\\(b_1\\) 0.5  0 \n\\(b_2\\) 0  0 \n\\(b_3\\) 0  0.5 \n\n\u00a0\u00a0\u00a0\u00a0\u00a0\n\nl r\n\\(b_1\\) 0  0 \n\\(b_2\\) 0  0.5 \n\\(b_3\\) 0  0.5 \n\n\n\n\\(\\lambda_A(a_1)\\)\n\\(\\lambda_A(a_2)\\)\n\\(\\lambda_A(a_3)\\)\n\n\n\n\n\nl r\n\\(a_1\\) 0.5  0 \n\\(a_2\\) 0  0.5 \n\\(a_3\\) 0  0 \n\n\u00a0\u00a0\u00a0\u00a0\u00a0\n\nl r\n\\(a_1\\) 0.5  0 \n\\(a_2\\) 0  0 \n\\(a_3\\) 0  0.5 \n\n\u00a0\u00a0\u00a0\u00a0\u00a0\n\nl r\n\\(a_1\\) 0  0 \n\\(a_2\\) 0  0.5 \n\\(a_3\\) 0  0.5 \n\n\n\n\\(\\lambda_B(b_1)\\)\n\\(\\lambda_B(b_2)\\)\n\\(\\lambda_B(b_3)\\)\n\n\nFigure 21\n\nConsider the state \\((d,r,a_3,b_3)\\). Both \\(a_3\\) and \\(b_3\\) correctly\nbelieve (i.e., assign probability 1 to) that the outcome is \\((d,r)\\)\n(we have \\(\\lambda_A(a_3)(r)=\\lambda_B(b_3)(d)=1\\)). This fact is not\ncommon knowledge: \\(a_3\\) assigns a 0.5 probability to Bob being of type\n\\(b_2\\), and type \\(b_2\\) assigns a 0.5 probability to Ann playing\n\\(l\\). Thus, Ann does not know that Bob knows that she is playing \\(r\\)\n(here, \u201cknowledge\u201d is identified with \u201cprobability\n1\u201d as it is in Aumann & Brandenburger 1995). Furthermore,\nwhile it is true that both Ann and Bob are rational, it is not common\nknowledge that they are rational. Indeed, the type \\(a_3\\) assigns a 0.5\nprobability to Bob being of type \\(b_2\\) and choosing \\(r\\); however, this\nis irrational since \\(b_2\\) believes that both of Ann\u2019s options\nare equally probable.\nThe example above is a situation where there is mutual knowledge of\nthe choices of the players. Indeed, it is not hard to see that in any\ntype space for a 2-player game \\(G\\), if \\((s,t)\\) is a state where there\nis mutual knowledge that player \\(i\\) is choosing \\(s_i\\) and the players\nare rational, then, \\(s\\) constitutes a (pure-strategy) Nash\nEquilibrium. There is a more general theorem concerning mixed strategy\nequilibrium. Recall that a conjecture for player \\(i\\) is a probability\nmeasure over the strategy choices of her opponents.\n\nTheorem 5.1  (Aumann & Brandenburger 1995: Theorem A)\nSuppose that \\(G\\) is a 2-person strategic game, \\((p_1,p_2)\\) are\nconjectures for players 1 and 2, and \\(\\T\\) is a type space for \\(G\\). If\n\\((s,t)\\) is a state in \\(\\T\\) where for \\(i=1,2\\), \\(t_i\\) assigns\nprobability 1 to the events (a) both players are rational (i.e.,\nmaximize expected utility), (b) the game is \\(G\\) and (c) for \\(i=1,2\\),\nplayer \\(i\\)\u2019s conjecture is \\(p_i\\), then \\((p_1, p_2)\\) constitutes\na Nash equilibrium.\n\nThe general version of this result, for arbitrary finite number of\nagents and allowing for mixed strategies, requires common\nknowledge of conjectures, i.e., of each player\u2019s\nprobabilistic beliefs in the other\u2019s choices. See Aumann &\nBrandenburger (1995: Theorem B) for precise formulation of the result,\nand, again, Spohn (1982) for an early version. See, also, Perea (2007)\nand Tan & Werlang (1988) for similar results about the Nash\nequilibrium.\n5.1.2 Philosophical Issues\nThis epistemic characterization of Nash equilibrium requires\nmutual knowledge and rather than beliefs. The result fails\nwhen agents can be mistaken about the strategy choice of the\nothers. This has lead some authors to criticize this epistemic\ncharacterization: See Gintis (2009) and Bruin (2010), for\ninstance. How could the players ever know what the others are\nchoosing? Is it not contrary to the very idea of a game, where the\nplayers are free to choose whatever they want (Baltag et\nal. 2009)?\nOne popular response to this criticism (Brandenburger 2010; Perea\n2012) is that the above result tells us something about Nash\nequilibrium as a solution concept, namely that it\nalleviates strategic uncertainty. Indeed, returning to the\nterminology introduced in Section 1.3, the\nepistemic conditions for Nash equilibrium are those that correspond to\nthe ex post state of information disclosure, \u201cwhen all\nis said and done\u201d, to put it figuratively. When players have\nreached full knowledge of what the others are going to do, there is\nnothing left to think about regarding the other players as rational,\ndeliberating agents. The consequences of each of the players\u2019\nactions are now certain. The only task that remains is to compute\nwhich action is recommended by the adopted choice rule, and this does\nnot involve any specific information about the other players\u2019\nbeliefs. Their choices are fixed, after all.\nThe idea here is not to reject the epistemic characterization of\nNash Equilibrium on the grounds that it rests on unrealistic\nassumptions, but, rather, to view it as a lesson learned about Nash\nEquilibrium itself. From an epistemic point of view, where one is\nfocused on strategic reasoning about what others are going to\ndo and are thinking, this solution concepts might be of less\ninterest.\nThere is another important lesson to draw from this epistemic\ncharacterization result. The widespread idea that game theory\n\u201cassumes common knowledge of rationality\u201d, perhaps in\nconjunction with the extensive use of equilibrium concepts in\ngame-theoretic analysis, has lead to misconception that the Nash\nEquilibrium either requires common knowledge of rationality,\nor that common knowledge of rationality is sufficient for the players\nto play according to a Nash equilibrium. To be sure, game theoretic\nmodels do assume that the structure of the game is common knowledge\n(though, see Section 5.3). Nonetheless, the\nabove result shows that both of these ideas are incorrect:\n\n\nCommon knowledge of rationality is neither necessary nor sufficient\nfor Nash Equilibrium.\n\nIn fact, as we just stressed, Nash equilibrium can be played under\nfull uncertainty, and a fortiori under higher-order\nuncertainty, about the rationality of others.\n5.1.3 Remarks on \u201cModal\u201d Characterizations of Nash Equilibrium\nIn recent years, a number of so-called \u201cmodal\u201d\ncharacterizations of Nash Equilibrium have been proposed, mostly using\ntechniques from modal logic (see Hoek & Pauly 2007 for\ndetails). These results typically devise a modal logical language to\ndescribe games in strategic form, typically including modalities for\nthe players\u2019 actions and preference, and show that the notion of\nprofile being a Nash Equilibrium language is definable in\nsuch a language.\nMost of these characterizations are not epistemic, and thus fall\noutside the scope of this entry. In context of this entry, it is\nimportant to note that most of these results aim at something\ndifferent than the epistemic characterization which we are discussing\nin this section. Mostly developed in Computer Sciences, these logical\nlanguages have been used to verify properties of multi-agents systems,\nnot to provide epistemic foundations to this solution\nconcept. However, note that in recent years, a number of logical\ncharacterizations of Nash equilibrium do explicitly use epistemic\nconcepts (see, for example, van Benthem et al. 2009; Lorini &\nSchwarzentruber 2010).\n5.2 Incorporating Admissibility and \u201cCautious\u201d Beliefs\nIt is not hard to find a game and an informational context where\nthere is at least one player without a unique \u201crational\nchoice\u201d. How should a rational player incorporate the\ninformation that more than one action is classified as\n\u201cchoice-worthy\u201d or \u201crationally permissible\u201d\n(according to some choice rule) for her opponent(s)? In such a\nsituation, it is natural to require that the player does not rule\nout the possibility that her opponent will pick a\n\u201cchoice-worthy\u201d option. More generally, the players should\nbe \u201ccautious\u201d about which of their opponents\u2019\noptions they rule out.\nAssuming that the players\u2019 beliefs are \u201ccautious\u201d\nis naturally related to weak dominance (recall the characterization of\nweak dominance, Section 3.2\nin which a strategy is weakly dominated iff it does not maximize\nexpected utility with respect to any full support probability\nmeasure). A key issue in epistemic game theory is the epistemic\nanalysis of iterated removal of weakly dominated strategies. Many\nauthors have pointed out puzzles surrounding such an analysis (Asheim\n& Dufwenberg 2003; Brandenburger, Friedenberg & Keisler 2008;\nCubitt & Sugden 1994; Samuelson 1992). For example, Samuelson\n(1992) showed (among other things) that the analogue of Theorem 4.1 is\nnot true for iterated removal of weakly dominated strategies. The main\nproblem is illustrated by the following game:\n\n\n \u00a0 Bob \nAnn \n\nl r\nu 1,1  1,0 \nd 1,0  0,1 \n\n\n\nFigure 22\n\nIn the above game, \\(d\\) is weakly dominated by \\(u\\) for Ann. If Bob\nknows that Ann is rational (in the sense that she will not choose a\nweakly dominated strategy), then he can rule out option \\(d\\). In the\nsmaller game, action \\(r\\) is now strictly dominated by \\(l\\) for Bob. If\nAnn knows that Bob is rational and that Bob knows that she is rational\n(and so, rules out option \\(d\\)), then she can rule out option\n\\(r\\). Assuming that the above reasoning is transparent to both Ann and\nBob, it is common knowledge that Ann will play \\(u\\) and Bob will play\n\\(l\\). But now, what is the reason for Bob to rule out the possibility\nthat Ann will play \\(d\\)? He knows that Ann knows that he is going to\nplay \\(l\\) and both \\(u\\) and \\(d\\) are best responses to \\(l\\). The problem\nis that assuming that the players\u2019 beliefs are cautious\nconflicts with the logic of iterated removal of weakly dominated\nstrategies. This issue is nicely described in a well-known microeconomics\ntextbook:\n\n[T]he argument for deletion of a weakly dominated strategy for\nplayer \\(i\\) is that he contemplates the possibility that every strategy\ncombination of his rivals occurs with positive probability. However,\nthis hypothesis clashes with the logic of iterated deletion, which\nassumes, precisely that eliminated strategies are not expected to\noccur. (Mas-Colell, Winston, & Green 1995: 240)\n\nThe extent of this conflict is nicely illustrated in Samuelson\n(1992). In particular, Samuelson (1992) shows that there is no\nepistemic-probability \nmodel[14]\n of the above game with\na state satisfying common knowledge of rationality (where\n\u201crationality\u201d means that players do not choose weakly\ndominated strategies). Prima facie, this is puzzling: What\nabout the epistemic-probability model consisting of a single state \\(w\\)\nassigned the profile \\((u, l)\\)? Isn\u2019t this a model of the above\ngame where there is a state satisfying common knowledge that the\nplayers do not choose weakly dominated strategies? The problem is that\nthe players do not have \u201ccautious\u201d beliefs in this model\n(in particular, Bob\u2019s beliefs are not cautious in the sense\ndescribed below). Recall that having a cautious belief means that a\nplayer cannot know which options her opponent(s)\nwill \npick[15]\n from a set of choice-worthy options (in the\nabove game, if Ann knows that Bob is choosing \\(l\\), then both\n\\(u\\) and \\(d\\) are \u201cchoice-worthy\u201d, so Bob\ncannot know that Ann is choosing \\(u\\)). This suggests an\nadditional requirement on a game model: Let \\(\\M=\\epprobmodel\\) be an\nepistemic-probability model. For each action \\(a\\in \\cup_{i\\in\\Agt}\nS_i\\), let \\({[\\![{a}]\\!]}=\\{w \\mid (\\sigma(w))_i=a\\}\\).\nIf \\(a\\in S_i\\) is rational for player \\(i\\) at state\n\\(w\\), then for all players \\(j\\ne i\\), \\({[\\![{a}]\\!]}\\cap \\Pi_j(w)\\ne\n\\emptyset\\).\nThis means that a player cannot know that her opponent\nwill not choose an action at a state \\(w\\) which is deemed rational\n(according to some choice rule). This property is called\n\u201cprivacy of tie-breaking\u201d by Cubitt and Sugden (2011: 8)\nand \u201cno extraneous beliefs\u201d by Asheim and Dufwenberg\n(2003).[16]\n For an extended discussion of the above\nassumption see Cubitt & Sugden (2011).\nGiven the above considerations, the epistemic analysis of iterated\nweak dominance is not a straightforward adaptation of the analysis of\niterated strict dominance discussed in the previous section. In\nparticular, any such analysis must resolve the conflict between\nstrategic reasoning where players rule out certain strategy\nchoices of their opponent(s) and admissibility considerations where\nplayers must consider all of their opponents\u2019\noptions possible. A number of authors have developed\nframeworks that do resolve this conflict (Brandenburger et al. 2008;\nAsheim & Dufwenberg 2003; Halpern & Pass 2009). We sketch one\nof these solutions below:\nThe key idea is to represent the players\u2019 beliefs as\na lexicographic probability system (LPS). An LPS is a finite\nsequence of probability measures \\((p_1,p_2,\\ldots,p_n)\\) with supports\n(The support of a probability measure \\(p\\) defined on\na set of states \\(W\\) is the set of all states that have nonzero\nprobability; formally, \\(Supp(p)=\\{w \\mid p(w)>0\\}\\)) that do not\noverlap. This is interpreted as follows: if \\((p_1,\\ldots,p_n)\\)\nrepresents Ann\u2019s beliefs, then \\(p_1\\) is Ann\u2019s\n\u201cinitial hypothesis\u201d about what Bob is going to do, \\(p_2\\)\nis Ann\u2019s secondary hypothesis, and so on. In the above game, we\ncan describe Bob\u2019s beliefs as follows: his initial hypothesis\nis that Ann will choose \\(U\\) with probability 1 and his secondary\nhypothesis is that she will choose \\(D\\) with probability 1. The\ninterpretation is that, although Bob does not rule out the possibility\nthat Ann will choose \\(D\\) (i.e., choose irrationally), he does consider\nit infinitely less likely than her choosing \\(U\\) (i.e.,\nchoosing rationally).\nSo, representing beliefs as lexicographic probability measures\nresolves the conflict between strategic reasoning and the assumption\nthat players do not play weakly dominated strategies. However, there\nis another, more fundamental, issue that arises in the epistemic\nanalysis of iterated weak dominance:\n\nUnder admissibility, Ann considers everything possible. But this is\nonly a decision-theoretic statement. Ann is in a game, so we imagine\nshe asks herself: \u201cWhat about Bob? What does he consider\npossible?\u201d If Ann truly considers everything possible, then it\nseems she should, in particular, allow for the possibility that Bob\ndoes not! Alternatively put, it seems that a full analysis of the\nadmissibility requirement should include the idea that other players\ndo not conform to the requirement.  (Brandenburger et al. 2008:\n313)\n\nThere are two main ingredients to the epistemic characterization of\niterated weak dominance. The first is to represent the players\u2019\nbeliefs as lexicographic probability systems. The second is to use a\nstronger notion of belief: A player assumes an event\n\\(E\\) provided \\(E\\) is infinitely more likely than \\(\\overline{E}\\) (on\nfinite spaces, this means each state in \\(E\\) is infinitely more likely\nthan states not in \\(E\\)). The key question is: What is the precise\nrelationship between the event \u201crationality and common\nassumption of rationality\u201d and the strategies that survive\niterated removal of weakly dominated strategies? The precise answer\nturns out to be surprisingly subtle\u2014the details are beyond the\nscope of this article (see Brandenburger et al. 2008).\n5.3 Incorporating Unawareness\nThe game models introduced in Section 2 have\nbeen used to describe the uncertainty that the players have about what\ntheir opponents are going to do and are thinking in a game\nsituation. In the analyses provided thus far, the structure\nof the game (i.e., who is playing, what are the preferences of the\ndifferent players, and which actions are available) is assumed to be\ncommon knowledge among the players. However, there are many situations\nwhere the players do not have such complete information about\nthe game. There is no inherent difficulty in using the models\nfrom Section 2 to describe situations where\nplayers are not perfectly informed about the structure of the\ngame (for example, where there is some uncertainty about available\nactions).\nThere is, however, a foundational issue that arises here. Suppose\nthat Ann considers it impossible that her opponent will\nchoose action \\(a\\). Now, there are many reasons why Ann would hold such\nan opinion. On the one hand, Ann may know something about what her\nopponent is going to do or is thinking which allows her to rule out\naction \\(a\\) as a live possibility\u2014i.e., given all the evidence\nAnn has about her opponent, she concludes that action \\(a\\) is just not\nsomething her opponent will do. On the other hand, Ann may not even\nconceive of the possibility that her opponent will choose action\n\\(a\\). She may have a completely different model of the game in mind\nthan her opponents. The foundational question is: Can the game models\nintroduced in Section 2 faithfully represent\nthis latter type of uncertainty?\nThe question is not whether one can formally describe what Ann\nknows and believes under the assumption that she considers it\nimpossible that her opponent will choose action \\(a\\). Indeed, an\nepistemic-probability model where Ann assigns probability zero to the\nevent that her opponent chooses action \\(a\\) is a perfectly good\ndescription of Ann\u2019s epistemic state. The problem is that this\nmodel blurs an important distinction between Ann\nbeing unaware that action \\(a\\) is a live possibility and\nAnn ruling out that action \\(a\\) is a viable option for her\nopponent. This distinction is illustrated by the following snippet\nfrom the well-known Sherlock Holmes\u2019 short story Silver Blaze\n(Doyle 1894):\n\n\u2026I saw by the inspector\u2019s face that his attention had\nbeen keenly aroused.  \u201cYou consider that to be\nimportant?\u201d he [Inspector Gregory] asked.\n\u201cExceedingly so.\u201d \u201cIs there any point to which\nyou would wish to draw my attention?\u201d \u201cTo the\ncurious incident of the dog in the night-time.\u201d \u201cThe\ndog did nothing in the night-time.\u201d \u201cThat was the\ncurious incident,\u201d remarked Sherlock Holmes.\n\nThe point is that Holmes is aware of a particular event (\u201cthe\ndog not barking\u201d) and uses this to come to a conclusion. The\ninspector is not aware of this event, and so cannot (without\nHolmes\u2019 help) come to the same conclusion. This is true of many\ndetective stories: clever detectives not only have the ability to\n\u201cconnect the dots\u201d, but they are also aware of\nwhich dots need to be connected. Can we describe the inspector\u2019s\nunawareness in an epistemic \nmodel?[17]\nSuppose that \\(U_i(E)\\) is the event that the player \\(i\\) is unaware\nof the event \\(E\\). Of course, if \\(i\\) is unaware of \\(E\\) then \\(i\\) does\nnot know that \\(E\\) is true (\\(U_i(E)\\subseteq\n\\overline{K_i(E)}\\), where \\(\\overline{X}\\) denotes the complement\nof the event \\(X\\)). Recall that in epistemic models (where the\nplayers\u2019 information is described by partitions), we have the\nnegative introspection property:\n\n\\[\\overline{K_i(E)}\\subseteq K_i(\\overline{K_i(E)}).\\] \n\nThis means that if \\(i\\) is unaware of \\(E\\),\nthen \\(i\\) knows that she does not know that \\(E\\). Thus, to capture a\nmore natural definition of \\(U_i(E)\\) where \n\n\\[U_i(E) \\subseteq \\overline{K_i(E)} \\cap \\overline{K_i(\\overline{K_i(E)})},\\] \n\nwe need to\nrepresent the players\u2019 knowledge in a possibility\nstructure where the \\(K_i\\) operators do not necessarily satisfy\nnegative introspection. A possibility structure is a tuple \\(\\langle W,\n\\{P_i\\}_{i\\in\\A}, \\sigma\\rangle\\) where \\(P_i:W\\rightarrow \\pow(W)\\). The\nonly difference with an epistemic model is that the \\(P_i(w)\\) do not\nnecessarily form a partition of \\(W\\). We do not go into details\nhere\u2014see Halpern (1999) for a complete discussion of possibility\nstructures and how they relate to epistemic models. The knowledge\noperator is defined as it is for epistemic models: for each event \\(E\\),\n\\(K_i(E)=\\{w \\mid P_i(w)\\subseteq E\\}\\). However, S. Modica and\nA. Rustichini (1994, 1999) argue that even the more general\npossibility structures cannot be used to describe a player\u2019s\nunawareness.\nA natural definition of unawareness on possibility structures is:\n\n\\[U(E) = \\overline{K(E)} \\cap \\overline{K(\\overline{K(E)})} \\cap\n\\overline{K(\\overline{K(\\overline{K(E)})}}\\cap \\cdots\n\\]\n\nThat is, an agent is unaware of \\(E\\) provided the agent does not\nknow that \\(E\\) obtains, does not know that she does not know that \\(E\\)\nobtains, and so on. Modica and Rustichini use a variant of the above\nSherlock Holmes story to show that there is a problem with this\ndefinition of unawareness.\nSuppose there are two signals: A dog barking (\\(d\\)) and a cat\nhowling (\\(c\\)). Furthermore, suppose there are three states \\(w_1\\),\n\\(w_2\\) in which the dog barks and \\(w_3\\) in which the cat howls. The\nevent that there is no intruder is \\(E=\\{w_1\\}\\) (the lack of the two\nsignals indicates that there was no\nintruder[18]).\n The following possibility structure\n(where there is an arrow from state \\(w\\) to state \\(v\\) provided \\(v\\in\nP(w)\\)) describes the inspector\u2019s epistemic state:\n\n\nFigure 23\n\nConsider the following calculations:\n\n\\(K(E)=\\{w_2\\}\\) (at \\(w_2\\), Watson knows there is a human intruder)\nand \\(-K(E)=\\{w_1,w_3\\}\\)\n\\(K(-K(E))=\\{w_3\\}\\) (at \\(w_3\\), Watson knows that she does not know\n\\(E\\)), and \\(-K(-K(E))=\\{w_1,w_2\\}\\).\n\\(-K(E)\\cap -K(-K(E))=\\{w_1\\}\\) and, in fact, \\(\\bigcap_{i=1}^\\infty\n(-K)^i(E)=\\{w_1\\}\\)\nLet \\(U(F)=\\bigcap_{i=1}^\\infty (-K)^i(F)\\). Then,\n\n\\(U(\\emptyset)=U(W)=U(\\{w_1\\})=U(\\{w_2,w_3\\})=\\emptyset\\)\n\\(U(E)=U(\\{w_3\\})=U(\\{w_1,w_3\\})=U(\\{w_1,w_2\\}=\\{w_1\\}\\)\n\n\nSo, \\(U(E)=\\{w_1\\}\\) and \\(U(U(E))=U(\\{w_1\\})=\\emptyset\\). This means\nthat at state \\(w_1\\), the Inspector is unaware of \\(E\\), but is not\nunaware that he is unaware of \\(E\\). More generally, Dekel et al. (1998)\nshow that there is no nontrivial unawareness operator \\(U\\) satisfying\nthe following properties:\n\n\\(U(E) \\subseteq \\overline{K(E)}\\cap \\overline{K(E)}\\)\n\\(K(U(E))=\\emptyset\\)\n\\(U(E)\\subseteq U(U(E))\\)\n\nThere is an extensive literature devoted to developing models that\ncan represent the players\u2019 unawareness. See Board, Chung, &\nSchipper (2011); Chen, Ely, & Luo (2012); E. Dekel et al. (1998);\nHalpern (2001a); Halpern & Rego (2008); and Heifetz, Meier, &\nSchipper (2006) for a discussion of issues related to this entry. The\nUnawareness Bibliography (see Other Internet\nResources) has an up-to-date list of papers in this area.\n6. A Paradox of Self-Reference in Game Models\nThe first step in any epistemic analysis of a game is to describe\nthe players\u2019 knowledge and beliefs using (a possible variant of)\none of the models introduced in Section 2. As we\nnoted already in Section 2.2, there will be\nstatements about what the players know and believe about the game\nsituation and about each other that are commonly known in some models\nbut not in others.\n\nIn any particular structure, certain beliefs, beliefs about belief,\n\u2026, will be present and others won\u2019t be. So, there is an\nimportant implicit assumption behind the choice of a structure. This\nis that it is \u201ctransparent\u201d to the players that the\nbeliefs in the type structure\u2014and only those beliefs\u2014are\npossible \u2026.The idea is that there is a \u201ccontext\u201d to\nthe strategic situation (e.g., history, conventions, etc.) and this\n\u201ccontext\u201d causes the players to rule out certain\nbeliefs. (Brandenburger & Friedenberg 2010: 801)\n\nRuling out certain configurations of beliefs\nconstitute substantive assumptions about the players\u2019\nreasoning during the decision making process. In other words,\nsubstantive assumptions are about how, and how much, information is\nimparted to the agents, over and above those that are intrinsic to the\nmathematical formulation of the structures used to describe the\nplayers\u2019 information. It is not hard to see that one always\nfinds substantive assumptions in finite structures: Given a countably\ninfinite set of atomic propositions, for instance, in finite\nstructures it will always be common knowledge that some logically\nconsistent combination of these basic facts are not realized,\nand a fortiori for logically consistent configurations of\ninformation and higher-order information about these basic facts. On\nthe other hand, monotonicity of the belief/knowledge operator is a\ntypical example of an assumption that is not\nsubstantive. More generally, there are no models of games, as we\ndefined in Section 2, where it is not common\nknowledge that the players believe all the logical consequences of\ntheir beliefs.[19]\nCan we compare models in terms of the number of substantive\nassumptions that are made? Are there models that make no, or at least\nas few as possible, substantive assumptions? These questions have been\nextensively discussed in the epistemic foundations of game\ntheory\u2014see the discussion in Samuelson (1992) and the references\nin Moscati (2009). Intuitively, a structure without any substantive\nassumptions must represent all possible states of (higher-order)\ninformation. Whether such a structure exists will depend, in part, on\nhow the players\u2019 informational attitudes are\nrepresented\u2014e.g., as (conditional/lexicographic) probability\nmeasures or set-valued knowledge/belief functions. These questions\nhave triggered interest in the existence of \u201crich\u201d models\ncontaining most, if not all, possible configurations of (higher-order)\nknowledge and beliefs.\nThere are different ways to understand what it means for a\nstructure to minimize the substantive assumptions about the\nplayers\u2019 higher-order information. We do not attempt a complete\noverview of this interesting literature here (see Brandenburger &\nKeisler (2006: sec. 11) and Siniscalchi (2008: sec. 3) for discussion\nand pointers to the relevant results). One approach considers the\nspace of all (Harsanyi type-/Kripke-/epistemic-plausibility-)\nstructures and tries to find a single structure that, in some suitable\nsense, \u201ccontains\u201d all other structures. Such a structure,\noften called called a universal structure (or a terminal\nobject in the language of category theory), if it exists,\nincorporates any substantive assumption that an analyst can\nimagine. Such structure have been shown to exists for Harsanyi type\nspaces (Mertens & Zamir 1985; Brandenburger & Dekel 1993). For\nKripke structures, the question has been answered in the negative\n(Heifetz & Samet 1998; Fagin, Geanakoplos, Halpern, & Vardi\n1999; Meier 2005), with some qualifications regarding the language\nthat is used to describe them (Heifetz 1999; Roy & Pacuit\n2013).\nA second approach takes an internal perspective by asking\nwhether, for a fixed set of states or types, the agents are\nmaking any substantive assumptions about what their opponents know or\nbelieve. The idea is to identify (in a given model) a set of\npossible conjectures about the players. For example, in a\nknowledge structure based on a set of states \\(W\\) this might be the set\nof all subsets of \\(W\\) or the set definable subsets of \\(W\\) in some\nsuitable logical language. A space is said to be complete if\neach agent correctly takes into account each possible conjecture about\nher opponents. A simple counting argument shows that there cannot\nexist a complete structure when the set of conjectures is all\nsubsets of the set of states (Brandenburger 2003). However, there is a\ndeeper result here which we discuss below.\nThe Brandenburger-Keisler Paradox\nAdam Brandenburger and H. Jerome Keisler (2006) introduce the\nfollowing two person, Russel-style paradox. The statement of the\nparadox involves two concepts: beliefs and\nassumptions. An assumption is a player\u2019s strongest\nbelief: it is a set of states that implies all other beliefs at a\ngiven state. We will say more about the interpretation of an\nassumption below. Suppose there are two players, Ann and Bob, and\nconsider the following description of beliefs.\n\n(S)\nAnn believes that Bob assumes that Ann believes that Bob\u2019s assumption is wrong.\n\n\nA paradox arises by asking the question\n\n(Q)\nDoes Ann believe that Bob\u2019s assumption is wrong?\n\nTo ease the discussion, let \\(C\\) be Bob\u2019s assumption in (S):\nthat is, \\(C\\) is the statement \u201cAnn believes that Bob\u2019s\nassumption is wrong.\u201d So, (Q) asks whether \\(C\\) is true or\nfalse. We will argue that \\(C\\) is true if, and only if, \\(C\\) is\nfalse.\nSuppose that \\(C\\) is true. Then, Ann does believe that Bob\u2019s\nassumption is wrong, and, by introspection, she believes that she\nbelieves this. That is to say, Ann believes that \\(C\\) is\ncorrect. Furthermore, according to (S), Ann believes that Bob\u2019s\nassumption is \\(C\\). So, Ann, in fact, believes that Bob\u2019s\nassumption is correct (she believes Bob\u2019s assumption is \\(C\\) and\nthat \\(C\\) is correct). So, \\(C\\) is false.\nSuppose that \\(C\\) is false. This means that Ann believes that\nBob\u2019s assumption is correct. That is, Ann believes that \\(C\\) is\ncorrect (By (S), Ann believes that Bob\u2019s assumption is\n\\(C\\)). Furthermore, by (S), we have that Ann believes that Bob\nassumes that Ann believes that \\(C\\) is wrong. So, Ann believes\nthat she believes that \\(C\\) is correct and she believes that Bob\nassumption is that she believes that \\(C\\) is wrong. So, it is true that\nshe believes Bob\u2019s assumptions is wrong (Ann believes that\nBob\u2019s assumption is she believes that \\(C\\) is wrong, but\nshe believes that is wrong: she believes that \\(C\\) is\ncorrect). So, \\(C\\) is true.\nBrandenburger and Keisler formalize the above argument in order to\nprove a very strong impossibility result about the existence of\nso-called assumption-complete structures. We need some\nnotation to state this result. It will be most convenient to work in\nqualitative type spaces for two players\n(Definition 2.7). A qualitative type space\nfor two players (cf. Definition 2.7. The set\nof states is not important in what follows, so we leave it out) is a\nstructure \\(\\langle \\{T_A, T_B\\}, \\{\\lambda_A, \\lambda_B\\}\\rangle\\)\nwhere\n\n\\[\\lambda_A:T_A\\rightarrow \\pow(T_B)\\qquad\\lambda_B:T_B\\rightarrow\\pow(T_A)\\]\n\nA set of conjectures about Ann is a subset\n\\(\\C_A\\subseteq \\pow(T_A)\\) (similarly, the set of conjectures about Bob\nis a subset \\(\\C_B\\subseteq \\pow(T_B)\\)). A structure \\(\\langle \\{T_A,\nT_B\\}, \\{\\lambda_A, \\lambda_B\\}\\rangle\\) is said to\nbe assumption-complete for the conjectures \\(\\C_A\\) and\n\\(\\C_B\\) provided for each conjecture in \\(\\C_A\\) there is a type that\nassumes that conjecture (similarly for Bob). Formally, for each\n\\(Y\\in\\C_B\\) there is a \\(t_0\\in T_A\\) such that \\(\\lambda_A(t_0)=Y\\), and\nsimilarly for Bob. As we remarked above, a simple counting argument\nshows that when \\(\\C_A=\\pow(T_A)\\) and \\(\\C_B=\\pow(T_B)\\), then\nassumption-complete models only exist in trivial cases. A much deeper\nresult is:\n\nTheorem 6.1 (Brandenburger & Keisler 2006: Theorem 5.4)\nThere is no assumption-complete type structure for the set of\nconjectures that contains the first-order definable subsets.\n\nSee the supplement for a discussion of the proof of this theorem\n(see Section 2).\nConsult Pacuit (2007) and  Abramsky & Zvesper (2010) for an extensive analysis and\ngeneralization of this result. But, it is not all bad news: Mariotti,\nMeier, & Piccione (2005) construct a complete structure where the\nset of conjectures are compact subsets of some well-behaved\ntopological space.\n7. Concluding Remarks\nThe epistemic view on games is that players should be seen as\nindividual decision makers, choosing what to do on the basis of their\nown preferences and the information they have in specific\ninformational contexts. What decision they will make\u2014the\ndescriptive question\u2014or what decision they should make\u2014the\nnormative question, depends on the decision-theoretic choice rule that\nthe player use, or should use, in a given context. We conclude with\ntwo general methodological issues about epistemic game theory and some\npointers to further reading.\n7.1 What is an epistemic game theory trying to accomplish?\nCommon knowledge of rationality is an informal assumption that game\ntheorists, philosophers and other social scientists often appeal to\nwhen analyzing social interactive situations. The epistemic program in\ngame theory demonstrates that there are many ways to understand what\nexactly it means to assume that there is \u201ccommon\nknowledge/belief of rationality\u201d in a game situation.\nBroadly speaking, much of the epistemic game theory literature is\nfocused on two types of projects. The goal of the first project is to\nmap out the relationship between different mathematical\nrepresentations of what the players know and believe about each other\nin a game situation. Research along these lines not only raises\ninteresting technical questions about how to compare and contrast\ndifferent mathematical models of the players\u2019 epistemic states,\nbut it also highlights the benefits and limits of an epistemic\nanalysis of games. The second project addresses the nature of rational\nchoice in game situations. The importance of this project is nicely\nexplained by Wolfgang Spohn:\n\n\u2026game theory\u2026is, to put it strongly, confused about\nthe rationality concept appropriate to it, its assumptions about its\nsubjects (the players) are very unclear, and, as a consequence, it is\nunclear about the decision rules to be applied\u2026.The basic\ndifficulty in defining rational behavior in game situations is the\nfact that in general each player\u2019s strategy will depend on his\nexpectations about the other players\u2019 strategies. Could we\nassume that his expectations were given, then his problem of strategy\nchoice would become an ordinary maximization problem: he could simply\nchoose a strategy maximizing his own payoff on the assumption that the\nother players would act in accordance with his given expectations. But\nthe point is that game theory cannot regard the players\u2019\nexpectations about each other\u2019s behavior as given; rather, one\nof the most important problems for game theory is precisely to decide\nwhat expectations intelligent players can rationally entertain about\nother intelligent players\u2019 behavior. (Spohn 1982: 267)\n\nMuch of the work in epistemic game theory can be viewed as an\nattempt to use precise representations of the players\u2019 knowledge\nand beliefs to help resolve some of the confusion alluded to in the\nabove quote.\n7.2 Alternatives to maximizing expected utility\nIn an epistemic analysis of a game, the specific recommendations or\npredictions for the players\u2019 choices are derived from\ndecision-theoretic choice rules. Maximization of expected utility, for\ninstance, underlies most of the results in the contemporary literature\non the epistemic foundations of game theory. From a methodological\nperspective, however, the choice rule that the modeler assumes the\nplayers are following is simply a parameter that can be varied. In\nrecent years, there have been some initial attempts to develop\nepistemic analyses with alternative choice rules, for\ninstance minregret Halpern & Pass (2009).\n7.3 Further reading\nThe reader interested in more extensive coverage of all or some of\nthe topics discussed in this entry should consult the following\narticles and books.\n\nLogic in Games by Johan van Benthem: This book uses the\ntools of modal logic broadly conceived to discuss many of the issues\nraised in this entry (2014, MIT Press).\nThe Language of Game Theory by Adam Brandenburger: A\ncollection of Brandenburger\u2019s key papers on epistemic game\ntheory (2014, World Scientific Series in Economic Theory).\nEpistemic Game Theory by Eddie Dekel and Marciano\nSiniscalchi: A survey paper aimed at economists covering the main\ntechnical results of epistemic game theory (2014, Available online).\nEpistemic Game Theory: Reasoning and Choice by\nAndr\u00e9s Perea: A non-technical introduction to epistemic game\ntheory (2012, Cambridge University Press).\nThe Bounds of Reason: Game Theory and the Unification of the\nBehavioral Sciences by Herbert Gintis: This book offers a broad\noverview of the social and behavioral science using the ideas of\nepistemic game theory (2009, Princeton University Press).\n\n",
    "bibliography": {
        "categories": [],
        "cat_ref_text": {
            "ref_list": [
                "Abramsky, S. &amp; J.A. Zvesper, 2012, \u201cFrom Lawvere to\nBrandenburger-Keisler: interactive forms of diagonalization and\nself-reference\u201d, in <em>Coalgebraic Methods in Computer\nScience</em> (LNCS, Vol. 7399,\npp. 1\u201319), <em>CoRR</em>, <em>abs/1006.0992</em>. ",
                "Alchourr\u00f3n, C.E., P. G\u00e4rdenfors, &amp; D. Makinson,\n1985, \u201cOn the logic of theory change: Partial meet contraction\nand revision functions\u201d, <em>Journal of Symbolic Logic</em>,\n50(2): 510\u2013530.",
                "Apt, K. &amp; J. Zvesper, 2010, \u201cThe role of monotonicity in\nthe epistemic analysis of strategic games\u201d, <em>Games</em>,\n1(4): 381\u2013394, doi:10.3390/g1040381",
                "Asheim, G. &amp; M. Dufwenberg, 2003, \u201cAdmissibility and\ncommon belief\u201d, <em>Game and Economic Behavior</em>, 42:\n208\u2013234.",
                "Aumann, R., 1976, \u201cAgreeing to disagree\u201d, <em>The\nAnnals of Statistics</em>, 4(6): 1236\u20131239.",
                "\u2013\u2013\u2013, 1987, \u201cCorrelated equilibrium as an\nexpression of Bayesian rationality\u201d, <em>Econometrica</em>,\n55(1): 1\u201318.",
                "\u2013\u2013\u2013, 1995, \u201cBackward induction and common\nknowledge of rationality\u201d, <em>Games and Economic Behavior</em>,\n8(1): 6\u201319.",
                "\u2013\u2013\u2013, 1999a, \u201cInteractive epistemology I:\nKnowledge\u201d, <em>International Journal of Game Theory</em>,\n28(3): 263\u2013300.",
                "\u2013\u2013\u2013, 1999b, \u201cInteractive epistemology II:\nProbability\u201d, <em>International Journal of Game Theory</em>,\n28(3): 301\u2013314.",
                "\u2013\u2013\u2013, 2010, \u201cInterview on epistemic\nlogic\u201d, in V. F. Hendricks &amp; O. Roy (Eds.), <em>Epistemic\nlogic: Five questions</em> (pp. 21\u201335). Automatic Press.",
                "Aumann, R. J., S. Hart, &amp; M. Perry, 1997, \u201cThe\nabsent-minded driver\u201d, <em>Games and Economic Behavior</em>,\n20(1): 102\u2013116. ",
                "Aumann, R. &amp; A. Brandenburger, 1995, \u201cEpistemic\nconditions for Nash equilibrium\u201d, <em>Econometrica</em>, 63(5):\n1161\u20131180.",
                "Aumann, R. &amp; S. Hart, 1994, <em>Handbook of game theory with\neconomic applications</em>\n(Vol. 2), Amsterdam: North Holland.",
                "Baltag, A. &amp; S. Smets, 2006, \u201cConditional doxastic\nmodels: A qualitative approach to dynamic belief revision\u201d,\nin <em>Electronic notes in theoretical computer science</em>\n(Vol. 165, pp. 5\u201321), Springer.",
                "Baltag, A., S. Smets, &amp; J. Zvesper, 2009, \u201cKeep\n\u2018hoping\u2019 for rationality: a solution to the backwards\ninduction paradox\u201d, <em>Synthese</em>, 169: 301\u2013333.",
                "Battigalli, P., 1997, \u201cOn rationalizability in extensive\ngames\u201d, <em>Journal of Economic Theory</em>, 74(1):\n40\u201361.",
                "Battigalli, P. &amp; A. Friedenberg, 2012, \u201cForward\ninduction reasoning revisited\u201d, <em>Theoretical Economics</em>,\n7(1): 57\u201398.",
                "Battigalli, P. &amp; M. Siniscalchi, 2002, \u201cStrong belief\nand forward induction reasoning\u201d, <em>Journal of Economic\nTheory</em>, 106(2): 356\u2013391.",
                "Battigalli, P., A. Di Tillio, &amp; D. Samet, 2013,\n\u201cStrategies and interactive beliefs in dynamic games\u201d,\nin <em>Advances in economics and econometrics: Theory and\napplications, Tenth World Congress, volume 1: economic\ntheory</em>, Cambridge: Cambridge\nUniversity Press.",
                "van Benthem, J., 2003, \u201cRational dynamic and epistemic logic\nin games\u201d, in S. Vannucci (Ed.), <em>Logic, game theory and\nsocial choice III</em>, University of Siena, Department of Political\nEconomy.",
                "\u2013\u2013\u2013, 2010, <em>Modal logic for open minds</em>,\nStanford, CA: CSLI Publications.",
                "\u2013\u2013\u2013, 2011, <em>Logical dynamics of information\nand interaction</em>, Cambridge: Cambridge University Press.",
                "van Benthem, J. &amp; A. Gheerbrant, 2010, \u201cGame solution,\nepistemic dynamics and fixed-point logics\u201d, <em>Fundamenta\nInformaticae</em>, 100: 1\u201323.",
                "van Benthem, J., P. Girard, &amp; O. Roy, 2009, \u201cEverything\nelse being equal: A modal logic for <em>Ceteris Paribus</em>\npreferences\u201d, <em>Journal of Philosophical Logic</em>, 38:\n83\u2013125.",
                "van Benthem, J., E. Pacuit, &amp; O. Roy, 2011, \u201cToward a\ntheory of play: A logical perspective on games and\ninteraction\u201d, <em>Games</em>, 2(1): 52\u201386.",
                "Bernheim, D., 1984, \u201cRationalizable strategic\nbehavior\u201d, <em>Econometrica</em>, 52: 1007\u20131028.",
                "Board, O., 2003, \u201cThe not-so-absent-minded\ndriver\u201d, <em>Research in Economics</em>, 57(3):\n189\u2013200.",
                "Board, O., K.S. Chung, &amp; B. Schipper, 2011, \u201cTwo models\nof unawareness: Comparing object-based and subjective-state-space\napproaches\u201d, <em>Synthese</em>, 179: 13\u201334.",
                "Bonanno, G., 1996, \u201cOn the logic of common\nbelief\u201d, <em>Mathematical Logical Quarterly</em>, 42:\n305\u2013311.",
                "\u2013\u2013\u2013, 2004, \u201cMemory and perfect recall in\nextensive games\u201d, <em>Games and Economic Behavior</em>, 47(2):\n237\u2013256.",
                "\u2013\u2013\u2013, 2013, \u201cA dynamic epistemic\ncharacterization of backward induction without\ncounterfactuals\u201d, <em>Games and Economic Behavior</em>, 78:\n31\u201343.",
                "Brandenburger, A., 2003, \u201cOn the existence of a\n\u201ccomplete\u201d possibility structure\u201d, in M. Basili,\nN. Dimitri, &amp; I. Gilboa (Eds.), <em>in Cognitive processes and\neconomic behavior</em> (pp. 30\u201334). Routledge.",
                "\u2013\u2013\u2013, 2007, \u201cA note on Kuhn\u2019s\ntheorem\u201d, in J. van Benthem, D. Gabbay, &amp; B. Loewe\n(Eds.), <em>Interactive logic, proceedings of the 7th Augustus de\nMorgan workshop, London</em> (pp. 71\u201388). Texts in Logic; Games,\nAmsterdam University Press.",
                "\u2013\u2013\u2013, 2010, \u201cOrigins of epistemic game\ntheory\u201d, in V. F. Hendricks &amp; O. Roy (Eds.), <em>Epistemic\nlogic: Five questions</em> (pp. 59\u201369). Automatic Press.",
                "Brandenburger, A. &amp; E. Dekel, 1987, \u201cRationalizability\nand correlated equilibria\u201d, <em>Econometrica</em>, 55(6):\n1391\u20131402.",
                "\u2013\u2013\u2013, 1993, \u201cHierarchies of beliefs and\ncommon knowledge\u201d, <em>Journal of Economic Theory</em>, 59.",
                "Brandenburger, A. &amp; A. Friedenberg, 2008, \u201cIntrinsic\ncorrelation in games\u201d, <em>Journal of Economic Theory</em>,\n141(1): 28\u201367.",
                "\u2013\u2013\u2013, 2010, \u201cSelf-admissible\nsets\u201d, <em>Journal of Economic Theory</em>, 145:\n785\u2013811.",
                "Brandenburger, A. &amp; H. Keisler, 2006, \u201cAn impossibility\ntheorem on beliefs in games\u201d, <em>Studia Logica</em>, 84(2):\n211\u2013240.",
                "Brandenburger, A., A. Friedenberg, &amp; H.J. Keisler, 2008,\n\u201cAdmissibility in games\u201d, <em>Econometrica</em>, 76(2):\n307\u2013352.",
                "de Bruin, B., 2010, <em>Explaining games : The epistemic\nprogramme in game theory</em>, New York City: Springer.",
                "Chen, Y.C., J. Ely, &amp; X. Luo, 2012, \u201cNote on\nunawareness: Negative introspection versus AU introspection (and KU\nintrospection)\u201d, <em>International Journal of Game Theory</em>,\n41(2): 325 - 329.",
                "Colman, A., 2003, \u201cCooperation, psychological game theory,\nand limitations of rationality in social\ninteractions\u201d, <em>Behavioral and Brain Sciences</em>, 26:\n139\u2013198.",
                "Cubitt, R.P. &amp; R. Sugden, 1994, \u201cRationally justifiable\nplay and the theory of non-cooperative games\u201d, <em>The Economic\nJournal</em>, 104(425): 798\u2013893.",
                "\u2013\u2013\u2013, 2014, \u201c Common reasoning in games: A\nLewisian analysis of common knowledge of\nrationality\u201d, <em>Economics and Philosophy</em>, 30(03):\n285\u2013329.",
                "Dekel, E., B. Lipman, &amp; A. Rustichini, 1998, \u201cStandard\nstate-space models preclude unawareness\u201d, <em>Econometrica</em>,\n66: 159\u2013173.",
                "Doyle, A.C., 1894, <em>The Memoirs of Sherlock Holmes</em>,\nMineola, NY: Dover Thrift Edition, 2010.",
                "Fagin, R., J. Geanakoplos, J. Halpern, &amp; M. Vardi, 1999,\n\u201cThe hierarchical approach to modeling knowledge and common\nknowledge\u201d, <em>International Journal of Game Theory</em>,\n28(3): 331\u2013365.",
                "Fagin, R., J. Halpern, &amp; N. Megiddo, 1990, \u201cA logic for\nreasoning about probabilities\u201d, <em>Information and\nComputation</em>, 87(1\u20132): 78\u2013128.",
                "Fagin, R., J. Halpern, Y. Moses, &amp; M. Vardi,\n1995, <em>Reasoning about\nknowledge</em>, Cambridge:  The MIT\nPress.",
                "Finetti, B., 1974, <em>Theory of probability, vols. 1 and 2</em>,\nNew York: Wiley.",
                "Friedenberg, A. &amp; H.J. Keisler, 2011, \u201cIterated\ndominance revisited\u201d, in <em>Proceedings of the behavioral and\nquantitative game theory: Conference on future\ndirections</em>, ACM, New York, NY.\n [<a href=\"https://www.math.wisc.edu/~keisler/idr-2011-08-12.pdf\" target=\"other\">available online</a>].",
                "Friedenberg, A. &amp; M. Meier, 2009, \u201cThe context of a\ngame\u201d, in <em>Proceedings of the 12th Conference on Theoretical\nAspects of Rationality and Knowledge</em>, pp. 134\u2013135\n [<a href=\"https://web.archive.org/web/20150329044244/http://www.public.asu.edu/~afrieden/cog.pdf\" target=\"other\">available online</a>].",
                "Gintis, H., 2009, <em>The bounds of reason: game theory and the\nunification of the behavioral sciences</em>, Princeton: Princeton\nUniversity Press.",
                "Halpern, J.Y., 1991, \u201cThe relationship between knowledge,\nbelief, and certainty\u201d, <em>Annals of Mathematics and Artificial\nIntelligence</em>, 4(3): 301\u2013322. \n[<a href=\"http://dx.doi.org/10.1007/BF01531062\" target=\"other\">available online</a>].",
                "\u2013\u2013\u2013, 1997, \u201cOn ambiguities in the\ninterpretation of game trees\u201d, <em>Games and Economic\nBehavior</em>, 20(1): 66\u201396.",
                "\u2013\u2013\u2013, 1999, \u201cSet-theoretic completeness for\nepistemic and conditional logic\u201d, <em>Annals of Mathematics and\nArtificial Intelligence</em>, 26: 1\u201327.",
                "\u2013\u2013\u2013, 2001a, \u201cAlternative semantics for\nunawareness\u201d, <em>Game and Economic Behavior</em>, 37:\n321\u2013339.",
                "\u2013\u2013\u2013, 2001b, \u201cSubstantive rationality and\nbackward induction\u201d, <em>Games and Economic Behavior</em>,\n37(2): 425\u2013435.",
                "\u2013\u2013\u2013, 2003, <em>Reasoning about uncertainty</em>,\nCambridge: The MIT Press.",
                "\u2013\u2013\u2013, 2010, \u201cLexiographic probability,\nconditional probability and nonstandard probability\u201d, <em>Games\nand Economic Behavior</em>, 68(1): 155\u2013179.",
                "Halpern, J.Y. &amp; R. Pass, 2009, \u201cA logical\ncharacterization of iterated admissibility\u201d, in A. Heifetz\n(Ed.), <em>Proceedings of the twelfth conference on theoretical\naspects of rationality and knowledge</em> (pp. 146\u2013155).",
                "\u2013\u2013\u2013, 2011, \u201cIterated regret minimization:\nA new solution concept\u201d, <em>Games and Economic\nBehavior</em>, 74(1): 184\u2013207\n [<a href=\"http://arxiv.org/pdf/0810.3023.pdf\" target=\"other\">available online</a>].",
                "Halpern, J.Y. &amp; L.C. Rego, 2008, \u201cInteractive\nunawareness revisited\u201d, <em>Games and Economic Behavior</em>,\n62(1): 232\u2013262.",
                "Harsanyi, J.C., 1967\u201368, \u201cGames with incomplete\ninformation played by \u2018Bayesian\u2019 players, parts\nI\u2013III\u201d, <em>Management Science</em>, 14: 159\u2013182;\n14: 320\u2013334; 14: 486\u2013502.",
                "Heifetz, A., 1999, \u201cHow canonical is the canonical model? A\ncomment on Aumann\u2019s interactive\nepistemology\u201d, <em>International Journal of Game Theory</em>,\n28(3): 435\u2013442.",
                "Heifetz, A. &amp; P. Mongin, 2001, \u201cProbability Logic for\nType Spaces\u201d, <em>Games and Economic Behavior</em>,\n35(1\u20132): 31\u201353.",
                "Heifetz, A. &amp; D. Samet, 1998, \u201cKnowledge spaces with\narbitrarily high rank\u201d, <em>Games and Economic Behavior</em>,\n22(2): 260\u2013273.",
                "Heifetz, A., M. Meier, &amp; B. Schipper, 2006, \u201cInteractive\nunawareness\u201d, <em>Journal of Economic Theory</em>, 130:\n78\u201394.",
                "Hendricks, V. &amp; J. Symons, 2009, \u201cEpistemic\nlogic\u201d, in E. N. Zalta (Ed.), <em>The Stanford Encyclopedia of\nPhilosophy</em> (Spring 2009 Edition), URL = \n &lt;<a href=\"https://plato.stanford.edu/archives/spr2009/entries/logic-epistemic/\">https://plato.stanford.edu/archives/spr2009/entries/logic-epistemic/</a>&gt;.",
                "Hoek, W. van der &amp; M. Pauly, 2007, \u201cModal logic for\ngames and information\u201d, in P. Blackburn, J. van Benthem, &amp;\nF. Wolter (Eds.), <em>Handbook of modal logic</em>\n(Vol. 3), Amsterdam:  Elsevier.",
                "Huber, F., 2009, \u201cFormal representations of belief\u201d,\nin E. N. Zalta (Ed.), <em>The Stanford Encyclopedia of Philosophy</em>\n(Summer 2009 Edition), URL = \n&lt;<a href=\"https://plato.stanford.edu/archives/sum2009/entries/formal-belief/\">https://plato.stanford.edu/archives/sum2009/entries/formal-belief/</a>&gt;.",
                "Joyce, J., 2004, \u201cBayesianism\u201d, in A. Mele &amp;\nP. Rawling (Eds.), <em>The Oxford handbook of\nrationality</em>, Oxford:  Oxford\nUniversity Press.",
                "Kadane, J.B. &amp; P.D. Larkey, 1982, \u201cSubjective\nprobability and the theory of games\u201d, <em>Management\nScience</em>, 28(2): 113\u2013120.\n[<a href=\"https://doi.org/10.1184/R1/6586943.v1\" target=\"other\">available online</a>]",
                "\u2013\u2013\u2013, 1983, \u201cThe confusion of is and ought\nin game theoretic contexts\u201d, <em>Management Science</em>,\n29(12): 1365\u20131379.\n[<a href=\"https://doi.org/10.1184/R1/6586976.v1\" target=\"other\">available online</a>]",
                "Kaneko, M. &amp; J. Kline, 1995, \u201cBehavior strategies, mixed\nstrategies and perfect recall\u201d, <em>International Journal of\nGame Theory</em>, 24: 127\u2013145.",
                "Kline, J., 2002, \u201cMinimum memory for equivalence\nbetween <em>Ex Ante</em> optimality and\ntime-consistency\u201d, <em>Games and Economic Behavior</em>, 38:\n278\u2013305.",
                "Kuhn, H., 1953, \u201cExtensive games and the problem of\ninformation\u201d, in H. Kuhn &amp; A. Tucker\n(Eds.), <em>Contributions to the theory of games, vol. II</em>,\nPrinceton: Princeton University Press.",
                "Lewis, D., 1969, <em>Convention</em>, Cambridge: Harvard\nUniversity Press.",
                "Leyton-Brown, K. &amp; Y. Shoham, 2008, <em>Essentials of game\ntheory: A concise, multidisciplinary introduction</em>, New York:\nMorgan &amp; Claypool.",
                "Lismont, L. &amp; P. Mongin, 1994, \u201cOn the logic of common\nbelief and common knowledge\u201d, <em>Theory and Decision</em>,\n37(1): 75\u2013106.",
                "\u2013\u2013\u2013, 2003, \u201cStrong Completeness Theorems\nfor Weak Logics of Common Belief\u201d, <em>Journal of Philosophical\nLogic</em>, 32(2): 115\u2013137.",
                "Liu, F., 2011, \u201cA two-level perspective on\npreference\u201d, <em>Journal of Philosophical Logic</em>, 40(3):\n421\u2013439.",
                "Lorini, E. &amp; F. Schwarzentruber, 2010, \u201cA modal logic of\nepistemic games\u201d, <em>Games</em>, 1(4): 478\u2013526.",
                "Mariotti, T., M. Meier, &amp; M. Piccione, 2005,\n\u201cHierarchies of beliefs for compact possibility\nmodels\u201d, <em>Journal of Mathematical Economics</em>, 41:\n303\u2013324.",
                "Mas-Colell, A., M. Winston, &amp; J. Green,\n1995, <em>Microeconomic theory</em>, Oxford: Oxford University\nPress.",
                "Meier, M., 2005, \u201cOn the nonexistence of universal\ninformation structures\u201d, <em>Journal of Economic Theory</em>,\n122(1): 132\u2013139.",
                "Mertens, J. &amp; S. Zamir, 1985, \u201cFormulation of Bayesian\nanalysis for games with incomplete\ninformation\u201d, <em>International Journal of Game Theory</em>,\n14(1): 1\u201329.",
                "Modica, S. &amp; A. Rustichini, 1994, \u201cAwareness and\npartitional information structures\u201d, <em>Theory and\nDecision</em>, 37: 107\u2013124.",
                "\u2013\u2013\u2013, 1999, \u201cUnawareness and partitional\ninformation structures\u201d, <em>Game and Economic Behavior</em>,\n27: 265\u2013298.",
                "Monderer, D. &amp; D. Samet, 1989, \u201cApproximating common\nknowledge with common beliefs\u201d, <em>Games and Economic\nBehavior</em>, 1(2): 170\u2013190.",
                "Morris, S., 1995, \u201cThe common prior assumption in economic\ntheory\u201d, <em>Economics and Philosophy</em>, 11(2):\n227\u2013253.",
                "Moscati, I., 2009, <em>Interactive and common knowledge in the\nstate-space model</em> (CESMEP Working Papers). University of\nTurin. [<a href=\"http://econpapers.repec.org/RePEc:uto:cesmep:200903\" target=\"other\">available online</a>].",
                "Myerson, R., 1997 [1991], <em>Game theory: Analysis of\nconflict</em>,  Cambridge: Harvard\nUniversity Press.",
                "Osborne, M., 2003, <em>An introduction to game\ntheory</em>, Oxford: Oxford University\nPress.",
                "Pacuit, E., 2007, \u201cUnderstanding the Brandenburger-Keisler \nparadox\u201d, <em>Studia Logica</em>, 86(3): 435\u2013454.",
                "Pacuit, E. &amp; O. Roy, 2011, \u201cA dynamic analysis of\ninteractive rationality\u201d, in H. van Ditmarsch, J. Lang, &amp;\nS. Ju (Eds.), <em>Proceedings of the third international workshop on\nlogic, rationality and interaction</em> (Vol. 6953,\npp. 244\u2013258).",
                "Pearce, D., 1984, \u201cRationalizable strategic behavior and the\nproblem of perfection\u201d, <em>Econometrica</em>, 52:\n1029\u20131050.",
                "Perea, A., 2007, \u201cA one-person doxastic characterization of\nNash strategies\u201d, <em>Synthese</em>, 158: 251\u2013271.",
                "\u2013\u2013\u2013, 2012, <em>Epistemic game theory: Reasoning\nand choice</em>, Cambridge: Cambridge\nUniversity Press.",
                "\u2013\u2013\u2013, 2014, \u201cBelief in the opponents\u2019\nfuture rationality\u201d, <em>Games and Economic Behavior</em>, 83:\n231\u2013254.",
                "Peterson, M., 2009, <em>An introduction to decision\ntheory</em>, Cambridge:  Cambridge\nUniversity Press.",
                "Piccione, M., &amp; A. Rubinstein, 1997a, \u201cOn the\ninterpretation of decision problems with imperfect\nrecall\u201d, <em>Games and Economic Behavior</em>, 20(1):\n3\u201324.",
                "\u2013\u2013\u2013, 1997b, \u201cThe absent-minded\ndriver\u2019s paradox: Synthesis and responses\u201d, <em>Games and\nEconomic Behavior</em>, 20(1): 121\u2013130.",
                "Rabinowicz, W., 1992, \u201cTortuous labyrinth: Noncooperative\nnormal-form games between hyperrational players\u201d, in\nC. Bicchieri &amp; M. L. D. Chiara (Eds.), <em>Knowledge, belief and\nstrategic interaction</em> (pp. 107\u2013125).",
                "Ross, D., 2010, \u201cGame theory\u201d, in E. N. Zalta\n(Ed.), <em>The Stanford Encyclopedia of Philosophy</em> (Fall 2010\nEdition), URL = \n&lt;<a href=\"https://plato.stanford.edu/archives/fall2010/entries/game-theory/\" target=\"other\">https://plato.stanford.edu/archives/fall2010/entries/game-theory/</a>&gt;.",
                "Roy, O. &amp; E. Pacuit, 2013, \u201cSubstantive assumptions in\ninteraction: A logical perspective\u201d, <em>Synthese</em>, 190(5):\n891\u2013908.",
                "Rubinstein, A., 1989, \u201cThe electronic mail game: Strategic\nbehavior under \u2018Almost common\nknowledge\u2019\u201d, <em>American Economic Review</em>, 79(3):\n385\u2013391.",
                "\u2013\u2013\u2013, 1991, \u201cComments on the interpretation\nof game theory\u201d, <em>Econometrica</em>, 59(4):\n909\u2013924.",
                "Samuelson, L., 1992, \u201cDominated strategies and common\nknowledge\u201d, <em>Game and Economic Behavior</em>, 4(2):\n284\u2013313.",
                "Schelling, T., 1960, <em>The Strategy of\nConflict</em>, Cambridge:  Harvard\nUniversity Press.",
                "Schwitzgebel, E., 2010, \u201cBelief\u201d, in E. N. Zalta\n(Ed.), <em>The Stanford Encyclopedia of Philosophy</em> (Winter 2010\nEdition), URL = \n&lt;<a href=\"https://plato.stanford.edu/archives/win2010/entries/belief/\" target=\"other\">https://plato.stanford.edu/archives/win2010/entries/belief/</a>&gt;.",
                "Selten, R., 1975, \u201cReexamination of the perfectness concept\nfor equilibrium points in extensive games\u201d, <em>International\nJournal of Game Theory</em>, 4(1): 25\u201355. \n [<a href=\"http://dx.doi.org/10.1007/BF01766400\" target=\"other\">available online</a>].",
                "Shoham, Y. &amp; K. Leyton-Brown, 2008, <em>Multiagent\nsystems</em>, Cambridge: Cambridge University Press.",
                "Siniscalchi, M., 2008, \u201cEpistemic game theory: Beliefs and\ntypes\u201d, in S. Durlauf &amp; L. Blume (Eds.), <em>The new\nPalgrave dictionary of economics</em>, Basingstoke: Palgrave\nMacmillan.",
                "Spohn, W., 1982, \u201cHow to make sense of game\ntheory\u201d, <em>Philosophy of economics: Proceedings, Munich, July\n1981</em>, W. Stegm\u00fcller, W. Balzer, &amp; W. Spohn (eds),\n239\u2013270, <em>Studies in Contemporary Economics</em>, Volume 2,\nBerlin: Springer-Verlag.",
                "Stalnaker, R., 1994, \u201cOn the evaluation of solution\nconcepts\u201d, <em>Theory and Decision</em>, 37(1):\n49\u201373.",
                "\u2013\u2013\u2013, 1996, \u201cKnowledge, belief and\ncounterfactual reasoning in games\u201d, <em>Economics and\nPhilosophy</em>, 12(02): 133\u2013163.",
                "\u2013\u2013\u2013, 1998, \u201cBelief revision in games:\nforward and backward induction\u201d, <em>Mathematical Social\nSciences</em>, 36(1): 31\u201356.",
                "\u2013\u2013\u2013, 1999, \u201cExtensive and strategic forms:\nGames and models for games\u201d, <em>Research in Economics</em>,\n53(3): 293\u2013319.",
                "\u2013\u2013\u2013, 2006, \u201cOn logics of knowledge and\nbelief\u201d, <em>Philosophical Studies</em>, 128(1):\n169\u2013199.",
                "Stuart Jr., H.W. &amp; H. Hu, 2002, \u201cAn epistemic analysis\nof the Harsanyi transformation\u201d, <em>International Journal of\nGame Theory</em>, 30(4): 517\u2013525.",
                "Tan, T.C.-C. &amp; S.R. da Costa Werlang, 1988, \u201cThe\nBayesian foundations of solution concepts of games\u201d, <em>Journal\nof Economic Theory</em>, 45(2): 370\u2013391,\n doi:10.1016/0022-0531(88)90276-1",
                "Titelbaum, M., 2013, \u201cTen reasons to care about the sleeping\nbeauty problem\u201d, <em>Philosophy Compass</em>, 8:\n1003\u20131017.",
                "Ullmann-Margalit, E. &amp; S. Morgenbesser, 1977, \u201cPicking\nand choosing\u201d, <em>Social Research</em>, 44: 757\u2013785.",
                "Vanderschraaf, P. &amp; G. Sillari, 2009, \u201cCommon\nknowledge\u201d, in E. N. Zalta (Ed.), <em>The Stanford Encyclopedia\nof Philosophy</em> (Spring 2009 Edition), URL =\n &lt;<a href=\"https://plato.stanford.edu/archives/spr2009/entries/common-knowledge/\" target=\"other\">https://plato.stanford.edu/archives/spr2009/entries/common-knowledge/</a>&gt;.",
                "de Weerd, H., R. Verbrugge, &amp; B. Verheij, 2013, \u201cHow\nmuch does it help to know what she knows you know? An agent-based\nsimulation study\u201d, <em>Artificial Intelligence</em>,\n199\u2013200: 67\u201392.",
                "Zvesper, J., 2010, <em>Playing with information</em> (PhD thesis),\nILLC, University of Amsterdam."
            ]
        },
        "raw_text": "<div id=\"bibliography\">\n<h2><a id=\"Bib\">Bibliography</a></h2>\n<ul class=\"hanging\">\n<li>Abramsky, S. &amp; J.A. Zvesper, 2012, \u201cFrom Lawvere to\nBrandenburger-Keisler: interactive forms of diagonalization and\nself-reference\u201d, in <em>Coalgebraic Methods in Computer\nScience</em> (LNCS, Vol. 7399,\npp. 1\u201319), <em>CoRR</em>, <em>abs/1006.0992</em>. </li>\n<li>Alchourr\u00f3n, C.E., P. G\u00e4rdenfors, &amp; D. Makinson,\n1985, \u201cOn the logic of theory change: Partial meet contraction\nand revision functions\u201d, <em>Journal of Symbolic Logic</em>,\n50(2): 510\u2013530.</li>\n<li>Apt, K. &amp; J. Zvesper, 2010, \u201cThe role of monotonicity in\nthe epistemic analysis of strategic games\u201d, <em>Games</em>,\n1(4): 381\u2013394, doi:10.3390/g1040381</li>\n<li>Asheim, G. &amp; M. Dufwenberg, 2003, \u201cAdmissibility and\ncommon belief\u201d, <em>Game and Economic Behavior</em>, 42:\n208\u2013234.</li>\n<li>Aumann, R., 1976, \u201cAgreeing to disagree\u201d, <em>The\nAnnals of Statistics</em>, 4(6): 1236\u20131239.</li>\n<li>\u2013\u2013\u2013, 1987, \u201cCorrelated equilibrium as an\nexpression of Bayesian rationality\u201d, <em>Econometrica</em>,\n55(1): 1\u201318.</li>\n<li>\u2013\u2013\u2013, 1995, \u201cBackward induction and common\nknowledge of rationality\u201d, <em>Games and Economic Behavior</em>,\n8(1): 6\u201319.</li>\n<li>\u2013\u2013\u2013, 1999a, \u201cInteractive epistemology I:\nKnowledge\u201d, <em>International Journal of Game Theory</em>,\n28(3): 263\u2013300.</li>\n<li>\u2013\u2013\u2013, 1999b, \u201cInteractive epistemology II:\nProbability\u201d, <em>International Journal of Game Theory</em>,\n28(3): 301\u2013314.</li>\n<li>\u2013\u2013\u2013, 2010, \u201cInterview on epistemic\nlogic\u201d, in V. F. Hendricks &amp; O. Roy (Eds.), <em>Epistemic\nlogic: Five questions</em> (pp. 21\u201335). Automatic Press.</li>\n<li>Aumann, R. J., S. Hart, &amp; M. Perry, 1997, \u201cThe\nabsent-minded driver\u201d, <em>Games and Economic Behavior</em>,\n20(1): 102\u2013116. </li>\n<li>Aumann, R. &amp; A. Brandenburger, 1995, \u201cEpistemic\nconditions for Nash equilibrium\u201d, <em>Econometrica</em>, 63(5):\n1161\u20131180.</li>\n<li>Aumann, R. &amp; S. Hart, 1994, <em>Handbook of game theory with\neconomic applications</em>\n(Vol. 2), Amsterdam: North Holland.</li>\n<li>Baltag, A. &amp; S. Smets, 2006, \u201cConditional doxastic\nmodels: A qualitative approach to dynamic belief revision\u201d,\nin <em>Electronic notes in theoretical computer science</em>\n(Vol. 165, pp. 5\u201321), Springer.</li>\n<li>Baltag, A., S. Smets, &amp; J. Zvesper, 2009, \u201cKeep\n\u2018hoping\u2019 for rationality: a solution to the backwards\ninduction paradox\u201d, <em>Synthese</em>, 169: 301\u2013333.</li>\n<li>Battigalli, P., 1997, \u201cOn rationalizability in extensive\ngames\u201d, <em>Journal of Economic Theory</em>, 74(1):\n40\u201361.</li>\n<li>Battigalli, P. &amp; A. Friedenberg, 2012, \u201cForward\ninduction reasoning revisited\u201d, <em>Theoretical Economics</em>,\n7(1): 57\u201398.</li>\n<li>Battigalli, P. &amp; M. Siniscalchi, 2002, \u201cStrong belief\nand forward induction reasoning\u201d, <em>Journal of Economic\nTheory</em>, 106(2): 356\u2013391.</li>\n<li>Battigalli, P., A. Di Tillio, &amp; D. Samet, 2013,\n\u201cStrategies and interactive beliefs in dynamic games\u201d,\nin <em>Advances in economics and econometrics: Theory and\napplications, Tenth World Congress, volume 1: economic\ntheory</em>, Cambridge: Cambridge\nUniversity Press.</li>\n<li>van Benthem, J., 2003, \u201cRational dynamic and epistemic logic\nin games\u201d, in S. Vannucci (Ed.), <em>Logic, game theory and\nsocial choice III</em>, University of Siena, Department of Political\nEconomy.</li>\n<li>\u2013\u2013\u2013, 2010, <em>Modal logic for open minds</em>,\nStanford, CA: CSLI Publications.</li>\n<li>\u2013\u2013\u2013, 2011, <em>Logical dynamics of information\nand interaction</em>, Cambridge: Cambridge University Press.</li>\n<li>van Benthem, J. &amp; A. Gheerbrant, 2010, \u201cGame solution,\nepistemic dynamics and fixed-point logics\u201d, <em>Fundamenta\nInformaticae</em>, 100: 1\u201323.</li>\n<li>van Benthem, J., P. Girard, &amp; O. Roy, 2009, \u201cEverything\nelse being equal: A modal logic for <em>Ceteris Paribus</em>\npreferences\u201d, <em>Journal of Philosophical Logic</em>, 38:\n83\u2013125.</li>\n<li>van Benthem, J., E. Pacuit, &amp; O. Roy, 2011, \u201cToward a\ntheory of play: A logical perspective on games and\ninteraction\u201d, <em>Games</em>, 2(1): 52\u201386.</li>\n<li>Bernheim, D., 1984, \u201cRationalizable strategic\nbehavior\u201d, <em>Econometrica</em>, 52: 1007\u20131028.</li>\n<li>Board, O., 2003, \u201cThe not-so-absent-minded\ndriver\u201d, <em>Research in Economics</em>, 57(3):\n189\u2013200.</li>\n<li>Board, O., K.S. Chung, &amp; B. Schipper, 2011, \u201cTwo models\nof unawareness: Comparing object-based and subjective-state-space\napproaches\u201d, <em>Synthese</em>, 179: 13\u201334.</li>\n<li>Bonanno, G., 1996, \u201cOn the logic of common\nbelief\u201d, <em>Mathematical Logical Quarterly</em>, 42:\n305\u2013311.</li>\n<li>\u2013\u2013\u2013, 2004, \u201cMemory and perfect recall in\nextensive games\u201d, <em>Games and Economic Behavior</em>, 47(2):\n237\u2013256.</li>\n<li>\u2013\u2013\u2013, 2013, \u201cA dynamic epistemic\ncharacterization of backward induction without\ncounterfactuals\u201d, <em>Games and Economic Behavior</em>, 78:\n31\u201343.</li>\n<li>Brandenburger, A., 2003, \u201cOn the existence of a\n\u201ccomplete\u201d possibility structure\u201d, in M. Basili,\nN. Dimitri, &amp; I. Gilboa (Eds.), <em>in Cognitive processes and\neconomic behavior</em> (pp. 30\u201334). Routledge.</li>\n<li>\u2013\u2013\u2013, 2007, \u201cA note on Kuhn\u2019s\ntheorem\u201d, in J. van Benthem, D. Gabbay, &amp; B. Loewe\n(Eds.), <em>Interactive logic, proceedings of the 7th Augustus de\nMorgan workshop, London</em> (pp. 71\u201388). Texts in Logic; Games,\nAmsterdam University Press.</li>\n<li>\u2013\u2013\u2013, 2010, \u201cOrigins of epistemic game\ntheory\u201d, in V. F. Hendricks &amp; O. Roy (Eds.), <em>Epistemic\nlogic: Five questions</em> (pp. 59\u201369). Automatic Press.</li>\n<li>Brandenburger, A. &amp; E. Dekel, 1987, \u201cRationalizability\nand correlated equilibria\u201d, <em>Econometrica</em>, 55(6):\n1391\u20131402.</li>\n<li>\u2013\u2013\u2013, 1993, \u201cHierarchies of beliefs and\ncommon knowledge\u201d, <em>Journal of Economic Theory</em>, 59.</li>\n<li>Brandenburger, A. &amp; A. Friedenberg, 2008, \u201cIntrinsic\ncorrelation in games\u201d, <em>Journal of Economic Theory</em>,\n141(1): 28\u201367.</li>\n<li>\u2013\u2013\u2013, 2010, \u201cSelf-admissible\nsets\u201d, <em>Journal of Economic Theory</em>, 145:\n785\u2013811.</li>\n<li>Brandenburger, A. &amp; H. Keisler, 2006, \u201cAn impossibility\ntheorem on beliefs in games\u201d, <em>Studia Logica</em>, 84(2):\n211\u2013240.</li>\n<li>Brandenburger, A., A. Friedenberg, &amp; H.J. Keisler, 2008,\n\u201cAdmissibility in games\u201d, <em>Econometrica</em>, 76(2):\n307\u2013352.</li>\n<li>de Bruin, B., 2010, <em>Explaining games : The epistemic\nprogramme in game theory</em>, New York City: Springer.</li>\n<li>Chen, Y.C., J. Ely, &amp; X. Luo, 2012, \u201cNote on\nunawareness: Negative introspection versus AU introspection (and KU\nintrospection)\u201d, <em>International Journal of Game Theory</em>,\n41(2): 325 - 329.</li>\n<li>Colman, A., 2003, \u201cCooperation, psychological game theory,\nand limitations of rationality in social\ninteractions\u201d, <em>Behavioral and Brain Sciences</em>, 26:\n139\u2013198.</li>\n<li>Cubitt, R.P. &amp; R. Sugden, 1994, \u201cRationally justifiable\nplay and the theory of non-cooperative games\u201d, <em>The Economic\nJournal</em>, 104(425): 798\u2013893.</li>\n<li>\u2013\u2013\u2013, 2014, \u201c Common reasoning in games: A\nLewisian analysis of common knowledge of\nrationality\u201d, <em>Economics and Philosophy</em>, 30(03):\n285\u2013329.</li>\n<li>Dekel, E., B. Lipman, &amp; A. Rustichini, 1998, \u201cStandard\nstate-space models preclude unawareness\u201d, <em>Econometrica</em>,\n66: 159\u2013173.</li>\n<li>Doyle, A.C., 1894, <em>The Memoirs of Sherlock Holmes</em>,\nMineola, NY: Dover Thrift Edition, 2010.</li>\n<li>Fagin, R., J. Geanakoplos, J. Halpern, &amp; M. Vardi, 1999,\n\u201cThe hierarchical approach to modeling knowledge and common\nknowledge\u201d, <em>International Journal of Game Theory</em>,\n28(3): 331\u2013365.</li>\n<li>Fagin, R., J. Halpern, &amp; N. Megiddo, 1990, \u201cA logic for\nreasoning about probabilities\u201d, <em>Information and\nComputation</em>, 87(1\u20132): 78\u2013128.</li>\n<li>Fagin, R., J. Halpern, Y. Moses, &amp; M. Vardi,\n1995, <em>Reasoning about\nknowledge</em>, Cambridge:  The MIT\nPress.</li>\n<li>Finetti, B., 1974, <em>Theory of probability, vols. 1 and 2</em>,\nNew York: Wiley.</li>\n<li>Friedenberg, A. &amp; H.J. Keisler, 2011, \u201cIterated\ndominance revisited\u201d, in <em>Proceedings of the behavioral and\nquantitative game theory: Conference on future\ndirections</em>, ACM, New York, NY.\n [<a href=\"https://www.math.wisc.edu/~keisler/idr-2011-08-12.pdf\" target=\"other\">available online</a>].</li>\n<li>Friedenberg, A. &amp; M. Meier, 2009, \u201cThe context of a\ngame\u201d, in <em>Proceedings of the 12th Conference on Theoretical\nAspects of Rationality and Knowledge</em>, pp. 134\u2013135\n [<a href=\"https://web.archive.org/web/20150329044244/http://www.public.asu.edu/~afrieden/cog.pdf\" target=\"other\">available online</a>].</li>\n<li>Gintis, H., 2009, <em>The bounds of reason: game theory and the\nunification of the behavioral sciences</em>, Princeton: Princeton\nUniversity Press.</li>\n<li>Halpern, J.Y., 1991, \u201cThe relationship between knowledge,\nbelief, and certainty\u201d, <em>Annals of Mathematics and Artificial\nIntelligence</em>, 4(3): 301\u2013322. \n[<a href=\"http://dx.doi.org/10.1007/BF01531062\" target=\"other\">available online</a>].</li>\n<li>\u2013\u2013\u2013, 1997, \u201cOn ambiguities in the\ninterpretation of game trees\u201d, <em>Games and Economic\nBehavior</em>, 20(1): 66\u201396.</li>\n<li>\u2013\u2013\u2013, 1999, \u201cSet-theoretic completeness for\nepistemic and conditional logic\u201d, <em>Annals of Mathematics and\nArtificial Intelligence</em>, 26: 1\u201327.</li>\n<li>\u2013\u2013\u2013, 2001a, \u201cAlternative semantics for\nunawareness\u201d, <em>Game and Economic Behavior</em>, 37:\n321\u2013339.</li>\n<li>\u2013\u2013\u2013, 2001b, \u201cSubstantive rationality and\nbackward induction\u201d, <em>Games and Economic Behavior</em>,\n37(2): 425\u2013435.</li>\n<li>\u2013\u2013\u2013, 2003, <em>Reasoning about uncertainty</em>,\nCambridge: The MIT Press.</li>\n<li>\u2013\u2013\u2013, 2010, \u201cLexiographic probability,\nconditional probability and nonstandard probability\u201d, <em>Games\nand Economic Behavior</em>, 68(1): 155\u2013179.</li>\n<li>Halpern, J.Y. &amp; R. Pass, 2009, \u201cA logical\ncharacterization of iterated admissibility\u201d, in A. Heifetz\n(Ed.), <em>Proceedings of the twelfth conference on theoretical\naspects of rationality and knowledge</em> (pp. 146\u2013155).</li>\n<li>\u2013\u2013\u2013, 2011, \u201cIterated regret minimization:\nA new solution concept\u201d, <em>Games and Economic\nBehavior</em>, 74(1): 184\u2013207\n [<a href=\"http://arxiv.org/pdf/0810.3023.pdf\" target=\"other\">available online</a>].</li>\n<li>Halpern, J.Y. &amp; L.C. Rego, 2008, \u201cInteractive\nunawareness revisited\u201d, <em>Games and Economic Behavior</em>,\n62(1): 232\u2013262.</li>\n<li>Harsanyi, J.C., 1967\u201368, \u201cGames with incomplete\ninformation played by \u2018Bayesian\u2019 players, parts\nI\u2013III\u201d, <em>Management Science</em>, 14: 159\u2013182;\n14: 320\u2013334; 14: 486\u2013502.</li>\n<li>Heifetz, A., 1999, \u201cHow canonical is the canonical model? A\ncomment on Aumann\u2019s interactive\nepistemology\u201d, <em>International Journal of Game Theory</em>,\n28(3): 435\u2013442.</li>\n<li>Heifetz, A. &amp; P. Mongin, 2001, \u201cProbability Logic for\nType Spaces\u201d, <em>Games and Economic Behavior</em>,\n35(1\u20132): 31\u201353.</li>\n<li>Heifetz, A. &amp; D. Samet, 1998, \u201cKnowledge spaces with\narbitrarily high rank\u201d, <em>Games and Economic Behavior</em>,\n22(2): 260\u2013273.</li>\n<li>Heifetz, A., M. Meier, &amp; B. Schipper, 2006, \u201cInteractive\nunawareness\u201d, <em>Journal of Economic Theory</em>, 130:\n78\u201394.</li>\n<li>Hendricks, V. &amp; J. Symons, 2009, \u201cEpistemic\nlogic\u201d, in E. N. Zalta (Ed.), <em>The Stanford Encyclopedia of\nPhilosophy</em> (Spring 2009 Edition), URL = \n &lt;<a href=\"https://plato.stanford.edu/archives/spr2009/entries/logic-epistemic/\">https://plato.stanford.edu/archives/spr2009/entries/logic-epistemic/</a>&gt;.</li>\n<li>Hoek, W. van der &amp; M. Pauly, 2007, \u201cModal logic for\ngames and information\u201d, in P. Blackburn, J. van Benthem, &amp;\nF. Wolter (Eds.), <em>Handbook of modal logic</em>\n(Vol. 3), Amsterdam:  Elsevier.</li>\n<li>Huber, F., 2009, \u201cFormal representations of belief\u201d,\nin E. N. Zalta (Ed.), <em>The Stanford Encyclopedia of Philosophy</em>\n(Summer 2009 Edition), URL = \n&lt;<a href=\"https://plato.stanford.edu/archives/sum2009/entries/formal-belief/\">https://plato.stanford.edu/archives/sum2009/entries/formal-belief/</a>&gt;.</li>\n<li>Joyce, J., 2004, \u201cBayesianism\u201d, in A. Mele &amp;\nP. Rawling (Eds.), <em>The Oxford handbook of\nrationality</em>, Oxford:  Oxford\nUniversity Press.</li>\n<li>Kadane, J.B. &amp; P.D. Larkey, 1982, \u201cSubjective\nprobability and the theory of games\u201d, <em>Management\nScience</em>, 28(2): 113\u2013120.\n[<a href=\"https://doi.org/10.1184/R1/6586943.v1\" target=\"other\">available online</a>]</li>\n<li>\u2013\u2013\u2013, 1983, \u201cThe confusion of is and ought\nin game theoretic contexts\u201d, <em>Management Science</em>,\n29(12): 1365\u20131379.\n[<a href=\"https://doi.org/10.1184/R1/6586976.v1\" target=\"other\">available online</a>]</li>\n<li>Kaneko, M. &amp; J. Kline, 1995, \u201cBehavior strategies, mixed\nstrategies and perfect recall\u201d, <em>International Journal of\nGame Theory</em>, 24: 127\u2013145.</li>\n<li>Kline, J., 2002, \u201cMinimum memory for equivalence\nbetween <em>Ex Ante</em> optimality and\ntime-consistency\u201d, <em>Games and Economic Behavior</em>, 38:\n278\u2013305.</li>\n<li>Kuhn, H., 1953, \u201cExtensive games and the problem of\ninformation\u201d, in H. Kuhn &amp; A. Tucker\n(Eds.), <em>Contributions to the theory of games, vol. II</em>,\nPrinceton: Princeton University Press.</li>\n<li>Lewis, D., 1969, <em>Convention</em>, Cambridge: Harvard\nUniversity Press.</li>\n<li>Leyton-Brown, K. &amp; Y. Shoham, 2008, <em>Essentials of game\ntheory: A concise, multidisciplinary introduction</em>, New York:\nMorgan &amp; Claypool.</li>\n<li>Lismont, L. &amp; P. Mongin, 1994, \u201cOn the logic of common\nbelief and common knowledge\u201d, <em>Theory and Decision</em>,\n37(1): 75\u2013106.</li>\n<li>\u2013\u2013\u2013, 2003, \u201cStrong Completeness Theorems\nfor Weak Logics of Common Belief\u201d, <em>Journal of Philosophical\nLogic</em>, 32(2): 115\u2013137.</li>\n<li>Liu, F., 2011, \u201cA two-level perspective on\npreference\u201d, <em>Journal of Philosophical Logic</em>, 40(3):\n421\u2013439.</li>\n<li>Lorini, E. &amp; F. Schwarzentruber, 2010, \u201cA modal logic of\nepistemic games\u201d, <em>Games</em>, 1(4): 478\u2013526.</li>\n<li>Mariotti, T., M. Meier, &amp; M. Piccione, 2005,\n\u201cHierarchies of beliefs for compact possibility\nmodels\u201d, <em>Journal of Mathematical Economics</em>, 41:\n303\u2013324.</li>\n<li>Mas-Colell, A., M. Winston, &amp; J. Green,\n1995, <em>Microeconomic theory</em>, Oxford: Oxford University\nPress.</li>\n<li>Meier, M., 2005, \u201cOn the nonexistence of universal\ninformation structures\u201d, <em>Journal of Economic Theory</em>,\n122(1): 132\u2013139.</li>\n<li>Mertens, J. &amp; S. Zamir, 1985, \u201cFormulation of Bayesian\nanalysis for games with incomplete\ninformation\u201d, <em>International Journal of Game Theory</em>,\n14(1): 1\u201329.</li>\n<li>Modica, S. &amp; A. Rustichini, 1994, \u201cAwareness and\npartitional information structures\u201d, <em>Theory and\nDecision</em>, 37: 107\u2013124.</li>\n<li>\u2013\u2013\u2013, 1999, \u201cUnawareness and partitional\ninformation structures\u201d, <em>Game and Economic Behavior</em>,\n27: 265\u2013298.</li>\n<li>Monderer, D. &amp; D. Samet, 1989, \u201cApproximating common\nknowledge with common beliefs\u201d, <em>Games and Economic\nBehavior</em>, 1(2): 170\u2013190.</li>\n<li>Morris, S., 1995, \u201cThe common prior assumption in economic\ntheory\u201d, <em>Economics and Philosophy</em>, 11(2):\n227\u2013253.</li>\n<li>Moscati, I., 2009, <em>Interactive and common knowledge in the\nstate-space model</em> (CESMEP Working Papers). University of\nTurin. [<a href=\"http://econpapers.repec.org/RePEc:uto:cesmep:200903\" target=\"other\">available online</a>].</li>\n<li>Myerson, R., 1997 [1991], <em>Game theory: Analysis of\nconflict</em>,  Cambridge: Harvard\nUniversity Press.</li>\n<li>Osborne, M., 2003, <em>An introduction to game\ntheory</em>, Oxford: Oxford University\nPress.</li>\n<li>Pacuit, E., 2007, \u201cUnderstanding the Brandenburger-Keisler \nparadox\u201d, <em>Studia Logica</em>, 86(3): 435\u2013454.</li>\n<li>Pacuit, E. &amp; O. Roy, 2011, \u201cA dynamic analysis of\ninteractive rationality\u201d, in H. van Ditmarsch, J. Lang, &amp;\nS. Ju (Eds.), <em>Proceedings of the third international workshop on\nlogic, rationality and interaction</em> (Vol. 6953,\npp. 244\u2013258).</li>\n<li>Pearce, D., 1984, \u201cRationalizable strategic behavior and the\nproblem of perfection\u201d, <em>Econometrica</em>, 52:\n1029\u20131050.</li>\n<li>Perea, A., 2007, \u201cA one-person doxastic characterization of\nNash strategies\u201d, <em>Synthese</em>, 158: 251\u2013271.</li>\n<li>\u2013\u2013\u2013, 2012, <em>Epistemic game theory: Reasoning\nand choice</em>, Cambridge: Cambridge\nUniversity Press.</li>\n<li>\u2013\u2013\u2013, 2014, \u201cBelief in the opponents\u2019\nfuture rationality\u201d, <em>Games and Economic Behavior</em>, 83:\n231\u2013254.</li>\n<li>Peterson, M., 2009, <em>An introduction to decision\ntheory</em>, Cambridge:  Cambridge\nUniversity Press.</li>\n<li>Piccione, M., &amp; A. Rubinstein, 1997a, \u201cOn the\ninterpretation of decision problems with imperfect\nrecall\u201d, <em>Games and Economic Behavior</em>, 20(1):\n3\u201324.</li>\n<li>\u2013\u2013\u2013, 1997b, \u201cThe absent-minded\ndriver\u2019s paradox: Synthesis and responses\u201d, <em>Games and\nEconomic Behavior</em>, 20(1): 121\u2013130.</li>\n<li>Rabinowicz, W., 1992, \u201cTortuous labyrinth: Noncooperative\nnormal-form games between hyperrational players\u201d, in\nC. Bicchieri &amp; M. L. D. Chiara (Eds.), <em>Knowledge, belief and\nstrategic interaction</em> (pp. 107\u2013125).</li>\n<li>Ross, D., 2010, \u201cGame theory\u201d, in E. N. Zalta\n(Ed.), <em>The Stanford Encyclopedia of Philosophy</em> (Fall 2010\nEdition), URL = \n&lt;<a href=\"https://plato.stanford.edu/archives/fall2010/entries/game-theory/\" target=\"other\">https://plato.stanford.edu/archives/fall2010/entries/game-theory/</a>&gt;.</li>\n<li>Roy, O. &amp; E. Pacuit, 2013, \u201cSubstantive assumptions in\ninteraction: A logical perspective\u201d, <em>Synthese</em>, 190(5):\n891\u2013908.</li>\n<li>Rubinstein, A., 1989, \u201cThe electronic mail game: Strategic\nbehavior under \u2018Almost common\nknowledge\u2019\u201d, <em>American Economic Review</em>, 79(3):\n385\u2013391.</li>\n<li>\u2013\u2013\u2013, 1991, \u201cComments on the interpretation\nof game theory\u201d, <em>Econometrica</em>, 59(4):\n909\u2013924.</li>\n<li>Samuelson, L., 1992, \u201cDominated strategies and common\nknowledge\u201d, <em>Game and Economic Behavior</em>, 4(2):\n284\u2013313.</li>\n<li>Schelling, T., 1960, <em>The Strategy of\nConflict</em>, Cambridge:  Harvard\nUniversity Press.</li>\n<li>Schwitzgebel, E., 2010, \u201cBelief\u201d, in E. N. Zalta\n(Ed.), <em>The Stanford Encyclopedia of Philosophy</em> (Winter 2010\nEdition), URL = \n&lt;<a href=\"https://plato.stanford.edu/archives/win2010/entries/belief/\" target=\"other\">https://plato.stanford.edu/archives/win2010/entries/belief/</a>&gt;.</li>\n<li>Selten, R., 1975, \u201cReexamination of the perfectness concept\nfor equilibrium points in extensive games\u201d, <em>International\nJournal of Game Theory</em>, 4(1): 25\u201355. \n [<a href=\"http://dx.doi.org/10.1007/BF01766400\" target=\"other\">available online</a>].</li>\n<li>Shoham, Y. &amp; K. Leyton-Brown, 2008, <em>Multiagent\nsystems</em>, Cambridge: Cambridge University Press.</li>\n<li>Siniscalchi, M., 2008, \u201cEpistemic game theory: Beliefs and\ntypes\u201d, in S. Durlauf &amp; L. Blume (Eds.), <em>The new\nPalgrave dictionary of economics</em>, Basingstoke: Palgrave\nMacmillan.</li>\n<li>Spohn, W., 1982, \u201cHow to make sense of game\ntheory\u201d, <em>Philosophy of economics: Proceedings, Munich, July\n1981</em>, W. Stegm\u00fcller, W. Balzer, &amp; W. Spohn (eds),\n239\u2013270, <em>Studies in Contemporary Economics</em>, Volume 2,\nBerlin: Springer-Verlag.</li>\n<li>Stalnaker, R., 1994, \u201cOn the evaluation of solution\nconcepts\u201d, <em>Theory and Decision</em>, 37(1):\n49\u201373.</li>\n<li>\u2013\u2013\u2013, 1996, \u201cKnowledge, belief and\ncounterfactual reasoning in games\u201d, <em>Economics and\nPhilosophy</em>, 12(02): 133\u2013163.</li>\n<li>\u2013\u2013\u2013, 1998, \u201cBelief revision in games:\nforward and backward induction\u201d, <em>Mathematical Social\nSciences</em>, 36(1): 31\u201356.</li>\n<li>\u2013\u2013\u2013, 1999, \u201cExtensive and strategic forms:\nGames and models for games\u201d, <em>Research in Economics</em>,\n53(3): 293\u2013319.</li>\n<li>\u2013\u2013\u2013, 2006, \u201cOn logics of knowledge and\nbelief\u201d, <em>Philosophical Studies</em>, 128(1):\n169\u2013199.</li>\n<li>Stuart Jr., H.W. &amp; H. Hu, 2002, \u201cAn epistemic analysis\nof the Harsanyi transformation\u201d, <em>International Journal of\nGame Theory</em>, 30(4): 517\u2013525.</li>\n<li>Tan, T.C.-C. &amp; S.R. da Costa Werlang, 1988, \u201cThe\nBayesian foundations of solution concepts of games\u201d, <em>Journal\nof Economic Theory</em>, 45(2): 370\u2013391,\n doi:10.1016/0022-0531(88)90276-1</li>\n<li>Titelbaum, M., 2013, \u201cTen reasons to care about the sleeping\nbeauty problem\u201d, <em>Philosophy Compass</em>, 8:\n1003\u20131017.</li>\n<li>Ullmann-Margalit, E. &amp; S. Morgenbesser, 1977, \u201cPicking\nand choosing\u201d, <em>Social Research</em>, 44: 757\u2013785.</li>\n<li>Vanderschraaf, P. &amp; G. Sillari, 2009, \u201cCommon\nknowledge\u201d, in E. N. Zalta (Ed.), <em>The Stanford Encyclopedia\nof Philosophy</em> (Spring 2009 Edition), URL =\n &lt;<a href=\"https://plato.stanford.edu/archives/spr2009/entries/common-knowledge/\" target=\"other\">https://plato.stanford.edu/archives/spr2009/entries/common-knowledge/</a>&gt;.</li>\n<li>de Weerd, H., R. Verbrugge, &amp; B. Verheij, 2013, \u201cHow\nmuch does it help to know what she knows you know? An agent-based\nsimulation study\u201d, <em>Artificial Intelligence</em>,\n199\u2013200: 67\u201392.</li>\n<li>Zvesper, J., 2010, <em>Playing with information</em> (PhD thesis),\nILLC, University of Amsterdam.</li>\n</ul>\n</div>"
    },
    "related_entries": {
        "entry_list": [
            "belief, formal representations of",
            "common knowledge",
            "epistemology: Bayesian",
            "game theory",
            "game theory: and ethics",
            "prisoner\u2019s dilemma"
        ],
        "entry_link": [
            {
                "../formal-belief/": "belief, formal representations of"
            },
            {
                "../common-knowledge/": "common knowledge"
            },
            {
                "../epistemology-bayesian/": "epistemology: Bayesian"
            },
            {
                "../game-theory/": "game theory"
            },
            {
                "../game-ethics/": "game theory: and ethics"
            },
            {
                "../prisoner-dilemma/": "prisoner\u2019s dilemma"
            }
        ]
    },
    "academic_tools": {
        "listed_text": [
            "<td valign=\"top\"><img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=epistemic-game\" target=\"other\">How to cite this entry</a>.",
            "<td valign=\"top\"><img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://leibniz.stanford.edu/friends/preview/epistemic-game/\" target=\"other\">Preview the PDF version of this entry</a> at the\n <a href=\"https://leibniz.stanford.edu/friends/\" target=\"other\">Friends of the SEP Society</a>.",
            "<td valign=\"top\"><img alt=\"inpho icon\" src=\"../../symbols/inpho.png\"/>",
            "<a href=\"https://www.inphoproject.org/entity?sep=epistemic-game&amp;redirect=True\" target=\"other\">Look up topics and thinkers related to this entry</a>\n at the Internet Philosophy Ontology Project (InPhO).",
            "<td valign=\"top\"><img alt=\"phil papers icon\" src=\"../../symbols/pp.gif\"/>",
            "<a href=\"http://philpapers.org/sep/epistemic-game/\" target=\"other\">Enhanced bibliography for this entry</a>\nat <a href=\"http://philpapers.org/\" target=\"other\">PhilPapers</a>, with links to its database."
        ],
        "listed_links": [
            {
                "https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=epistemic-game": "How to cite this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/preview/epistemic-game/": "Preview the PDF version of this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/": "Friends of the SEP Society"
            },
            {
                "https://www.inphoproject.org/entity?sep=epistemic-game&redirect=True": "Look up topics and thinkers related to this entry"
            },
            {
                "http://philpapers.org/sep/epistemic-game/": "Enhanced bibliography for this entry"
            },
            {
                "http://philpapers.org/": "PhilPapers"
            }
        ]
    },
    "other_internet_resources": {
        "listed_text": [
            "Baltag, A., and S. Smets, 2009, \n\u201c<a href=\"http://alexandru.tiddlyspot.com/#%5B%5BESSLLI09%20COURSE%5D%5D\" target=\"other\">Dynamic logics for interactive belief revision</a>,\u201d  \n  slides for ESSLLI 2009 Course."
        ],
        "listed_links": [
            {
                "http://alexandru.tiddlyspot.com/#%5B%5BESSLLI09%20COURSE%5D%5D": "Dynamic logics for interactive belief revision"
            }
        ]
    }
}