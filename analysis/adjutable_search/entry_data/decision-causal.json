{
    "url": "decision-causal",
    "title": "Causal Decision Theory",
    "authorship": {
        "year": "Copyright \u00a9 2020",
        "author_text": "Paul Weirich\n<weirichp@missouri.edu>",
        "author_links": [
            {
                "https://philosophy.missouri.edu/people/weirich": "Paul Weirich"
            },
            {
                "mailto:weirichp%40missouri%2eedu": "weirichp@missouri.edu"
            }
        ],
        "raw_html": "<div id=\"article-copyright\">\n<p>\n<a href=\"../../info.html#c\">Copyright \u00a9 2020</a> by\n\n<br/>\n<a href=\"https://philosophy.missouri.edu/people/weirich\" target=\"other\">Paul Weirich</a>\n&lt;<a href=\"mailto:weirichp%40missouri%2eedu\"><em>weirichp<abbr title=\" at \">@</abbr>missouri<abbr title=\" dot \">.</abbr>edu</em></a>&gt;\n    </p>\n</div>"
    },
    "pubinfo": [
        "First published Sat Oct 25, 2008",
        "substantive revision Fri Nov 6, 2020"
    ],
    "preamble": "\n\nCausal decision theory adopts principles of rational choice that\nattend to an act\u2019s consequences. It maintains that an account of\nrational choice must use causality to identify the considerations that\nmake a choice rational.\n\nGiven a set of options constituting a decision problem, decision\ntheory recommends an option that maximizes utility, that is, an option\nwhose utility equals or exceeds the utility of every other option. It\nevaluates an option\u2019s utility by calculating the option\u2019s\nexpected utility. It uses probabilities and utilities of an\noption\u2019s possible outcomes to define an option\u2019s expected\nutility. The probabilities depend on the option. Causal decision\ntheory takes the dependence to be causal rather than merely\nevidential.\n\nThis essay explains causal decision theory, reviews its history,\ndescribes current research in causal decision theory, and surveys the\ntheory\u2019s philosophical foundations. The literature on causal\ndecision theory is vast, and this essay covers only a portion of\nit.\n",
    "toc": [
        {
            "#ExpeUtil": "1. Expected Utility"
        },
        {
            "#Hist": "2. History"
        },
        {
            "#NewcProb": "2.1 Newcomb\u2019s Problem"
        },
        {
            "#StalSolu": "2.2 Stalnaker\u2019s Solution"
        },
        {
            "#Vari": "2.3 Variants"
        },
        {
            "#ReprTheo": "2.4 Representation Theorems"
        },
        {
            "#Obje": "2.5 Objections"
        },
        {
            "#CurrIssu": "3. Current Issues"
        },
        {
            "#ProbUtil": "3.1 Probability and Utility"
        },
        {
            "#PartInva": "3.2 Partition Invariance"
        },
        {
            "#Outc": "3.3 Outcomes"
        },
        {
            "#Acts": "3.4 Acts"
        },
        {
            "#GeneExpeUtil": "3.5 Generalizing Expected Utility"
        },
        {
            "#Rati": "3.6 Ratification"
        },
        {
            "#RelaTopiConcRema": "4. Related Topics and Concluding Remarks"
        },
        {
            "#Bib": "Bibliography"
        },
        {
            "#Aca": "Academic Tools"
        },
        {
            "#Oth": "Other Internet Resources"
        },
        {
            "#Rel": "Related Entries"
        }
    ],
    "main_text": "\n1. Expected Utility\n\nSuppose that a student is considering whether to study for an exam. He\nreasons that if he will pass the exam, then studying is wasted effort.\nAlso, if he will not pass the exam, then studying is wasted effort. He\nconcludes that because whatever will happen, studying is wasted\neffort, it is better not to study. This reasoning errs because\nstudying raises the probability of passing the exam. Deliberations\nshould take account of an act\u2019s influence on the probability of\nits possible outcomes.\nAn act\u2019s expected utility is a probability-weighted average\nof its possible outcomes\u2019 utilities. Possible states of the\nworld that are mutually exclusive and jointly exhaustive, and so form\na partition, generate an act\u2019s possible outcomes. An act-state\npair specifies an outcome. In the example, the act of studying and the\nstate of passing form an outcome comprising the effort of studying and\nthe benefit of passing. The expected utility of studying is the\nprobability of passing if one studies times the utility of studying\nand passing plus the probability of not passing if one studies times\nthe utility of studying and not passing. In compact notation,\n\n\\[\n\\textit{EU} (S) = P(P \\mbox{ if } S) \\util (S \\amp P) + P({\\sim}P \\mbox{ if } S) \\util (S \\amp{\\sim}P).\n\\] \n\n\nEach product specifies the probability and utility of a possible\noutcome. The sum is a probability-weighted average of the possible\noutcomes\u2019 utilities.\n\nHow should decision theory interpret the probability of a state \\(S\\)\nif one performs an act \\(A\\), that is, \\(P(S \\mbox{ if }A)\\)?\nProbability theory offers a handy suggestion. It has an account of\nconditional probabilities that decision theory may adopt. Decision\ntheory may take \\(P(S \\mbox{ if }A)\\) as the probability of the state\nconditional on the act. Then \\(P(S \\mbox{ if }A)\\) equals \\(P(S\\mid A)\\),\nwhich probability theory defines as \\(P(S \\amp A)/P(A)\\) when \\(P(A)\n\\ne 0\\). Some theorists call expected utility computed using\nconditional probabilities conditional expected utility. I call it\nexpected utility tout court because the formula using\nconditional probabilities generalizes a simpler formula for expected\nutility that uses nonconditional probabilities of states. Also, some\ntheorists call an act\u2019s expected utility its utility tout\ncourt because an act\u2019s expected utility appraises the act\nand yields the act\u2019s utility in ideal cases. I call it expected\nutility because a person by mistake may attach more or less utility to\na bet than its expected utility warrants. The equality of an\nact\u2019s utility and its expected utility is normative rather than\ndefinitional.\nExpected utilities obtained from conditional probabilities steer\nthe student\u2019s deliberations in the right\ndirection.\n\n\\[\\textit{EU} (S) = P(P\\mid S)\\util (S \\amp P) + P({\\sim}P\\mid S)\\util\n(S \\amp{\\sim}P), \\]\n\nand \n\n\\[\\textit{EU} ({\\sim}S) = P(P\\mid {\\sim}S)\\util ({\\sim}S \\amp P) +\nP({\\sim}P\\mid {\\sim}S)\\util ({\\sim}S \\amp{\\sim}P).\n\\] \n\n\nBecause of studying\u2019s effect on the probability of passing,\n\\(P(P\\mid S) \\gt P(P\\mid {\\sim}S)\\) and \\(P({\\sim}P\\mid S) \\lt\nP({\\sim}P\\mid {\\sim}S)\\). So \\(\\textit{EU} (S) \\gt \\textit{EU}\n({\\sim}S)\\), assuming that studying\u2019s increase in the\nprobability of passing compensates for the effort of\nstudying. Maximization of expected utility recommends studying.\n\nThe handy interpretation of the probability of a state if one performs\nan act, however, is not completely satisfactory. Suppose that one\ntosses a coin with an unknown bias and obtains heads. This result is\nevidence that the next toss will yield heads, although it does not\ncausally influence the next toss\u2019s result. An event\u2019s\nprobability conditional on another event indicates the evidence that\nthe second event provides for the first. If the two events are\ncorrelated, the second may provide evidence for the first without\ncausally influencing it. Causation entails correlation, but\ncorrelation does not entail causation. Deliberations should attend to\nan act\u2019s causal influence on a state rather than an act\u2019s\nevidence for a state. A good decision aims to produce a good outcome\nrather than evidence of a good outcome. It aims for the good and not\njust signs of the good. Often efficacy and auspiciousness go hand in\nhand. When they come apart, an agent should perform an efficacious act\nrather than an auspicious act.\n\nConsider the Prisoner\u2019s Dilemma, a stock example of game theory.\nTwo people isolated from each other may each act either cooperatively\nor uncooperatively. They each do better if they each act cooperatively\nthan if they each act uncooperatively. However, each does better if he\nacts uncooperatively, no matter what the other does. Acting\nuncooperatively dominates acting cooperatively. Suppose, in addition,\nthat the two players are psychological twins. Each thinks as the other\nthinks. Moreover, they know this fact about themselves. Then if one\nplayer acts cooperatively, he concludes that his counterpart also acts\ncooperatively. His acting cooperatively is good evidence that his\ncounterpart does the same. Nonetheless, his acting cooperatively does\nnot cause his counterpart to act cooperatively. He has no contact with\nhis counterpart. Because he is better off not acting cooperatively\nwhatever his counterpart does, not acting cooperatively is the better\ncourse. Acting cooperatively is auspicious but not efficacious.\n\nTo make expected utility track efficacy rather than auspiciousness,\ncausal decision theory interprets the probability of a state if one\nperforms an act as a type of causal probability rather than as a\nstandard conditional probability. In the Prisoner\u2019s Dilemma with\ntwins, consider the probability of one player\u2019s acting\ncooperatively given that the other player does. This conditional\nprobability is high. Next, consider the causal probability of one\nplayer\u2019s acting cooperatively if the other player does. Because\nthe players are isolated, this probability equals the probability of\nthe first player\u2019s acting cooperatively. It is low if that\nplayer follows dominance. Using conditional probabilities, the\nexpected utility of acting cooperatively exceeds the expected utility\nof acting uncooperatively. However, using causal probabilities, the\nexpected utility of acting uncooperatively exceeds the expected\nutility of acting cooperatively. Switching from conditional to causal\nprobabilities makes expected-utility maximization yield acting\nuncooperatively.\n2. History\n\nThis section tours causal decision theory\u2019s history and along\nthe way presents various formulations of the theory.\n2.1 Newcomb\u2019s Problem\n\nRobert Nozick (1969) presented a dilemma for decision theory. He\nconstructed an example in which the standard principle of dominance\nconflicts with the standard principle of expected-utility\nmaximization. Nozick called the example Newcomb\u2019s Problem after\nthe physicist, William Newcomb, who first formulated the problem.\n\nIn Newcomb\u2019s Problem an agent may choose either to take an\nopaque box or to take both the opaque box and a transparent box. The\ntransparent box contains one thousand dollars that the agent plainly\nsees. The opaque box contains either nothing or one million dollars,\ndepending on a prediction already made. The prediction was about the\nagent\u2019s choice. If the prediction was that the agent will take\nboth boxes, then the opaque box is empty. On the other hand, if the\nprediction was that the agent will take just the opaque box, then the\nopaque box contains a million dollars. The prediction is reliable. The\nagent knows all these features of his decision problem.\n\nFigure 1 displays the agent\u2019s options and their outcomes. A row\nrepresents an option, a column a state of the world, and a cell an\noption\u2019s outcome in a state of the world.\n\n\n\n\nPrediction\n\none-boxing\nPrediction\n\ntwo-boxing \n\nTake one box\n\\(\\$M\\)\n\\(\\$0\\) \n\nTake two boxes\n\\(\\$M + \\$T\\)\n\\(\\$T\\) \n\n\nFigure 1. Newcomb\u2019s Problem\n\n\nBecause the outcome of two-boxing is better by \\(\\$T\\) than the\noutcome of one-boxing given each prediction, two-boxing dominates\none-boxing. Two-boxing is the rational choice according to the\nprinciple of dominance. Because the prediction is reliable, a\nprediction of one-boxing has a high probability given one-boxing.\nSimilarly, a prediction of two-boxing has a high probability given\ntwo-boxing. Hence, using conditional probabilities to compute expected\nutilities, one-boxing\u2019s expected utility exceeds\ntwo-boxing\u2019s expected utility. One-boxing is the rational choice\naccording to the principle of expected-utility maximization.\n\nDecision theory should address all possible decision problems and not\njust realistic decision problems. However, if Newcomb\u2019s problem\nseems untroubling because unrealistic, realistic versions of the\nproblem are plentiful. The essential feature of Newcomb\u2019s\nproblem is an inferior act\u2019s correlation with a good state that\nit does not causally promote. In realistic, medical Newcomb problems,\na medical condition and a behavioral symptom have a common cause and\nare correlated although neither causes the other. If the behavior is\nattractive, dominance recommends it although expected utility\nmaximization prohibits it. Also, Allan Gibbard and William Harper\n(1978: Sec. 12) and David Lewis (1979) observe that a Prisoner\u2019s\nDilemma with psychological twins poses a Newcomb problem for each\nplayer. For each player, the other player\u2019s act is a state\naffecting the outcome. Acting cooperatively is a sign, but not a\ncause, of the other player\u2019s acting cooperatively. Dominance\nrecommends acting uncooperatively, whereas expected utility computed\nwith conditional probabilities recommends acting cooperatively. In\nsome realistic instances of the Prisoner\u2019s Dilemma, the\nplayers\u2019 anticipated similarity of thought creates a conflict\nbetween the principle of dominance and the principle of\nexpected-utility maximization.\n\nArif Ahmed (2018) offers a collection of essays on Newcomb\u2019s\nproblem. Kenny Easwaran (forthcoming) distinguishes Newcomb-like problems\naccording to opportunities for causal intervention.\n2.2 Stalnaker\u2019s Solution\n\nRobert Stalnaker (1968) presented truth conditions for subjunctive\nconditionals. A subjunctive conditional is true if and only if in the\nnearest antecedent-world, its consequent is true. (This analysis is\nunderstood so that a subjunctive conditional is true if its antecedent\nis true in no world.) Stalnaker used analysis of subjunctive\nconditionals to ground their role in decision theory and in a\nresolution of Newcomb\u2019s problem.\nIn a letter to Lewis, Stalnaker (1972) proposed a way of\nreconciling decision principles in Newcomb\u2019s problem. He\nsuggested calculating an act\u2019s expected utility using\nprobabilities of conditionals in place of conditional\nprobabilities. Accordingly,\n\n\\[\n\\textit{EU} (A) = \\sum_i P(A \\gt S_i)\\util (A \\amp S_i),\n\\] \n\n\nwhere \\(A \\gt S_i\\) stands for the conditional that if \\(A\\) were\nperformed then \\(S_i\\) would obtain. Thus, instead of using the\nprobability of a prediction of one-boxing given one-boxing, one should\nuse the probability of the conditional that if the agent were to pick\njust one box, then the prediction would have been one-boxing. Because\nthe agent\u2019s act does not cause the prediction, the probability\nof the conditional equals the probability that the prediction is\none-boxing. Also, consider the conditional that if the agent were to\npick both boxes, then the prediction would have been one-boxing. Its\nprobability similarly equals the probability that the prediction is\none-boxing. The act the agent performs does not affect any\nprediction\u2019s probability because the prediction occurs prior to\nthe act. Consequently, using probabilities of conditionals to compute\nexpected utility, two-boxing\u2019s expected utility exceeds\none-boxing\u2019s expected utility. Therefore, the principle of\nexpected-utility maximization makes the same recommendation as does\nthe principle of dominance.\n\nGibbard and Harper (1978) elaborated and made public Stalnaker\u2019s\nresolution of Newcomb\u2019s problem. They distinguished causal\ndecision theory, which uses probabilities of subjunctive conditionals,\nfrom evidential decision theory, which uses conditional probabilities.\nBecause in decision problems probabilities of subjunctive conditionals\ntrack causal relations, using them to calculate an option\u2019s\nexpected utility makes decision theory causal.\n\nGibbard and Harper distinguished two types of expected utility. One\ntype they called value and represented with \\(V\\). It indicates\nnews-value or auspiciousness. The other type they called utility and\nrepresented with \\(U\\). It indicates efficacy in attainment of goals.\nA calculation of an act\u2019s expected value uses conditional\nprobabilities, and a calculation of its expected utility uses\nprobabilities of conditionals. They argued that expected utility,\ncalculated with probabilities of conditionals, yields genuine expected\nutility.\n\nAs Gibbard and Harper introduce \\(V\\) and \\(U\\), both rest on an\nassessment \\(D\\) (for desirability) of maximally specific outcomes.\nInstead of adopting a formula for expected utility that uses an\nassessment of outcomes neutral with respect to evidential and causal\ndecision theory, this essay follows Stalnaker (1972) in adopting a\nformula that uses utility to evaluate outcomes.\n2.3 Variants\n\nConsider a conditional asserting that if an option were adopted, then\na certain state would obtain. Gibbard and Harper assume, to illustrate\nthe main ideas of causal decision theory, that the conditional has a\ntruth-value, and that, given its falsity, if the option were adopted,\nthen the state would not obtain. This assumption may be unwarranted if\nthe option is flipping a coin, and the relevant state is obtaining\nheads. It may be false (or indeterminate) that if the agent were to\nflip the coin, he would obtain heads. Similarly, the corresponding\nconditional about obtaining tails may be false (or indeterminate).\nThen probabilities of conditionals are not suitable for calculating\nthe option\u2019s expected utility. The relevant probabilities do not\nsum to one (or do not even exist). To circumvent such impasses, some\ntheorists calculate causally-sensitive expected utilities without\nprobabilities of subjunctive conditionals. Causal decision theory has\nmany formulations.\nBrian Skyrms (1980: Sec IIC; 1982) presented a version of causal\ndecision theory that dispenses with probabilities of subjunctive\nconditionals. His theory separates factors that the agent\u2019s act\nmay influence from factors that the agent\u2019s act may not\ninfluence. It lets \\(K_i\\) stand for a possible full specification of\nfactors that an agent may not influence and lets \\(C_j\\) stand for a\npossible (but not necessarily full) specification of factors that the\nagent may influence. The set of \\(K_i\\) forms a partition, and the set\nof \\(C_j\\) forms a partition. The formula for an act\u2019s expected\nutility first calculates its expected utility using factors the agent\nmay influence, with respect to each possible combination of factors\noutside the agent\u2019s influence. Then it computes a\nprobability-weighted average of those conditional expected\nutilities. An act\u2019s expected utility calculated this way is the\nact\u2019s \\(K\\)-expectation, \\(\\textit{EU}_k(A)\\). According to\nSkyrms\u2019s definition,\n\n\\[\\textit{EU}_k(A) = \\sum_i P(K_i)\\sum_j P(C_j \\mid K_i \\amp A)\\util (C_j\n\\amp K_i \\amp A).\\]\n\n\nSkyrms holds that an agent should select an act that maximizes\n\\(K\\)-expectation.\nLewis (1981) presented a version of causal decision theory that\ncalculates expected utility using probabilities of dependency\nhypotheses instead of probabilities of subjunctive conditionals. A\ndependency hypothesis for an agent at a time is a maximally specific\nproposition about how the things the agent cares about do and do not\ndepend causally on his present acts. An option\u2019s expected\nutility is its probability-weighted average utility with respect to a\npartition of dependency hypotheses \\(K_i\\). Lewis defines the expected\nutility of an option \\(A\\) as\n\n\\[\n\\textit{EU} (A) = \\sum_i P(K_i)\\util (K_i \\amp A)\n\\] \n\n\nand holds that to act rationally is to realize an option that\nmaximizes expected utility. His formula for an option\u2019s expected\nutility is the same as Skyrms\u2019s assuming that \\(U(K_i \\amp A)\\)\nmay be expanded with respect to a partition of factors the agent may\ninfluence, using the formula\n\n\\[\nU(K_i \\amp A) = \\sum_j P(C_j\\mid K_i \\amp A)\\util (C_j \\amp K_i \\amp A).\n\\]\n\n\nSkyrms\u2019s and Lewis\u2019s calculations of expected utility\ndispense with causal probabilities. They build causality into states\nof the world so that causal probabilities are unnecessary. In cases\nsuch as Newcomb\u2019s problem, their calculations yield the same\nrecommendations as calculations of expected utility employing\nprobabilities of subjunctive conditionals. The various versions of\ncausal decision theory make equivalent recommendations when cases meet\ntheir background assumptions. Adam Bales (2016) compares versions in\nspecial cases that do not meet the background assumptions.\n2.4 Representation Theorems\n\nDecision theory often introduces probability and utility with\nrepresentation theorems. These theorems show that if preferences among\nacts meet certain constraints, such as transitivity, then there exist\na probability function and a utility function (given a choice of\nscale) that generate expected utilities agreeing with preferences.\nDavid Krantz, R. Duncan Luce, Patrick Suppes, and Amos Tversky (1971)\noffer a good, general introduction to the purposes and methods of\nconstructing representations theorems. In\n Section 3.1,\n I discuss the theorems\u2019 function in decision theory.\n\nRichard Jeffrey ([1965] 1983) presented a representation theorem for\nevidential decision theory, using its formula for expected utility.\nBrad Armendt (1986, 1988a) presented a representation theorem for\ncausal decision theory, using its formula for expected utility. James\nJoyce (1999) constructed a very general representation theorem that\nyields either causal or evidential decision theory depending on the\ninterpretation of probability that the formula for expected utility\nadopts.\n2.5 Objections\n\nThe most common objection to causal decision theory is that it yields\nthe wrong choice in Newcomb\u2019s problem. It yields two-boxing,\nwhereas one-boxing is correct. Terry Horgan (1981 [1985]), Paul\nHorwich (1987: Chap. 11), and Caspar Hare and Brian Hedden (2016) for\nexample, promote one-boxing. The main rationale for one-boxing is that\none-boxers fare better than do two-boxers. Causal decision theorists\nrespond that Newcomb\u2019s problem is an unusual case that rewards\nirrationality. One-boxing is irrational even if one-boxers\nprosper. Bales (2018) rejects the argument that two-boxing is\nirrational\n\nSome theorists hold that one-boxing is plainly rational if the\nprediction is completely reliable. They maintain that if the\nprediction is certainly accurate, then choice reduces to taking\n\\(\\$M\\) or taking \\(\\$T\\). This view oversimplifies. If an agent\none-boxes, then that act is certain to yield \\(\\$M\\). However, the\nagent still would have done better by taking both boxes. Dominance\nstill recommends two-boxing. Making the prediction certain to be\naccurate does not change the character of the problem. Efficacy still\ntrumps auspiciousness, as Howard Sobel (1994: Chap. 5) argues.\n\nA way of reconciling the two sides of the debate about Newcomb\u2019s\nproblem acknowledges that a rational person should prepare for the\nproblem by cultivating a disposition to one-box. Then whenever the\nproblem arises, the disposition will prompt a prediction of one-boxing\nand afterwards the act of one-boxing (still freely chosen). Causal\ndecision theory may acknowledge the value of this preparation. It may\nconclude that cultivating a disposition to one-box is rational\nalthough one-boxing itself is irrational. Hence, if in Newcomb\u2019s\nproblem an agent two-boxes, causal decision theory may concede that\nthe agent did not rationally prepare for the problem. It nonetheless\nmaintains that two-boxing itself is rational. Although two-boxing is\nnot the act of a maximally rational agent, it is rational given the\ncircumstances of Newcomb\u2019s problem.\n\nCausal decision theory may also explain that it advances a claim about\nthe evaluation of an act given the agent\u2019s circumstances in\nNewcomb\u2019s problem. It asserts two-boxing\u2019s conditional\nrationality. Conditional and nonconditional rationality treat mistakes\ndifferently. In contrast with conditional rationality, nonconditional\nrationality does not grant past mistakes. It evaluates an act taking\naccount of the influence of past mistakes. However, conditional\nrationality accepts present circumstances as they are and does not\ndiscredit an act because it stems from past mistakes. Causal decision\ntheory maintains that two-boxing is rational, granting the\nagent\u2019s circumstances and so ignoring any mistakes leading to\nthose circumstances, such as irrational preparation for\nNewcomb\u2019s problem.\n\nAnother objection to causal decision theory concedes that two-boxing\nis the rational choice in Newcomb\u2019s problem but rejects causal\nprinciples of choice that yield two-boxing. It seeks noncausal\nprinciples that yield two-boxing. Positivism is a source of aversion\nto decision principles incorporating causation. Some decision\ntheorists shun causation because no positivist account specifies its\nnature. Without a definition of causation in terms of observable\nphenomena, they prefer that decision theory avoid causation. Causal\ndecision theory\u2019s response to this objection is both to\ndiscredit positivism and also to clarify causation so that puzzles\nconcerning it no longer give decision theory any reason to avoid\nit.\n\nEvidential decision theory has weaker metaphysical assumptions than\nhas causal decision theory, even if causation has impeccable\nmetaphysical credentials. Some decision theorists do not omit\ncausation because of metaphysical scruples but for conceptual economy.\nJeffrey ([1965] 1983, 2004), for the sake of parsimony, formulates\ndecision principles that do not rely on causal relations.\n\nEllery Eells (1981, 1982) contends that evidential decision theory\nyields causal decision theory\u2019s recommendations but, more\neconomically, without reliance on causal apparatus. In particular,\nevidential decision theory yields two-boxing in Newcomb\u2019s\nproblem. An agent\u2019s reflection on his evidence makes conditional\nprobabilities support two-boxing.\n\nA noncontentious elaboration of Newcomb\u2019s problem posits that\nthe agent\u2019s choice and its prediction have a common cause. The\nagent\u2019s choice is evidence of the common cause and evidence of\nthe choice\u2019s prediction. Once an agent acquires the probability\nof the common cause, he may put aside the evidence his choice provides\nabout the prediction. That evidence is superfluous. Given the\nprobability of the common cause, the probability of a prediction of\none-boxing is constant with respect to his options. Similarly, the\nprobability of a prediction of two-boxing is constant with respect to\nhis options. Because the probability of a prediction is the same\nconditional on either option, the expected utility of two-boxing\nexceeds the expected utility of one-boxing according to evidential\ndecision theory. Horgan (1981 [1985]) and Huw Price (1986) make\nsimilar points.\n\nSuppose that an event \\(S\\) is a sign of a cause \\(C\\) that produces\nan effect \\(E\\). For the probability of \\(E\\), knowing whether \\(C\\)\nholds makes superfluous knowing whether \\(S\\) holds. Observation of\n\\(C\\) screens off the evidence that \\(S\\) provides for \\(E\\).\nThat is, \\(P(E\\mid C \\amp S) = P(E\\mid C)\\). In Newcomb\u2019s problem,\nassuming that the agent is rational, his beliefs and desires are a\ncommon cause of his choice and the prediction. So his choice is a sign\nof the prediction\u2019s content. For the probability of a prediction\nof one-boxing, knowing one\u2019s beliefs and desires makes\nsuperfluous knowing the choice that they yield. Knowledge of the\ncommon cause screens off evidence that the choice provides about the\nprediction. Hence, the probability of a prediction of one-boxing is\nconstant with respect to one\u2019s choice, and maximization of\nevidential expected-utility agrees with the principle of dominance.\nThis defense of evidential decision theory is called the tickle\ndefense because it assumes that an introspected condition screens off\nthe correlation between choice and prediction.\n\nEells\u2019s defense of evidential decision theory assumes that an\nagent chooses according to beliefs and desires and knows his beliefs\nand desires. Some agents may not choose this way and may not have this\nknowledge. Decision theory should prescribe a rational choice for such\nagents, and evidential decision theory may not do that correctly, as\nLewis (1981: 10\u201311) and John Pollock (2010) argue. Armendt\n(1988b: 326\u2013329) and David Papineau (2001: 252\u2013255) concur\nthat the phenomenon of screening off does not in all cases make\nevidential decision theory yield the results of causal decision\ntheory.\n\nHorwich (1987: Chap. 11) rejects Eells\u2019s argument because, even\nif an agent knows that her choice springs from her beliefs and\ndesires, she may be unaware of the mechanism by which her beliefs and\ndesires produce her choice. The agent may doubt that she chooses by\nmaximizing expected utility. Then in Newcomb\u2019s problem her\nchoice may offer relevant evidence about the prediction. Eells (1984a)\nconstructs a dynamic version of the tickle defense to meet this\nobjection. Sobel (1994: Chap. 2) discusses that version of the\ndefense. He argues that it does not yield evidential decision\ntheory\u2019s agreement with causal decision theory in all decision\nproblems in which an act furnishes evidence concerning the state of\nthe world. Moreover, it does not establish that an evidential theory\nof rational desire agrees with a causal theory of rational desire. He\nconcludes that even in cases where evidential decision theory yields\nthe right recommendation, it does not yield it for the right\nreasons.\n\nPrice (2012) proposes a blend of evidential and causal decision theory\nand motivates it with an analysis of cases in which an agent has\nforeknowledge of an event occurring by chance. Causal decision theory\non its own accommodates such cases, argues Bales (2016). \nAhmed (2014a) champions evidential decision theory and advances several\nobjections to causal decision theory. His objections assume some\ncontroversial points about rational choice, including a controversial\nprinciple for sequences of choices.\n\nA common view distinguishes principles for evaluating choices from\nprinciples for evaluating sequences of choices. The principle of\nutility maximization evaluates an agent\u2019s choice as a resolution\nof a decision problem only if the agent has direct control of each\noption in the decision problem, that is, only if the agent can at will\nimmediately adopt any option in the decision problem. The principle\ndoes not evaluate an agent\u2019s sequence of multiple choices\nbecause the agent does not have direct control of such a sequence. She\nrealizes a sequence of multiple choices only by making each choice in\nthe sequence at the time for it; she cannot at will immediately\nrealize the entire sequence. Rationality evaluates an option in an\nagent\u2019s direct control by comparing it with alternatives but\nevaluates a sequence in an agent\u2019s indirect control by\nevaluating the directly controlled options in the sequence; a sequence\nof choices is rational if the choices in the sequence are rational.\nAdopting this common method of evaluating sequences of choices fends\noff objections to causal decision theory that assume rival\nmethods.\n3. Current Issues\n\nDecision theory is an active area of research. Current work addresses\na number of problems. Causal decision theory\u2019s approach to those\nproblems arises from its nonpositivistic methodology and its attention\nto causation. This section mentions some topics on causal decision\ntheory\u2019s agenda.\n3.1 Probability and Utility\n\nPrinciples of causal decision theory use probabilities and utilities.\nThe interpretation of probabilities and utilities is a matter of\ndebate. One tradition defines them in terms of functions that\nrepresentation theorems introduce to depict preferences. The\nrepresentation theorems show that if preferences meet certain\nstructural axioms, then if they also meet certain normative axioms,\nthey are as if they follow expected utility. That is, preferences\nfollow expected utility calculated using probability and utility\nfunctions constructed so that preferences follow expected utility.\nExpected utility calculated this way differs from expected utility\ncalculated using probability and utility assignments grounded in\nattitudes toward possible outcomes. For example, a person confused\nabout bets concerning a coin toss may have preferences among those\nbets that are as if he assigns probability 60% to heads, when, in\nfact, the evidence of past tosses leads him to assign probability 40%\nto heads. Consequently, when preferences meet a representation\ntheorem\u2019s structural axioms, the theorem\u2019s normative\naxioms justify only conformity with expected utility fabricated to\nagree with preferences and do not justify conformity with expected\nutility in the traditional sense. Defining probability and utility\nusing the representation theorems thus weakens the traditional\nprinciple of expected utility. It becomes merely a principle of\ncoherence among preferences.\n\nInstead of using the representation theorems to define probabilities\nand utilities, decision theory may use them to establish\nprobabilities\u2019 and utilities\u2019 measurability when\npreferences meet structural and normative axioms. This employment of\nthe representation theorems allows decision theory to advance the\ntraditional principle of expected utility and thereby enrich its\ntreatment of rational decisions. Decision theory may justify that\ntraditional principle by deriving it from general principles of\nevaluation, as in Weirich (2001).\n\nA broad account of probabilities and utilities takes them to indicate\nattitudes toward propositions. They are rational degrees of belief and\nrational degrees of desire, respectively. This account of\nprobabilities and utilities recognizes their existence in cases where\nthey are not inferable from preferences or their other effects but\ninstead are inferable from their causes, such as an agent\u2019s\ninformation about objective probabilities, or are not inferable at all\n(except perhaps by introspection). The account relies on arguments\nthat degrees of belief and degrees of desire, if rational, conform to\nstandard principles of probability and utility. Bolstering these\narguments is work for causal decision theory.\n\nBesides clarifying its general interpretation of probability and\nutility, causal decision theory searches for the particular\nprobabilities and utilities that yield the best version of its\nprinciple to maximize expected utility. The causal probabilities in\nits formula for expected utility may be probabilities of subjunctive\nconditionals or various substitutes. Versions that use probabilities\nof subjunctive conditionals must settle on an analysis of those\nconditionals. Lewis (1973: Chap. 1) modifies Stalnaker\u2019s\nanalysis to count a subjunctive conditional true if and only if as\nantecedent worlds come closer and closer to the actual world, there is\na point beyond which the consequent is true in all the worlds at least\nthat close. Joyce (1999: 161\u2013180) advances probability images,\nas Lewis (1976) introduces them, as substitutes for probabilities of\nsubjunctive conditionals. The probability image of a state \\(S\\) under\nsubjunctive supposition of an act \\(A\\) is the probability of \\(S\\)\naccording to an assignment that shifts the probability of\n\\({\\sim}A\\)-worlds to nearby \\(A\\)-worlds. Causal relations among an\nact and possible states guide probability\u2019s reassignment.\nA common formula for an act\u2019s expected utility takes the\nutility for an act-state pair, the utility of the act\u2019s outcome\nin the state, to be the utility of the act\u2019s and the\nstate\u2019s conjunction:\n\n\\[\n\\textit{EU} (A) = \\sum_i P(A \\gt S_i)\\util (A \\amp S_i).\n\\] \n\n\nDoes causal decision theory need an alternative, more\ncausally-sensitive utility for an act-state pair?  Weirich (1980)\nargues that it does. A person contemplating a wager that the capital\nof Missouri is Jefferson City entertains the consequences if\nhe were to make the wager given that St. Louis is\nMissouri\u2019s capital. A rational deliberator subjunctively\nsupposes an act attending to causal relations and indicatively\nsupposes a state attending to evidential relations, but can suppose an\nact\u2019s and a state\u2019s conjunction only one way. Furthermore,\nusing the utility of an act\u2019s and a state\u2019s conjunction\nprevents an act\u2019s expected utility from being\npartition-invariant. The next subsection elaborates this point.\n3.2 Partition Invariance\n\nAn act\u2019s expected utility is partition invariant if and only if\nit is the same under all partitions of states. Partition invariance is\na vital property of an act\u2019s expected utility. If acts\u2019\nexpected utilities lack this property, then decision theory may use\nonly expected utilities computed from selected partitions. Expected\nutility\u2019s partition invariance makes an act\u2019s expected\nutility independent of selection of a partition of states and thereby\nincreases expected utility\u2019s explanatory power.\n\nPartition invariance ensures that various representations of the same\ndecision problem yield solutions that agree. Take Newcomb\u2019s\nproblem with Figure 2\u2019s representation.\n\n\n\n\nRight prediction\nWrong prediction \n\nTake only one box\n\\(\\$M\\)\n$0 \n\nTake two boxes\n\\(\\$T\\)\n \\(\\$M + \\$T\\) \n\n\nFigure 2. New States for Newcomb\u2019s\nProblem\n\n\nDominance does not apply to this representation. It nonetheless\nsettles the problem\u2019s solution because it applies to a decision\nproblem if it applies to any accurate representation of the problem,\nsuch as Figure 1\u2019s representation of the problem. If expected\nutilities are partition-sensitive, then acts that maximize expected\nutility may be partition-sensitive. The principle of expected utility\ndoes not yield a decision problem\u2019s solution, however, if acts\nof maximum expected-utility change from one partition to another. In\nthat case an act is not a solution to a decision problem simply\nbecause it maximizes expected utility under some accurate\nrepresentation of the problem. Too many acts have the same\ncredential.\n\nThe expected utility principle, using probabilities of conditionals,\napplies to Figure 2\u2019s representation of Newcomb\u2019s problem.\nLetting \\(P1\\) stand for a prediction of one-boxing and \\(P2\\) stand\nfor a prediction of two-boxing, the acts\u2019 expected utilities\nare: \n\n\\[\n\\begin{align}\n\\textit{EU} (1)   & =   P(1 \\gt R)\\util (\\$M) + P(1 \\gt W)0\\\\\n                &  =   P(P1)\\util (\\$M)\\\\\n \\textit{EU} (2)   & =   P(2 \\gt R)\\util (\\$T) + P(2 \\gt W)\\util (\\$M + \\$T)\\\\\n                &  =   P(P2)\\util (\\$T) + P(P1)\\util (\\$M + \\$T)\\\\\n\\end{align}\n\\]\n\n\nHence \\(\\textit{EU}(1) \\lt EU(2)\\). This result agrees with the\nverdict of causal decision theory given other accurate representations\nof the problem. Provided that causal decision theory uses a\npartition-invariant formula for expected utility, its recommendations\nare independent of a decision problem\u2019s representation.\nLewis (1981: 12\u201313) observes that the formula\n\n\\[\nEU(A) = \\sum_i P(S_i)\\util (A \\amp S_i)\n\\] \n\n\nis not partition invariant. Its results depend on the partition of\nstates. If a state is a set of worlds with equal utilities, then with\nrespect to a partition of such states every act has the same expected\nutility. An element \\(S_i\\) of the partition obscures the effects of\n\\(A\\) that the utility of an outcome should evaluate. Lewis overcomes\nthis problem by using only partitions of dependency\nhypotheses. However, causal decision theory may craft a\npartition-invariant formula for expected utility by adopting a\nsubstitute for \\(U(A \\amp S_i)\\).\nSobel (1994: Chap. 9) investigates partition invariance. Putting\nhis work in this essay\u2019s notation, he proceeds as\nfollows. First, he takes a canonical computation of an option\u2019s\nexpected utility to use worlds as states. His basic formula is\n\n\\[\n\\textit{EU} (A) = \\sum_i P(A \\gt W_i)\\util (W_i).\n\\] \n\n\nA world \\(W_i\\) absorbs an act performed in it. Only the worlds in\nwhich \\(A\\) holds contribute positive probabilities and so affect the\nsum. Next, Sobel searches for other computations, using coarse-grained\nstates, that are equivalent to the canonical computation. A suitable\nspecification of utilities achieves partition invariance given his\nassumptions. According to a theorem he proves (1994: 185),\n\n\\[\nU(A) = \\sum_i P(S_i)\\util (A \\mbox{ given } S_i)\n\\] \n\n\nfor any partition of states.\nJoyce (2000: S11) also articulates for causal decision theory a\npartition-invariant formula for an act\u2019s expected utility. He\nachieves partition invariance, assuming that\n\n\\[\n\\textit{EU} (A) = \\sum_i P(A \\gt S_i)\\util (A \\amp S_i),\n\\] \n\nby stipulating that \\(U(A \\amp S_i)\\) equals\n\n\\[\n\\sum_{ij} P^A(W_j\\mid S_i)\\util (W_j),\n\\] \n\n\nwhere \\(W_j\\) is a world and \\(P^A\\) stands for the probability image\nof \\(A\\). Weirich (2001: Secs. 3.2, 4.2.2), as Sobel does, substitutes\n\\(U(A \\mbox{ given }S_i)\\) for \\(U(A \\amp S_i)\\) in the formula for\nexpected utility and interprets \\(U(A \\mbox{ given }S_i)\\) as the\nutility of the outcome that \\(A\\)\u2019s realization would produce if\n\\(S\\) obtains. Accordingly, \\(U(A \\mbox{ given }S_i)\\) responds to\n\\(A\\)\u2019s causal consequences in worlds where \\(S_i\\) holds. Then\nthe formula\n\n\\[\n\\textit{EU} (A) = \\sum_i P(S_i) \\util (A \\mbox{ given }S_i)\n\\] \n\n\nis invariant with respect to partitions in which states are\nprobabilistically independent of the act. A more complex formula,\n\n\\[\n\\textit{EU} (A) = \\sum_i P(S_i \\mbox{ if }A)\\util (A \\mbox{ given }\n(S_i \\mbox{ if } A)),\n\\] \n\n\nassuming a causal interpretation of its probabilities, relaxes all\nrestriction on partitions. \\(U(A \\mbox{ given }(S_i \\mbox{ if }A))\\)\nis the utility of the outcome if \\(A\\) were realized, given that it is\nthe case that \\(S_i\\) would obtain if \\(A\\) were realized.\n3.3 Outcomes\n\nOne issue concerning outcomes is their comprehensiveness. Are an\nact\u2019s outcomes possible worlds, temporal aftermaths, or causal\nconsequences? Gibbard and Harper ([1978] 1981: 166\u2013168) mention\nthe possibility of narrowing outcomes to causal consequences, as\npractical applicability advocates. The narrowing must be judicious,\nhowever, because the expected-utility principle requires that outcomes\ninclude every relevant consideration. For example, if an agent is\naverse to risk, then each of a risky act\u2019s possible outcomes\nmust include the risk the act generates. Its inclusion tends to lower\neach possible outcome\u2019s utility.\nIn Sobel\u2019s canonical formula for expected utility,\n\n\\[\n\\textit{EU} (A) = \\sum_i P(A \\gt W_i)\\util (W_i).\n\\] \n\n\nThe formula, from one perspective, omits states of the world because\nthe outcomes themselves form a partition. The distinction between\nstates and outcomes dissolves because worlds play the role of both\nstates and outcomes. States are dispensable means of generating\noutcomes that are exclusive and exhaustive. According to a basic\nprinciple, an act\u2019s expected utility is a probability-weighted\naverage of possible outcomes that are exclusive and exhaustive, such\nas the worlds to which the act may lead.\nSuppose that a world\u2019s utility comes from realization of\nbasic intrinsic desires and aversions. Granting that the utilities of\ntheir realizations are additive, the utility of a world is a sum of\nthe utilities of their realizations. Then besides being a\nprobability-weighted average of the utilities of worlds to which it\nmay lead, an option\u2019s expected utility is also a\nprobability-weighted average of the realizations of basic intrinsic\ndesires and aversions. In this formula for its expected utility,\nstates play no explicit role:\n\n\\[\n\\textit{EU} (A) = \\sum_i P(A \\gt B_i)\\util (B_i),\n\\] \n\n\nwhere \\(B_i\\) ranges over possible realizations of basic intrinsic\ndesires and aversions. The formula considers for each basic desire and\naversion the prospect of its realization if the act were performed. It\ntakes the act\u2019s expected utility as the sum of the\nprospects\u2019 utilities. The formula provides an economical\nrepresentation of an act\u2019s expected utility. It eliminates\nstates and obtains expected utility directly from outcomes taken as\nrealizations of basic desires and aversions.\n\nTo illustrate calculation of an act\u2019s expected utility using\nbasic intrinsic desires and aversions, suppose that an agent has no\nbasic intrinsic aversions and just two basic intrinsic desires, one\nfor health and the other for wisdom. The utility of health is 4, and\nthe utility of wisdom is 8. In the formula for expected utility, a\nworld covers only matters about which the agent cares. In the example,\na world is a proposition specifying whether the agent has health and\nwhether he has wisdom. Accordingly, there are four worlds:\n\n\\[\n\\begin{align}\nH  \\amp W, \\\\\nH   \\amp {\\sim}W, \\\\\n{\\sim}H   \\amp  W, \\\\\n{\\sim}H   \\amp {\\sim}W.\\\\\n\\end{align}\n\\] \n\n Suppose that \\(A\\) is equally likely to generate any\nworld. Using worlds, \n\n\\[\n\\begin{align}\n\\textit{EU} (A) & = P(A \\gt(H \\amp W))\\util (H \\amp W) \\\\\n&\\qquad + P(A \\gt(H \\amp{\\sim}W))\\util (H \\amp{\\sim}W) \\\\\n&\\qquad + P(A \\gt({\\sim}H \\amp W))\\util ({\\sim}H \\amp W) \\\\\n&\\qquad + P(A \\gt({\\sim}H \\amp{\\sim}W))\\util ({\\sim}H \\amp{\\sim}W) \\\\\n& = (0.25)(12) + (0.25)(4) + (0.25)(8) + (0.25)(0) \\\\\n& = 6.\\\\\n\\end{align}\n\n\\] \n\n Using basic intrinsic attitudes,\n\n\\[\n\\begin{align}\n\\textit{EU} (A) &= P(A \\gt H)\\util (H) + P(A \\gt W)\\util (W) \\\\\n& = (0.5)(4) + (0.5)(8) \\\\\n& = 6.\n\\end{align}\n\\]\n\n The two methods of computing an option\u2019s utility\nare equivalent given that, under supposition of an act\u2019s\nrealization, the probability of a basic intrinsic desire\u2019s or\naversion\u2019s realization is the sum of the probabilities of the\nworlds that realize it.\n3.4 Acts\n\nIn deliberations, a first-person action proposition represents an act.\nThe proposition has a subject-predicate structure and refers directly\nto the agent, its subject, without the intermediary of a concept of\nthe agent. A centered world represents the proposition. Such a world\nnot only specifies individuals and their properties and relations, but\nalso specifies which individual is the agent and where and when his\ndecision problem arises. Realization of the act is realization of a\nworld with, at its center, the agent at the time and place of his\ndecision problem.\n\nIsaac Levi (2000) objects to any decision theory that attaches\nprobabilities to acts. He holds that deliberation crowds out\nprediction. While deliberating, an agent does not have beliefs or\ndegrees of belief about the act that she will perform. Levi holds that\nNewcomb\u2019s problem, and evidential and causal decision theories\nthat address it, involve mistaken assignments of probabilities to an\nagent\u2019s acts. He rejects both Jeffrey\u2019s ([1965] 1983)\nevidential decision theory and Joyce\u2019s (1999) causal decision\ntheory because they allow an agent to assign probabilities to her acts\nduring deliberation.\n\nIn opposition to Levi\u2019s views, Joyce (2002) argues that (1)\ncausal decision theory need not accommodate an agent\u2019s assigning\nprobabilities to her acts, but (2) a deliberating agent may\nlegitimately assign probabilities to her acts. Evidential decision\ntheory computes an act\u2019s expected utility using the probability\nof a state given the act, \\(P(S\\mid A)\\), defined as \\(P(S \\amp A)/P(A)\\).\nThe fraction\u2019s denominator assigns a probability to an act.\nCausal decision theory replaces \\(P(S\\mid A)\\) with \\(P(A \\gt S)\\) or a\nsimilar causal probability. It need not assign a probability to an\nact.\n\nMay an agent deliberating assign probabilities to her possible acts?\nYes, a deliberator may sensibly assign probabilities to any events,\nincluding her acts. Causal decision theory may accommodate such\nprobabilities by forgoing their measurement with betting quotients.\nAccording to that method of measurement, willingness to make bets\nindicates probabilities. Suppose that a person is willing to take\neither side of a bet in which the stake for the event is \\(x\\) and the\nstake against the event is \\(y\\). Then the probability the person\nassigns to the event is the betting quotient \\(x/(x + y)\\). This\nmethod of measurement may fail when the event is an agent\u2019s own\nfuture act. A bet on an act\u2019s realization may influence the\nact\u2019s probability, as a thermometer\u2019s temperature may\ninfluence the temperature of a liquid it measures.\n\nJoyce (2007: 552\u2013561) considers whether Newcomb problems are\ngenuine decision problems despite strong correlations between states\nand acts. He concludes that, yes, despite those correlations, an agent\nmay view her decision as causing her act. An agent\u2019s decision\nsupports a belief about her act independently of prior correlations\nbetween states and her act. According to a principle of evidential\nautonomy (2007: 557), \n\n\nA deliberating agent who regards herself as free need not proportion\nher beliefs about her own acts to the antecedent evidence that she has\nfor thinking that she will perform them. \n\n\nShe should proportion her beliefs to her total evidence, including her\nself-supporting beliefs about her own acts. Those beliefs provide new\nrelevant evidence about her acts.\n\nHow should an agent deliberating about an act understand the\nbackground for her act? She should not adopt a backtracking\nsupposition of her act. Standing on the edge of a cliff, she should\nnot suppose that if she were to jump, she would have a parachute to\nbreak her fall. Also, she should not imagine gratuitous changes in her\nbasic desires. She should not imagine that if she were to choose\nchocolate instead of vanilla, despite currently preferring vanilla,\nthat she would then prefer chocolate. She should imagine that her\nbasic desires are constant as she imagines the various acts she may\nperform, and, moreover, should adopt during deliberations the pretense\nthat her will generates her act independently of her basic desires and\naversions.\n\nChristopher Hitchcock (1996) holds that an agent should pretend that\nher act is free of causal influence. Doing this makes partitions of\nstates yielding probabilities for decision agree with partitions of\nstates yielding probabilities defining causal relevance. As a result,\nprobabilities in causal decision theory may form a foundation for\nprobabilities in the probabilistic theory of causation. Causal\ndecision theory, in particular, the version using dependency\nhypotheses, grounds theories of probabilistic causation.\n3.5 Generalizing Expected Utility\n\nProblems such as Pascal\u2019s Wager and the St. Petersburg paradox\nsuggest that decision theory needs a means of handling infinite\nutilities and expected utilities. Suppose that an option\u2019s\npossible outcomes all have finite utilities. Nonetheless, if those\nutilities are infinitely many and unbounded, then the option\u2019s\nexpected utility may be infinite. Alan H\u00e1jek and Harris Nover\n(2006) also show that the option may have no expected utility. The\norder of possible outcomes, which is arbitrary, may affect convergence\nof their utilities\u2019 probability-weighted average and the value\nto which the average converges if it does converge. Causal decision\ntheory should generalize its principle of expected-utility\nmaximization to handle such cases.\n\nAlso, common principles of causal decision theory advance standards of\nrationality that are too demanding to apply to humans. They are\nstandards for ideal agents in ideal circumstances (a precise\nformulation of the idealizations may vary from theorist to theorist).\nMaking causal decision theory realistic requires relaxing\nidealizations that its principles assume. A generalization of the\nprinciple of expected-utility maximization, for example, may relax\nidealizations to accommodate limited cognitive abilities. Weirich\n(2004) and Pollock (2006) take steps in this direction. Appropriate\ngeneralizations distinguish taking maximization of expected utility as\na procedure for making a decision and taking it as a standard for\nevaluating a decision even after the decision has been made.\n3.6 Ratification\n\nGibbard and Harper (1978: Sec. 11) present a problem for causal\ndecision theory using an example drawn from literature. A man in\nDamascus knows that he has an appointment with Death at midnight. He\nwill escape Death if he manages at midnight not to be at the place of\nhis appointment. He can be in either Damascus or Aleppo at midnight.\nAs the man knows, Death is a good predictor of his whereabouts. If he\nstays in Damascus, he thereby has evidence that Death will look for\nhim in Damascus. However, if he goes to Aleppo he thereby has evidence\nthat Death will look for him in Aleppo. Wherever he decides to be at\nmidnight, he has evidence that he would be better off at the other\nplace. No decision is stable. Decision instability arises in cases in\nwhich a choice provides evidence for its outcome, and each choice\nprovides evidence that another choice would have been better. Reed\nRichter (1984, 1986) uses cases of decision instability to argue\nagainst causal decision theory. The theory needs a resolution of the\nproblem of decision instability.\n\nA common analysis of the problem classifies options as either\nself-ratifying or not self-ratifying. Jeffrey ([1965] 1983) introduced\nratification as a component of evidential decision theory. His version\nof the theory evaluates a decision according to the expected utility\nof the act it selects. The distinction between an act and a decision\nto perform the act grounds his definition of an option\u2019s\nself-ratification and his principle to make self-ratifying, or\nratifiable, decisions. According to his definition ([1965] 1983: 16),\n\n\n\nA ratifiable decision is a decision to perform an act of maximum\nestimated desirability relative to the probability matrix the agent\nthinks he would have if he finally decided to perform that act. \n\n\nEstimated desirability is expected utility. An agent\u2019s\nprobability matrix is an array of rows and columns for acts and\nstates, respectively, with each cell formed by the intersection of an\nact\u2019s row and a state\u2019s column containing the probability\nof the state given that the agent is about to perform the act. Before\nperforming an act, an agent may assess the act in light of a decision\nto perform it. Information the decision carries may affect the\nact\u2019s expected utility and its ranking with respect to other\nacts.\n\nJeffrey used ratification as a means of making evidential decision\ntheory yield the same recommendations as causal decision theory. In\nNewcomb\u2019s problem, for instance, two-boxing is the only\nself-ratifying option. However, Jeffrey (2004: 113n) concedes that\nevidential decision theory\u2019s reliance on ratification does not\nmake it agree with causal decision theory in all cases. Moreover,\nJoyce (2007) argues that the motivation for ratification appeals to\ncausal relations, so that even if it yields correct recommendations\nusing Jeffrey\u2019s formula for expected-utility, it still does not\nyield a purely evidential decision theory.\n\nCausal decision theory\u2019s account of self-ratification may put\naside Jeffrey\u2019s method of evaluating a decision by evaluating\nthe act it selects. Because the decision and the act differ, they may\nhave different consequences. For example, a decision may fail to\ngenerate the act it selects. Hence, the decision\u2019s expected\nutility may differ from the act\u2019s expected utility. Driving\nthrough a flooded section of highway may have high expected utility\nbecause it minimizes travel time to one\u2019s destination. However,\nthe decision to drive through the flooded section may have low\nexpected utility because for all one knows the water may be deep\nenough to swamp the car. Using an act\u2019s expected utility to\nassess a decision to perform the act leads to faulty evaluations of\ndecisions. It is better to evaluate a decision by comparing its\nexpected utility to the expected utilities of rival decisions. A\ndecision\u2019s expected utility depends on the probability of its\nexecution as well as the expected consequences of the act it\nselects.\n\nWeirich (1985) and Harper (1986) define ratification in terms of an\noption\u2019s expected utility given its realization rather than\ngiven a decision to realize it. An option is self-ratifying if and\nonly if it maximizes expected utility given its realization. This\naccount of ratification accommodates cases in which an option and a\ndecision to realize it have different expected utilities. Weirich and\nHarper also assume causal decision theory\u2019s formula for expected\nutility. In the case of Death in Damascus, causal decision theory\nconcludes that the threatened man lacks a self-ratifying option. A\nself-ratifying option emerges, however, if the man may flip a coin to\nmake his decision. Adopting the probability distribution for locations\nis called a mixed strategy, whereas choices of location are called\npure strategies. Assuming that Death cannot predict the coin\nflip\u2019s outcome, the mixed strategy is self-ratifying.\n\nDuring deliberations to resolve a decision problem, an agent may\nrevise the probabilities she assigns to pure strategies in light of\ncomputations of their expected utilities using earlier probability\nassignments. The process of revision may culminate in a stable\nprobability assignment that represents a mixed strategy. Skyrms (1982,\n1990) and Eells (1984b) investigate these dynamics of deliberation.\nSome open issues are whether adoption of a mixed strategy resolves a\ndecision problem and whether a pure strategy arising from a mixed\nstrategy that constitutes an equilibrium of deliberations is rational\nif the pure strategy itself is not self-ratifying.\n\nAndy Egan (2007) argues that causal decision theory yields the wrong\nrecommendation in decision problems with an option that provides\nevidence concerning its outcome. He entertains the case of an assassin\nwho deliberates about pulling the trigger, knowing that the\noption\u2019s realization provides evidence of a brain lesion that\nruins his aim. Egan maintains that causal decision theory mistakenly\nignores the evidence that the option provides. However, versions of\ncausal decision theory that incorporate ratification are innocent of\nthe charges. Ratification takes account of evidence an option provides\nconcerning its outcome.\n\nAny version of the expected utility principle, whether it uses\nconditional probabilities or probabilities of conditionals, must\nspecify the information that guides assignments of probabilities and\nutilities. Principles of nonconditional expected-utility maximization\nuse the same information for all options, and hence exclude\ninformation about an option\u2019s realization. The principle of\nratification uses for each option information that includes the\noption\u2019s realization. It is a principle of conditional\nexpected-utility maximization. Egan\u2019s cases count against\nnonconditional expected-utility maximization, and not against causal\ndecision theory. Conditional expected-utility maximization using\ncausal decision theory\u2019s formula for expected utility addresses\nthe cases he presents.\n\nEgan\u2019s examples do not refute causal decision theory but present\na challenge for it. Suppose that in a decision problem no\nself-ratifying option exists, or multiple self-ratifying options\nexist. How should a rational agent proceed, granting that a decision\nprinciple should take account of information that an option provides?\nThis is an open problem in causal decision theory (and in any decision\ntheory acknowledging that an option\u2019s realization may constitute\nevidence concerning its outcome). Ratification analyzes decision\ninstability but is not a complete response to it.\n\nIn response to Egan, Frank Arntzenius (2008) and Joyce (2012) argue\nthat in some decision problems an agent\u2019s rational deliberations\nusing freely available information do not settle on a single option\nbut instead settle on a probability distribution over options. They\nacknowledge that the agent may regret the option issuing from these\ndeliberations but differ about the regret\u2019s significance.\nArntzenius holds that the regret counts against the option\u2019s\nrationality, whereas Joyce denies this. Ahmed (2012) and Ralph\nWedgwood (2013) reject Arntzenius\u2019s and Joyce\u2019s responses\nto Egan because they hold that deliberations should settle on an\noption. Wedgwood introduces a novel decision principle to accommodate\nEgan\u2019s decision problems. Ahmed contends that Egan\u2019s\nanalysis of these decision problems has a flaw because when it is\nextended to some other decision problems, it declares every option\nirrational.\n\nAhmed (2014b) criticizes causal decision theory in cases of\ndecision instability.  Also, in such cases, Jack Spencer and Ian Wells\n(2019, preprint 2017) criticize a principle of causal dominance\nattributed to causal decision theory. This decision principle\nprohibits adopting an option if another causally dominates it. Joyce\n(2018) defends causal decision theory against Ahmed\u2019s and Spencer and\nWells\u2019s charges using points about deliberational dynamics. Armendt\n(2019) and Bales (2020) also defend causal decision theory\u2019s handling\nof decision instability.\n\nPoints about ratification in decision problems clarify points about\nequilibrium in game theory because in games of strategy a\nplayer\u2019s choice often furnishes evidence about other\nplayers\u2019 choices. Decision theory underlies game theory because\na game\u2019s solution identifies rational choices in the decision\nproblems the game creates for the players. Solutions to games\ndistinguish correlation and causation, as do decision principles.\nBecause in simultaneous-move games two agent\u2019s strategies may be\ncorrelated but not related as cause and effect, solutions to such\ngames do not have the same properties as solutions to sequential\ngames. Causal decision theory attends to distinctions on which\nsolutions to games depend. It supports game theory\u2019s account of\ninteractive decisions. Joyce and Gibbard (2016) describe the role of\nratification in game theory, and Stalnaker (2018) describes causal\ndecision theory\u2019s place in game theory.\n\nThe existence of self-ratifying mixed strategies in decision problems\nsuch as Death in Damascus suggests that ratification, as causal\ndecision theory explains it, supports participation in a Nash\nequilibrium of a game. Such an equilibrium assigns a strategy to each\nplayer so that each strategy in the assignment is a best response to\nthe others. Suppose that two people are playing Matching Pennies.\nSimultaneously, each displays a penny. One player tries to make the\nsides match, and the other player tries to prevent a match. If the\nfirst player succeeds, he gets both pennies. Otherwise, the second\nplayer gets both pennies. Suppose that each player is good at\npredicting the other player, and each player knows this. Then if the\nfirst player displays heads, he has reason to think that the second\nplayer displays tails. Also, if the first player displays tails, he\nhas reason to think that the second player displays heads. Because\nMatching Pennies is a simultaneous-move game, neither player\u2019s\nstrategy influences the other player\u2019s strategy, but each\nplayer\u2019s strategy is evidence of the other player\u2019s\nstrategy. Mixed strategies help resolve decision instability in this\ncase. If the first player flips his penny to settle the side to\ndisplay, then his mixed strategy is self-ratifying. The second\nplayer\u2019s situation is similar, and she also reaches a\nself-ratifying strategy by flipping her penny. The combination of\nself-ratifying strategies is a Nash equilibrium of the game.\n\nWeirich (2004: Chap. 9) presents a method of selecting among multiple\nself-ratifying strategies, and hence a method by which a group of\nplayers may coordinate to realize a particular Nash equilibrium when\nseveral exist. Although decision instability is an open problem,\ncausal decision theory has resources for addressing it. The\ntheory\u2019s eventual resolution of the problem will offer game\ntheory a justification for participation in a Nash equilibrium of a\ngame.\n4. Related Topics and Concluding Remarks\n\nCausal decision theory has foundations in various areas of philosophy.\nFor example, it relies on metaphysics for an account of causation. It\nalso relies on inductive logic for an account of inferences concerning\ncausation. A comprehensive causal decision theory treats not only\ncausal probabilities\u2019 generation of options\u2019 expected\nutilities, but also evidence\u2019s generation of causal\nprobabilities.\n\nResearch concerning causation contributes to the metaphysical\nfoundations of causal decision theory. Nancy Cartwright (1979), for\nexample, draws on ideas about causation to flesh out details of causal\ndecision theory. Also, some accounts of causation distinguish types of\ncauses. Both oxygen and a flame are metaphysical causes of\ntinder\u2019s combustion. However, only the flame is causally\nresponsible for, and so a normative cause of, the combustion. Causal\nresponsibility for an event accrues to just the salient metaphysical\ncauses of the event. Causal decision theory is interested not only in\nevents for which an act is causally responsible, but also in other\nevents for which an act is a metaphysical cause. Expected utilities\nthat guide decisions are comprehensive.\n\nJudea Pearl (2000) and also Peter Spirtes, Clark Glymour, and Richard\nScheines (2000) present methods of inferring causal relations from\nstatistical data. They use directed acyclic graphs and associated\nprobability distributions to construct causal models. In a decision\nproblem, a causal model yields a way of calculating an act\u2019s\neffect. A causal graph and its probability distribution express a\ndependency hypothesis and yield each act\u2019s causal influence\ngiven that hypothesis. They specify the causal probability of a state\nunder supposition of an act. An act\u2019s expected utility is a\nprobability-weighted average of its expected utility according to the\ndependency hypotheses that candidate causal models represent, as\nWeirich (2015: 225\u2013236) explains.\n\nA causal model\u2019s directed graph and probability distribution\nindicate causal relations among event types. As Pearl (2000: 30) and\nSprites et al. (2000: 11) explain, a causal model meets the causal\nMarkov condition if and only if with respect to its probability\ndistribution each event type in its directed graph is independent of\nall the event type\u2019s nondescendants, given its parents. Given a\nmodel meeting the condition, knowledge of all an event\u2019s direct\ncauses makes other information statistically irrelevant to the\nevent\u2019s occurrence, except for information about the event and\nits effects. Knowledge of an event\u2019s direct causes screens off\nevidence from indirect causes and independent effects of its causes.\nGiven a typical causal model for Newcomb\u2019s problem, knowledge of\nthe common cause of a decision and a prediction screens off the\ncorrelation between the decision and the prediction.\n\nDirected acyclic graphs present causal structure clearly, and so\nclarify in decision theory points that depend on causal structure. For\nexample, Eells (2000) observes that choice is not genuine unless a\ndecision screens off an act\u2019s correlation with states. Joyce\n(2007: 546) uses a causal graph to depict how this may happen in a\nNewcomb problem that arises in a Prisoner\u2019s Dilemma with a\npsychological twin. He shows that the Newcomb problem is a genuine\nchoice despite correlation of acts and states because a decision\nscreens off that correlation. Wolfgang Spohn (2012) constructs for\nNewcomb\u2019s problem a causal model that distinguishes a decision\nand its execution and argues that given the model causal decision\ntheory recommends one-boxing. An act in a decision problem may\nconstitute an intervention in the causal model for the decision\nproblem, as Meek and Glamour (1994) explain. Hitchcock (2016) and Joyce and Gibbard (2016)\nmaintain that treating an act as an intervention enriches causal\ndecision theory.\n\nTimothy Williamson (2007: Chap. 5) studies the epistemology of\ncounterfactual, or subjunctive, conditionals. He points out their role\nin contingency planning and decision making. According to his account,\none learns a subjunctive conditional if one robustly obtains its\nconsequent when imagining its antecedent. Experience disciplines\nimagination. The experience leading to a judgment that a subjunctive\nconditional holds may be neither strictly enabling nor strictly\nevidential so that knowledge of the conditional is neither purely\na priori nor purely a posteriori. Williamson claims\nthat knowledge of subjunctive conditionals is foundational so that\ndecision theory appropriately grounds knowledge of an act\u2019s\nchoiceworthiness in knowledge of such conditionals.\n\nMost texts on decision theory are consistent with causal decision\ntheory. Many do not treat the special cases, such as Newcomb\u2019s\nproblem, that motivate a distinction between causal and evidential\ndecision theory. For example, Leonard Savage (1954) analyzes only\ndecision problems in which options do not affect probabilities of\nstates, as his account of utility makes clear (1954: 73). Causal and\nevidential decision theories reach the same recommendations in these\nproblems. Causal decision theory is the prevailing form of decision\ntheory among those who distinguish causal and evidential decision\ntheory.\n",
    "bibliography": {
        "categories": [],
        "cat_ref_text": {
            "ref_list": [
                "Ahmed, Arif, 2012, \u201cPush the Button\u201d, <em>Philosophy\nof Science</em>, 79: 386\u2013395.",
                "\u2013\u2013\u2013, 2014a, <em>Evidence, Decision and\nCausality</em>, Cambridge: Cambridge University Press.",
                "\u2013\u2013\u2013, 2014b, \u201cDicing with\nDeath,\u201d <em>Analysis</em> 74: 587\u2013592.",
                "\u2013\u2013\u2013 (ed.), 2018, <em>Newcomb\u2019s\nProblem</em>, Cambridge: Cambridge University Press.",
                "Armendt, Brad, 1986, \u201cA Foundation for Causal Decision\nTheory\u201d, <em>Topoi</em>, 5(1): 3\u201319.\ndoi:10.1007/BF00137825",
                "\u2013\u2013\u2013, 1988a, \u201cConditional Preference and\nCausal Expected Utility\u201d, in William Harper and Brian Skyrms\n(eds.), <em>Causation in Decision, Belief Change, and Statistics</em>,\nVol. II, pp. 3\u201324, Dordrecht: Kluwer.",
                "\u2013\u2013\u2013, 1988b, \u201cImpartiality and Causal\nDecision Theory\u201d, in Arthur Fine and Jarrett Leplin (eds.),\n<em>PSA: Proceedings of Biennial Meeting of the Philosophy of Science\nAssociation 1988</em> (Volume I), pp. 326\u2013336, East Lansing, MI:\nPhilosophy of Science Association.",
                "\u2013\u2013\u2013, 2019, \u201cCausal Decision Theory and\nDecision Instability,\u201d <em>Journal of Philosophy</em>, 116:\n263\u2013277.",
                "Arntzenius, Frank, 2008, \u201cNo Regrets, or: Edith Piaf Revamps\nDecision Theory\u201d, <em>Erkenntnis</em>, 68(2): 277\u2013297.\ndoi:10.1007/s10670-007-9084-8",
                "Bales, Adam, 2016, \u201cThe Pauper\u2019s Problem: Chance,\nForeknowledge and Causal Decision Theory\u201d, <em>Philosophical\nStudies</em>, 173(6): 1497\u20131516.\ndoi:10.1007/s11098-015-0560-8",
                "\u2013\u2013\u2013, 2018, \u201cRichness and Rationality:\nCausal Decision Theory and the WAR Argument,\u201d <em>Synthese</em>,\n195: 259\u2013267.",
                "\u2013\u2013\u2013, 2020, \u201cIntentions and Instability: A\nDefense of Causal Decision Theory,\u201d <em>Philosophical\nStudies</em>, 177: 793\u2013804.",
                "Cartwright, Nancy, 1979, \u201cCausal Laws and Effective\nStrategies\u201d, <em>No\u00fbs</em>, 13(4): 419\u2013437.\ndoi:10.2307/2215337",
                "Easwaran, Kenny, forthcoming, \u201cA Classification of Newcomb\nProblems and Decision Theories,\u201d <em>Synthese</em>, first online\n30 May 2019.  doi:10.1007/s11229-019-02272-z",
                "Eells, Ellery, 1981, \u201cCausality, Utility, and\nDecision\u201d, <em>Synthese</em>, 48(2): 295\u2013329.\ndoi:10.1007/BF01063891",
                "\u2013\u2013\u2013, 1982, <em>Rational Decision and\nCausality</em>, Cambridge: Cambridge University Press.",
                "\u2013\u2013\u2013, 1984a, \u201cNewcomb\u2019s Many\nSolutions\u201d, <em>Theory and Decision</em>, 16(1): 59\u2013105.\ndoi:10.1007/BF00141675",
                "\u2013\u2013\u2013, 1984b, \u201cMetatickles and the Dynamics\nof Deliberation\u201d, <em>Theory and Decision</em>, 17(1):\n71\u201395. doi:10.1007/BF00140057",
                "\u2013\u2013\u2013, 2000, \u201cReview: <em>The Foundations of\nCausal Decision Theory</em>, by James Joyce\u201d, <em>British\nJournal for the Philosophy of Science</em>, 51(4): 893\u2013900.\ndoi:10.1093/bjps/51.4.893 ",
                "Egan, Andy, 2007, \u201cSome Counterexamples to Causal Decision\nTheory\u201d, <em>Philosophical Review</em>, 116(1): 93\u2013114.\n10.1215/00318108-2006-023",
                "Gibbard, Allan and William Harper, 1978 [1981],\n\u201cCounterfactuals and Two Kinds of Expected Utility\u201d, in\nClifford Alan Hooker, James L. Leach, and Edward Francis McClennan\n(eds.), <em>Foundations and Applications of Decision Theory</em>\n(University of Western Ontario Series in Philosophy of Science, 13a),\nDordrecht: D. Reidel, pp. 125\u2013162.\ndoi:10.1007/978-94-009-9789-9_5 Reprinted in Harper, Stalnaker, and\nPearce 1981: 153\u2013190. doi:10.1007/978-94-009-9117-0_8",
                "H\u00e1jek, Alan and Harris Nover, 2006, \u201cPerplexing\nExpectations\u201d, <em>Mind</em>, 115(459): 703\u2013720.\n10.1093/mind/fzl703",
                "Hare, Caspar and Brian Hedden, 2016, \u201cSelf-Reinforcing and\nSelf-Frustrating Decisions,\u201d <em>No\u00fbs</em>, 50:\n604\u2013628.",
                "Harper, William, 1986, \u201cMixed Strategies and Ratifiability\nin Causal Decision Theory\u201d, <em>Erkenntnis</em>, 24(1):\n25\u201336. doi:10.1007/BF00183199",
                "Harper, William, Robert Stalnaker, and Glenn Pearce (eds.),\n1981,<em>Ifs: Conditionals, Belief, Decision, Chance, and Time</em>\n(University of Western Ontario Series in Philosophy of Science, 15),\nDordrecht: Reidel.",
                "Hitchcock, Christopher Read, 1996, \u201cCausal Decision Theory\nand Decision-Theoretic Causation\u201d, <em>No\u00fbs</em>, 30(4):\n508\u2013526. doi:10.2307/2216116 ",
                "\u2013\u2013\u2013, 2016, \u201cConditioning, Intervening, and\nDecision\u201d, <em>Synthese</em>, 193(4): 1157\u20131176.\ndoi:10.1007/s11229-015-0710-8",
                "Horgan, Terry, 1981 [1985], \u201cCounterfactuals and\nNewcomb\u2019s Problem\u201d, <em>The Journal of Philosophy</em>,\n78(6): 331\u2013356. doi:10.2307/2026128 Reprinted in Richmond\nCampbell and Lanning Sowden (eds.), 1985, <em>Paradoxes of Rationality\nand Cooperation: Prisoner\u2019s Dilemma and Newcomb\u2019s\nProblem</em>, Vancouver: University of British Columbia Press, pp.\n159\u2013182.",
                "Horwich, Paul, 1987, <em>Asymmetries in Time</em>, Cambridge, MA:\nMIT Press.",
                "Jeffrey, Richard C., [1965] 1983, <em>The Logic of Decision</em>,\nsecond edition, Chicago: University of Chicago Press. [The 1990\npaperback edition includes some revisions.]",
                "\u2013\u2013\u2013, 2004, <em>Subjective Probability: The Real\nThing</em>, Cambridge: Cambridge University Press.",
                "Joyce, James M., 1999, <em>The Foundations of Causal Decision\nTheory</em>, Cambridge: Cambridge University Press.",
                "\u2013\u2013\u2013, 2000, \u201cWhy We Still Need the Logic of\nDecision\u201d, <em>Philosophy of Science</em>, 67: S1\u2013S13.\ndoi:10.1086/392804",
                "\u2013\u2013\u2013, 2002, \u201cLevi on Causal Decision Theory\nand the Possibility of Predicting One\u2019s Own Actions\u201d,\n<em>Philosophical Studies</em>, 110(1): 69\u2013102.\ndoi:10.1023/A:1019839429878",
                "\u2013\u2013\u2013, 2007, \u201cAre Newcomb Problems Really\nDecisions?\u201d <em>Synthese</em>, 156(3): 537\u2013562.\ndoi:10.1007/s11229-006-9137-6",
                "\u2013\u2013\u2013, 2012, \u201cRegret and Instability in\nCausal Decision Theory\u201d, <em>Synthese</em>, 187(1):\n123\u2013145. doi:10.1007/s11229-011-0022-6",
                "\u2013\u2013\u2013, 2018, \u201cDeliberation and Stability in\nNewcomb Problems and Pseudo-Newcomb Problems,\u201d in Arif Ahmed\n(ed.), <em>Newcomb\u2019s Problem</em>, Cambridge:\nCambridge University Press, pp. 138\u2013159.",
                "Joyce, James and Allan Gibbard, 2016, \u201cCausal Decision\nTheory\u201d, in Horacio Arl\u00f8-Costa, Vincent F. Hendricks, and\nJohan van Benthem (eds.), <em>Readings in Formal Epistemology</em>,\nBerlin: Springer, pp. 457\u2013491.",
                "Krantz, David, R., Duncan Luce, Patrick Suppes, and Amos Tversky,\n1971, <em>The Foundations of Measurement</em> (Volume 1: <em>Additive\nand Polynomial Representations</em>), New York: Academic Press.",
                "Levi, Isaac, 2000, \u201cReview Essay on <em>The Foundations of\nCausal Decision Theory</em>, by James Joyce\u201d, <em>Journal of\nPhilosophy</em>, 97(7): 387\u2013402. doi:10.2307/2678411 ",
                "Lewis, David, 1973, <em>Counterfactuals</em>, Cambridge, MA:\nHarvard University Press.",
                "\u2013\u2013\u2013, 1976, \u201cProbabilities of Conditionals\nand Conditional Probabilities\u201d, <em>Philosophical Review</em>,\n85(3): 297\u2013315. doi:10.2307/2184045 ",
                "\u2013\u2013\u2013, 1979, \u201cPrisoner\u2019s Dilemma is a\nNewcomb Problem\u201d, <em>Philosophy and Public Affairs</em>, 8(3):\n235\u2013240.",
                "\u2013\u2013\u2013, 1981, \u201cCausal Decision Theory\u201d,\n<em>Australasian Journal of Philosophy</em>, 59(1): 5\u201330.\ndoi:10.1080/00048408112340011",
                "Meek, Christopher and Clark Glymour, 1994, \u201cConditioning and\nIntervening\u201d, <em>British Journal for the Philosophy of\nScience</em>, 45(4): 1001\u20131021. doi:10.1093/bjps/45.4.1001 ",
                "Nozick, Robert, 1969, \u201cNewcomb\u2019s Problem and Two\nPrinciples of Choice\u201d, in Nicholas Rescher (ed.), <em>Essays in\nHonor of Carl G. Hempel</em>, Dordrecht: Reidel,\npp. 114\u2013146.",
                "Papineau, David, 2001, \u201cEvidentialism Reconsidered\u201d,\n<em>No\u00fbs</em>, 35(2): 239\u2013259.",
                "Pearl, Judea, 2000, <em>Causality: Models, Reasoning, and\nInference</em>, Cambridge: Cambridge University Press; second edition,\n2009.",
                "Pollock, John, 2006, <em>Thinking about Acting: Logical\nFoundations for Rational Decision Making</em>, New York: Oxford\nUniversity Press.",
                "\u2013\u2013\u2013, 2010, \u201cA Resource-Bounded Agent\nAddresses the Newcomb Problem\u201d, <em>Synthese</em>, 176(1):\n57\u201382. doi:10.1007/s11229-009-9484-1",
                "Price, Huw, 1986, \u201cAgainst Causal Decision Theory\u201d,\n<em>Synthese</em>, 67(2): 195\u2013212. doi:10.1007/BF00540068",
                "\u2013\u2013\u2013, 2012, \u201cCausation, Chance, and the\nRational Significance of Supernatural Evidence\u201d,\n<em>Philosophical Review</em>, 121(4): 483\u2013538.\ndoi:10.1215/00318108-1630912 ",
                "Richter, Reed, 1984, \u201cRationality Revisited\u201d,\n<em>Australasian Journal of Philosophy</em>, 62(4): 392\u2013403.\ndoi:10.1080/00048408412341601",
                "\u2013\u2013\u2013, 1986, \u201cFurther Comments on Decision\nInstability\u201d, <em>Australasian Journal of Philosophy</em>,\n64(3): 345\u2013349. doi:10.1080/00048408612342571",
                "Savage, Leonard, 1954, <em>The Foundations of Statistics</em>, New\nYork: Wiley.",
                "Skyrms, Brian, 1980, <em>Causal Necessity: A Pragmatic\nInvestigation of the Necessity of Laws</em>, New Haven, CT: Yale\nUniversity Press.",
                "\u2013\u2013\u2013, 1982, \u201cCausal Decision Theory\u201d,\n<em>Journal of Philosophy</em>, 79(11): 695\u2013711.\ndoi:10.2307/2026547",
                "\u2013\u2013\u2013, 1990, <em>The Dynamics of Rational\nDeliberation</em>, Cambridge, MA: Harvard University Press.",
                "Sobel, Jordan Howard, 1994, <em>Taking Chances: Essays on Rational\nChoice</em>, Cambridge: Cambridge University Press.",
                "Spencer, Jack and Ian Wells, 2019, \u201cWhy Take Both\nBoxes?\u201d <em>Philosophy and Phenomenological Research</em>, 99:\n27\u201348.",
                "Spirtes, Peter, Clark Glymour, and Richard Scheines, 2000,\n<em>Causation, Prediction, and Search</em>, second edition, Cambridge,\nMA: MIT Press.",
                "Spohn, Wolfgang, 2012, \u201cReversing 30 Years of Discussion:\nWhy Causal Decision Theorists Should One-Box\u201d,\n<em>Synthese</em>, 187(1): 95\u2013122.\ndoi:10.1007/s11229-011-0023-5",
                "Stalnaker, Robert C., 1968, \u201cA Theory of\nConditionals\u201d, in <em>Studies in Logical Theory</em> (American\nPhilosphical Quarterly Monograph series, 2 ), Oxford: Blackwell,\n98\u2013112. Reprinted in in Harper, Stalnaker, and Pearce 1981:\n41\u201356. doi:10.1007/978-94-009-9117-0_2",
                "\u2013\u2013\u2013, 1972 [1981], \u201cLetter to David\nLewis\u201d, May 21; printed in Harper, Stalnaker, and Pearce 1981:\n151\u2013152. doi:10.1007/978-94-009-9117-0_7",
                "\u2013\u2013\u2013, 2018, \u201cGame Theory and Decision\nTheory (Causal and Evidential),\u201d in Arif Ahmed\n(ed.), <em>Newcomb\u2019s Problem</em>, Cambridge: Cambridge\nUniversity Press, pp. 180\u2013200.",
                "Wedgwood, Ralph, 2013, \u201cGandalf\u2019s Solution to the\nNewcomb Problem\u201d, <em>Synthese</em>, 190(14): 2643\u20132675.\ndoi:10.1007/s11229-011-9900-1",
                "Weirich, Paul, 1980, \u201cConditional Utility and Its Place in\nDecision Theory\u201d, <em>Journal of Philosophy</em>, 77(11):\n702\u2013715.",
                "\u2013\u2013\u2013, 1985, \u201cDecision Instability\u201d,\n<em>Australasian Journal of Philosophy</em>, 63(4): 465\u2013472.\ndoi:10.1080/00048408512342061",
                "\u2013\u2013\u2013, 2001, <em>Decision Space: Multidimensional\nUtility Analysis</em>, Cambridge: Cambridge University Press.",
                "\u2013\u2013\u2013, 2004, <em>Realistic Decision Theory: Rules\nfor Nonideal Agents in Nonideal Circumstances</em>, New York: Oxford\nUniversity Press.",
                "\u2013\u2013\u2013, 2015, <em>Models of Decision-Making:\nSimplifying Choices</em>, Cambridge: Cambridge University Press.",
                "Williamson, Timothy, 2007, <em>The Philosophy of Philosophy</em>,\nMalden, MA: Blackwell."
            ]
        },
        "raw_text": "<div id=\"bibliography\">\n<h2 id=\"Bib\">Bibliography</h2>\n<ul class=\"hanging\">\n<li>Ahmed, Arif, 2012, \u201cPush the Button\u201d, <em>Philosophy\nof Science</em>, 79: 386\u2013395.</li>\n<li>\u2013\u2013\u2013, 2014a, <em>Evidence, Decision and\nCausality</em>, Cambridge: Cambridge University Press.</li>\n<li>\u2013\u2013\u2013, 2014b, \u201cDicing with\nDeath,\u201d <em>Analysis</em> 74: 587\u2013592.</li>\n<li>\u2013\u2013\u2013 (ed.), 2018, <em>Newcomb\u2019s\nProblem</em>, Cambridge: Cambridge University Press.</li>\n<li>Armendt, Brad, 1986, \u201cA Foundation for Causal Decision\nTheory\u201d, <em>Topoi</em>, 5(1): 3\u201319.\ndoi:10.1007/BF00137825</li>\n<li>\u2013\u2013\u2013, 1988a, \u201cConditional Preference and\nCausal Expected Utility\u201d, in William Harper and Brian Skyrms\n(eds.), <em>Causation in Decision, Belief Change, and Statistics</em>,\nVol. II, pp. 3\u201324, Dordrecht: Kluwer.</li>\n<li>\u2013\u2013\u2013, 1988b, \u201cImpartiality and Causal\nDecision Theory\u201d, in Arthur Fine and Jarrett Leplin (eds.),\n<em>PSA: Proceedings of Biennial Meeting of the Philosophy of Science\nAssociation 1988</em> (Volume I), pp. 326\u2013336, East Lansing, MI:\nPhilosophy of Science Association.</li>\n<li>\u2013\u2013\u2013, 2019, \u201cCausal Decision Theory and\nDecision Instability,\u201d <em>Journal of Philosophy</em>, 116:\n263\u2013277.</li>\n<li>Arntzenius, Frank, 2008, \u201cNo Regrets, or: Edith Piaf Revamps\nDecision Theory\u201d, <em>Erkenntnis</em>, 68(2): 277\u2013297.\ndoi:10.1007/s10670-007-9084-8</li>\n<li>Bales, Adam, 2016, \u201cThe Pauper\u2019s Problem: Chance,\nForeknowledge and Causal Decision Theory\u201d, <em>Philosophical\nStudies</em>, 173(6): 1497\u20131516.\ndoi:10.1007/s11098-015-0560-8</li>\n<li>\u2013\u2013\u2013, 2018, \u201cRichness and Rationality:\nCausal Decision Theory and the WAR Argument,\u201d <em>Synthese</em>,\n195: 259\u2013267.</li>\n<li>\u2013\u2013\u2013, 2020, \u201cIntentions and Instability: A\nDefense of Causal Decision Theory,\u201d <em>Philosophical\nStudies</em>, 177: 793\u2013804.</li>\n<li>Cartwright, Nancy, 1979, \u201cCausal Laws and Effective\nStrategies\u201d, <em>No\u00fbs</em>, 13(4): 419\u2013437.\ndoi:10.2307/2215337</li>\n<li>Easwaran, Kenny, forthcoming, \u201cA Classification of Newcomb\nProblems and Decision Theories,\u201d <em>Synthese</em>, first online\n30 May 2019.  doi:10.1007/s11229-019-02272-z</li>\n<li>Eells, Ellery, 1981, \u201cCausality, Utility, and\nDecision\u201d, <em>Synthese</em>, 48(2): 295\u2013329.\ndoi:10.1007/BF01063891</li>\n<li>\u2013\u2013\u2013, 1982, <em>Rational Decision and\nCausality</em>, Cambridge: Cambridge University Press.</li>\n<li>\u2013\u2013\u2013, 1984a, \u201cNewcomb\u2019s Many\nSolutions\u201d, <em>Theory and Decision</em>, 16(1): 59\u2013105.\ndoi:10.1007/BF00141675</li>\n<li>\u2013\u2013\u2013, 1984b, \u201cMetatickles and the Dynamics\nof Deliberation\u201d, <em>Theory and Decision</em>, 17(1):\n71\u201395. doi:10.1007/BF00140057</li>\n<li>\u2013\u2013\u2013, 2000, \u201cReview: <em>The Foundations of\nCausal Decision Theory</em>, by James Joyce\u201d, <em>British\nJournal for the Philosophy of Science</em>, 51(4): 893\u2013900.\ndoi:10.1093/bjps/51.4.893 </li>\n<li>Egan, Andy, 2007, \u201cSome Counterexamples to Causal Decision\nTheory\u201d, <em>Philosophical Review</em>, 116(1): 93\u2013114.\n10.1215/00318108-2006-023</li>\n<li>Gibbard, Allan and William Harper, 1978 [1981],\n\u201cCounterfactuals and Two Kinds of Expected Utility\u201d, in\nClifford Alan Hooker, James L. Leach, and Edward Francis McClennan\n(eds.), <em>Foundations and Applications of Decision Theory</em>\n(University of Western Ontario Series in Philosophy of Science, 13a),\nDordrecht: D. Reidel, pp. 125\u2013162.\ndoi:10.1007/978-94-009-9789-9_5 Reprinted in Harper, Stalnaker, and\nPearce 1981: 153\u2013190. doi:10.1007/978-94-009-9117-0_8</li>\n<li>H\u00e1jek, Alan and Harris Nover, 2006, \u201cPerplexing\nExpectations\u201d, <em>Mind</em>, 115(459): 703\u2013720.\n10.1093/mind/fzl703</li>\n<li>Hare, Caspar and Brian Hedden, 2016, \u201cSelf-Reinforcing and\nSelf-Frustrating Decisions,\u201d <em>No\u00fbs</em>, 50:\n604\u2013628.</li>\n<li>Harper, William, 1986, \u201cMixed Strategies and Ratifiability\nin Causal Decision Theory\u201d, <em>Erkenntnis</em>, 24(1):\n25\u201336. doi:10.1007/BF00183199</li>\n<li>Harper, William, Robert Stalnaker, and Glenn Pearce (eds.),\n1981,<em>Ifs: Conditionals, Belief, Decision, Chance, and Time</em>\n(University of Western Ontario Series in Philosophy of Science, 15),\nDordrecht: Reidel.</li>\n<li>Hitchcock, Christopher Read, 1996, \u201cCausal Decision Theory\nand Decision-Theoretic Causation\u201d, <em>No\u00fbs</em>, 30(4):\n508\u2013526. doi:10.2307/2216116 </li>\n<li>\u2013\u2013\u2013, 2016, \u201cConditioning, Intervening, and\nDecision\u201d, <em>Synthese</em>, 193(4): 1157\u20131176.\ndoi:10.1007/s11229-015-0710-8</li>\n<li>Horgan, Terry, 1981 [1985], \u201cCounterfactuals and\nNewcomb\u2019s Problem\u201d, <em>The Journal of Philosophy</em>,\n78(6): 331\u2013356. doi:10.2307/2026128 Reprinted in Richmond\nCampbell and Lanning Sowden (eds.), 1985, <em>Paradoxes of Rationality\nand Cooperation: Prisoner\u2019s Dilemma and Newcomb\u2019s\nProblem</em>, Vancouver: University of British Columbia Press, pp.\n159\u2013182.</li>\n<li>Horwich, Paul, 1987, <em>Asymmetries in Time</em>, Cambridge, MA:\nMIT Press.</li>\n<li>Jeffrey, Richard C., [1965] 1983, <em>The Logic of Decision</em>,\nsecond edition, Chicago: University of Chicago Press. [The 1990\npaperback edition includes some revisions.]</li>\n<li>\u2013\u2013\u2013, 2004, <em>Subjective Probability: The Real\nThing</em>, Cambridge: Cambridge University Press.</li>\n<li>Joyce, James M., 1999, <em>The Foundations of Causal Decision\nTheory</em>, Cambridge: Cambridge University Press.</li>\n<li>\u2013\u2013\u2013, 2000, \u201cWhy We Still Need the Logic of\nDecision\u201d, <em>Philosophy of Science</em>, 67: S1\u2013S13.\ndoi:10.1086/392804</li>\n<li>\u2013\u2013\u2013, 2002, \u201cLevi on Causal Decision Theory\nand the Possibility of Predicting One\u2019s Own Actions\u201d,\n<em>Philosophical Studies</em>, 110(1): 69\u2013102.\ndoi:10.1023/A:1019839429878</li>\n<li>\u2013\u2013\u2013, 2007, \u201cAre Newcomb Problems Really\nDecisions?\u201d <em>Synthese</em>, 156(3): 537\u2013562.\ndoi:10.1007/s11229-006-9137-6</li>\n<li>\u2013\u2013\u2013, 2012, \u201cRegret and Instability in\nCausal Decision Theory\u201d, <em>Synthese</em>, 187(1):\n123\u2013145. doi:10.1007/s11229-011-0022-6</li>\n<li>\u2013\u2013\u2013, 2018, \u201cDeliberation and Stability in\nNewcomb Problems and Pseudo-Newcomb Problems,\u201d in Arif Ahmed\n(ed.), <em>Newcomb\u2019s Problem</em>, Cambridge:\nCambridge University Press, pp. 138\u2013159.</li>\n<li>Joyce, James and Allan Gibbard, 2016, \u201cCausal Decision\nTheory\u201d, in Horacio Arl\u00f8-Costa, Vincent F. Hendricks, and\nJohan van Benthem (eds.), <em>Readings in Formal Epistemology</em>,\nBerlin: Springer, pp. 457\u2013491.</li>\n<li>Krantz, David, R., Duncan Luce, Patrick Suppes, and Amos Tversky,\n1971, <em>The Foundations of Measurement</em> (Volume 1: <em>Additive\nand Polynomial Representations</em>), New York: Academic Press.</li>\n<li>Levi, Isaac, 2000, \u201cReview Essay on <em>The Foundations of\nCausal Decision Theory</em>, by James Joyce\u201d, <em>Journal of\nPhilosophy</em>, 97(7): 387\u2013402. doi:10.2307/2678411 </li>\n<li>Lewis, David, 1973, <em>Counterfactuals</em>, Cambridge, MA:\nHarvard University Press.</li>\n<li>\u2013\u2013\u2013, 1976, \u201cProbabilities of Conditionals\nand Conditional Probabilities\u201d, <em>Philosophical Review</em>,\n85(3): 297\u2013315. doi:10.2307/2184045 </li>\n<li>\u2013\u2013\u2013, 1979, \u201cPrisoner\u2019s Dilemma is a\nNewcomb Problem\u201d, <em>Philosophy and Public Affairs</em>, 8(3):\n235\u2013240.</li>\n<li>\u2013\u2013\u2013, 1981, \u201cCausal Decision Theory\u201d,\n<em>Australasian Journal of Philosophy</em>, 59(1): 5\u201330.\ndoi:10.1080/00048408112340011</li>\n<li>Meek, Christopher and Clark Glymour, 1994, \u201cConditioning and\nIntervening\u201d, <em>British Journal for the Philosophy of\nScience</em>, 45(4): 1001\u20131021. doi:10.1093/bjps/45.4.1001 </li>\n<li>Nozick, Robert, 1969, \u201cNewcomb\u2019s Problem and Two\nPrinciples of Choice\u201d, in Nicholas Rescher (ed.), <em>Essays in\nHonor of Carl G. Hempel</em>, Dordrecht: Reidel,\npp. 114\u2013146.</li>\n<li>Papineau, David, 2001, \u201cEvidentialism Reconsidered\u201d,\n<em>No\u00fbs</em>, 35(2): 239\u2013259.</li>\n<li>Pearl, Judea, 2000, <em>Causality: Models, Reasoning, and\nInference</em>, Cambridge: Cambridge University Press; second edition,\n2009.</li>\n<li>Pollock, John, 2006, <em>Thinking about Acting: Logical\nFoundations for Rational Decision Making</em>, New York: Oxford\nUniversity Press.</li>\n<li>\u2013\u2013\u2013, 2010, \u201cA Resource-Bounded Agent\nAddresses the Newcomb Problem\u201d, <em>Synthese</em>, 176(1):\n57\u201382. doi:10.1007/s11229-009-9484-1</li>\n<li>Price, Huw, 1986, \u201cAgainst Causal Decision Theory\u201d,\n<em>Synthese</em>, 67(2): 195\u2013212. doi:10.1007/BF00540068</li>\n<li>\u2013\u2013\u2013, 2012, \u201cCausation, Chance, and the\nRational Significance of Supernatural Evidence\u201d,\n<em>Philosophical Review</em>, 121(4): 483\u2013538.\ndoi:10.1215/00318108-1630912 </li>\n<li>Richter, Reed, 1984, \u201cRationality Revisited\u201d,\n<em>Australasian Journal of Philosophy</em>, 62(4): 392\u2013403.\ndoi:10.1080/00048408412341601</li>\n<li>\u2013\u2013\u2013, 1986, \u201cFurther Comments on Decision\nInstability\u201d, <em>Australasian Journal of Philosophy</em>,\n64(3): 345\u2013349. doi:10.1080/00048408612342571</li>\n<li>Savage, Leonard, 1954, <em>The Foundations of Statistics</em>, New\nYork: Wiley.</li>\n<li>Skyrms, Brian, 1980, <em>Causal Necessity: A Pragmatic\nInvestigation of the Necessity of Laws</em>, New Haven, CT: Yale\nUniversity Press.</li>\n<li>\u2013\u2013\u2013, 1982, \u201cCausal Decision Theory\u201d,\n<em>Journal of Philosophy</em>, 79(11): 695\u2013711.\ndoi:10.2307/2026547</li>\n<li>\u2013\u2013\u2013, 1990, <em>The Dynamics of Rational\nDeliberation</em>, Cambridge, MA: Harvard University Press.</li>\n<li>Sobel, Jordan Howard, 1994, <em>Taking Chances: Essays on Rational\nChoice</em>, Cambridge: Cambridge University Press.</li>\n<li>Spencer, Jack and Ian Wells, 2019, \u201cWhy Take Both\nBoxes?\u201d <em>Philosophy and Phenomenological Research</em>, 99:\n27\u201348.</li>\n<li>Spirtes, Peter, Clark Glymour, and Richard Scheines, 2000,\n<em>Causation, Prediction, and Search</em>, second edition, Cambridge,\nMA: MIT Press.</li>\n<li>Spohn, Wolfgang, 2012, \u201cReversing 30 Years of Discussion:\nWhy Causal Decision Theorists Should One-Box\u201d,\n<em>Synthese</em>, 187(1): 95\u2013122.\ndoi:10.1007/s11229-011-0023-5</li>\n<li>Stalnaker, Robert C., 1968, \u201cA Theory of\nConditionals\u201d, in <em>Studies in Logical Theory</em> (American\nPhilosphical Quarterly Monograph series, 2 ), Oxford: Blackwell,\n98\u2013112. Reprinted in in Harper, Stalnaker, and Pearce 1981:\n41\u201356. doi:10.1007/978-94-009-9117-0_2</li>\n<li>\u2013\u2013\u2013, 1972 [1981], \u201cLetter to David\nLewis\u201d, May 21; printed in Harper, Stalnaker, and Pearce 1981:\n151\u2013152. doi:10.1007/978-94-009-9117-0_7</li>\n<li>\u2013\u2013\u2013, 2018, \u201cGame Theory and Decision\nTheory (Causal and Evidential),\u201d in Arif Ahmed\n(ed.), <em>Newcomb\u2019s Problem</em>, Cambridge: Cambridge\nUniversity Press, pp. 180\u2013200.</li>\n<li>Wedgwood, Ralph, 2013, \u201cGandalf\u2019s Solution to the\nNewcomb Problem\u201d, <em>Synthese</em>, 190(14): 2643\u20132675.\ndoi:10.1007/s11229-011-9900-1</li>\n<li>Weirich, Paul, 1980, \u201cConditional Utility and Its Place in\nDecision Theory\u201d, <em>Journal of Philosophy</em>, 77(11):\n702\u2013715.</li>\n<li>\u2013\u2013\u2013, 1985, \u201cDecision Instability\u201d,\n<em>Australasian Journal of Philosophy</em>, 63(4): 465\u2013472.\ndoi:10.1080/00048408512342061</li>\n<li>\u2013\u2013\u2013, 2001, <em>Decision Space: Multidimensional\nUtility Analysis</em>, Cambridge: Cambridge University Press.</li>\n<li>\u2013\u2013\u2013, 2004, <em>Realistic Decision Theory: Rules\nfor Nonideal Agents in Nonideal Circumstances</em>, New York: Oxford\nUniversity Press.</li>\n<li>\u2013\u2013\u2013, 2015, <em>Models of Decision-Making:\nSimplifying Choices</em>, Cambridge: Cambridge University Press.</li>\n<li>Williamson, Timothy, 2007, <em>The Philosophy of Philosophy</em>,\nMalden, MA: Blackwell.</li>\n</ul>\n</div>"
    },
    "related_entries": {
        "entry_list": [
            "causation: counterfactual theories of",
            "causation: the metaphysics of",
            "conditionals",
            "conditionals: counterfactual",
            "decision theory",
            "game theory",
            "normative-theories-of-rational-choice:-expected-utility",
            "prisoner\u2019s dilemma",
            "probability, interpretations of"
        ],
        "entry_link": [
            {
                "../causation-counterfactual/": "causation: counterfactual theories of"
            },
            {
                "../causation-metaphysics/": "causation: the metaphysics of"
            },
            {
                "../conditionals/": "conditionals"
            },
            {
                "../counterfactuals/": "conditionals: counterfactual"
            },
            {
                "../decision-theory/": "decision theory"
            },
            {
                "../game-theory/": "game theory"
            },
            {
                "../prisoner-dilemma/": "prisoner\u2019s dilemma"
            },
            {
                "../probability-interpret/": "probability, interpretations of"
            }
        ]
    },
    "academic_tools": {
        "listed_text": [
            "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=decision-causal\" target=\"other\">How to cite this entry</a>.",
            "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://leibniz.stanford.edu/friends/preview/decision-causal/\" target=\"other\">Preview the PDF version of this entry</a> at the\n <a href=\"https://leibniz.stanford.edu/friends/\" target=\"other\">Friends of the SEP Society</a>.",
            "<img alt=\"inpho icon\" src=\"../../symbols/inpho.png\"/>",
            "<a href=\"https://www.inphoproject.org/entity?sep=decision-causal&amp;redirect=True\" target=\"other\">Look up topics and thinkers related to this entry</a>\n at the Internet Philosophy Ontology Project (InPhO).",
            "<img alt=\"phil papers icon\" src=\"../../symbols/pp.gif\"/>",
            "<a href=\"https://philpapers.org/sep/decision-causal/\" target=\"other\">Enhanced bibliography for this entry</a>\nat <a href=\"https://philpapers.org/\" target=\"other\">PhilPapers</a>, with links to its database."
        ],
        "listed_links": [
            {
                "https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=decision-causal": "How to cite this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/preview/decision-causal/": "Preview the PDF version of this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/": "Friends of the SEP Society"
            },
            {
                "https://www.inphoproject.org/entity?sep=decision-causal&redirect=True": "Look up topics and thinkers related to this entry"
            },
            {
                "https://philpapers.org/sep/decision-causal/": "Enhanced bibliography for this entry"
            },
            {
                "https://philpapers.org/": "PhilPapers"
            }
        ]
    },
    "other_internet_resources": {
        "listed_text": [
            "<a href=\"http://ocw.mit.edu/courses/linguistics-and-philosophy/24\u2013222-decisions-games-and-rational-choice-spring-2008/\" target=\"other\">MIT Course on Decision Theory</a>,\n offered by Robert Stalnaker.",
            "<a href=\"http://en.wikipedia.org/wiki/Decision_theory\" target=\"other\">Decision Theory</a>,\n as of this writing (November 4, 2016), the Wikipedia site has a good\noverall introduction to decision theory and a list of references."
        ],
        "listed_links": [
            {
                "http://ocw.mit.edu/courses/linguistics-and-philosophy/24\u2013222-decisions-games-and-rational-choice-spring-2008/": "MIT Course on Decision Theory"
            },
            {
                "http://en.wikipedia.org/wiki/Decision_theory": "Decision Theory"
            }
        ]
    }
}