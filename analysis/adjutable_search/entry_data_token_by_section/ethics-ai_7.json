{
    "main_text": "Main Debates || Bias in Decision Systems\n2.4 Bias in Decision Systems\n\nAutomated AI decision support systems and \u201cpredictive\nanalytics\u201d operate on data and produce a decision as\n\u201coutput\u201d. This output may range from the relatively\ntrivial to the highly significant: \u201cthis restaurant matches your\npreferences\u201d, \u201cthe patient in this X-ray has completed\nbone growth\u201d, \u201capplication to credit card declined\u201d,\n\u201cdonor organ will be given to another patient\u201d,\n\u201cbail is denied\u201d, or \u201ctarget identified and\nengaged\u201d. Data analysis is often used in \u201cpredictive\nanalytics\u201d in business, healthcare, and other fields, to foresee\nfuture developments\u2014since prediction is easier, it will also\nbecome a cheaper commodity. One use of prediction is in\n\u201cpredictive policing\u201d (NIJ 2014 [OIR]), which many fear\nmight lead to an erosion of public liberties (Ferguson 2017) because\nit can take away power from the people whose behaviour is predicted.\nIt appears, however, that many of the worries about policing depend on\nfuturistic scenarios where law enforcement foresees and punishes\nplanned actions, rather than waiting until a crime has been committed\n(like in the 2002 film \u201cMinority Report\u201d). One concern is\nthat these systems might perpetuate bias that was already in the data\nused to set up the system, e.g., by increasing police patrols in an\narea and discovering more crime in that area. Actual \u201cpredictive\npolicing\u201d or \u201cintelligence led policing\u201d techniques\nmainly concern the question of where and when police forces will be\nneeded most. Also, police officers can be provided with more data,\noffering them more control and facilitating better decisions, in\nworkflow support software (e.g., \u201cArcGIS\u201d). Whether this\nis problematic depends on the appropriate level of trust in the\ntechnical quality of these systems, and on the evaluation of aims of\nthe police work itself. Perhaps a recent paper title points in the\nright direction here: \u201cAI ethics in predictive policing: From\nmodels of threat to an ethics of care\u201d (Asaro 2019).\n\nBias typically surfaces when unfair judgments are made because the\nindividual making the judgment is influenced by a characteristic that\nis actually irrelevant to the matter at hand, typically a\ndiscriminatory preconception about members of a group. So, one form of\nbias is a learned cognitive feature of a person, often not made\nexplicit. The person concerned may not be aware of having that\nbias\u2014they may even be honestly and explicitly opposed to a bias\nthey are found to have (e.g., through priming, cf. Graham and Lowery\n2004). On fairness vs. bias in machine learning, see Binns (2018).\n\nApart from the social phenomenon of learned bias, the human cognitive\nsystem is generally prone to have various kinds of \u201ccognitive\nbiases\u201d, e.g., the \u201cconfirmation bias\u201d: humans tend\nto interpret information as confirming what they already believe. This\nsecond form of bias is often said to impede performance in rational\njudgment (Kahnemann 2011)\u2014though at least some cognitive biases\ngenerate an evolutionary advantage, e.g., economical use of resources\nfor intuitive judgment. There is a question whether AI systems could\nor should have such cognitive bias.\n\nA third form of bias is present in data when it exhibits systematic\nerror, e.g., \u201cstatistical bias\u201d. Strictly, any given\ndataset will only be unbiased for a single kind of issue, so the mere\ncreation of a dataset involves the danger that it may be used for a\ndifferent kind of issue, and then turn out to be biased for that kind.\nMachine learning on the basis of such data would then not only fail to\nrecognise the bias, but codify and automate the \u201chistorical\nbias\u201d. Such historical bias was discovered in an automated\nrecruitment screening system at Amazon (discontinued early 2017) that\ndiscriminated against women\u2014presumably because the company had a\nhistory of discriminating against women in the hiring process. The\n\u201cCorrectional Offender Management Profiling for Alternative\nSanctions\u201d (COMPAS), a system to predict whether a defendant\nwould re-offend, was found to be as successful (65.2% accuracy) as a\ngroup of random humans (Dressel and Farid 2018) and to produce more\nfalse positives and less false negatives for black defendants. The\nproblem with such systems is thus bias plus humans placing excessive\ntrust in the systems. The political dimensions of such automated\nsystems in the USA are investigated in Eubanks (2018).\n\nThere are significant technical efforts to detect and remove bias from\nAI systems, but it is fair to say that these are in early stages: see\nUK Institute for Ethical AI & Machine Learning (Brownsword,\nScotford, and Yeung 2017; Yeung and Lodge 2019). It appears that\ntechnological fixes have their limits in that they need a mathematical\nnotion of fairness, which is hard to come by (Whittaker et al. 2018:\n24ff; Selbst et al. 2019), as is a formal notion of \u201crace\u201d\n(see Benthall and Haynes 2019). An institutional proposal is in (Veale\nand Binns 2017).\n",
    "section_title": "2.4 Bias in Decision Systems",
    "entry_title": "Ethics of Artificial Intelligence and Robotics",
    "hierarchy_title": "Ethics of Artificial Intelligence and Robotics || Main Debates || Bias in Decision Systems",
    "tokenized_text": [
        "main",
        "debate",
        "bias",
        "decision",
        "system",
        "bias",
        "decision",
        "system",
        "automated",
        "ai",
        "decision",
        "support",
        "system",
        "predictive",
        "analytics",
        "operate",
        "data",
        "produce",
        "decision",
        "output",
        "output",
        "may",
        "range",
        "relatively",
        "trivial",
        "highly",
        "significant",
        "restaurant",
        "match",
        "preference",
        "patient",
        "xray",
        "completed",
        "bone",
        "growth",
        "application",
        "credit",
        "card",
        "declined",
        "donor",
        "organ",
        "given",
        "another",
        "patient",
        "bail",
        "denied",
        "target",
        "identified",
        "engaged",
        "data",
        "analysis",
        "often",
        "used",
        "predictive",
        "analytics",
        "business",
        "healthcare",
        "field",
        "foresee",
        "future",
        "developmentssince",
        "prediction",
        "easier",
        "also",
        "become",
        "cheaper",
        "commodity",
        "one",
        "use",
        "prediction",
        "predictive",
        "policing",
        "nij",
        "oir",
        "many",
        "fear",
        "might",
        "lead",
        "erosion",
        "public",
        "liberty",
        "ferguson",
        "take",
        "away",
        "power",
        "people",
        "whose",
        "behaviour",
        "predicted",
        "appears",
        "however",
        "many",
        "worry",
        "policing",
        "depend",
        "futuristic",
        "scenario",
        "law",
        "enforcement",
        "foresees",
        "punishes",
        "planned",
        "action",
        "rather",
        "waiting",
        "crime",
        "committed",
        "like",
        "film",
        "minority",
        "report",
        "one",
        "concern",
        "system",
        "might",
        "perpetuate",
        "bias",
        "already",
        "data",
        "used",
        "set",
        "system",
        "eg",
        "increasing",
        "police",
        "patrol",
        "area",
        "discovering",
        "crime",
        "area",
        "actual",
        "predictive",
        "policing",
        "intelligence",
        "led",
        "policing",
        "technique",
        "mainly",
        "concern",
        "question",
        "police",
        "force",
        "needed",
        "also",
        "police",
        "officer",
        "provided",
        "data",
        "offering",
        "control",
        "facilitating",
        "better",
        "decision",
        "workflow",
        "support",
        "software",
        "eg",
        "arcgis",
        "whether",
        "problematic",
        "depends",
        "appropriate",
        "level",
        "trust",
        "technical",
        "quality",
        "system",
        "evaluation",
        "aim",
        "police",
        "work",
        "perhaps",
        "recent",
        "paper",
        "title",
        "point",
        "right",
        "direction",
        "ai",
        "ethic",
        "predictive",
        "policing",
        "model",
        "threat",
        "ethic",
        "care",
        "asaro",
        "bias",
        "typically",
        "surface",
        "unfair",
        "judgment",
        "made",
        "individual",
        "making",
        "judgment",
        "influenced",
        "characteristic",
        "actually",
        "irrelevant",
        "matter",
        "hand",
        "typically",
        "discriminatory",
        "preconception",
        "member",
        "group",
        "one",
        "form",
        "bias",
        "learned",
        "cognitive",
        "feature",
        "person",
        "often",
        "made",
        "explicit",
        "person",
        "concerned",
        "may",
        "aware",
        "biasthey",
        "may",
        "even",
        "honestly",
        "explicitly",
        "opposed",
        "bias",
        "found",
        "eg",
        "priming",
        "cf",
        "graham",
        "lowery",
        "fairness",
        "vs",
        "bias",
        "machine",
        "learning",
        "see",
        "binns",
        "apart",
        "social",
        "phenomenon",
        "learned",
        "bias",
        "human",
        "cognitive",
        "system",
        "generally",
        "prone",
        "various",
        "kind",
        "cognitive",
        "bias",
        "eg",
        "confirmation",
        "bias",
        "human",
        "tend",
        "interpret",
        "information",
        "confirming",
        "already",
        "believe",
        "second",
        "form",
        "bias",
        "often",
        "said",
        "impede",
        "performance",
        "rational",
        "judgment",
        "kahnemann",
        "though",
        "least",
        "cognitive",
        "bias",
        "generate",
        "evolutionary",
        "advantage",
        "eg",
        "economical",
        "use",
        "resource",
        "intuitive",
        "judgment",
        "question",
        "whether",
        "ai",
        "system",
        "could",
        "cognitive",
        "bias",
        "third",
        "form",
        "bias",
        "present",
        "data",
        "exhibit",
        "systematic",
        "error",
        "eg",
        "statistical",
        "bias",
        "strictly",
        "given",
        "dataset",
        "unbiased",
        "single",
        "kind",
        "issue",
        "mere",
        "creation",
        "dataset",
        "involves",
        "danger",
        "may",
        "used",
        "different",
        "kind",
        "issue",
        "turn",
        "biased",
        "kind",
        "machine",
        "learning",
        "basis",
        "data",
        "would",
        "fail",
        "recognise",
        "bias",
        "codify",
        "automate",
        "historical",
        "bias",
        "historical",
        "bias",
        "discovered",
        "automated",
        "recruitment",
        "screening",
        "system",
        "amazon",
        "discontinued",
        "early",
        "discriminated",
        "womenpresumably",
        "company",
        "history",
        "discriminating",
        "woman",
        "hiring",
        "process",
        "correctional",
        "offender",
        "management",
        "profiling",
        "alternative",
        "sanction",
        "compas",
        "system",
        "predict",
        "whether",
        "defendant",
        "would",
        "reoffend",
        "found",
        "successful",
        "accuracy",
        "group",
        "random",
        "human",
        "dressel",
        "farid",
        "produce",
        "false",
        "positive",
        "le",
        "false",
        "negative",
        "black",
        "defendant",
        "problem",
        "system",
        "thus",
        "bias",
        "plus",
        "human",
        "placing",
        "excessive",
        "trust",
        "system",
        "political",
        "dimension",
        "automated",
        "system",
        "usa",
        "investigated",
        "eubanks",
        "significant",
        "technical",
        "effort",
        "detect",
        "remove",
        "bias",
        "ai",
        "system",
        "fair",
        "say",
        "early",
        "stage",
        "see",
        "uk",
        "institute",
        "ethical",
        "ai",
        "machine",
        "learning",
        "brownsword",
        "scotford",
        "yeung",
        "yeung",
        "lodge",
        "appears",
        "technological",
        "fix",
        "limit",
        "need",
        "mathematical",
        "notion",
        "fairness",
        "hard",
        "come",
        "whittaker",
        "et",
        "al",
        "ff",
        "selbst",
        "et",
        "al",
        "formal",
        "notion",
        "race",
        "see",
        "benthall",
        "haynes",
        "institutional",
        "proposal",
        "veale",
        "binns"
    ]
}