{
    "url": "content-causal",
    "title": "Causal Theories of Mental Content",
    "authorship": {
        "year": "Copyright \u00a9 2021",
        "author_text": "Fred Adams\n\nKen Aizawa",
        "author_links": [
            {
                "https://www.lingcogsci.udel.edu/people/faculty/Frederick%20Adams": "Fred Adams"
            },
            {
                "https://ncas.rutgers.edu/about-us/faculty-staff/kenneth-aizawa": "Ken Aizawa"
            }
        ],
        "raw_html": "<div id=\"article-copyright\">\n<p>\n<a href=\"../../info.html#c\">Copyright \u00a9 2021</a> by\n\n<br/>\n<a href=\"https://www.lingcogsci.udel.edu/people/faculty/Frederick%20Adams\" target=\"other\">Fred Adams</a>\n<br/>\n<a href=\"https://ncas.rutgers.edu/about-us/faculty-staff/kenneth-aizawa\" target=\"other\">Ken Aizawa</a>\n</p>\n</div>"
    },
    "pubinfo": [
        "First published Thu Feb 4, 2010",
        "substantive revision Thu Aug 12, 2021"
    ],
    "preamble": "\n\nCausal theories of mental content attempt to explain how thoughts can\nbe about things. They attempt to explain how one can think about, for\nexample, dogs. These theories begin with the idea that there are\nmental representations and that thoughts are meaningful in virtue of a\ncausal connection between a mental representation and some part of the\nworld that is represented. In other words, the point of departure for\nthese theories is that thoughts of dogs are about dogs because dogs\ncause the mental representations of dogs.\n",
    "toc": [
        {
            "#Int": "1. Introduction"
        },
        {
            "#SomHisTheCon": "2. Some Historical and Theoretical Context"
        },
        {
            "#SpeCauTheMenCon": "3. Specific Causal Theories of Mental Content"
        },
        {
            "#NorCon": "3.1 Normal Conditions"
        },
        {
            "#EvoFun": "3.2 Evolutionary Functions"
        },
        {
            "#DevFun": "3.3 Developmental Functions"
        },
        {
            "#AsyDepThe": "3.4 Asymmetric Dependency Theory"
        },
        {
            "#BesTesThe": "3.5 Best Test Theory"
        },
        {
            "#GenObjCauTheMenCon": "4. General Objections to Causal Theories of Mental Content"
        },
        {
            "#CauTheDoNotWorForLogMatRel": "4.1 Causal Theories do not Work for Logical and Mathematical Relations"
        },
        {
            "#CauTheDoNotWorForVacTer": "4.2 Causal Theories do not Work for Vacuous Terms"
        },
        {
            "#CauTheDoNotWorForPheInt": "4.3 Causal Theories do not Work for Phenomenal Intentionality"
        },
        {
            "#CauTheDoNotWorForCerRefTho": "4.4 Causal Theories do not Work for Certain Reflexive Thoughts"
        },
        {
            "#CauTheConTheMedPer": "4.5 Causal Theories do not Work for Reliable Misrepresentations"
        },
        {
            "#CauTheConImpPsyLaw": "4.6 Causal Theories Conflict with the Theory Mediation of Perception"
        },
        {
            "#CauTheConImpPsyLaw": "4.7 Causal Theories Conflict with the Implementation of Psychological Laws"
        },
        {
            "#CauTheDoNotProMetThe": "4.8 Causal Theories do not Provide a Metasemantic Theory"
        },
        {
            "#ConRem": "5. Concluding Remarks"
        },
        {
            "#Bib": "Bibliography"
        },
        {
            "#Aca": "Academic Tools"
        },
        {
            "#Oth": "Other Internet Resources"
        },
        {
            "#Rel": "Related Entries"
        }
    ],
    "main_text": "\n1. Introduction\n\nContent is what is said, asserted, thought, believed, desired, hoped\nfor, etc. Mental content is the content had by mental states and\nprocesses. Causal theories of mental content attempt to explain what\ngives thoughts, beliefs, desires, and so forth their contents. They\nattempt to explain how thoughts can be about\n things.[1]\n2. Some Historical and Theoretical Context\n\nAlthough one might find precursors to causal theories of mental\ncontent scattered throughout the history of philosophy, the current\ninterest in the topic was spurred, in part, by perceived inadequacies\nin \u201csimilarity\u201d or \u201cpicture\u201d theories of\nmental representation. Where meaning and representation are asymmetric\nrelations\u2014that is, a syntactic item \u201cX\u201d might mean\nor represent X, but X does not (typically) mean or represent\n\u201cX\u201d\u2014similarity and resemblance are symmetric\nrelations. Dennis Stampe (1977), who played an important role in\ninitiating contemporary interest in causal theories, drew attention to\nrelated problems. Consider a photograph of one of two identical twins.\nWhat makes it a photo of Judy, rather than her identical twin Trudy?\nBy assumption, it cannot be the similarity of the photo to one twin\nrather than the other, since the twins are identical. Moreover, one\ncan have a photo of Judy even though the photo happens not to look\nvery much like her at all. What apparently makes a photo of Judy a\nphoto of Judy is that she was causally implicated, in the right way,\nin the production of the photo. Reinforcing the hunch that causation\ncould be relevant to meaning and representation is the observation\nthat there is a sense in which the number of rings in a tree stump\nrepresents the age of the tree when it died and that the presence of\nsmoke means fire. The history of contemporary developments of causal\ntheories of mental content consists largely of specifying what it is\nfor something to be causally implicated in the right way in the\nproduction of meaning and refining the sense in which smoke represents\nfire to the sense in which a person\u2019s thoughts, sometimes at\nleast, represent the world.\n\nIf one wanted to trace a simple historical arc for recent causal\ntheories, one would have to begin with the seminal 1977 paper by\nDennis Stampe, \u201cToward a Causal Theory of Linguistic\nRepresentation.\u201d Among the many important features of this paper\nis its having set much of the conceptual and theoretical stage to be\ndescribed in greater detail below. It drew a contrast between causal\ntheories and \u201cpicture theories\u201d that try to explain\nrepresentational content by appeal to some form of similarity between\na representation and the thing represented. It also drew attention to\nthe problem of distinguishing the content determining causes of a\nrepresentation from adventitious non-content determining causes. So,\nfor example, one will want \u201cX\u201d to mean dogs because dogs\ncauses dogs, but one does not want \u201cX\u201d to mean\nblow-to-the-head, even though blows to the head might cause the\noccurrence of an \u201cX\u201d. (Much more of this will be described\nbelow.) Finally, it also provided some attempts to address this\nproblem, such as an appeal to the function a thing might have.\n\nFred Dretske\u2019s 1981 Knowledge and the Flow of\nInformation offered a much expanded treatment of a type of causal\ntheory. Rather than basing semantic content on a causal connection\nper se, Dretske began with a type of informational connection\nderived from the mathematical theory of information. This has led some\nto refer to Dretske\u2019s theory as \u201cinformation\nsemantics\u201d. Dretske also appealed to the notion of function in\nan attempt to distinguish content determining causes from adventitious\nnon-content determining causes. This has led some to refer to\nDretske\u2019s theory as a \u201cteleoinformational\u201d theory or\na \u201cteleosemantic\u201d theory. Dretske\u2019s 1988 book,\nExplaining Behavior, further refined his earlier\ntreatment.\n\nJerry Fodor\u2019s 1984 \u201cSemantics, Wisconsin Style\u201d gave\nthe problem of distinguishing content-determining causes from\nnon-content determining causes its best-known guise as \u201cthe\ndisjunction problem\u201d. How can a causal theory of content say\nthat \u201cX\u201d has the non-disjunctive content dog, rather than\nthe disjunctive content dog-or-blow-to-the-head, when both dogs and\nblows to the head cause instances of \u201cX\u201d? By 1987, in\nPsychosemantics, Fodor published his first attempt at an\nalternative method of solving the disjunction problem, the Asymmetric\n(Causal) Dependency Theory. This theory was further refined for the\ntitle essay in Fodor\u2019s 1990 book A Theory of Content and\nOther Essays.\n\nAlthough these causal theories have subsequently spawned a significant\ncritical literature, other related causal theories have also been\nadvanced. Two of these are teleosemantic theories that are sometimes\ncontrasted with causal theories. (Cf., e.g., Papineau (1984), Millikan\n(1989), and the entry on \n  teleological theories of mental content.) \nOther more purely causal theories are Dan Lloyd\u2019s (1987, 1989)\nDialectical Theory of Representation, Robert Rupert\u2019s (1999)\nBest Test Theory (see section 3.5 below), Marius Usher\u2019s (2001)\nStatistical Referential Theory, and Dan Ryder\u2019s (2004) SINBAD\nneurosemantics.\n\nCausal theories of mental content are typically developed in the\ncontext of four principal assumptions. First, they typically\npresuppose that there is a difference between derived and underived\n meaning.[2]\n Normal humans can use one thing, such as \u201c%\u201d, to mean\npercent. They can use certain large red octagons to mean that one is\nto stop at an intersection. In such cases, there are collective\narrangements that confer relatively specific meanings on relatively\nspecific objects. In the case of human minds, however, it is proposed\nthat thoughts can have the meanings or contents they do without\nrecourse to collective arrangements. It is possible to think about\npercentage or ways of negotiating intersections prior to collective\nsocial arrangements. It, therefore, appears that the contents of our\nthoughts do not acquire the content they do in the way that\n\u201c%\u201d and certain large red octagons do. Causal theories of\nmental content presuppose that mental contents are underived, hence\nattempt to explain how underived meaning arises.\n\nSecond, causal theories of mental content distinguish what has come to\nbe known as natural meaning and non-natural\n meaning.[3]\n Cases where an object or event X has natural meaning are those in\nwhich, given certain background conditions, the existence or\noccurrence of X \u201centails\u201d the existence or occurrence of\nsome state of affairs. If smoke in the unspoiled forest naturally\nmeans fire then, given the presence of smoke, there was fire. Under\nthe relevant background conditions, the effect indicates or naturally\nmeans the cause. An important feature of natural meaning is that it\ndoes not generate falsity. If smoke naturally means fire, then there\nmust really be a fire. By contrast, many non-naturally meaningful\nthings can be false. Sentences, for example, can be meaningful and\nfalse. The utterance \u201cColleen currently has measles\u201d means\nthat Colleen currently has measles but does not entail that Colleen\ncurrently has measles in the way that Colleen\u2019s spots do entail\nthat she has measles. Like sentences, thoughts are also meaningful,\nbut often false. Thus, it is generally supposed that mental content\nmust be a form of non-natural underived\n meaning.[4]\n\nThird, these theories assume that it is possible to explain the origin\nof underived content without appeal to other semantic or contentful\nnotions. So, it is assumed that there is more to the project than\nsimply saying that one\u2019s thoughts mean that Colleen currently\nhas the measles because one\u2019s thoughts are about Colleen\ncurrently having the measles. Explicating meaning in terms of\naboutness, or aboutness in terms of meaning, or either in terms of\nsome still further semantic notion, does not go as far as is commonly\ndesired by those who develop causal theories of mental content. To\nnote some additional terminology, it is often said that causal\ntheories of mental content attempt to naturalize non-natural,\nunderived meaning. To put the matter less technically, one might say\nthat causal theories of mental content presuppose that it is possible\nfor a purely physical system to bear underived content. Thus, they\npresuppose that if one were to build a genuinely thinking robot or\ncomputer, one would have to design it in such a way that some of its\ninternal components would bear non-natural, underived content in\nvirtue of purely physical conditions. To get a feel for the difference\nbetween a naturalized theory and an unnaturalized theory of content,\none might note the theory developed by Grice (1948). Grice developed\nan unnaturalized theory. Speaking of linguistic items, Grice held that\n\u2018Speaker S non-naturally means something by\n\u201cX\u201d\u2019 is roughly equivalent to \u2018S intended the\nutterance of \u201cX\u201d to produce some effect in an audience by\nmeans of the recognition of this intention.\u2019 Grice did not\nexplicate the origin of mental content of speaker\u2019s intentions\nor audience recognition, hence he did not attempt to naturalize the\nmeaning of linguistic items.\n\nFourth, it is commonly presupposed that naturalistic analyses of\nnon-natural, underived meanings will apply, in the first instance,\nto the contents of thought. The physical items \u201cX\u201d that\nare supposed to be bearers of causally determined content will,\ntherefore, be something like the firings of a particular neuron or set\nof neurons. These contents of thoughts are said to be captured in what\nis sometimes called a \u201clanguage of thought\u201d or\n\u201cmentalese.\u201d The contents of items in natural languages,\nsuch as English, Japanese, and French, will then be given a separate\nanalysis, presumably in terms of a naturalistic account of non-natural\nderived meanings. It is, of course, possible to suppose that it is\nnatural language, or some other system of communication, that first\ndevelops content, which can then serve as a basis upon which to\nprovide an account of mental content. Among the reasons that threaten\nthis order of dependency is the fact that cognitive agents appear to\nhave evolved before systems of communication. Another reason is that\nhuman infants at least appear to have some sophisticated cognitive\ncapacities involving mental representation, before they speak or\nunderstand natural languages. Yet another reason is that, although\nsome social animals may have systems of communication complex enough\nto support the genesis of mental content, other non-social cognizing\nanimals may not.\n\nIt is worth noting that, in recent years, this last presupposition has\nsometimes been abandoned by philosophers attempting to understand\nanimal signaling or animal communication, as when toads emit mating\ncalls or vervet monkeys cry out when seeing a cheetah, eagle, or\nsnake. See, for example, Stegmann, 2005, 2009, Skyrms, 2008, 2010a, b,\n2012, and Birch, 2014. In other words, there have been efforts to use\nthe sorts of apparatus originally developed for theories of mental\ncontent, plus or minus a bit, as apparatus for handling animal\nsignaling. These approaches seem to allow that there are mental\nrepresentations in the brains of the signaling/communicating animals,\nbut do not reply on the content of the mental representations to\nprovide the representational contents of the signals. In this way, the\ncontents of the signals are not derived from the contents of the\nmental representations.\n3. Specific Causal Theories of Mental Content\n\nThe unifying inspiration for causal theories of mental content is that\nsome syntactic item \u201cX\u201d means X because \u201cX\u201ds\nare caused by\n Xs.[5]\n Matters cannot be this simple, however, since in general one expects\nthat some causes of \u201cX\u201d are not among the\ncontent-specifying causes of \u201cX\u201ds. There are numerous\nexamples illustrating this point, each illustrating a kind of cause\nthat must not typically be among the content-determining causes of\n\u201cX\u201d:\n\nSuppose there is some syntactic item \u201cX\u201d that is a\nputative mental representation of a dog. Dogs will presumably cause\ntokens of \u201cX\u201d, but so might foxes at odd angles, with some\nobstructions, at a distance, or under poor lighting conditions. The\ncausal theorist will need some principle that allows her to say that\nthe causal links between dogs and \u201cX\u201ds will be\ncontent-determining, where the causal links between, say, foxes and\n\u201cX\u201ds will not. Mice and shrews, mules and donkeys, German\nShepherds and wolves, dogs and paper m\u00e2ch\u00e9 dogs, dogs and\nstuffed dogs, and any number of confusable groups would do to make\nthis point.\nA syntactic item \u201cX\u201d with the putative content of dog\nmight also be caused by a dose of LSD, a set of strategically placed\nand activated microelectrodes, a brain tumor, or quantum mechanical\nfluctuations. Who knows what mental representations might be triggered\nby these things? LSD, microelectrodes, etc., should (typically) not be\namong the content-determining causes of most mental\nrepresentations.\nUpon hearing the question \u201cWhat kind of animal is named\n\u2018Fido\u2019?\u201d a person might token the syntactic item\n\u201cX\u201d. One will want at least some cases in which this\n\u201cX\u201d means dog, but to get this result the causal theorist\nwill not want the question to be among the content-determining causes\nof \u201cX\u201d.\nIn seeing a dog, there is a causal pathway between the dog through\nthe visual system (and perhaps beyond) to a token of \u201cX\u201d.\nWhat in this causal pathway from the dog to \u201cX\u201d\nconstitutes the content-determining element? In virtue of what is it\nthe case that \u201cX\u201d means dog, rather than retinal\nprojection of a dog, or any number of other possible points along the\npathway? Clearly there is a similar problem for other sense\nmodalities. In hearing a dog, there is a causal pathway between the\ndog through the auditory system (and perhaps beyond) to a token of\n\u201cX\u201d. What makes \u201cX\u201d mean dog, rather than\nsound of a dog (barking?) or eardrum vibration or motion in the stapes\nbone of the inner ear? One might press essentially the same point by\nasking what makes \u201cX\u201d mean dog, rather than some complex\nfunction of all the diverse causal intermediaries between dogs and\n\u201cX\u201d.\n\n\nThe foregoing problem cases are generally developed under the rubric\nof \u201cfalse beliefs\u201d or \u201cthe disjunction\nproblem\u201d in the following way and can be traced to Fodor (1984).\nNo one is perfect, so a theory of content should be able to explicate\nwhat is going on when a person makes a mistake, such as mistaking a\nfox for a dog. The first thought is that this happens when a fox (at a\ndistance or in poor lighting conditions) causes the occurrence of a\ntoken of \u201cX\u201d and, since \u201cX\u201d means dog, one has\nmistaken a fox for a dog. The problem with this first thought arises\nwith the invocation of the idea that \u201cX\u201d means dog. Why\nsay that \u201cX\u201d means dog, rather than dog or fox? On a\ncausal account, we need some principled reason to say that the content\nof \u201cX\u201d is dog, hence that the token of \u201cX\u201d is\nfalsely tokened by the fox, rather than the content of \u201cX\u201d\nis dog or fox, hence that the token of \u201cX\u201d is truly\ntokened by the fox. What basis is there for saying that\n\u201cX\u201d means dog, rather than dog or fox? Because there\nappears always to be this option of making the content of a term some\ndisjunction of items, the problem has been called \u201cthe\ndisjunction\n problem\u201d.[6]\n\nAs was noted above, what unifies causal theories of mental content is\nsome version of the idea that \u201cX\u201ds being causally\nconnected to Xs makes \u201cX\u201ds mean Xs. What divides causal\ntheories of mental content, most notably, is the different approaches\nthey take to separating the content-determining causes from the\nnon-content-determining causes. Some of these different theories\nappeal to normal conditions, others to functions generated by natural\nselection, others to functions acquired ontogenetically, and still\nothers to dependencies among laws. At present there is no approach\nthat is commonly agreed to correctly separate the content-determining\ncauses from the non-content determining causes while at the same time\nrespecting the need not to invoke existing semantic concepts. Although\neach attempt may have technical problems of its own, the recurring\nproblem is that the attempts to separate content-determining from\nnon-content-determining causes threaten to smuggle in semantic\nelements.\n\nIn this section, we will review the internal problematic of causal\ntheories by examining how each theory fares on our battery of test\ncases (I)\u2013(IV), along with other objections from time to time.\nThis provides a simple, readily understood organization of the project\nof developing a causal theory of mental content, but it does this at a\nprice. The primary literature is not arranged exactly in this way. The\npositive theories found in the primary literature are typically more\nnuanced than what we present here. Moreover, the criticisms are not\narranged into the kind of test battery we have with cases\n(I)\u2013(IV). One paper might bring forward cases (I) and (III)\nagainst theory A, where another paper might bring forward cases (I)\nand (II) against theory B. Nor are the examples in our test battery\nexactly the ones developed in the primary literature. In other words,\nthe price one pays for this simplicity of organization is that we have\nsomething less like a literature review and more like a theoretical\nand conceptual toolbox for understanding causal theories.\n3.1 Normal Conditions\n\nTrees usually grow a certain way. Each year, there is the passage of\nthe four seasons with a tree growing more quickly at some times and\nmore slowly at others. As a result, each year a tree adds a\n\u201cring\u201d to its girth in such a way that one might say that\neach ring means a year of growth. If we find a tree stump that has\ntwelve rings, then that means that the tree was twelve years old when\nit died. But, it is not an entirely inviolable law that a tree grows a\nring each year. Such a law, if it is one, is at most a ceteris\nparibus law. It holds only given certain background conditions,\nsuch as that weather conditions are normal. If the weather conditions\nare especially bad one season, then perhaps the tree will not grow\nenough to produce a new ring. One might, therefore, propose that if\nconditions are normal, then n rings means that the tree was n years\nold when it died. This idea makes its first appearance when Stampe\n(1977) invokes it as part of his theory of \u201cfidelity\nconditions.\u201d\n\nAn appeal to normal conditions would seem to be an obvious way in\nwhich to bracket at least some non-content-determining causes of a\nwould-be mental representation \u201cX\u201d. It is only the causes\nthat operate under normal conditions that are content-determining. So,\nwhen it comes to human brains, under normal conditions one is not\nunder the influence of hallucinogens nor is one\u2019s head being\ninvaded by an elaborate configuration of microelectrodes. So, even\nthough LSD and microelectrodes would, counterfactually speaking, cause\na token neural event \u201cX\u201d, these causes would not be among\nthe content-determining causes of \u201cX\u201d. Moreover, one can\ntake normal conditions of viewing to include good lighting, a\nparticular perspective, a particular viewing distance, a lack of\n(seriously) occluding objects, and so forth, so that foxes in dim\nlight, viewed from the bottom up, at a remove of a mile, or through a\ndense fog, would not be among the content-determining causes of\n\u201cX\u201d. Under normal viewing conditions, one does not confuse\na fox with a dog, so foxes are not to be counted as part of the\ncontent of \u201cX\u201d. Moreover, if one does confuse a fox with a\ndog under normal viewing conditions, then perhaps one does not really\nhave a mental representation of a dog, but maybe only a mental\nrepresentation of a member of the taxonomic family canidae.\n\nAlthough an appeal to normal conditions initially appears promising,\nit does not seem to be sufficient to rule out the causal\nintermediaries between objects in the environment and \u201cX\u201d.\nEven under normal conditions of viewing that include good lighting, a\nparticular perspective, a particular viewing distance, a lack of\n(seriously) occluding objects, and so forth, it is still the case that\nboth dogs and, say, retinal projections of dogs, lead to tokens of\n\u201cX\u201d. Why does the content of \u201cX\u201d not include\nretinal projections of dogs or any of the other causal intermediaries?\nNor do normal conditions suffice to keep questions from getting in\namong the content-determining causes. What abnormal conditions are\nthere when the question, \u201cWhat kind of animal is named\n\u2018Fido\u2019?,\u201d leads to a tokening of an \u201cX\u201d\nwith the putative meaning of dog? Suppose there are instances of\nquantum mechanical fluctuations in the nervous system, wherein\nspontaneous changes in neurons lead to tokens of \u201cX\u201d. Do\nnormal conditions block these out? So, there are problem cases in\nwhich appeals to normal conditions do not seem to work. Fodor (1990b)\ndiscusses this problem with proximal stimulations in connection with\nhis asymmetric dependency theory, but it is one that clearly\nchallenges the causal theory plus normal conditions approach.\n\nNext, suppose that we tightly construe normal conditions to eliminate\nthe kinds of problem cases described above. So, when completely\nfleshed out, under normal conditions only dogs cause \u201cX\u201ds.\nWhat one intuitively wants is to be able to say that, under normal\nconditions of good lighting, proper viewing distance, etc.\n\u201cX\u201d means dog. But, another possibility is that in such a\nsituation \u201cX\u201d does not mean dog, but\ndog-under-normal-conditions-of-good-lighting, proper-viewing-distance,\netc. Why take one interpretation over another? One needs a principled\nbasis for distinguishing the cause of \u201cX\u201d from\nthe many causally contributing factors. In other words, we still have\nthe problem of bracketing non-content-determining causes, only in a\nslightly reformulated manner. This sort of objection may be found in\nFodor (1984).\n\nNow set the preceding problem aside. There is still another developed\nin Fodor (1984). Suppose that \u201cX\u201d does mean dog under\nconditions of good lighting, lack of serious occlusions, etc. Do not\nmerely suppose that \u201cX\u201d is caused by dogs under conditions\nof good light, lack of serious occlusions, etc.; grant that\n\u201cX\u201d really does mean dog under these conditions. Even\nthen, why does \u201cX\u201d, the firing of the neuronal circuit,\nstill mean dog, when those conditions do not hold? Why does\n\u201cX\u201d still mean dog under, say, degraded lighting\nconditions? After all, we could abide by another apparently true\nconditional regarding these other conditions, namely, if the lighting\nconditions were not so good, there were no serious occlusions, etc.,\nthen the neuronal circuit\u2019s firing would mean dog or fox. Even\nif \u201cX\u201d means X under one set of conditions C1,\nwhy doesn\u2019t \u201cX\u201d mean Y under a different set of\nconditions C2? It looks as though one could say that\nC1 provides normal conditions under which \u201cX\u201d\nmeans X and C2 provides normal conditions under which\n\u201cX\u201d means Y. We need some non-semantic notions to enable\nus to fix on one interpretation, rather than the other. At this point,\none might look to a notion of functions to solve these\n problems.[7]\n3.2 Evolutionary Functions\n\nMany physical objects have functions. (Stampe (1977) was the first to\nnote this as a fact that might help causal theories of content.) A\nfamiliar mercury thermometer has the function of indicating\ntemperature. But, such a thermometer works against a set of background\nconditions which include the atmospheric pressure. The atmospheric\npressure influences the volume of the vacuum that forms above the\ncolumn of mercury in the glass tube. So, the height of the column of\nmercury is the product of two causally relevant features, the ambient\natmospheric temperature and the ambient atmospheric pressure. This\nsuggests that one and the same physical device with the same causal\ndependencies can be used in different ways. A column of mercury in a\nglass tube can be used to measure temperature, but it is possible to\nput it to use as a pressure gauge. Which thing a column of mercury\nmeasures is determined by its function.\n\nThis observation suggests a way to specify which causes of\n\u201cX\u201d determine its content. The content of \u201cX\u201d,\nsay, the firing of some neurons, is determined by dogs, and not foxes,\nbecause it is the function of those neurons to register the presence\nof dogs, but not foxes. Further, the content of \u201cX\u201d does\nnot include LSD, microelectrodes, or quantum mechanical fluctuations,\nbecause it is not the function of \u201cX\u201d to fire in response\nto LSD, microelectrodes, or quantum mechanical fluctuations in the\nbrain. Similarly, the content of \u201cX\u201d does not include\nproximal sensory projections of dogs, because the function of the\nneurons is to register the presence of the dogs, not the sensory\nstimulations. It is the objective features of the world that matter to\nan organism, not its sensory states. Finally, it is the function of\n\u201cX\u201d to register the presence of dogs, but not the presence\nof questions, such as \u2018What kind of animal is named \n\u201cFido\u201d?\u2019,\nthat leads to \u201cX\u201d meaning dogs. Functions, thus, provide\na prima facie attractive means of properly winnowing down the\ncauses of \u201cX\u201d to those that are genuinely content\ndetermining.\n\nIn addition, the theory of evolution by natural selection apparently\nprovides a non-semantic, non-intentional basis upon which to explicate\nfunctions and, in turn, semantic content. Individual organisms vary in\ntheir characteristics, such as how their neurons respond to features\nof the environment. Some of these differences in how neurons respond\nmake a difference to an organism\u2019s survival and reproduction.\nFinally, some of these very differences may be heritable. Natural\nselection, commonly understood as this differential reproduction of\nheritable variation, is purely causal. Suppose that there is a\npopulation of rabbits. Further suppose that either by a genetic\nmutation or by the recombination of existing genes, some of these\nrabbits develop neurons that are wired into their visual systems in\nsuch a way that they fire (more or less reliably) in the presence of\ndogs. Further, the firing of these neurons is wired into a freezing\nbehavior in these rabbits. Because of this configuration, the rabbits\nwith the \u201cdog neurons\u201d are less likely to be detected by\ndogs, hence more likely to survive and reproduce. Finally, because the\ngenes for these neurons are heritable, the offspring of these\ndog-sensitive rabbits will themselves be dog-sensitive. Over time, the\nnumber of the dog-sensitive rabbits will increase, thereby displacing\nthe dog-insensitive rabbits. So, natural selection will, in such a\nscenario, give rise to mental representations of dogs. Insofar as such\na story is plausible, there is hope that natural selection and the\ngenesis of functions can provide a naturalistically acceptable means\nof delimiting content-determining causes.\n3.2.1 Objections to Evolutionary Functions\n\nThere is no doubt that individual variation, differential\nreproduction, and inheritance can be understood in a purely causal\nmanner. Yet, there remains skepticism about how naturalistically one\ncan describe what natural selection can select for. There are doubts\nabout the extent to which the objects of selection really can be\nspecified without illicit importation of intentional notions. Fodor\n(1989, 1990a) give voice to some of this skepticism. Prima\nfacie, it makes sense to say that the neurons in our hypothetical\nrabbits fire in response to the presence of dogs, hence that there is\nselection for dog representations. But, it makes just as much sense,\none might worry, to say that it is sensitivity to dog-look-alikes that\nleads to the greater fitness of the rabbits with the new\n neurons.[8]\n There are genes for the dog-look-alike neurons and these genes are\nheritable. Moreover, those rabbits that freeze in response to\ndog-look-alikes are more likely to survive and reproduce than are\nthose that do not so freeze, hence one might say that the freezing is\nin response to dog-look-alikes. So, our ability to say that the\nmeaning of the rabbits\u2019 mental representation \u201cX\u201d is\ndog, rather than dog-look-alike, depends on our ability to say that it\nis the dog-sensitivity of \u201cX\u201d, rather than the\ndog-look-alike-sensitivity of \u201cX\u201d, that keeps the rabbits\nalive longer. Of course, being dog-sensitive and being\ndog-look-alike-sensitive are connected, but the problem here is that\nboth being dog-look-alike-sensitive and being dog-sensitive can\nincrease fitness in ways that lead to the fixation of a genotype. And\nit can well be that it is avoidance of dogs that keeps a rabbit alive,\nbut one still needs some principled basis for saying that the rabbits\navoid dogs by being sensitive to dogs, rather than by being sensitive\nto dog-look-alikes. The latter appears to be good enough for the\ndifferential reproduction of heritable variation to do its work. Where\nwe risk importing semantic notions into the mix is in understanding\nselection intentionally, rather than purely causally. We need a notion\nof \u201cselection for\u201d that is both general enough to work for\nall the mental contents causal theorists aspire to address and that\ndoes not tacitly import semantic notions.\n\nIn response to this sort of objection, it has been proposed that the\ncorrect explanation of a rabbit\u2019s evolutionary success with,\nsay, \u201cX\u201d, is not that this enables the rabbit to avoid\ndog-look-alikes, but that it enables them to avoid dogs. It is dogs,\nbut not mere dog-look-alikes, that prey on rabbits. (This sort of\nresponse is developed in Millikan (1991) and Neander (1995).) Yet, the\nrejoinder is that if we really want to get at the correct explanation\nof a rabbit-cum-\u201cX\u201d system, then we should not suppose\nthat \u201cX\u201d means dog. Instead, we should say that it is in\nvirtue of the fact that \u201cX\u201d picks up on something like,\nsay, predator of such and such characteristics that the\n\u201cX\u201d alarm system increases the chance of a rabbit\u2019s\nsurvival. (This sort of rejoinder may be found in Agar (1993).)\n\nThis problem aside, there is also some concern about the extent to\nwhich it is plausible to suppose that natural selection could act on\nthe fine details of the operation of the brain, such as the firing of\nneurons in the presence of dogs. (This is an objection raised in Fodor\n(1990c)). Natural selection might operate to increase the size of the\nbrain so there is more cortical mass for cognitive processing. Natural\nselection might also operate to increase the folding of the brain so\nas to maximize the cortical surface area that can be contained within\nthe brain. Natural selection might also lead to compartmentalization\nof the brain, so that one particular region could be dedicated to\nvisual processing, another to auditory processing, and still another\nto face processing. Yet, many would take it to be implausible to\nsuppose that natural selection works at the level of individual mental\nrepresentations. The brain is too plastic and there is too much\nindividual variation in the brains of mammals to admit of selection\nacting in this way. Moreover, such far reaching effects of natural\nselection would lead to innate ideas not merely of colors and shapes,\nbut of dogs, cats, cars, skyscrapers, and movie stars. Rather than\nsupposing that functions are determined by natural selection across\nmultiple generations, many philosophers contend that it is more\nplausible that the functions that underlie mental representations are\nacquired through cognitive development.\n3.3 Developmental Functions\n\nHypothesizing that certain activities or events within the brain mean\nwhat they do, in part, because of some function that develops over the\ncourse of an individual\u2019s lifetime shares many of the attractive\nfeatures of the hypothesis that these same activities or events mean\nwhat they do, in part, because of some evolutionarily acquired\nfunction. One again can say that it is not the function of\n\u201cX\u201d to register the presence of LSD, microelectrodes,\nfoxes, stuffed dogs, or paper m\u00e2ch\u00e9 dogs, or questions,\nbut it is their function to report on dogs. Moreover, it does not\ninvoke dubious suppositions about an intimate connection between\nnatural selection and the precise details of neuronal hardware and its\noperation. A functional account based on ontogenetic function\nacquisition or learning seems to be an improvement. This is the core\nof the approach taken in Dretske (1981; 1988).\n\nThe function acquisition story proposes that during development, an\norganism is trained to discriminate real flesh and blood dogs from\nquestions, foxes, stuffed dogs, paper m\u00e2ch\u00e9 dogs under\nconditions of good lighting, without occlusions, or distractions. A\nteacher ensures that training proceeds according to plan. Once\n\u201cX\u201d has acquired the function to respond to dogs, the\ntraining is over. Thereafter, any instances in which \u201cX\u201d\nis triggered by foxes, stuffed dogs, paper m\u00e2ch\u00e9 dogs,\nLSD, microelectrodes, etc., are false tokenings and figure into false\nbeliefs.\n3.3.1 Objections to Developmental Functions\n\nAmong the most familiar objections to this proposal is that there is\nno principled distinction between when a creature is learning and when\nit is done learning. Instances in which a creature entertains the\nhypothesis that \u201cX\u201d means X, instances in which the\ncreature entertains the hypothesis that \u201cX\u201d means Y,\ninstances in which the creature straightforwardly uses \u201cX\u201d\nto mean X, and instances in which the creature straightforwardly uses\n\u201cX\u201d to mean Y are thoroughly intermingled. The problem is\nperhaps more clearly illustrated with tokens of natural language,\nwhere children will use words struggling through correct and incorrect\nuses of a word before (perhaps) finally settling on a correct usage.\nThere seems to be no principled way to specify if learning has stopped\nor whether there is instead \u201clifelong learning\u201d. This is\namong the objections to be found in Fodor (1984).\n\nThis, however, is a relatively technical objection. Further reflection\nsuggests that there may be an underlying appeal to the intentions of\nthe teacher. Let us revisit the learning story. Suppose that during\nthe learning period the subject is trained to use \u201cX\u201d as a\nmental representation of dogs. Now, let the student graduate from\n\u201cX\u201d using school and immediately thereafter see a fox.\nSeeing this fox causes a token of \u201cX\u201d and one would like\nto say that this is an instance of mistaking a fox for a dog, hence a\nfalse tokening. But, consider the situation counterfactually. If the\nstudent had seen the fox during the training period just before\ngraduation, the fox would have triggered a token of \u201cX\u201d.\nThis suggests that we might just as well say that the student learned\nthat \u201cX\u201d means fox or dog as that the student learned that\n\u201cX\u201d means dog. Thus, we might just as well say that, after\ntraining, the graduate does not falsely think of a dog, but truly\nthinks of a fox or a dog. The threat of running afoul of naturalist\nscruples comes if one attempts to say, in one way or another, that it\nis because the teacher meant for the student to learn that\n\u201cX\u201d means dog, rather than \u201cX\u201d means fox or\ndog. The threatened violation of naturalism comes in invoking the\nteacher\u2019s intentions. This, too, is an objection to be found in\nFodor (1984).\n3.4 Asymmetric Dependency Theory\n\nThe preceding attempts to distinguish the content-determining causes\nfrom non-content-determining causes focused on the background or\nboundary conditions under which the distinct types of causes may be\nthought to act. Fodor\u2019s Asymmetric Dependency Theory (ADT),\nhowever, represents a bold alternative to these approaches. Although\nFodor (1987, 1990a, b, 1994) contain numerous variations on the\ndetails of the theory, the core idea is that the content-determining\ncause is in an important sense fundamental, where the\nnon-content-determining causes are non-fundamental. The sense of being\nfundamental is that the non-content-determining causes depend on the\ncontent-determining cause; the non-content-determining causes would\nnot exist if not for the content-determining cause. Put a bit more\ntechnically, there are numerous laws such as \u2018Y1\ncauses \u201cX\u201d,\u2019 \u2018Y2 causes\n\u201cX\u201d,\u2019 etc., but none of these laws would exist were\nit not a law that X causes \u201cX\u201d. The fact that the \u2018X\ncauses \u201cX\u201d\u2019 law does not in the same way depend on\nany of the Y1, Y2, \u2026, Yn laws\nmakes the dependence asymmetric. Hence, there is an asymmetric\ndependency between the laws. The intuition here is that the question,\n\u2018What kind of animal is called \u201cFido\u201d?\u2019 will\ncause an occurrence of the representation \u201cX\u201d only because\nof the fact that dogs cause \u201cX\u201d. Instances of foxes cause\ninstances of \u201cX\u201d only because foxes are mistaken for dogs\nand dogs cause instances of \u201cX\u201d.\n\nCausation is typically understood to have a temporal dimension. First\nthere is event C and this event C subsequently leads to event E. Thus,\nwhen the ADT is sometimes referred to as the \u201cAsymmetric Causal\nDependency Theory,\u201d the term \u201ccausal\u201d might suggest\na diachronic picture in which there is, first, an X-\u201cX\u201d\nlaw which subsequently gives rise to the various Y-\u201cX\u201d\nlaws. Such a diachronic interpretation, however, would lead to\ncounterexamples for the ADT approach. Fodor (1987) discusses this\npossibility. Consider Pavlovian conditioning. Food causes salivation\nin a dog. Then a bell causes salivation in the dog. It is likely that\nthe bell causes salivation only because the food causes it. Yet,\nsalivation hardly means food. It may well naturally mean that food is\npresent, but salivation is not a thought or thought content and it is\nnot ripe for false semantic tokening. Or take a more exotic kind of\ncase. Suppose that one comes to apply \u201cX\u201d to dogs, but\nonly by means of observations of foxes. This would be a weird case of\n\u201clearning\u201d, but if things were to go this way, one would\nnot want \u201cX\u201d to mean fox. To block this kind of objection,\nthe theory maintains the dependency between the fundamental\nX-\u201cX\u201d law and the non-fundamental Y-\u201cX\u201d laws\nis synchronic. The dependency is such that if one were to break the\nX-\u201cX\u201d law at time t, then one would thereby\ninstantaneously break all the Y-\u201cX\u201d laws at that time.\n\nThe core of ADT, therefore, comes down to this. \u201cX\u201d means\nX if\n\n\u2018Xs cause \u201cX\u201ds\u2019 is a law,\nFor all Ys that are not Xs, if Ys qua Ys actually cause\n\u201cX\u201ds, then the Y\u2019s causing \u201cX\u201ds is\nasymmetrically dependent on the Xs causing \u201cX\u201ds,\nThe dependence in (2) is synchronic (not\ndiachronic).\n\n\nThis seems to get a number of cases right. The reason that questions\nlike \u201cWhat kind of animal is named \u2018Fido\u2019?\u201d or\n\u201cWhat is a Sheltie?\u201d trigger \u201cX\u201d, meaning dog,\nis that dogs are able to trigger \u201cX\u201ds. Foxes only trigger\n\u201cX\u201ds, meaning dog, because dogs are able to trigger them.\nMoreover, it appears to solve the disjunction problem. Suppose we have\na \u2018dogs cause \u201cX\u201ds\u2019 law and a \u2018dogs or\nfoxes cause \u201cX\u201ds\u2019 law. If one breaks the \u2018dogs\ncause \u201cX\u201ds\u2019 law, then one thereby breaks the\n\u2018dogs or foxes cause \u201cX\u201ds\u2019 law, since the only\nreason either dogs or foxes cause \u201cX\u201ds is because dogs do.\nMoreover, if one breaks the \u2018dogs or foxes cause\n\u201cX\u201ds\u2019 law, one does not thereby break the\n\u2018dogs cause \u201cX\u201ds\u2019 law, since dogs alone might\nsuffice to cause \u201cX\u201ds. So, the \u2018dogs or foxes cause\n\u201cX\u201ds\u2019 law depends on the \u2018dogs cause\n\u201cX\u201ds\u2019 law, but not vice versa. Asymmetric dependency\nof laws gives the right\n results.[9]\n3.4.1 Objections to ADT\n\nAdams and Aizawa (1994) mention an important class of causes that the\nADT does not appear to handle, namely, the \u201cnon-psychological\ninterventions\u201d. We have all along assumed the \u201cX\u201d is\nsome sort of brain event, such as the firing of some neurons. But, it\nis plausible that some interventions, such as a dose of hallucinogen\nor maybe some carefully placed microelectrodes, could trigger such\nbrain events, quite apart from the connection of those brain events to\nother events in the external world. If essentially all brain events\nare so artificially inducible, then it would appear that for all\nputative mental representations, there will be some laws, such as\n\u2018microelectrodes cause \u201cX\u201ds,\u2019 that do not\ndepend on laws such as \u2018dogs causes \u201cX\u201ds.\u2019 If\nthis is the case, then the second condition of the ADT would rarely or\nnever be satisfied, so that the theory would have little relevance to\nactual cognitive scientific practice.\n\nFodor (1990a) discusses challenges that arise with the fact that the\nperception of objects involves causal intermediaries. Suppose that\nthere is a dog-\u201cX\u201d law that is mediated entirely by\nsensory mechanisms. In fact, suppose unrealistically that the\ndog-\u201cX\u201d law is mediated by a single visual sensory\nprojection. In other words let the dog-\u201cX\u201d law be mediated\nby the combination of a dog-dogsp law and a\ndogsp-\u201cX\u201d law. Under these conditions, it\nappears that \u201cX\u201d means dogsp, rather than dog.\nCondition (1) is satisfied, since there is a\ndogsp-\u201cX\u201d law. Condition (2) is satisfied,\nsince if one were to break the dogsp-\u201cX\u201d law\none would thereby break the dog-\u201cX\u201d law (i.e., there is a\ndependence of one law one the other) and breaking the\ndog-\u201cX\u201d law would not necessarily break the\ndogsp-\u201cX\u201d law (i.e., the dependence is not\nsymmetric). The dependence is asymmetric, because one can break the\ndog-\u201cX\u201d law by breaking the dog-dogsp law (by\nchanging the way dogs look) without thereby breaking the\ndogsp-\u201cX\u201d law. Finally, condition (3) is\nsatisfied, since the dependence of the dog-\u201cX\u201d law on the\ndogsp-\u201cX\u201d law is synchronic.\n\nThe foregoing version of the sensory projections problem relies on\nwhat was noted to be the unrealistic assumption that the\ndog-\u201cX\u201d law is mediated by a single visual sensory\nprojection. Relaxing the assumption does not so much solve the problem\nas transform it. So, adopt the more realistic assumption that the\ndog-\u201cX\u201d law is sustained by a combination of a large set\nof dog-sensory projection laws and a large set of\ndogsp-\u201cX\u201d laws. In the first set, we have laws\nconnecting dogs to particular patterns of retinal stimulation, laws\nconnecting dogs to particular patterns of acoustic stimulation, etc.\nIn the second set, we have certain psychological laws connecting\nparticular patterns of retinal stimulation to \u201cX\u201d, certain\npsychological laws connecting particular patterns of acoustic\nstimulation to \u201cX\u201d, etc. In this sort of situation, there\nthreatens to be no \u201cfundamental\u201d law, no law on which all\nother laws asymmetrically depend. If one breaks the\ndog-\u201cX\u201d law one does not thereby break any of the sensory\nprojection-\u201cX\u201d laws, since the former can be broken by\ndissolving all of the dog-sensory projection laws. If, however, one\nbreaks any one of the particular dogsp-\u201cX\u201d\nlaws, e.g. one connecting a particular doggish visual appearance to\n\u201cX\u201d, one does not thereby break the dog-\u201cX\u201d\nlaw. The other sensory projections might sustain the\ndog-\u201cX\u201d law. Moreover, breaking the law connecting a\nparticular doggish look to \u201cX\u201d will not thereby break a\nlaw connecting a particular doggish sound to \u201cX\u201d. Without\na \u201cfundamental\u201d law, there is no meaning in virtue of the\nconditions of the ADT. Further, the applicability of the ADT appears\nto be dramatically reduced insofar as connections between mental\nrepresentations and properties in the world are mediated by sensory\nprojections. (See Neander, 2013, Schulte, 2018, Artiga &\nSebasti\u00e1n, 2020, for discussion of the distality problem for\nother causal theories.)\n\nAnother problem arises with items or kinds that are indistinguishable.\nAdams and Aizawa (1994), and, implicitly, McLaughlin, (1991), among\nothers, have discussed this problem. As one example, consider the time\nat which the two minerals, jadeite and nephrite, were chemically\nindistinguishable and were both thought to be jade. As another, one\nmight appeal to H2O and XYZ (the stuff of philosophical\nthought experiments, the water look-alike substance found on\ntwin-earth). Let X = jadeite and Y = nephrite and let there be laws\n\u2018jadeite causes \u201cX\u201d\u2019 and \u2018nephrite\ncauses \u201cX\u201d\u2019. Can \u201cX\u201d mean jadeite? No.\nCondition (1) is satisfied, since it is a law that \u2018jadeite\ncauses \u201cX\u201d\u2019. Condition (3) is satisfied, since\nbreaking the jadeite-\u201cX\u201d law will immediately break the\nnephrite-\u201cX\u201d law. If jadeite cannot trigger an\n\u201cX\u201d, then neither can nephrite, since the two are\nindistinguishable. That is, there is a synchronic dependence of the\n\u2018nephrite causes \u201cX\u201d\u2019 law on the\n\u2018jadeite causes \u201cX\u201d law. The problem arises with\ncondition (2). Breaking the jadeite-\u201cX\u201d law will thereby\nbreak the nephrite-\u201cX\u201d law, but breaking the\nnephrite-\u201cX\u201d law will also thereby break the\njadeite-\u201cX\u201d law. Condition (2) cannot be satisfied, since\nthere is a symmetric dependence between the jadeite-\u201cX\u201d\nlaw and the nephrite-\u201cX\u201d law. By parity of reasoning,\n\u201cX\u201d cannot mean nephrite. So, can \u201cX\u201d mean\njade? No. As before, conditions (1) and (3) could be satisfied, since\nthere could be a jade-\u201cX\u201d law and the\njadeite-\u201cX\u201d law and the nephrite-\u201cX\u201d law could\nsynchronically depend on it. The problem is, again, with condition\n(2). Presumably breaking the jade-\u201cX\u201d law would break the\njadeite-\u201cX\u201d and nephrite-\u201cX\u201d law, but breaking\neither of them would break the jade-\u201cX\u201d law. The problem\nis, again, with symmetric dependencies.\n\nHere is a problem that we earlier found in conjunction with other\ncausal theories. Despite the bold new idea underlying the ADT method\nof partitioning off non-content-determining causes, it too appears to\nsneak in naturalistically unacceptable assumptions. Like all causal\ntheories of mental content, the asymmetric causal dependencies are\nsupposed to be the basis upon which meaning is created; the\ndependencies are not themselves supposed to be a product, or\nbyproduct, of meaning. Yet, ADT appears to violate this naturalistic\npre-condition for causal theories. (This kind of objection may be\nfound in Seager (1993), Adams & Aizawa (1994), (1994b), Wallis\n(1995), and Gibson (1996)). Ys are supposed to cause \u201cX\u201ds\nonly because Xs do and this must not be because of any semantic facts\nabout \u201cX\u201ds. So, what sort of mechanism would bring about\nsuch asymmetric dependencies among things connected to the syntactic\nitem \u201cX\u201d? In fact, why wouldn\u2019t lots of things be\nable to cause \u201cX\u201ds besides Xs, quite independently of the\nfact that Xs do? The instantiation of \u201cX\u201ds in the brain\nis, say, some set of neurochemical events. There should be natural\ncauses capable of producing such events in one\u2019s brain under a\nvariety of circumstances. Why on earth would foxes be able to cause\nthe neurochemical \u201cX\u201d events in us only because dogs can?\nOne might be tempted to observe that \u201cX\u201d means dog,\n\u201cY\u201d means fox, we associate foxes with dogs and that is\nwhy foxes cause \u201cX\u201ds only because dogs cause\n\u201cX\u201ds. We would not associate foxes with \u201cX\u201ds\nunless we associated \u201cX\u201ds with dogs and foxes with dogs.\nThis answer, however, involves deriving the asymmetric causal\ndependencies from meanings, which violates the background assumption\nof the naturalization project. Unless there is a better explanation of\nsuch asymmetrical dependencies, it may well be that the theory is\nmisguided in attempting to rest meaning upon them.\n3.5 Best Test Theory\n\nA relatively more recent causal theory is Robert Rupert\u2019s (1999)\nBest Test Theory (BTT) for the meanings of natural kind terms. Unlike\nmost causal theories, this one is restricted in scope to just natural\nkinds and terms for natural kinds. To mark this restriction, we will\nlet represented kinds be denoted by K\u2019s, rather than our usual\nX\u2019s.\n\nBest Test Theory: If a subject S bears no\nextension-fixing intentions toward \u201cX\u201d and \u201cX\u201d\nis an atomic natural kind term in S\u2019s language of thought (i.e.,\nnot a compound of two or more other natural kind terms), then\n\u201cX\u201d has as its extension the members of natural kind K if\nand only if members of K are more efficient in their causing of\n\u201cX\u201d in S than are the members of any other natural\nkind.\n\nTo put the idea succinctly, \u201cX\u201d means, or refers to, those\nthings that are the most powerful stimulants of \u201cX\u201d. That\nsaid, we need an account of what it is for a member of a natural kind\nto be more efficient in causing \u201cX\u201ds than are other\nnatural kinds. We need an account of how to measure the power of a\nstimulus. This might be explained in terms of a kind of biography.\n\n\n\n\n \n\u201cX1\u201d\n\u201cX2\u201d\n\u201cX3\u201d\n\u201cX4\u201d\n\u201cX5\u201d \n\nK1\n\n1\n1\n1\n \n\nK1\n1\n1\n1\n\n \n\nK1\n1\n1\n\n\n \n\nK1\n\n1\n1\n1\n \n\nK1\n1\n1\n1\n\n \n\nK1\n1\n1\n\n\n \n\nK2\n1\n\n\n\n \n\nK2\n1\n\n\n\n \n\nK2\n1\n1\n\n\n \n\nK3\n\n1\n\n\n \n\nK3\n\n1\n\n\n \n\nK3\n\n1\n\n\n \n\nK3\n1\n\n\n\n  \n\n\nFigure 1. A spreadsheet biography\n\n\nConsider an organism S that (a) causally interacts with three\ndifferent natural kinds, K1-K3, in its\nenvironment and (b) has a language of thought with five terms\n\u201cX1\u201d-\u201cX5\u201d. Further,\nsuppose that each time S interacts with an individual of kind\nKi this causes an occurrence of one or more of\n\u201cX1\u201d-\u201cX5\u201d. We can then\ncreate a kind of \u201cspreadsheet biography\u201d or \u201clog of\nmental activity\u201d for S in which there is a column for each of\n\u201cX1\u201d-\u201cX5\u201d and a row for\neach instance in which a member of K1-K3 causes\none or more instances of\n\u201cX1\u201d-\u201cX5\u201d. Each mental\nrepresentation \u201cXi\u201d that Ki triggers\nreceives a \u201c1\u201d in its column. Thus, a single spreadsheet\nbiography might look like that shown in Figure 1.\n\nTo determine what a given term \u201cXi\u201d means, we\nfind the kind Ki that is most effective at causing\n\u201cXi\u201d. This can be computed from S\u2019s\nbiography. For each Ki and \u201cXi\u201d, we\ncompute the frequency with which Ki triggers\n\u201cXi\u201d. \u201cX1\u201d is tokened\nfour out of six times that K1 is encountered, three out of\nthree times that K2 is encountered, and one out of four\ntimes that K3 is encountered. \u201cXi\u201d\nmeans that Ki that has the highest sample frequency. Thus,\nin this case, \u201cX1\u201d means K2. Just to\nbe clear, when BTT claims that \u201cXi\u201d means the\nKi that is the most powerful stimulant of \u201cX\u201d,\nthis is not to say that \u201cX\u201d means the most common\nstimulant of \u201cX\u201d. In our spreadsheet biography,\nK1 is the most common stimulant of\n\u201cX1\u201d, since it triggers\n\u201cX1\u201d four times, where K2 triggers\nit only three times, and K3 triggers it only one time. This\nis why, according to BTT, \u201cX1\u201d means\nK2, rather than K1 or K3.\n\nHow does the BTT handle our range of test cases? Consider, first, the\nstandard form of the disjunction problem, the case of \u201cX\u201d\nmeaning dog, rather than dog or fox-on-a-dark-night-at-a-distance.\nSince the latter is not apparently a natural kind, \u201cX\u201d\ncannot mean\n that.[10]\n Moreover, \u201cX\u201d means dog, rather than fox, because the\nonly times the many foxes that S encounters can trigger\n\u201cX1\u201ds is on dark nights at a distance, where\ndogs trigger \u201cX\u201ds more consistently under a wider range of\nconditions.\n\nHow does the BTT address the apparent problem of \u201cbrain\ninterventions,\u201d such as LSD, microelectrodes, or brain tumors?\nThe answer is multi-faceted. The quickest method for taking much of\nthe sting out of these cases is to note that they generally do not\narise for most individuals. The Best Test Theory relies on personal\nbiographies in which only actual instances of kinds triggering mental\nrepresentations are used to specify causal efficiency. The\ncounterfactual truth that, were a stimulating microelectrode to be\napplied to, say, a particular neuron, it would perfectly reliably\nproduce an \u201cX\u201d token simply does not matter for the\ntheory. So, for all those individuals who do not take LSD, do not have\nmicroelectrodes inserted in their brains, do not have brain tumors,\netc., these sorts of counterfactual possibilities are irrelevant. A\nsecond line of defense against \u201cbrain interventions\u201d\nappeals to the limitation to natural kinds. The BTT might set aside\nmicroelectrodes, since they do not constitute a natural kind. Maybe\nbrain tumors are; maybe not. Unfortunately, however, LSD is a very\nstrong candidate for a chemical natural kind. Still the BTT is not\nwithout a third line of defense for handling these cases. One might\nsuppose that LSD and brain tumors act on the brain in a rather diffuse\nmanner. Sometimes a dose of LSD triggers \u201cXi\u201d,\nanother time it triggers \u201cXj\u201d, and another time\nit triggers \u201cXk\u201d. One might then propose that,\nif one counts all these episodes with LSD, none of these will act\noften enough on, say, \u201cXi\u201d to get it to mean\nLSD, rather than, say, dog. This is the sort of strategy that Rupert\ninvokes to keep mental symbols from meaning omnipresent, but\nnon-specific causes such as the heart. The heart might causally\ncontribute to \u201cX1\u201d, but it also contributes to\nso many other \u201cXi\u201ds, that the heart will turn\nout not to be the most efficient cause of\n\u201cX1\u201d.\n\nWhat about questions? Presumably questions as a category will count as\nan instance of a linguistic natural kind. Moreover, particular\nsentences will also count. So, the restriction of the BTT to natural\nkinds is of little use here. So, what of causal efficiency? Many\nsentences appear to provoke a wide range of possible responses. In\nresponse to, \u201cI went to the zoo last week,\u201d S could think\nof lions, tigers, bear, giraffes, monkeys, and any number of other\nnatural kinds. But, the question, \u201cWhat animal goes \u2018oink,\noink\u2019?\u201d\u2014perhaps uttered in \u201cMotherese\u201d\nin a clear deliberate fashion so that it is readily comprehensible to\na child\u2014will be rather efficient in generating thoughts of a\npig. Moreover, it could be more efficient than actual pigs, since a\nchild might have more experience with the question than with actual\npigs, often not figuring out that actual pigs are pigs. In such\nsituations, \u201cpig\u201d would turn out to mean \u201cWhat\nanimal goes \u2018oink, oink\u2019?,\u201d rather than pig. So,\nthere appear to be cases in which BTT could make prima facie\nincorrect content assignments.\n\nWhat, finally, of proximal projections of natural kinds? One plausible\nline might be to maintain that proximal projections of natural kinds\nare not themselves natural kinds, hence that they are automatically\nexcluded from the scope of the theory. This plausible line, however,\nmight be the only available line. Presumably, in the course of\nS\u2019s life, the only way dogs can cause \u201cX\u201ds is by way\nof causal mediators between the dogs and the \u201cX\u201ds. Thus,\neach episode in which a dog causes an \u201cX\u201d is also an\nepisode in which a sensory projection of a dog causes an\n\u201cX\u201d. So, dog efficiency for \u201cX\u201d can be no\nhigher the efficiency of dog sensory projections. And, if it is\npossible for there to be a sensory projection of a dog without there\nbeing an actual dog, then the efficiency of the projections would be\ngreater than the efficiency of the dogs. So, \u201cX\u201d could not\nmean dog. But, this problem is not necessarily damaging to BTT.\n\nSince the BTT has not received a critical response in the literature,\nwe will not devote a section to objections to it. Instead, we will\nleave well enough alone with our somewhat speculative treatment of how\nBTT might handle our familiar test cases. The general upshot is that\nthe combination of actual causal efficiency over the course of an\nindividual\u2019s lifetime along with the restriction to natural\nkinds provides a surprisingly rich means of addressing some\nlong-standing problems.\n4. General Objections to Causal Theories of Mental Content\n\nIn the preceding section, we surveyed issues that face the philosopher\nattempting to work out the details of a causal theory of mental\ncontent. These issues are, therefore, one might say, internal to\ncausal theories. In this section, however, we shall review some of the\nobjections that have been brought forward to the very idea of a causal\ntheory of mental content. As such, these objections might be construed\nas external to the project of developing a causal theory of mental\ncontent. Some of these are coeval with causal theories and have been\naddressed in the literature, but some are relatively recent and have\nnot been discussed in the literature. The first objections, discussed\nin subsections 4.1\u20134.4, in one way or another push against the\nidea that all content could be explained by appeal to a causal theory,\nbut leave open the possibility that one or another causal theory might\nprovide sufficiency conditions for meaning. The last objections, those\ndiscussed in subsections 4.5\u20134.6 challenge the ability of causal\ntheories to provide even sufficiency conditions for mental\ncontent.\n4.1 Causal Theories do not Work for Logical and Mathematical Relations\n\nOne might think that the meanings of terms that denote mathematical or\nlogical relations could not be handled by a causal theory. How could a\nmental version of the symbol \u201c+\u201d be causally connected to\nthe addition function? How could a mental version of the logical\nsymbol \u201c\u00ac\u201d be causally connected to the negation truth\nfunction? The addition function and the negation function are abstract\nobjects. To avoid this problem, causal theories typically acquiesce\nand maintain that their conditions are merely sufficient conditions on\nmeaning. If an object meets the conditions, then that object bears\nmeaning. But, the conditions are not necessary for meaning, so that\nrepresentations of abstract objects get their meaning in some other\nway. Perhaps conceptual role semantics, wherein the meanings of terms\nare defined in terms of the meanings of other terms, could be made to\nwork for these other theories.\n4.2 Causal Theories do not Work for Vacuous Terms\n\nAnother class of potential problem cases are vacuous terms. So, for\nexample, people can think about unicorns, fountains of youth, or the\nplanet Vulcan. Cases such as these are discussed in Stampe (1977) and\nFodor (1990a), among other places. These things would be physical\nobjects were they to exist, but they do not, so one cannot causally\ninteract with them. In principle, one could say that thoughts about\nsuch things are not counterexamples to causal theories, since causal\ntheories are meant only to offer sufficiency conditions for meaning.\nBut, this in principle reply appears to be ad hoc. It is not\nwarranted, for example, by the fact that these excluded meanings\ninvolve abstract objects. There are, however, a number of options that\nmight be explored here.\n\nOne strategy would be to turn to the basic ontology of one\u2019s\ncausal theory of mental content. This is where a theory based on\nnomological relations might be superior to a version that is based on\ncausal relations between individuals. One might say that there can be\na unicorn-\u201cunicorn\u201d law, even if there are no actual\nunicorns. This story, however, would break down for mental\nrepresentations of individuals, such as the putative planet Vulcan.\nThere is no law that connects a mental representation to an\nindividual; laws are relations among properties.\n\nAnother strategy would be to propose that some thought symbols are\ncomplex and can decompose into meaningful primitive constituents. One\ncould then allow that \u201cX\u201d is a kind of abbreviation for,\nor logical construction of, or defined in terms of \u201cY1,\u201d\n\u201cY2,\u201d and \u201cY3,\u201d and that a causal theory\napplies to \u201cY1,\u201d \u201cY2,\u201d and \u201cY3.\u201d\nSo, for example, one might have a thought of a unicorn, but rather\nthan having a single unicorn mental representation there is another\nrepresentation made up of a representation of a horse, a\nrepresentation of a horn, and a representation of the relationship\nbetween the horse and the horn. \u201cHorse\u201d,\n\u201chorn\u201d, and \u201cpossession,\u201d may then have\ninstantiated properties as their contents.\n4.3 Causal Theories do not Work for Phenomenal Intentionality\n\nHorgan and Tienson (2002) object to what they describe as\n\u201cstrong externalist theories\u201d that maintain that causal\nconnections are necessary for content. They argue, first, that mental\nlife involves a lot of intentional content that is constituted by\nphenomenology alone. Perceptual states, such as seeing a red apple,\nare intentional. They are about apples. Believing that there are more\nthan 10 Mersenne primes and hoping to discover a new Mersenne prime\nare also intentional states, in this case about Mersenne primes. But,\nall these intentional states have a phenomenology\u2014something it\nis like to be in these states. There is something it is like to see a\nred apple, something different that it is like to believe there are\nmore than 10 Mersenne primes, and something different still that it is\nlike to hope to discover a new Mersenne prime. Horgan and Tienson\npropose that there can be phenomenological duplicates\u2014two\nindividuals with exactly the same phenomenology. Assume nothing about\nthese duplicates other than that they are phenomenological duplicates.\nIn such a situation, one can be neutral regarding how much of their\nphenomenological experience is veridical and how much illusory. So,\none can be neutral on whether or not a duplicate sees a red apple or\nwhether there really are more than 10 Mersenne primes. This suggests\nthat there is a kind of intentionality\u2014that shared by the\nduplicates\u2014that is purely phenomenological. Second, Horgan and\nTienson argue that phenomenology constitutively depends only on narrow\nfactors. They observe that one\u2019s experiences are often caused or\ntriggered by events in the environment, but that these environmental\ncauses are only parts of causal chains that lead to the phenomenology\nitself. They do not constitute that phenomenology. The states that\nconstitute, or provide the supervenience base for, the phenomenology\nare not the elements of the causal chain leading back into the\nenvironment. If we combine the conclusions of these two arguments, we\nget Horgan and Tienson\u2019s principal argument against any causal\ntheory that would maintain that causal connections are necessary for\ncontent.\n\nP1. There is intentional content that is constituted by phenomenology\nalone.\n\nP2. Phenomenology is constituted only by narrow factors.\n\n\nTherefore,\n\nC. There is intentional content that is constituted only by narrow\nfactors.\n\n\nThus, versions of causal theories that suppose that all content must\nbe based on causal connections are fundamentally mistaken. For those\nversions of causal theories that offer only sufficiency conditions on\nsemantic content, however, Horgan and Tienson\u2019s argument may be\ntaken to provide a specific limitation on the scope of causal\ntheories, namely, that causal theories do not work for intentional\ncontent that is constituted by phenomenology alone.\n\nA relatively familiar challenge to this argument may be found in\ncertain representational theories of phenomenological properties.\n(See, for example, Dretske (1988) and Tye (1997).) According to these\nviews, the phenomenology of a mental state derives from that\nstate\u2019s representational properties, but the representational\nproperties are determined by external factors, such as the environment\nin which an organism finds itself. Thus, such representationalist\ntheories challenge premise P2 of Horgan and Tienson\u2019s\nargument.\n4.4 Causal Theories do not Work for Certain Reflexive Thoughts\n\nBuras (2009) presents another argument that is perhaps best thought of\nas providing a novel reason to think that causal theories of mental\nrepresentation only offer sufficiency conditions on meaning. This\nargument begins with the premise that some mental states are about\nthemselves. To motivate this claim, Buras notes that some sentences\nare about themselves. So, by analogy with, \u201cThis sentence is\nfalse,\u201d which is about itself, one might think that there is a\nthought, \u201cThis thought is false,\u201d that is also about\nitself. Or, how about \u201cThis thought is realized in brain\ntissue\u201d or \u201cThis thought was caused by LSD\u201d? These\nappear to be about themselves. Buras\u2019 second premise is that\nnothing is a cause of itself. So, \u201cThis thought is false\u201d\nis about itself, but could not be caused by itself. So, the sentence\n\u201cThis thought is false\u201d could not mean that it itself is\nfalse in virtue of the fact that \u201cThis thought is false\u201d\nwas caused by its being false. So, \u201cThis thought is false\u201d\nmust get its meaning in some other way. It must get its meaning in\nvirtue of some other conditions of meaning acquisition.\n\nThis is not, however, exactly the way Buras develops his argument. In\nthe first place, he treats causal theories of mental content as\nmaintaining that, if \u201cX\u201d means X, then X causes\n\u201cX\u201d. (Cf. Buras, 2009, p. 118). He cites Stampe (1977),\nDretske (1988), and Fodor (1987) as maintaining this. Yet, Stampe,\nDretske, and Fodor explicitly formulate their theories in terms of\nsufficiency conditions, so that (roughly) \u201cX\u201d means X, if\nXs causes \u201cX\u201ds, etc. (See, for example, Stampe (1977), pp.\n82\u20133, Dretske (1988), p. 52, and Fodor (187), p. 100). In the\nsecond place, Buras seems to draw a conclusion that is orthogonal to\nthe truth or falsity of causal theories of mental content. He begins\nhis paper with an impressively succinct statement of his argument.\n\nSome mental states are about themselves. Nothing is a cause of itself.\nSo some mental states are not about their causes; they are about\nthings distinct from their causes (Buras, 2009, p. 117).\n\nThe causal theorist can admit that some mental states are not about\ntheir causes, since some states are thoughts and thoughts mean what\nthey do in virtue of, say, the meanings of mental sentences. These\nmental sentences might mean what they do in virtue of the meanings of\nprimitive mental representations (which may or may not mean what they\ndo in virtue of a causal theory of meaning) and the way in which those\nprimitive mental representations are put together. As was mentioned in\nsection 2, such a syntactically and semantically combinatorial\nlanguage of thought is a familiar background assumption for causal\ntheories. The conclusion that Buras may want, instead, is that there\nare some thoughts that do not mean what they do in virtue of what\ncauses them. So, through some slight amendments, one can understand\nBuras to be presenting a clarification of the scope of causal theories\nof mental content or as a challenge to a particularly strong version\nof causal theories, a version that takes them as offering a necessary\ncondition on meaning.\n4.5 Causal Theories do Not Work for Reliable Misrepresentations\n\nAs noted above, one of the central challenges for causal theories of\nmental content has been to discriminate between a \u201ccore\u201d\ncontent-determining causal connection, as between cows and\n\u201ccow\u201ds, and \u201cperipheral\u201d\nnon-content-determining causal connections, as between horses and\n\u201ccow\u201ds. Cases of reliable misrepresentations are\nrepresentations which always misrepresent in the same way. In such\ncases, there is supposed to be no \u201ccore\u201d\ncontent-determining causal connection; there are no X\u2019s to which\n\u201cX\u201ds are causally connected. Instead, there are only\n\u201cperipheral\u201d causal connections. Mendelovici, (2013),\nfollowing a discussion by Hohman, (2002), suggests that color\nrepresentations may be like\n this.[11]\n Color anti-realism, according to which there are no colors in the\nworld, seems to be committed to the view that color representations\nare not caused by colors in the world. Color representations may be\nreliably tokened by something in the world, but not by colors that are\nin the world.\n\nIn some instances, reliable misrepresentations provide another take on\nsome of the familiar content-determination problems. So, take attempts\nto use normal conditions to distinguish between content-determining\ncauses and non-content-determining causes. Even in normal conditions,\ncolor representations are not caused by colors, but by, say, surface\nreflectances under certain conditions of illumination, just in the way\nthat, even in normal conditions cow representations are sometimes not\ncaused by cows, but by, say, a question such as, \u201cWhat kind of\nanimal is sometimes named \u2018Bessie\u2019?\u201d Take a version\nof the asymmetric dependency theory. On this theory applied to color\nterms, it might seem that there is no red-to-\u201cred\u201d law on\nwhich all the other laws depends in much the same way that it might\nseem there is no unicorn-to-\u201cunicorn\u201d law on which all\nother laws depends. (Cf. Fodor (1987, pp. 163\u20134) and (1990, pp.\n100\u20131)).\n\nUnlike the more familiar cases, Mendelovici, (2013), does not argue\nthat there actually are such problematic cases. The argument is not\nthat there are actual cases of reliable misrepresentations, but merely\nthat reliable misrepresentations are possible and that this is enough\nto create trouble for causal theories of mental representation. One\nsort of trouble stems from the need for a pattern of psychological\nexplanation. Let a mental representation \u201cX\u201d mean\nintrinsically-heavy. Such a representation is a misrepresentation,\nsince there is no such property of being intrinsically heavy. Such a\nmisrepresentation is, nonetheless, reliable (i.e. consistent), since\nit is consistently tokened by all the same sort of things on earth.\nBut, one can see how an agent using \u201cX\u201d could make a\nreasonable, yet mistaken, inference to the conclusion that an object\nthat causes a tokening of \u201cX\u201d on earth would be hard to\nlift on the moon. To allow such a pattern of explanation, Mendelovici\nargues, a causal theorist must allow for reliable misrepresentation. A\ntheory of what mental representations are should not preclude such\npatterns of explanation. Another sort of trouble stems from the idea\nthat if a theory of meaning does not allow for reliable\nmisrepresentation, but requires that there be a connection between\n\u201cX\u201ds and Xs, then this would be constitute a commitment to\na realist metaphysics for Xs. While there can be good reasons for\nrealism, the needs of a theory of content would not seem to be a\nproper source for them.\n\nArtiga, 2013, provides a defense of teleosemantic theories in the face\nof Mendelovici\u2019s examples of reliable misrepresentation. Some of\nArtiga\u2019s arguments might also be used by advocates of causal\ntheories of mental content. Mendelovici, (2016), replies to Artiga,\n2013, by providing refinements and a further defense of the view that\nreliable misrepresentations are a problem for causal theories of\nmental content.\n4.6 Causal Theories Conflict with the Theory Mediation of Perception\n\nCummins (1997) argues that causal theories of mental content are\nincompatible with the fact that one\u2019s perception of objects in\nthe physical environment is typically mediated by a theory. His\nargument proceeds in two stages. In one stage, he argues that, on a\ncausal theory, for each primitive \u201cX\u201d there must be some\nbit of machinery or mechanism that is responsible for detecting Xs.\nBut, since a finite device, such as the human brain, contains only a\nfinite amount of material, it can only generate a finite number of\nprimitive representations. Next, he observes that thought is\nproductive\u2014that it can, in principle, generate an unbounded\nnumber of semantically distinct representations. This means that to\ngenerate the stock of mental representations corresponding to each of\nthese distinct thoughts, one must have a syntactically and\nsemantically combinatorial system of mental representation of the sort\nfound in a language of thought (LOT). More explicitly, this scheme of\nmental representation must have the following properties:\n\nIt has a finite number of semantically primitive expressions.\nEvery expression is a concatenation of one or more primitive\nexpressions.\nThe content of any complex expression is a function of the\ncontents of the primitives and the way those primitives are\nconcatenated into the whole expression.\n\n\nThe conclusion of this first stage is, therefore, that a causal theory\nof mental representation requires a LOT. In the other stage of his\nargument, Cummins observes that, for a wide range of objects, their\nperception is mediated by a body of theory. Thus, to perceive\ndogs\u2014for dogs to cause \u201cdogs\u201d\u2014one has to know\nthings such as that dogs have tails, dogs have fur, and dogs four\nlegs. But, to know that dogs have tails, fur, and four legs, one needs\na set of mental representations, such as \u201ctail\u201d,\n\u201cfur\u201d, \u201cfour\u201d, and \u201clegs\u201d. Now the\nproblem fully emerges. According to causal theories, having an\n\u201cX\u201d representation requires the ability to detect dogs.\nBut, the ability to detect dogs requires a theory of dogs. But, having\na theory of dogs requires already having a LOT\u2014a system of\nmental representation. One cannot generate mental representations\nwithout already having\n them.[12]\n4.7 Causal Theories Conflict with the Implementation of Psychological Laws\n\nJason Bridges (2006) argues that the core hypothesis of informational\nsemantics conflicts with the idea that psychological laws are\nnon-basic. As we have just observed, causal theories are often taken\nto offer mere sufficiency conditions for meaning. Suppose, therefore,\nthat we suitably restrict the scope of a causal theory and understand\nits core hypothesis as asserting that all \u201cX\u201ds with the\ncontent X are reliably caused by X. (Nothing in the logic of\nBridges\u2019 argument depends on any additional conditions on a\nputative causal theory of mental content, so for simplicity we can\nfollow Bridges in restricting attention to this simple version.)\nBridges proposes that this core claim of a causal theory of mental\ncontent is a constitution thesis. It specifies what constitutes the\nmeaning relation (at least in some restricted domain). Thus, if one\nwere to ask, \u201cWhy is it that all \u2018X\u2019s with content X\nare reliably caused by Xs?,\u201d the answer is roughly,\n\u201cThat\u2019s just what it is for \u2018X\u2019 to have the\ncontent X\u201d. Being caused in that way is what constitutes having\nthat meaning. So, when a theory invokes this kind of constitutive\nrelation, there is this kind of constitutive explanation. So, the\nfirst premise of Bridges\u2019 argument is that causal theories\nspecify a constitutive relation between meaning and reliable causal\nconnection.\n\nBridges next observes that causal theorists typically maintain that\nthe putative fact that all \u201cX\u201ds are reliably caused by Xs\nis mediated by underlying mechanisms of one sort or another. So,\n\u201cX\u201ds might be reliably caused by dogs in part through the\nmediation of a person\u2019s visual system or auditory system.\nOne\u2019s visual apparatus might causally connect particular\npatterns of color and luminance produced by dogs to \u201cX\u201ds.\nOne might put the point somewhat differently by saying that a causal\ntheorist\u2019s hypothetical \u201cXs causes \u2018X\u2019s\u201d\nlaw is not a basic or fundamental law of nature, but an implemented\nlaw.\n\nBridges\u2019 third premise is a principle that he takes to be nearly\nself-evident, once understood. We can develop a better first-pass\nunderstanding of Bridges\u2019 argument if, at the risk of distorting\nthe argument, we consider a slightly simplified version of this\nprinciple:\n\n(S) If it is a true constitutive claim that all fs are\ngs, then it\u2019s not an implemented law that all\nfs are gs.\n\n\nTo illustrate the principle, suppose we say that gold is identical to\nthe element with atomic number 79, that all gold has atomic number 79.\nThen suppose one were to ask, \u201cWhy is it that all gold has the\natomic number 79?\u201d The answer would be, \u201cGold just\nis the element with atomic number 79.\u201d This would be a\nconstitutive explanation. According to (S), however, this constitutive\nexplanation precludes giving a further mechanistic explanation of why\ngold has atomic number 79. There is no mechanism by which gold gets\natomic number 79. Having atomic number 79 just is what makes gold\ngold.\n\nSo, here is the argument\n\nP1. It is a true constitutive claim that all \u201cX\u201ds with\ncontent X are reliably caused by Xs.\n\nP2. If it is a true constitutive claim that all \u201cX\u201ds with\ncontent X are reliably caused by Xs, then it is not an implemented law\nthat all \u201cX\u201ds with content X are reliably caused by Xs.\n\n\nTherefore, by modus ponens on P1 and P2,\n\nC1. It is not an implemented law that all \u201cX\u201ds with\ncontent X are reliably caused by Xs.\n\n\nBut, C1 contradicts the common assumption\n\nP3. It is an implemented law that all \u201cX\u201ds with content X\nare reliably caused by\n Xs.[13]\n\n\nRupert (2008) challenges the first premise of Bridges\u2019 argument\non two scores. First, he notes that claims about constitutive natures\nhave modal implications which at least some naturalistic philosophers\nhave found objectionable. Second, he claims that natural scientists do\nnot appeal to constitutive natures, so that one need not develop a\ntheory of mental content that invokes them.\n4.8 Causal Theories do not Provide a Metasemantic Theory\n\nIn discussing informational variants of causal theories, Artiga &\nSebasti\u00e1n, 2020, have proposed a new \u201cmetasemantic\u201d\nproblem. They correctly note that there is a difference between\nexplaining why \u201cX\u201d means X, rather than Y, and explaining\nwhy \u201cX\u201d has a meaning at all. Moreover, they correctly\nnote that having an answer to this first question does not necessarily\nanswer the second question. But, it is unclear that the metasemantic\nproblem is serious. If one\u2019s theory is that \u201cX\u201d\nmeans X if A, B, and C, then that would seem to provide an answer to\nthe metasemantic question. Why does \u201cX\u201d mean anything at\nall? Because \u201cX\u201d meets sufficiency conditions A, B, and C.\nIn other words, insofar as there is a question to be answered, it\nappears that all theories provide one.\n5. Concluding Remarks\n\nAlthough philosophers and cognitive scientists frequently propose to\ndispense with (one or another sort of) mental representation (cf.,\ne.g., Stich, 1983, Brooks, 1991, van Gelder, 1995, Haugeland, 1999,\nJohnson, 2007, Chemero, 2009), this is universally accepted to be a\nrevolutionary shift in thinking about minds. Short of taking on board\nsuch radical views, one will naturally want some explanation of how\nmental representations arise. In attempting such explanations, causal\ntheories have been widely perceived to have numerous attractive\nfeatures. If, for example, one use for mental representations is to\nhelp one keep track of events in the world, then some causal\nconnection between mind and world makes sense. This attractiveness has\nbeen enough to motivate new causal theories (e.g. Rupert, 1999, Usher,\n2001, and Ryder, 2004), despite the widespread recognition of serious\nchallenges to an earlier generation of theories developed by Stampe,\nDretske, Fodor, and others.\n",
    "bibliography": {
        "categories": [],
        "cat_ref_text": {
            "ref_list": [
                "Adams, F., 1979, \u201cA Goal-State Theory of Function\nAttribution,\u201d <em>Canadian Journal of Philosophy</em>, 9:\n493\u2013518.",
                "\u2013\u2013\u2013, 2003a, \u201cThoughts and their Contents:\nNaturalized Semantics,\u201d in S. Stich and T. Warfield (eds.),\n<em>The Blackwell Guide to Philosophy of Mind</em>, Oxford: Basil\nBlackwell, pp. 143\u2013171.",
                "\u2013\u2013\u2013, 2003b, \u201cThe Informational Turn in\nPhilosophy,\u201d <em>Minds and Machines</em>, 13:\n471\u2013501.",
                "Adams, F. and Aizawa, K., 1992, \u201c\u2018X\u2019 Means X:\nSemantics Fodor-Style,\u201d <em>Minds and Machines</em>, 2:\n175\u2013183.",
                "\u2013\u2013\u2013, 1994a, \u201cFodorian Semantics,\u201d in\nS. Stich and T. Warfield (eds.), <em>Mental Representations</em>,\nOxford: Basil Blackwell, pp. 223\u2013242.",
                "\u2013\u2013\u2013, 1994b, \u201c\u2018X\u2019 Means X:\nFodor/Warfield Semantics,\u201d <em>Minds and Machines</em>, 4:\n215\u2013231.",
                "Adams, F., Drebushenko, D., Fuller, G., and Stecker, R., 1990,\n\u201cNarrow Content: Fodor\u2019s Folly,\u201d <em>Mind &amp;\nLanguage</em>, 5: 213\u2013229.",
                "Adams, F. and Dietrich, L., 2004, \u201cWhat\u2019s in a(n\nEmpty) Name?,\u201d <em>Pacific Philosophical Quarterly</em>, 85:\n125\u2013148.",
                "Adams, F. and Enc, B., 1988, \u201cNot Quite by Accident,\u201d\n<em>Dialogue</em>, 27: 287\u2013297.",
                "Adams, F. and Stecker, R., 1994, \u201cVacuous Singular\nTerms,\u201d <em>Mind &amp; Language</em>, 71: 1\u201312.",
                "Agar, N., 1993. \u201c What do frogs really believe?,\u201d\n<em>Australasian Journal of Philosophy</em>, 9: 387\u2013401.",
                "Aizawa, K., 1994, \u201cLloyd\u2019s Dialectical Theory of\nRepresentation,\u201d <em>Mind &amp; Language</em>, 9:\n1\u201324.",
                "Antony, L. and Levine, J., 1991, \u201cThe Nomic and the\nRobust,\u201d in B. Loewer and G. Rey (eds.), <em>Meaning in Mind:\nFodor and His Critics</em>, Oxford: Basil Blackwell, pp.\n1\u201316.",
                "Artiga, M., &amp; Sebasti\u00e1n, M. A., 2020, \u201cInformational\nTheories of Content and Mental Representation,\u201d <i>Review of Philosophy\nand Psychology</i>, 11: 613\u201327.",
                "Baker, L., 1989, \u201cOn a Causal Theory of Content,\u201d\n<em>Philosophical Perspectives</em>, 3: 165\u2013186.",
                "\u2013\u2013\u2013, 1991, \u201cHas Content Been\nNaturalized?,\u201d in B. Loewer and G. Rey (eds.), <em>Meaning in\nMind: Fodor and His Critics</em>, Oxford: Basil Blackwell, pp.\n17\u201332.",
                "Bar-On, D., 1995, \u201c\u2018Meaning\u2019 Reconstructed:\nGrice and the Naturalizing of Semantics,\u201d <em>Pacific\nPhilosophical Quarterly</em>, 76: 83\u2013116.",
                "Boghossian, P., 1991, \u201cNaturalizing Content,\u201d in B.\nLoewer and G. Rey (eds.), <em>Meaning in Mind: Fodor and His\nCritics</em>, Oxford: Basil Blackwell, pp. 65\u201386.",
                "Bridges, J., 2006, \u201cDoes Informational Semantics Commit\nEuthyphro\u2019s Fallacy?,\u201d <em>No\u00fbs</em>, 40:\n522\u2013547.",
                "Brooks, R., 1991, \u201cIntelligence without\nRepresentation,\u201d<em>Artificial Intelligence</em>, 47:\n139\u2013159.",
                "Buras, T., 2009, \u201cAn Argument against Causal Theories of\nMental Content,\u201d <em>American Philosophical Quarterly</em>, 46:\n117\u2013129.",
                "Cain, M. J., 2009, \u201cFodor\u2019s Attempt to Naturalize\nMental Content,\u201d <em>The Philosophical Quarterly</em>, 49:\n520\u2013526.",
                "Chemero, A., 2009, <em>Radical Embodied Cognitive Science</em>,\nCambridge, MA: The MIT Press.",
                "Cummins, R., 1989, <em>Meaning and Mental Representation</em>,\nCambridge, MA: MIT/Bradford.",
                "\u2013\u2013\u2013, 1997, \u201cThe LOT of the Causal Theory\nof Mental Content,\u201d <em>Journal of Philosophy</em>, 94:\n535\u2013542.",
                "Dennett, D., 1987, \u201cReview of J. Fodor\u2019s\n<em>Psychosemantics</em>,\u201d <em>Journal of Philosophy</em>, 85:\n384\u2013389.",
                "Dretske, F., 1981, <em>Knowledge and the Flow of Information</em>,\nCambridge, MA: MIT/Bradford Press.",
                "\u2013\u2013\u2013, 1983, \u201cPrecis of <em>Knowledge and\nthe Flow of Information</em>,\u201d <em>Behavioral and Brain\nSciences</em>, 6: 55\u201363.",
                "\u2013\u2013\u2013, 1986, \u201cMisrepresentation,\u201d in\nR. Bogdan (ed.), <em>Belief</em>, Oxford: Oxford University Press, pp.\n17\u201336.",
                "\u2013\u2013\u2013, 1988, <em>Explaining Behavior: Reasons in a\nWorld of Causes</em>, Cambridge, MA: MIT/Bradford.",
                "\u2013\u2013\u2013, 1999, <em>Naturalizing the Mind</em>,\nCambridge, MA: MIT Press.",
                "En\u00e7, B., 1982, \u201cIntentional States of Mechanical\nDevices,\u201d <em>Mind</em>, 91: 161\u2013182.",
                "En\u00e7, B. and Adams, F., 1998, \u201cFunctions and\nGoal-Directedness,\u201d in C. Allen, M. Bekoff and G. Lauder (eds.),\n<em>Nature\u2019s Purposes</em>, Cambridge, MA: MIT/Bradford, pp.\n371\u2013394.",
                "Fodor, J., 1984, \u201cSemantics, Wisconsin Style,\u201d\n<em>Synthese</em>, 59: 231\u2013250. (Reprinted in Fodor,\n1990a).",
                "\u2013\u2013\u2013, 1987, <em>Psychosemantics: The Problem of\nMeaning in the Philosophy of Mind</em>, Cambridge, MA:\nMIT/Bradford.",
                "\u2013\u2013\u2013, 1990a, <em>A Theory of Content and Other\nEssays</em>, Cambridge, MA: MIT/Bradford Press.",
                "\u2013\u2013\u2013, 1990b, \u201cInformation and\nRepresentation,\u201d in P. Hanson (ed.), <em>Information, Language,\nand Cognition</em>, Vancouver: University of British Columbia Press,\npp. 175\u2013190.",
                "\u2013\u2013\u2013, 1990c, \u201cPsychosemantics or Where do\nTruth Conditions come from?,\u201d in W. Lycan (ed.), <em>Mind and\nCognition</em>, Oxford: Basil Blackwell, pp. 312\u2013337.",
                "\u2013\u2013\u2013, 1991, \u201cReplies,\u201d in B. Loewer\nand G. Rey (eds.), <em>Meaning in Mind: Fodor and His Critics</em>,\nOxford: Basil Blackwell, pp. 255\u2013319.",
                "\u2013\u2013\u2013, 1994, <em>The Elm and the Expert</em>,\nCambridge, MA: MIT/Bradford.",
                "\u2013\u2013\u2013, 1998a, <em>Concepts: Where Cognitive\nScience Went Wrong</em>, Oxford: Oxford University Press.",
                "\u2013\u2013\u2013, 1998b, <em>In Critical Condition: Polemical\nEssays on Cognitive Science and the Philosophy of Mind</em>,\nCambridge, MA: MIT/Bradford Press.",
                "Gibson, M., 1996, \u201cAsymmetric Dependencies, Ideal\nConditions, and Meaning,\u201d <em>Philosophical Psychology</em>, 9:\n235\u2013259.",
                "Godfrey-Smith, P., 1989, \u201cMisinformation,\u201d\n<em>Canadian Journal of Philosophy</em>, 19: 533\u2013550.",
                "\u2013\u2013\u2013, 1992, \u201cIndication and\nAdaptation,\u201d <em>Synthese</em>, 92: 283\u2013312.",
                "Grice, H., 1957, \u201cMeaning,\u201d <i>The Philosophical\nReview</i>, 66: 377\u201388.",
                "\u2013\u2013\u2013, 1989, <em>Studies in the Way of Words</em>,\nCambridge: Harvard University Press.",
                "Haugeland, J., 1999, \u201cMind Embodied and Embedded,\u201d in\nJ. Haugeland (ed.), <em>Having Thought</em>, pp. 207\u2013237.",
                "Horgan, T., and Tienson, J., 2002, \u201cThe Intentionality of\nPhenomenology and the Phenomenology of Intentionality,\u201d in D.\nChalmers, <em>Philosophy of Mind: Classical and Contemporary\nReadings</em>, Oxford: Oxford University Press, pp.\n520\u2013933.",
                "Johnson, M., 2007, <em>The Meaning of the Body: Aesthetics of\nHuman Understanding</em>, Chicago, IL: University of Chicago\nPress.",
                "Jones, T., Mulaire, E., and Stich, S., 1991, \u201cStaving off\nCatastrophe: A Critical Notice of Jerry Fodor\u2019s\nPsychosemantics,\u201d <em>Mind &amp; Language</em>, 6:\n58\u201382.",
                "Lloyd, D., 1987, \u201cMental Representation from the Bottom\nup,\u201d <em>Synthese</em>, 70: 23\u201378.",
                "\u2013\u2013\u2013, 1989, <em>Simple minds</em>, Cambridge, MA:\nThe MIT Press.",
                "Loar, B., 1991, \u201cCan We Explain Intentionality?,\u201d in\nB. Loewer and G. Rey (eds.), <em>Meaning in Mind: Fodor and His</em>\nCritics, Oxford: Basil Blackwell, pp. 119\u2013135.",
                "Loewer, B., 1987, \u201cFrom Information to\nIntentionality,\u201d <em>Synthese</em>, 70: 287\u2013317.",
                "Maloney, C., 1990, \u201cMental Representation,\u201d\n<em>Philosophy of Science</em>, 57: 445\u2013458.",
                "Maloney, J., 1994, \u201cContent: Covariation, Control and\nContingency,\u201d <em>Synthese</em>, 100: 241\u2013290.",
                "Manfredi, P. and Summerfield, D., 1992, \u201cRobustness without\nAsymmetry: A Flaw in Fodor\u2019s Theory of Content,\u201d\n<em>Philosophical Studies</em>, 66: 261\u2013283.",
                "McLaughlin, B. P., 1991, \u201cBelief individuation and Dretske\non naturalizing content,\u201d in B. P. McLaughlin (ed.), <em>Dretske\nand His Critics</em>, Oxford: Basil Blackwell, pp. 157\u201379.",
                "\u2013\u2013\u2013, 2016, \u201cThe Skewed View From Here:\nNormal Geometrical Misperception,\u201d <em>Philosophical\nTopics</em>, 44: 231\u201399.",
                "Mendelovici, A., 2013, \u201cReliable misrepresentation and\ntracking theories of mental representation,\u201d <em>Philosophical\nStudies</em>, 165: 421\u2013443.",
                "\u2013\u2013\u2013, 2016, \u201cWhy tracking theories should\nallow for clean cases of reliable misrepresentation,\u201d\n<em>Disputatio</em>, 8: 57\u201392.",
                "Millikan, R., 1984, <em>Language, Thought and Other Biological\nCategories</em>, Cambridge, MA: MIT Press.",
                "\u2013\u2013\u2013, 1989, \u201cBiosemantics,\u201d\n<em>Journal of Philosophy</em>, 86: 281\u201397.",
                "\u2013\u2013\u2013, 2001, \u201cWhat Has Natural Information\nto Do with Intentional Representation?,\u201d in D. M. Walsh (ed.),\n<em>Naturalism, Evolution and Mind</em>, Cambridge: Cambridge\nUniversity Press, pp. 105\u2013125.",
                "Neander, K., 1995, \u201cMisrepresenting and\nMalfunctioning,\u201d <em>Philosophical Studies</em>, 79:\n109\u2013141.",
                "\u2013\u2013\u2013, 1996, \u201cDretske\u2019s Innate\nModesty,\u201d <em>Australasian Journal of Philosophy</em>, 74:\n258\u2013274.",
                "Papineau, D., 1984, \u201cRepresentation and Explanation,\u201d\n<em>Philosophy of Science</em>, 51: 550\u201372.",
                "\u2013\u2013\u2013, 1998, \u201cTeleosemantics and\nIndeterminacy,\u201d <em>Australasian Journal of Philosophy</em>, 76:\n1\u201314.",
                "Pineda, D., 1998, \u201cInformation and Content,\u201d\n<em>Philosophical Issues</em>, 9: 381\u2013387.",
                "Possin, K., 1988, \u201cSticky Problems with Stampe on\nRepresentations,\u201d <em>Australasian Journal of Philosophy</em>,\n66: 75\u201382.",
                "Price, C., 1998, \u201cDeterminate functions,\u201d\n<em>No\u00fbs, 32</em>: 54\u201375.",
                "Rupert, R., 1999, \u201cThe Best Test Theory of Extension: First\nPrinciple(s),\u201d <em>Mind &amp; Language</em>, 14:\n321\u2013355.",
                "\u2013\u2013\u2013, 2001, \u201cCoining Terms in the Language\nof Thought: Innateness, Emergence, and the Lot of Cummins\u2019s\nArgument against the Causal Theory of Mental Content,\u201d\n<em>Journal of Philosophy</em>, 98: 499\u2013530.",
                "\u2013\u2013\u2013, 2008, \u201cCausal Theories of Mental\nContent,\u201d <em>Philosophy Compass</em>, 3: 353\u201380.",
                "Ryder, D., 2004, \u201cSINBAD Neurosemantics: A Theory of Mental\nRepresentation,\u201d <em>Mind &amp; Language</em>, 19:\n211\u2013240.",
                "Schulte, P., 2012, \u201cHow Frogs See the World: Putting\nMillikan\u2019s Teleosemantics to the Test,\u201d\n<i>Philosophia</i>, 40: 483\u201396.",
                "\u2013\u2013\u2013, 2015, \u201cPerceptual Representations: A\nTeleosemantic Answer to the Breadth-of-Application Problem,\u201d\n<i>Biology &amp; Philosophy</i>, 30: 119\u201336.",
                "\u2013\u2013\u2013, 2018, \u201cPerceiving the world outside:\nHow to solve the distality problem for informational\nteleosemantics,\u201d <i>The Philosophical Quarterly</i>, 68:\n349\u201369.",
                "Skyrms, B., 2008, \u201cSignals,\u201d <em>Philosophy of\nScience</em>, 75: 489\u2013500.",
                "\u2013\u2013\u2013, 2010a, <em>Signals: Evolution, Learning,\nand Information</em>, Oxford: Oxford University Press",
                "\u2013\u2013\u2013, 2010b, \u201cThe flow of information in\nsignaling games,\u201d <em>Philosophical Studies</em>, 147:\n155\u201365.",
                "\u2013\u2013\u2013, 2012, \u201cLearning to signal with probe\nand adjust,\u201d <em>Episteme</em>, 9: 139\u201350.",
                "Stampe, D., 1975, \u201cShow and Tell,\u201d in B. Freed, A.\nMarras, and P. Maynard (eds.), <em>Forms of Representation</em>,\nAmsterdam: North-Holland, pp. 221\u2013245.",
                "\u2013\u2013\u2013, 1977, \u201cToward a Causal Theory of\nLinguistic Representation,\u201d in P. French, H. K. Wettstein, and\nT. E. Uehling (eds.), <em>Midwest Studies in Philosophy</em>, vol. 2,\nMinneapolis: University of Minnesota Press, pp. 42\u201363.",
                "\u2013\u2013\u2013, 1986, \u201cVerification and a Causal\nAccount of Meaning,\u201d <em>Synthese</em>, 69: 107\u2013137.",
                "\u2013\u2013\u2013, 1990, \u201cContent, Context, and\nExplanation,\u201d in E. Villanueva, <em>Information, Semantics, and\nEpistemology</em>, Oxford: Blackwell, pp. 134\u2013152.",
                "Stegmann, U. E., 2005, \u201cJohn Maynard Smith\u2019s notion of\nanimal signals,\u201d <em>Biology and Philosophy</em>, 20:\n1011\u201325.",
                "\u2013\u2013\u2013, 2009, \u201cA consumer-based\nteleosemantics for animal signals,\u201d <em>Philosophy of\nScience</em>, 76: 864\u201375.",
                "Sterelny, K., 1990, <em>The Representational Theory of Mind</em>,\nOxford: Blackwell.",
                "Stich, S., 1983, <em>From Folk Psychology to Cognitive\nScience</em>, Cambridge, MA: The MIT Press.",
                "Sturdee, D., 1997, \u201cThe Semantic Shuffle: Shifting Emphasis\nin Dretske\u2019s Account of Representational Content,\u201d\n<em>Erkenntnis, 47</em>: 89\u2013103.",
                "Tye, M., 1995, <em>Ten Problems of Consciousness: A\nRepresentational Theory of Mind</em>, Cambridge, MA: MIT Press.",
                "Usher, M., 2001, \u201cA Statistical Referential Theory of\nContent: Using Information Theory to Account for\nMisrepresentation,\u201d <em>Mind and Language</em>, 16:\n311\u2013334.",
                "\u2013\u2013\u2013, 2004, \u201cComment on Ryder\u2019s\nSINBAD Neurosemantics: Is Teleofunction Isomorphism the Way to\nUnderstand Representations?,\u201d <em>Mind and Language</em>, 19:\n241\u2013248.",
                "Van Gelder, T. 1995, \u201cWhat Might Cognition Be, If not\nComputation?,\u201d <em>The Journal of Philosophy</em>, 91:\n345\u2013381.",
                "Wallis, C., 1994, \u201cRepresentation and the Imperfect\nIdeal,\u201d <em>Philosophy of Science</em>, 61: 407\u2013428.",
                "\u2013\u2013\u2013, 1995, \u201cAsymmetrical Dependence,\nRepresentation, and Cognitive Science,\u201d <em>The Southern Journal\nof Philosophy</em>, 33: 373\u2013401.",
                "Warfield, T., 1994, \u201cFodorian Semantics: A Reply to Adams\nand Aizawa,\u201d <em>Minds and Machines</em>, 4: 205\u2013214.",
                "Wright, L., 1973, \u201cFunctions,\u201d <em>Philosophical\nReview</em>, 82: 139\u2013168."
            ]
        },
        "raw_text": "<div id=\"bibliography\">\n<h2><a name=\"Bib\">Bibliography</a></h2>\n<ul class=\"hanging\">\n<li>Adams, F., 1979, \u201cA Goal-State Theory of Function\nAttribution,\u201d <em>Canadian Journal of Philosophy</em>, 9:\n493\u2013518.</li>\n<li>\u2013\u2013\u2013, 2003a, \u201cThoughts and their Contents:\nNaturalized Semantics,\u201d in S. Stich and T. Warfield (eds.),\n<em>The Blackwell Guide to Philosophy of Mind</em>, Oxford: Basil\nBlackwell, pp. 143\u2013171.</li>\n<li>\u2013\u2013\u2013, 2003b, \u201cThe Informational Turn in\nPhilosophy,\u201d <em>Minds and Machines</em>, 13:\n471\u2013501.</li>\n<li>Adams, F. and Aizawa, K., 1992, \u201c\u2018X\u2019 Means X:\nSemantics Fodor-Style,\u201d <em>Minds and Machines</em>, 2:\n175\u2013183.</li>\n<li>\u2013\u2013\u2013, 1994a, \u201cFodorian Semantics,\u201d in\nS. Stich and T. Warfield (eds.), <em>Mental Representations</em>,\nOxford: Basil Blackwell, pp. 223\u2013242.</li>\n<li>\u2013\u2013\u2013, 1994b, \u201c\u2018X\u2019 Means X:\nFodor/Warfield Semantics,\u201d <em>Minds and Machines</em>, 4:\n215\u2013231.</li>\n<li>Adams, F., Drebushenko, D., Fuller, G., and Stecker, R., 1990,\n\u201cNarrow Content: Fodor\u2019s Folly,\u201d <em>Mind &amp;\nLanguage</em>, 5: 213\u2013229.</li>\n<li>Adams, F. and Dietrich, L., 2004, \u201cWhat\u2019s in a(n\nEmpty) Name?,\u201d <em>Pacific Philosophical Quarterly</em>, 85:\n125\u2013148.</li>\n<li>Adams, F. and Enc, B., 1988, \u201cNot Quite by Accident,\u201d\n<em>Dialogue</em>, 27: 287\u2013297.</li>\n<li>Adams, F. and Stecker, R., 1994, \u201cVacuous Singular\nTerms,\u201d <em>Mind &amp; Language</em>, 71: 1\u201312.</li>\n<li>Agar, N., 1993. \u201c What do frogs really believe?,\u201d\n<em>Australasian Journal of Philosophy</em>, 9: 387\u2013401.</li>\n<li>Aizawa, K., 1994, \u201cLloyd\u2019s Dialectical Theory of\nRepresentation,\u201d <em>Mind &amp; Language</em>, 9:\n1\u201324.</li>\n<li>Antony, L. and Levine, J., 1991, \u201cThe Nomic and the\nRobust,\u201d in B. Loewer and G. Rey (eds.), <em>Meaning in Mind:\nFodor and His Critics</em>, Oxford: Basil Blackwell, pp.\n1\u201316.</li>\n<li>Artiga, M., &amp; Sebasti\u00e1n, M. A., 2020, \u201cInformational\nTheories of Content and Mental Representation,\u201d <i>Review of Philosophy\nand Psychology</i>, 11: 613\u201327.</li>\n<li>Baker, L., 1989, \u201cOn a Causal Theory of Content,\u201d\n<em>Philosophical Perspectives</em>, 3: 165\u2013186.</li>\n<li>\u2013\u2013\u2013, 1991, \u201cHas Content Been\nNaturalized?,\u201d in B. Loewer and G. Rey (eds.), <em>Meaning in\nMind: Fodor and His Critics</em>, Oxford: Basil Blackwell, pp.\n17\u201332.</li>\n<li>Bar-On, D., 1995, \u201c\u2018Meaning\u2019 Reconstructed:\nGrice and the Naturalizing of Semantics,\u201d <em>Pacific\nPhilosophical Quarterly</em>, 76: 83\u2013116.</li>\n<li>Boghossian, P., 1991, \u201cNaturalizing Content,\u201d in B.\nLoewer and G. Rey (eds.), <em>Meaning in Mind: Fodor and His\nCritics</em>, Oxford: Basil Blackwell, pp. 65\u201386.</li>\n<li>Bridges, J., 2006, \u201cDoes Informational Semantics Commit\nEuthyphro\u2019s Fallacy?,\u201d <em>No\u00fbs</em>, 40:\n522\u2013547.</li>\n<li>Brooks, R., 1991, \u201cIntelligence without\nRepresentation,\u201d<em>Artificial Intelligence</em>, 47:\n139\u2013159.</li>\n<li>Buras, T., 2009, \u201cAn Argument against Causal Theories of\nMental Content,\u201d <em>American Philosophical Quarterly</em>, 46:\n117\u2013129.</li>\n<li>Cain, M. J., 2009, \u201cFodor\u2019s Attempt to Naturalize\nMental Content,\u201d <em>The Philosophical Quarterly</em>, 49:\n520\u2013526.</li>\n<li>Chemero, A., 2009, <em>Radical Embodied Cognitive Science</em>,\nCambridge, MA: The MIT Press.</li>\n<li>Cummins, R., 1989, <em>Meaning and Mental Representation</em>,\nCambridge, MA: MIT/Bradford.</li>\n<li>\u2013\u2013\u2013, 1997, \u201cThe LOT of the Causal Theory\nof Mental Content,\u201d <em>Journal of Philosophy</em>, 94:\n535\u2013542.</li>\n<li>Dennett, D., 1987, \u201cReview of J. Fodor\u2019s\n<em>Psychosemantics</em>,\u201d <em>Journal of Philosophy</em>, 85:\n384\u2013389.</li>\n<li>Dretske, F., 1981, <em>Knowledge and the Flow of Information</em>,\nCambridge, MA: MIT/Bradford Press.</li>\n<li>\u2013\u2013\u2013, 1983, \u201cPrecis of <em>Knowledge and\nthe Flow of Information</em>,\u201d <em>Behavioral and Brain\nSciences</em>, 6: 55\u201363.</li>\n<li>\u2013\u2013\u2013, 1986, \u201cMisrepresentation,\u201d in\nR. Bogdan (ed.), <em>Belief</em>, Oxford: Oxford University Press, pp.\n17\u201336.</li>\n<li>\u2013\u2013\u2013, 1988, <em>Explaining Behavior: Reasons in a\nWorld of Causes</em>, Cambridge, MA: MIT/Bradford.</li>\n<li>\u2013\u2013\u2013, 1999, <em>Naturalizing the Mind</em>,\nCambridge, MA: MIT Press.</li>\n<li>En\u00e7, B., 1982, \u201cIntentional States of Mechanical\nDevices,\u201d <em>Mind</em>, 91: 161\u2013182.</li>\n<li>En\u00e7, B. and Adams, F., 1998, \u201cFunctions and\nGoal-Directedness,\u201d in C. Allen, M. Bekoff and G. Lauder (eds.),\n<em>Nature\u2019s Purposes</em>, Cambridge, MA: MIT/Bradford, pp.\n371\u2013394.</li>\n<li>Fodor, J., 1984, \u201cSemantics, Wisconsin Style,\u201d\n<em>Synthese</em>, 59: 231\u2013250. (Reprinted in Fodor,\n1990a).</li>\n<li>\u2013\u2013\u2013, 1987, <em>Psychosemantics: The Problem of\nMeaning in the Philosophy of Mind</em>, Cambridge, MA:\nMIT/Bradford.</li>\n<li>\u2013\u2013\u2013, 1990a, <em>A Theory of Content and Other\nEssays</em>, Cambridge, MA: MIT/Bradford Press.</li>\n<li>\u2013\u2013\u2013, 1990b, \u201cInformation and\nRepresentation,\u201d in P. Hanson (ed.), <em>Information, Language,\nand Cognition</em>, Vancouver: University of British Columbia Press,\npp. 175\u2013190.</li>\n<li>\u2013\u2013\u2013, 1990c, \u201cPsychosemantics or Where do\nTruth Conditions come from?,\u201d in W. Lycan (ed.), <em>Mind and\nCognition</em>, Oxford: Basil Blackwell, pp. 312\u2013337.</li>\n<li>\u2013\u2013\u2013, 1991, \u201cReplies,\u201d in B. Loewer\nand G. Rey (eds.), <em>Meaning in Mind: Fodor and His Critics</em>,\nOxford: Basil Blackwell, pp. 255\u2013319.</li>\n<li>\u2013\u2013\u2013, 1994, <em>The Elm and the Expert</em>,\nCambridge, MA: MIT/Bradford.</li>\n<li>\u2013\u2013\u2013, 1998a, <em>Concepts: Where Cognitive\nScience Went Wrong</em>, Oxford: Oxford University Press.</li>\n<li>\u2013\u2013\u2013, 1998b, <em>In Critical Condition: Polemical\nEssays on Cognitive Science and the Philosophy of Mind</em>,\nCambridge, MA: MIT/Bradford Press.</li>\n<li>Gibson, M., 1996, \u201cAsymmetric Dependencies, Ideal\nConditions, and Meaning,\u201d <em>Philosophical Psychology</em>, 9:\n235\u2013259.</li>\n<li>Godfrey-Smith, P., 1989, \u201cMisinformation,\u201d\n<em>Canadian Journal of Philosophy</em>, 19: 533\u2013550.</li>\n<li>\u2013\u2013\u2013, 1992, \u201cIndication and\nAdaptation,\u201d <em>Synthese</em>, 92: 283\u2013312.</li>\n<li>Grice, H., 1957, \u201cMeaning,\u201d <i>The Philosophical\nReview</i>, 66: 377\u201388.</li>\n<li>\u2013\u2013\u2013, 1989, <em>Studies in the Way of Words</em>,\nCambridge: Harvard University Press.</li>\n<li>Haugeland, J., 1999, \u201cMind Embodied and Embedded,\u201d in\nJ. Haugeland (ed.), <em>Having Thought</em>, pp. 207\u2013237.</li>\n<li>Horgan, T., and Tienson, J., 2002, \u201cThe Intentionality of\nPhenomenology and the Phenomenology of Intentionality,\u201d in D.\nChalmers, <em>Philosophy of Mind: Classical and Contemporary\nReadings</em>, Oxford: Oxford University Press, pp.\n520\u2013933.</li>\n<li>Johnson, M., 2007, <em>The Meaning of the Body: Aesthetics of\nHuman Understanding</em>, Chicago, IL: University of Chicago\nPress.</li>\n<li>Jones, T., Mulaire, E., and Stich, S., 1991, \u201cStaving off\nCatastrophe: A Critical Notice of Jerry Fodor\u2019s\nPsychosemantics,\u201d <em>Mind &amp; Language</em>, 6:\n58\u201382.</li>\n<li>Lloyd, D., 1987, \u201cMental Representation from the Bottom\nup,\u201d <em>Synthese</em>, 70: 23\u201378.</li>\n<li>\u2013\u2013\u2013, 1989, <em>Simple minds</em>, Cambridge, MA:\nThe MIT Press.</li>\n<li>Loar, B., 1991, \u201cCan We Explain Intentionality?,\u201d in\nB. Loewer and G. Rey (eds.), <em>Meaning in Mind: Fodor and His</em>\nCritics, Oxford: Basil Blackwell, pp. 119\u2013135.</li>\n<li>Loewer, B., 1987, \u201cFrom Information to\nIntentionality,\u201d <em>Synthese</em>, 70: 287\u2013317.</li>\n<li>Maloney, C., 1990, \u201cMental Representation,\u201d\n<em>Philosophy of Science</em>, 57: 445\u2013458.</li>\n<li>Maloney, J., 1994, \u201cContent: Covariation, Control and\nContingency,\u201d <em>Synthese</em>, 100: 241\u2013290.</li>\n<li>Manfredi, P. and Summerfield, D., 1992, \u201cRobustness without\nAsymmetry: A Flaw in Fodor\u2019s Theory of Content,\u201d\n<em>Philosophical Studies</em>, 66: 261\u2013283.</li>\n<li>McLaughlin, B. P., 1991, \u201cBelief individuation and Dretske\non naturalizing content,\u201d in B. P. McLaughlin (ed.), <em>Dretske\nand His Critics</em>, Oxford: Basil Blackwell, pp. 157\u201379.</li>\n<li>\u2013\u2013\u2013, 2016, \u201cThe Skewed View From Here:\nNormal Geometrical Misperception,\u201d <em>Philosophical\nTopics</em>, 44: 231\u201399.</li>\n<li>Mendelovici, A., 2013, \u201cReliable misrepresentation and\ntracking theories of mental representation,\u201d <em>Philosophical\nStudies</em>, 165: 421\u2013443.</li>\n<li>\u2013\u2013\u2013, 2016, \u201cWhy tracking theories should\nallow for clean cases of reliable misrepresentation,\u201d\n<em>Disputatio</em>, 8: 57\u201392.</li>\n<li>Millikan, R., 1984, <em>Language, Thought and Other Biological\nCategories</em>, Cambridge, MA: MIT Press.</li>\n<li>\u2013\u2013\u2013, 1989, \u201cBiosemantics,\u201d\n<em>Journal of Philosophy</em>, 86: 281\u201397.</li>\n<li>\u2013\u2013\u2013, 2001, \u201cWhat Has Natural Information\nto Do with Intentional Representation?,\u201d in D. M. Walsh (ed.),\n<em>Naturalism, Evolution and Mind</em>, Cambridge: Cambridge\nUniversity Press, pp. 105\u2013125.</li>\n<li>Neander, K., 1995, \u201cMisrepresenting and\nMalfunctioning,\u201d <em>Philosophical Studies</em>, 79:\n109\u2013141.</li>\n<li>\u2013\u2013\u2013, 1996, \u201cDretske\u2019s Innate\nModesty,\u201d <em>Australasian Journal of Philosophy</em>, 74:\n258\u2013274.</li>\n<li>Papineau, D., 1984, \u201cRepresentation and Explanation,\u201d\n<em>Philosophy of Science</em>, 51: 550\u201372.</li>\n<li>\u2013\u2013\u2013, 1998, \u201cTeleosemantics and\nIndeterminacy,\u201d <em>Australasian Journal of Philosophy</em>, 76:\n1\u201314.</li>\n<li>Pineda, D., 1998, \u201cInformation and Content,\u201d\n<em>Philosophical Issues</em>, 9: 381\u2013387.</li>\n<li>Possin, K., 1988, \u201cSticky Problems with Stampe on\nRepresentations,\u201d <em>Australasian Journal of Philosophy</em>,\n66: 75\u201382.</li>\n<li>Price, C., 1998, \u201cDeterminate functions,\u201d\n<em>No\u00fbs, 32</em>: 54\u201375.</li>\n<li>Rupert, R., 1999, \u201cThe Best Test Theory of Extension: First\nPrinciple(s),\u201d <em>Mind &amp; Language</em>, 14:\n321\u2013355.</li>\n<li>\u2013\u2013\u2013, 2001, \u201cCoining Terms in the Language\nof Thought: Innateness, Emergence, and the Lot of Cummins\u2019s\nArgument against the Causal Theory of Mental Content,\u201d\n<em>Journal of Philosophy</em>, 98: 499\u2013530.</li>\n<li>\u2013\u2013\u2013, 2008, \u201cCausal Theories of Mental\nContent,\u201d <em>Philosophy Compass</em>, 3: 353\u201380.</li>\n<li>Ryder, D., 2004, \u201cSINBAD Neurosemantics: A Theory of Mental\nRepresentation,\u201d <em>Mind &amp; Language</em>, 19:\n211\u2013240.</li>\n<li>Schulte, P., 2012, \u201cHow Frogs See the World: Putting\nMillikan\u2019s Teleosemantics to the Test,\u201d\n<i>Philosophia</i>, 40: 483\u201396.</li>\n<li>\u2013\u2013\u2013, 2015, \u201cPerceptual Representations: A\nTeleosemantic Answer to the Breadth-of-Application Problem,\u201d\n<i>Biology &amp; Philosophy</i>, 30: 119\u201336.</li>\n<li>\u2013\u2013\u2013, 2018, \u201cPerceiving the world outside:\nHow to solve the distality problem for informational\nteleosemantics,\u201d <i>The Philosophical Quarterly</i>, 68:\n349\u201369.</li>\n<li>Skyrms, B., 2008, \u201cSignals,\u201d <em>Philosophy of\nScience</em>, 75: 489\u2013500.</li>\n<li>\u2013\u2013\u2013, 2010a, <em>Signals: Evolution, Learning,\nand Information</em>, Oxford: Oxford University Press</li>\n<li>\u2013\u2013\u2013, 2010b, \u201cThe flow of information in\nsignaling games,\u201d <em>Philosophical Studies</em>, 147:\n155\u201365.</li>\n<li>\u2013\u2013\u2013, 2012, \u201cLearning to signal with probe\nand adjust,\u201d <em>Episteme</em>, 9: 139\u201350.</li>\n<li>Stampe, D., 1975, \u201cShow and Tell,\u201d in B. Freed, A.\nMarras, and P. Maynard (eds.), <em>Forms of Representation</em>,\nAmsterdam: North-Holland, pp. 221\u2013245.</li>\n<li>\u2013\u2013\u2013, 1977, \u201cToward a Causal Theory of\nLinguistic Representation,\u201d in P. French, H. K. Wettstein, and\nT. E. Uehling (eds.), <em>Midwest Studies in Philosophy</em>, vol. 2,\nMinneapolis: University of Minnesota Press, pp. 42\u201363.</li>\n<li>\u2013\u2013\u2013, 1986, \u201cVerification and a Causal\nAccount of Meaning,\u201d <em>Synthese</em>, 69: 107\u2013137.</li>\n<li>\u2013\u2013\u2013, 1990, \u201cContent, Context, and\nExplanation,\u201d in E. Villanueva, <em>Information, Semantics, and\nEpistemology</em>, Oxford: Blackwell, pp. 134\u2013152.</li>\n<li>Stegmann, U. E., 2005, \u201cJohn Maynard Smith\u2019s notion of\nanimal signals,\u201d <em>Biology and Philosophy</em>, 20:\n1011\u201325.</li>\n<li>\u2013\u2013\u2013, 2009, \u201cA consumer-based\nteleosemantics for animal signals,\u201d <em>Philosophy of\nScience</em>, 76: 864\u201375.</li>\n<li>Sterelny, K., 1990, <em>The Representational Theory of Mind</em>,\nOxford: Blackwell.</li>\n<li>Stich, S., 1983, <em>From Folk Psychology to Cognitive\nScience</em>, Cambridge, MA: The MIT Press.</li>\n<li>Sturdee, D., 1997, \u201cThe Semantic Shuffle: Shifting Emphasis\nin Dretske\u2019s Account of Representational Content,\u201d\n<em>Erkenntnis, 47</em>: 89\u2013103.</li>\n<li>Tye, M., 1995, <em>Ten Problems of Consciousness: A\nRepresentational Theory of Mind</em>, Cambridge, MA: MIT Press.</li>\n<li>Usher, M., 2001, \u201cA Statistical Referential Theory of\nContent: Using Information Theory to Account for\nMisrepresentation,\u201d <em>Mind and Language</em>, 16:\n311\u2013334.</li>\n<li>\u2013\u2013\u2013, 2004, \u201cComment on Ryder\u2019s\nSINBAD Neurosemantics: Is Teleofunction Isomorphism the Way to\nUnderstand Representations?,\u201d <em>Mind and Language</em>, 19:\n241\u2013248.</li>\n<li>Van Gelder, T. 1995, \u201cWhat Might Cognition Be, If not\nComputation?,\u201d <em>The Journal of Philosophy</em>, 91:\n345\u2013381.</li>\n<li>Wallis, C., 1994, \u201cRepresentation and the Imperfect\nIdeal,\u201d <em>Philosophy of Science</em>, 61: 407\u2013428.</li>\n<li>\u2013\u2013\u2013, 1995, \u201cAsymmetrical Dependence,\nRepresentation, and Cognitive Science,\u201d <em>The Southern Journal\nof Philosophy</em>, 33: 373\u2013401.</li>\n<li>Warfield, T., 1994, \u201cFodorian Semantics: A Reply to Adams\nand Aizawa,\u201d <em>Minds and Machines</em>, 4: 205\u2013214.</li>\n<li>Wright, L., 1973, \u201cFunctions,\u201d <em>Philosophical\nReview</em>, 82: 139\u2013168.</li>\n</ul>\n</div>"
    },
    "related_entries": {
        "entry_list": [
            "consciousness: representational theories of",
            "externalism about the mind",
            "intentionality",
            "language of thought hypothesis",
            "meaning, theories of",
            "mental content: narrow",
            "mental content: nonconceptual",
            "mental content: teleological theories of"
        ],
        "entry_link": [
            {
                "../consciousness-representational/": "consciousness: representational theories of"
            },
            {
                "../content-externalism/": "externalism about the mind"
            },
            {
                "../intentionality/": "intentionality"
            },
            {
                "../language-thought/": "language of thought hypothesis"
            },
            {
                "../meaning/": "meaning, theories of"
            },
            {
                "../content-narrow/": "mental content: narrow"
            },
            {
                "../content-nonconceptual/": "mental content: nonconceptual"
            },
            {
                "../content-teleological/": "mental content: teleological theories of"
            }
        ]
    },
    "academic_tools": {
        "listed_text": [
            "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=content-causal\" target=\"other\">How to cite this entry</a>.",
            "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://leibniz.stanford.edu/friends/preview/content-causal/\" target=\"other\">Preview the PDF version of this entry</a> at the\n <a href=\"https://leibniz.stanford.edu/friends/\" target=\"other\">Friends of the SEP Society</a>.",
            "<img alt=\"inpho icon\" src=\"../../symbols/inpho.png\"/>",
            "<a href=\"https://www.inphoproject.org/entity?sep=content-causal&amp;redirect=True\" target=\"other\">Look up topics and thinkers related to this entry</a>\n at the Internet Philosophy Ontology Project (InPhO).",
            "<img alt=\"phil papers icon\" src=\"../../symbols/pp.gif\"/>",
            "<a href=\"https://philpapers.org/sep/content-causal/\" target=\"other\">Enhanced bibliography for this entry</a>\nat <a href=\"https://philpapers.org/\" target=\"other\">PhilPapers</a>, with links to its database."
        ],
        "listed_links": [
            {
                "https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=content-causal": "How to cite this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/preview/content-causal/": "Preview the PDF version of this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/": "Friends of the SEP Society"
            },
            {
                "https://www.inphoproject.org/entity?sep=content-causal&redirect=True": "Look up topics and thinkers related to this entry"
            },
            {
                "https://philpapers.org/sep/content-causal/": "Enhanced bibliography for this entry"
            },
            {
                "https://philpapers.org/": "PhilPapers"
            }
        ]
    },
    "other_internet_resources": {
        "listed_text": [],
        "listed_links": []
    }
}