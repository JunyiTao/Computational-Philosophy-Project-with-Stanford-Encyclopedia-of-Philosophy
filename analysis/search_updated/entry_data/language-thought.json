{
    "url": "language-thought",
    "title": "The Language of Thought Hypothesis",
    "authorship": {
        "year": "Copyright \u00a9 2023",
        "author_text": "Michael Rescorla\n<rescorla@ucla.edu>",
        "author_links": [
            {
                "mailto:rescorla%40ucla%2eedu": "rescorla@ucla.edu"
            }
        ],
        "raw_html": "<div id=\"article-copyright\">\n<p>\n<a href=\"../../info.html#c\">Copyright \u00a9 2023</a> by\n\n<br/>\nMichael Rescorla\n&lt;<a href=\"mailto:rescorla%40ucla%2eedu\"><em>rescorla<abbr title=\" at \">@</abbr>ucla<abbr title=\" dot \">.</abbr>edu</em></a>&gt;\n    </p>\n</div>"
    },
    "pubinfo": [
        "First published Tue May 28, 2019",
        "substantive revision Mon Oct 16, 2023"
    ],
    "preamble": "\n\nThe language of thought hypothesis (LOTH) proposes that\nthinking occurs in a mental language. Often called Mentalese,\nthe mental language resembles spoken language in several key respects:\nit contains words that can combine into sentences; the words and\nsentences are meaningful; and each sentence\u2019s meaning depends in\na systematic way upon the meanings of its component words and the way\nthose words are combined. For example, there is a Mentalese\nword whale that denotes whales, and there is a\nMentalese word mammal that denotes\nmammals. These words can combine into a Mentalese\nsentence whales are mammals, which means that\nwhales are mammals. To believe that whales are mammals is to bear an\nappropriate psychological relation to this sentence. During a\nprototypical deductive inference, I might transform the Mentalese\nsentence whales are mammals and the Mentalese\nsentence Moby Dick is a whale into the\nMentalese sentence Moby Dick is a mammal. As I\nexecute the inference, I enter into a succession of mental states that\ninstantiate those sentences.\n\nLOTH emerged gradually through the writings of Augustine, Boethius,\nThomas Aquinas, John Duns Scotus, and many others. William of Ockham\noffered the first systematic treatment in his Summa Logicae\n(c. 1323), which meticulously analyzed the meaning and structure of\nMentalese expressions. LOTH was quite popular during the late medieval\nera, but it slipped from view in the sixteenth and seventeenth\ncenturies. From that point through the mid-twentieth century, it\nplayed little serious role within theorizing about the mind.\n\nIn the 1970s, LOTH underwent a dramatic revival. The watershed was\npublication of Jerry Fodor\u2019s The Language of Thought\n(1975). Fodor argued abductively: our current best scientific theories\nof psychological activity postulate Mentalese; we therefore have good\nreason to accept that Mentalese exists. Fodor\u2019s analysis exerted\ntremendous impact. LOTH once again became a focus of discussion, some\nsupportive and some critical. Debates over the existence and nature of\nMentalese continue to figure prominently within philosophy and\ncognitive science. These debates have pivotal importance for our\nunderstanding of how the mind works.\n",
    "toc": [
        {
            "#MentLang": "1. Mental Language"
        },
        {
            "#ReprTheoThou": "1.1 The Representational Theory of Thought"
        },
        {
            "#CompSema": "1.2 Compositional Semantics"
        },
        {
            "#LogiStru": "1.3 Logical Structure"
        },
        {
            "#ScopLOTH": "2. Scope of LOTH"
        },
        {
            "#MentComp": "3. Mental Computation"
        },
        {
            "#ArguForLOTH": "4. Arguments for LOTH"
        },
        {
            "#ArguCognSciePrac": "4.1 Argument from Cognitive Science Practice"
        },
        {
            "#ArguProdThou": "4.2 Argument from the Productivity of Thought"
        },
        {
            "#ArguSystThou": "4.3 Argument from the Systematicity of Thought"
        },
        {
            "#ArguSystThin": "4.4 Argument from the Systematicity of Thinking"
        },
        {
            "#ConnChal": "5. The Connectionist Challenge"
        },
        {
            "#RegrObjeLOTH": "6. Regress Objections to LOTH"
        },
        {
            "#LearLang": "6.1 Learning a Language"
        },
        {
            "#UndeLang": "6.2 Understanding a Language"
        },
        {
            "#NatuMind": "7. Naturalizing the Mind"
        },
        {
            "#IndiMentExpr": "8. Individuation of Mentalese Expressions"
        },
        {
            "#Bib": "Bibliography"
        },
        {
            "#Aca": "Academic Tools"
        },
        {
            "#Oth": "Other Internet Resources"
        },
        {
            "#Rel": "Related Entries"
        }
    ],
    "main_text": "\n1. Mental Language\n\nWhat does it mean to posit a mental language? Or to say that thinking\noccurs in this language? Just how \u201clanguage-like\u201d is\nMentalese supposed to be? To address these questions, we will isolate\nsome core commitments that are widely shared among LOT theorists.\n1.1 The Representational Theory of Thought\n\nFolk psychology routinely explains and predicts behavior by citing\nmental states, including beliefs, desires, intentions, fears, hopes,\nand so on. To explain why Mary walked to the refrigerator, we might\nnote that she believed there was orange juice in the refrigerator and\nwanted to drink orange juice. Mental states such as belief and desire\nare called propositional attitudes. They can be specified\nusing locutions of the form\n\n\nX believes that p.\n\nX desires that p.\n\nX intends that p.\n\nX fears that p.\n\netc.\n\n\nBy replacing \u201cp\u201d with a sentence, we specify the\ncontent of X\u2019s mental state. Propositional attitudes have\n intentionality\n or aboutness: they are about a subject matter. For that\nreason, they are often called intentional states.\n\nThe term \u201cpropositional attitude\u201d originates with Russell\n(1918\u20131919 [1985]) and reflects his own preferred analysis: that\npropositional attitudes are relations to\n propositions.\n A proposition is an abstract entity that determines a\ntruth-condition. To illustrate, suppose John believes that\nParis is north of London. Then John\u2019s belief is a relation to\nthe proposition that Paris is north of London, and this\nproposition is true iff Paris is north of London. Beyond the thesis\nthat propositions determine truth-conditions, there is little\nagreement about what propositions are like. The literature offers many\noptions, mainly derived from theories of Frege (1892 [1997]), Russell\n(1918\u20131919 [1985]), and Wittgenstein (1921 [1922]).\n\nFodor (1981: 177\u2013203; 1987: 16\u201326) proposes a theory of\npropositional attitudes that assigns a central role to mental\nrepresentations. A\n mental representation\n is a mental item with semantic properties (such as a\ndenotation, or a meaning, or a truth-condition, etc.). To believe that\np, or hope that p, or intend that p, is to bear\nan appropriate relation to a mental representation whose meaning is\nthat p. For example, there is a relation belief* between\nthinkers and mental representations, where the following biconditional\nis true no matter what English sentence one substitutes for\n\u201cp\u201d:\n\n\nX believes that p iff there is a mental representation\nS such that X believes* S and S means that\np.\n\n\nMore generally:\n\n(1) Each propositional\nattitude A corresponds to a unique psychological relation\nA*, where the following biconditional is true no matter what\nsentence one substitutes for \u201cp\u201d: X\nAs that p iff there is a mental representation\nS such that X bears A* to S and S\nmeans that p.\n\n\nOn this analysis, mental representations are the most direct objects\nof propositional attitudes. A propositional attitude inherits its\nsemantic properties, including its truth-condition, from the mental\nrepresentation that is its object.\n\nProponents of (1) typically invoke\n functionalism\n to analyze A*. Each psychological relation A* is\nassociated with a distinctive functional role: a role that\nS plays within your mental activity just in case you bear\nA* to S. When specifying what it is to believe*\nS, for example, we might mention how S serves as a basis\nfor inferential reasoning, how it interacts with desires to produce\nactions, and so on. Precise functional roles are to be discovered by\nscientific psychology. Following Schiffer (1981), it is common to use\nthe term \u201cbelief-box\u201d as a placeholder for the functional\nrole corresponding to belief*: to believe* S is to place\nS in your belief box. Similarly for \u201cdesire-box\u201d,\netc.\n\n(1) is compatible with the view that propositional attitudes are\nrelations to propositions. One might analyze the locution\n\u201cS means that p\u201d as involving a relation\nbetween S and a proposition expressed by S. It would\nthen follow that someone who believes* S stands in a\npsychologically important relation to the proposition expressed by\nS. Fodor (1987: 17) adopts this approach. He combines a\ncommitment to mental representations with a commitment to\npropositions. In contrast, Field (2001: 30\u201382) declines to\npostulate propositions when analyzing \u201cS means that\np\u201d. He posits mental representations with semantic\nproperties, but he does not posit propositions expressed by the mental\nrepresentations.\n\nThe distinction between\n types and tokens\n is crucial for understanding (1). A mental representation is a\nrepeatable type that can be instantiated on different occasions. In\nthe current literature, it is generally assumed that a mental\nrepresentation\u2019s tokens are neurological. For present purposes,\nthe key point is that mental representations are instantiated by\nmental events. Here we construe the category of\n events\n broadly so as to include both occurrences (e.g., I form an\nintention to drink orange juice) and enduring states (e.g.,\nmy longstanding belief that Abraham Lincoln was president of the\nUnited States). When mental event e instantiates representation\nS, we say that S is tokened and that e\nis a tokening of S. For example, if I believe that\nwhales are mammals, then my belief (a mental event) is a tokening of a\nmental representation whose meaning is that whales are mammals.\n\nAccording to Fodor (1987: 17), thinking consists in chains of mental\nevents that instantiate mental representations:\n\n(2) Thought processes\nare causal sequences of tokenings of mental\nrepresentations.\n\n\nA paradigm example is deductive inference: I transition from\nbelieving* the premises to believing* the conclusion. The first mental\nevent (my belief* in the premises) causes the second (my belief* in\nthe conclusion).\n\n(1) and (2) fit together naturally as a package that one might call\nthe representational theory of thought (RTT). RTT postulates\nmental representations that serve as the objects of propositional\nattitudes and that constitute the domain of thought\n processes.[1]\n\nRTT as stated requires qualification. There is a clear sense in which\nyou believe that there are no elephants on Jupiter. However, you\nprobably never considered the question until now. It is not plausible\nthat your belief box previously contained a mental representation with\nthe meaning that there are no elephants on Jupiter. Fodor (1987:\n20\u201326) responds to this sort of example by restricting (1) to\ncore cases. Core cases are those where the propositional\nattitude figures as a causally efficacious episode in a mental\nprocess. Your tacit belief that there are no elephants on Jupiter does\nnot figure in your reasoning or decision-making, although it can come\nto do so if the question becomes salient and you consciously judge\nthat there are no elephants on Jupiter. So long as the belief remains\ntacit, (1) need not apply. In general, Fodor says, an intentional\nmental state that is causally efficacious must involve explicit\ntokening of an appropriate mental representation. In a slogan:\n\u201cNo Intentional Causation without Explicit Representation\u201d\n(Fodor 1987: 25). Thus, we should not construe (1) as an attempt at\nfaithfully analyzing informal discourse about propositional attitudes.\nFodor does not seek to replicate folk psychological categories. He\naims to identify mental states that resemble the propositional\nattitudes adduced within folk psychology, that play roughly similar\nroles in mental activity, and that can support systematic\ntheorizing.\n\nDennett\u2019s (1977 [1981]) review of The Language of\nThought raises a widely cited objection to RTT:\n\n\nIn a recent conversation with the designer of a chess-playing program\nI heard the following criticism of a rival program: \u201cit thinks\nit should get its queen out early\u201d. This ascribes a\npropositional attitude to the program in a very useful and predictive\nway, for as the designer went on to say, one can usefully count on\nchasing that queen around the board. But for all the many levels of\nexplicit representation to be found in that program, nowhere is\nanything roughly synonymous with \u201cI should get my queen out\nearly\u201d explicitly tokened. The level of analysis to which the\ndesigner\u2019s remark belongs describes features of the program that\nare, in an entirely innocent way, emergent properties of the\ncomputational processes that have \u201cengineering reality\u201d. I\nsee no reason to believe that the relation between belief-talk and\npsychological talk will be any more direct.\n\n\nIn Dennett\u2019s example, the chess-playing machine does not\nexplicitly represent that it should get the queen out early, yet in\nsome sense it acts upon a belief that it should do so. Analogous\nexamples arise for human cognition. For example, we often follow rules\nof deductive inference without explicitly representing the rules.\n\nTo assess Dennett\u2019s objection, we must distinguish sharply\nbetween mental representations and rules governing the manipulation of\nmental representations (Fodor 1987: 25). RTT does not require that\nevery such rule be explicitly represented. Some rules may be\nexplicitly represented\u2014we can imagine a reasoning system that\nexplicitly represents deductive inference rules to which it conforms.\nBut the rules need not be explicitly represented. They may\nmerely be implicit in the system\u2019s operations. Only when\nconsultation of a rule figures as a causally efficacious episode in\nmental activity does RTT require that the rule be explicitly\nrepresented. Dennett\u2019s chess machine explicitly represents chess\nboard configurations and perhaps some rules for manipulating chess\npieces. It never consults any rule akin to Get the Queen out\nearly. For that reason, we should not expect that the machine\nexplicitly represents this rule even if the rule is in some sense\nbuilt into the machine\u2019s programming. Similarly, typical\nthinkers do not consult inference rules when engaging in deductive\ninference. So RTT does not demand that a typical thinker explicitly\nrepresent inference rules, even if she conforms to them and in some\nsense tacitly believes that she should conform to them.\n1.2 Compositional Semantics\n\nNatural language is\n compositional:\n complex linguistic expressions are built from simpler linguistic\nexpressions, and the meaning of a complex expression is a function of\nthe meanings of its constituents together with the way those\nconstituents are combined. Compositional semantics describes\nin a systematic way how semantic properties of a complex expression\ndepend upon semantic properties of its constituents and the way those\nconstituents are combined. For example, the truth-condition of a\nconjunction is determined as follows: the conjunction is true iff both\nconjuncts are true.\n\nHistorical and contemporary LOT theorists universally agree that\nMentalese is compositional:\n\n\nCompositionality of mental representations\n(COMP): Mental representations have a compositional\nsemantics: complex representations are composed of simple\nconstituents, and the meaning of a complex representation depends upon\nthe meanings of its constituents together with the constituency\nstructure into which those constituents are arranged.\n\n\nClearly, mental language and natural language must differ in many\nimportant respects. For example, Mentalese surely does not have a\nphonology. It may not have a morphology either. Nevertheless, COMP\narticulates a fundamental point of similarity. Just like natural\nlanguage, Mentalese contains complex symbols amenable to semantic\nanalysis.\n\nWhat is it for one representation to be a \u201cconstituent\u201d of\nanother? According to Fodor (2008: 108), \u201cconstituent structure\nis a species of the part/whole relation\u201d. Not all parts\nof a linguistic expression are constituents: \u201cJohn ran\u201d is\na constituent of \u201cJohn ran and Mary jumped\u201d, but\n\u201cran and Mary\u201d is not a constituent because it is not\nsemantically interpretable. The important point for our purposes is\nthat all constituents are parts. When a complex representation is\ntokened, so are its parts. For example, \n\n\nintending that \\(P \\amp Q\\) requires having a sentence in your\nintention box\u2026 one of whose parts is a token of the very same\ntype that\u2019s in the intention box when you intend that \\(P\\), and\nanother of whose parts is a token of the very same type that\u2019s\nin the intention box when you intend that \\(Q\\). (Fodor 1987: 139)\n\n\n\nMore generally: mental event \\(e\\) instantiates a complex mental\nrepresentation only if \\(e\\) instantiates all of the\nrepresentation\u2019s constituent parts. In that sense, \\(e\\) itself\nhas internal complexity.\n\nThe complexity of mental events figures crucially here, as highlighted\nby Fodor in the following passage (1987: 136):\n\n\nPractically everybody thinks that the objects of intentional states\nare in some way complex\u2026 [For example], what you believe when\nyou believe that \\(P \\amp Q\\) is\u2026 something composite, whose\nelements are\u2014as it might be\u2014the proposition that P\nand the proposition that Q. But the (putative) complexity of\nthe intentional object of a mental state does not, of course,\nentail the complexity of the mental state itself\u2026 LOT claims\nthat mental states\u2014and not just their propositional\nobjects\u2014typically have constituent structure.\n\n\nMany philosophers, including Frege and Russell, regard propositions as\nstructured entities. These philosophers apply a part/whole model to\npropositions but not necessarily to mental events during which\nthinkers entertain propositions. LOTH as developed by Fodor applies\nthe part/whole model to the mental events themselves: \n\n\nwhat\u2019s at issue here is the complexity of mental events and not\nmerely the complexity of the propositions that are their intentional\nobjects. (Fodor 1987: 142) \n\n\nOn this approach, a key element of LOTH is the thesis that mental\nevents have semantically relevant complexity.\n\nContemporary proponents of LOTH endorse RTT+COMP. Historical\nproponents also believed something in the vicinity (Normore 1990,\n2009; Panaccio 1999 [2017]), although of course they did not use\nmodern terminology to formulate their views. We may regard RTT+COMP as\na minimalist formulation of LOTH, bearing in mind that many\nphilosophers have used the phrase \u201clanguage of thought\nhypothesis\u201d to denote one of the stronger theses discussed\nbelow. As befits a minimalist formulation, RTT+COMP leaves unresolved\nnumerous questions about the nature, structure, and psychological role\nof Mentalese expressions.\n1.3 Logical Structure\n\nIn practice, LOT theorists usually adopt a more specific view of the\ncompositional semantics for Mentalese. They claim that Mentalese\nexpressions have\n logical form\n (Fodor 2008: 21). More specifically, they claim that Mentalese\ncontains analogues to the familiar logical connectives (and,\nor, not, if-then, some,\nall, the). Iterative application of logical\nconnectives generates complex expressions from simpler expressions.\nThe meaning of a logically complex expression depends upon the\nmeanings of its parts and upon its logical structure. Thus, LOT\ntheorists usually endorse a doctrine along the following lines:\n\n\nLogically structured mental representations\n(LOGIC): Some mental representations have logical\nstructure. The compositional semantics for these mental\nrepresentations resembles the compositional semantics for logically\nstructured natural language expressions.\n\n\nMedieval LOT theorists used syllogistic and propositional logic to\nanalyze the semantics of Mentalese (King 2005; Normore 1990).\nContemporary proponents instead use the predicate calculus,\nwhich was discovered by Frege (1879 [1967]) and whose semantics was\nfirst systematically articulated by Tarski (1933 [1983]). The view is\nthat Mentalese contains primitive words\u2014including predicates,\nsingular terms, and logical connectives\u2014and that these words\ncombine to form complex sentences governed by something like the\nsemantics of the predicate calculus.\n\nThe notion of a Mentalese word corresponds roughly to the\nintuitive notion of a concept. In fact, Fodor (1998: 70)\nconstrues a concept as a Mentalese word together with its denotation.\nFor example, a thinker has the concept of a cat only if she has in her\nrepertoire a Mentalese word that denotes cats.\n\nLogical structure is just one possible paradigm for the structure of\nmental representations. Human society employs a wide range of\nnon-sentential representations, including pictures, maps, diagrams,\nand graphs. Non-sentential representations typically contain parts\narranged into a compositionally significant structure. In many cases,\nit is not obvious that the resulting complex representations have\nlogical structure. For example, maps do not seem to contain logical\nconnectives (Fodor 1991: 295; Millikan 1993: 302; Pylyshyn 2003:\n424\u20135). Nor is it evident that they contain predicates (Camp\n2018; Rescorla 2009c), although some philosophers contend that they do\n(Blumson 2012; Casati & Varzi 1999; Kulvicki 2015).\n\nTheorists often posit mental representations that conform to COMP but\nthat lack logical structure. The British empiricists postulated\nideas, which they characterized in broadly imagistic terms.\nThey emphasized that simple ideas can combine to form complex ideas.\nThey held that the representational import of a complex idea depends\nupon the representational import of its parts and the way those parts\nare combined. So they accepted COMP or something close to it\n(depending on what exactly \u201cconstituency\u201d amounts\n to).[2]\n They did not say in much detail how compounding of ideas was supposed\nto work, but imagistic structure seems to be the paradigm in at least\nsome passages. LOGIC plays no significant role in their\n writings.[3]\n Partly inspired by the British empiricists, Prinz (2002) and Barsalou\n(1999) analyze cognition in terms of image-like representations\nderived from perception. Armstrong (1973) and Braddon-Mitchell and\nJackson (2007) propose that propositional attitudes are relations not\nto mental sentences but to mental maps analogous in important\nrespects to ordinary concrete maps.\n\nOne problem facing imagistic and cartographic theories of thought is\nthat propositional attitudes are often logically complex (e.g., John\nbelieves that if Pl\u00e1cido Domingo does not sing then either\nGustavo Dudamel will conduct or the concert will be cancelled).\nImages and maps do not seem to support logical operations: the\nnegation of a map is not a map; the disjunction of two maps is not a\nmap; similarly for other logical operations; and similarly for images.\nGiven that images and maps do not support logical operations, theories\nthat analyze thought in exclusively imagistic or cartographic terms\nwill struggle to explain logically complex propositional\n attitudes.[4]\n\nThere is room here for a pluralist position that allows mental\nrepresentations of different kinds: some with logical structure, some\nmore analogous to pictures, or maps, or diagrams, and so on. The\npluralist position is widespread within cognitive science, which\nposits a range of formats for mental representation (Block 1983; Camp\n2009; Johnson-Laird 2004: 187; Kosslyn 1980; Mandelbaum et al. 2022;\nMcDermott 2001: 69; Pinker 2005: 7; Sloman 1978: 144\u201376). Fodor\nhimself (1975: 184\u2013195) suggests a view on which imagistic\nmental representations co-exist alongside, and interact with,\nlogically structured Mentalese expressions.\n\nGiven the prominent role played by logical structure within historical\nand contemporary discussion of Mentalese, one might take LOGIC to be\ndefinitive of LOTH. One might insist that mental representations\ncomprise a mental language only if they have logical\nstructure. We need not evaluate the merits of this terminological\nchoice.\n2. Scope of LOTH\n\nRTT concerns propositional attitudes and the mental processes in which\nthey figure, such as deductive inference, reasoning, decision-making,\nand planning. It does not address perception, motor control,\nimagination, dreaming, pattern recognition, linguistic processing, or\nany other mental activity distinct from high-level cognition. Hence\nthe emphasis upon a language of thought: a system of mental\nrepresentations that underlie thinking, as opposed to perceiving,\nimagining, etc. Nevertheless, talk about a mental language generalizes\nnaturally from high-level cognition to other mental phenomena.\n\nPerception is a good example. The perceptual system\ntransforms proximal sensory stimulations (e.g., retinal stimulations)\ninto perceptual estimates of environmental conditions (e.g., estimates\nof shapes, sizes, colors, locations, etc.). Helmholtz (1867 [1925])\nproposed that the transition from proximal sensory input to perceptual\nestimates features an unconscious inference, similar in key\nrespects to high-level conscious inference yet inaccessible to\nconsciousness. Helmholtz\u2019s proposal is foundational to\ncontemporary perceptual psychology, which constructs detailed\nmathematical models of unconscious perceptual inference (Knill &\nRichards 1996; Rescorla 2015). Fodor (1975: 44\u201355) argues that\nthis scientific research program presupposes mental representations.\nThe representations participate in unconscious inferences or\ninference-like transitions executed by the perceptual\n system.[5]\n\nNavigation is another good example. Tolman (1948)\nhypothesized that rats navigate using cognitive maps: mental\nrepresentations that represent the layout of the spatial environment.\nThe cognitive map hypothesis, advanced during the heyday of\nbehaviorism, initially encountered great scorn. It remained a fringe\nposition well into the 1970s, long after the demise of behaviorism.\nEventually, mounting behavioral and neurophysiological evidence won it\nmany converts (Gallistel 1990; Gallistel & Matzel 2013; Jacobs\n& Menzel 2014; O\u2019Keefe & Nadel 1978; Weiner et al.\n2011). Although a few researchers remain skeptical (Mackintosh 20002),\nthere is now a broad consensus that mammals (and possibly even some\ninsects) navigate using mental representations of spatial layout.\nRescorla (2017b) summarizes the case for cognitive maps and reviews\nsome of their core properties.\n\nTo what extent should we expect perceptual representations and\ncognitive maps to resemble the mental representations that figure in\nhigh-level human thought? It is generally agreed that all these mental\nrepresentations have compositional structure. For example, the\nperceptual system can bind together a representation of shape and a\nrepresentation of size to form a complex representation that an object\nhas a certain shape and size; the representational import of the\ncomplex representation depends in a systematic way upon the\nrepresentational import of the component representations. On the other\nhand, it is not clear that perceptual representations have\nlogical structure (Block 2023: 182\u2013190; Burge 2022:\n190\u2013201), including even predicative structure (Burge 2010:\n540\u2013544; Burge 2022: 44\u201345: Fodor 2008: 169\u2013195).\nNor is it evident that cognitive maps contain logical connectives or\npredicates (Rescorla 2009a, 2009b). Perceptual processing and\nnon-human navigation certainly do not seem to instantiate mental\nprocesses that would exploit putative logical structure. In\nparticular, they do not seem to instantiate deductive inference.\n\nThese observations provide ammunition for pluralism about\nrepresentational format. Pluralists can posit one system of\ncompositionally structured mental representations for perception,\nanother for navigation, another for high-level cognition, and so on.\nDifferent representational systems potentially feature different\ncompositional mechanisms. As indicated in\n section 1.3,\n pluralism figures prominently in contemporary cognitive science.\nPluralists face some pressing questions. Which compositional\nmechanisms figure in which psychological domains? Which\nrepresentational formats support which mental operations? How do\ndifferent representational formats interface with each other? Further\nresearch bridging philosophy and cognitive science is needed to\naddress such questions.\n3. Mental Computation\n\nModern proponents of LOTH typically endorse the\n computational theory of mind\n (CTM), which claims that the mind is a computational system. Some\nauthors use the phrase \u201clanguage of thought hypothesis\u201d so\nthat it definitionally includes CTM as one component.\n\nIn a seminal contribution, Turing (1936) introduced what is now called\nthe\n Turing machine:\n an abstract model of an idealized computing device. A Turing machine\ncontains a central processor, governed by precise mechanical rules,\nthat manipulates symbols inscribed along a linear array of memory\nlocations. Impressed by the enormous power of the Turing machine\nformalism, many researchers seek to construct computational models of\ncore mental processes, including reasoning, decision-making, and\nproblem solving. This enterprise bifurcates into two main branches.\nThe first branch is artificial intelligence (AI), which aims\nto build \u201cthinking machines\u201d. Here the goal is primarily\nan engineering one\u2014to build a system that instantiates or at\nleast simulates thought\u2014without any pretense at capturing how\nthe human mind works. The second branch, computational\npsychology, aims to construct computational models of human\nmental activity. AI and computational psychology both emerged in the\n1960s as crucial elements in the new interdisciplinary initiative\n cognitive science,\n which studies the mind by drawing upon psychology, computer science\n(especially AI), linguistics, philosophy, economics (especially game\ntheory and behavioral economics), anthropology, and neuroscience.\n\nFrom the 1960s to the early 1980s, computational models offered within\npsychology were mainly Turing-style models. These models embody a\nviewpoint known as the classical computational theory of mind\n(CCTM). According to CCTM, the mind is a computational system similar\nin important respects to a Turing machine, and certain core mental\nprocesses are computations similar in important respects to\ncomputations executed by a Turing machine.\n\nCCTM fits together nicely with RTT+COMP. Turing-style computation\noperates over symbols, so any Turing-style mental computations must\noperate over mental symbols. The essence of RTT+COMP is postulation of\nmental symbols. Fodor (1975, 1981) advocates RTT+COMP+CCTM. He holds\nthat certain core mental processes are Turing-style computations over\nMentalese expressions.\n\nOne can endorse RTT+COMP without endorsing CCTM. By positing a system\nof compositionally structured mental representations, one does not\ncommit oneself to saying that operations over the representations are\ncomputational. Historical LOT theorists could not even\nformulate CCTM, for the simple reason that the Turing formalism had\nnot been discovered. In the modern era, Harman (1973) and Sellars\n(1975) endorse something like RTT+COMP but not CCTM. Horgan and\nTienson (1996) endorse RTT+COMP+CTM but not CCTM, i.e.,\nclassical CTM. They favor a version of CTM grounded in\n connectionism,\n an alternative computational framework that differs quite\nsignificantly from Turing\u2019s approach. Thus, proponents of\nRTT+COMP need not accept that mental activity instantiates\nTuring-style computation.\n\nFodor (1981) combines RTT+COMP+CCTM with a view that one might call\nthe formal-syntactic conception of computation (FSC).\nAccording to FSC, computation manipulates symbols in virtue of their\nformal syntactic properties but not their semantic properties.\n\nFSC draws inspiration from modern logic, which emphasizes the\nformalization of deductive reasoning. To formalize, we\nspecify a formal language whose component linguistic\nexpressions are individuated non-semantically (e.g., by their\ngeometric shapes). We describe the expressions as pieces of formal\nsyntax, without considering what if anything the expressions mean. We\nthen specify inference rules in syntactic, non-semantic\nterms. Well-chosen inference rules will carry true premises to true\nconclusions. By combining formalization with Turing-style computation,\nwe can build a physical machine that manipulates symbols based solely\non the formal syntax of the symbols. If we program the machine to\nimplement appropriate inference rules, then its syntactic\nmanipulations will transform true premises into true conclusions.\n\nCCTM+FSC says that the mind is a formal syntactic computing system:\nmental activity consists in computation over symbols with formal\nsyntactic properties; computational transitions are sensitive to the\nsymbols\u2019 formal syntactic properties but not their semantic\nproperties. The key term \u201csensitive\u201d is rather imprecise,\nallowing some latitude as to the precise import of CCTM+FSC.\nIntuitively, the picture is that a mental symbol\u2019s formal syntax\nrather than its semantics determines how mental computation\nmanipulates it. The mind is a \u201csyntactic engine\u201d.\n\nFodor (1987: 18\u201320) argues that CCTM+FSC helps illuminate a\ncrucial feature of cognition: semantic coherence. For the\nmost part, our thinking does not move randomly from thought to\nthought. Rather, thoughts are causally connected in a way that\nrespects their semantics. For example, deductive inference carries\ntrue beliefs to true beliefs. More generally, thinking tends to\nrespect epistemic properties such as warrant and degree of\nconfirmation. In some sense, then, our thinking tends to cohere with\nsemantic relations among thoughts. How is semantic coherence achieved?\nHow does our thinking manage to track semantic properties? CCTM+FSC\ngives one possible answer. It shows how a physical system operating in\naccord with physical laws can execute computations that coherently\ntrack semantic properties. By treating the mind as a syntax-driven\nmachine, we explain how mental activity achieves semantic coherence.\nWe thereby answer the question: How is rationality mechanically\npossible?\n\nFodor\u2019s argument convinced many researchers that CCTM+FSC\ndecisively advances our understanding of the mind\u2019s relation to\nthe physical world. But not everyone agrees that CCTM+FSC adequately\nintegrates semantics into the causal order. A common worry is that the\nformal syntactic picture veers dangerously close to\nepiphenomenalism (Block 1990; Kazez 1994). Pre-theoretically,\nsemantic properties of mental states seem highly relevant to mental\nand behavioral outcomes. For example, if I form an intention to walk\nto the grocery store, then the fact that my intention concerns the\ngrocery store rather than the post office helps explain why I walk to\nthe grocery store rather than the post office. Burge (2010) and\nPeacocke (1994) argue that cognitive science theorizing likewise\nassigns causal and explanatory importance to semantic properties. The\nworry is that CCTM+FSC cannot accommodate the causal and explanatory\nimportance of semantic properties because it depicts them as causally\nirrelevant: formal syntax, not semantics, drives mental computation\nforward. Semantics looks epiphenomenal, with syntax doing all the work\n(Stich 1983).\n\nFodor (1990, 1994) expends considerable energy trying to allay\nepiphenomenalist worries. Advancing a detailed theory of the relation\nbetween Mentalese syntax and Mentalese semantics, he insists that FSC\ncan honor the causal and explanatory relevance of semantic properties.\nFodor\u2019s treatment is widely regarded as problematic (Arjo 1996;\nAydede 1997b, 1998; Aydede & Robbins 2001; Perry 1998; Prinz 2011;\nWakefield 2002), although Rupert (2008) and Schneider (2005) espouse\nsomewhat similar positions.\n\nPartly in response to epiphenomenalist worries, some authors recommend\nthat we replace FSC with an alternative semantic conception\nof computation (Block 1990; Burge 2010: 95\u2013101; Figdor 2009;\nO\u2019Brien & Opie 2006; Peacocke 1994, 1999; Rescorla 2012a).\nSemantic computationalists claim that computational transitions are\nsometimes sensitive to semantic properties, perhaps in addition to\nsyntactic properties. More specifically, semantic computationalists\ninsist that mental computation is sometimes sensitive to\nsemantics. Thus, they reject any suggestion that the mind is a\n\u201csyntactic engine\u201d or that mental computation is sensitive\nonly to formal\n syntax.[6]\n To illustrate, consider Mentalese conjunction. This mental symbol\nexpresses the truth-table for conjunction. According to semantic\ncomputationalists, the symbol\u2019s meaning is relevant (both\ncausally and explanatorily) to mechanical operations over it. That the\nsymbol expresses the truth-table for conjunction rather than, say,\ndisjunction influences the course of computation. We should therefore\nreject any suggestion that mental computation is sensitive to the\nsymbol\u2019s syntactic properties rather than its semantic\nproperties. The claim is not that mental computation explicitly\nrepresents semantic properties of mental symbols. All parties\nagree that, in general, it does not. There is no homunculus inside\nyour head interpreting your mental language. The claim is rather that\nsemantic properties influence how mental computation proceeds.\n(Compare: the momentum of a baseball thrown at a window causally\ninfluences whether the window breaks, even though the window does not\nexplicitly represent the baseball\u2019s momentum.)\n\nProponents of the semantic conception differ as to how exactly they\ngloss the core claim that some computations are\n\u201csensitive\u201d to semantic properties. They also differ in\ntheir stance towards CCTM. Block (1990) and Rescorla (2014a) focus\nupon CCTM. They argue that a symbol\u2019s semantic properties can\nimpact mechanical operations executed by a Turing-style computational\nsystem. In contrast, O\u2019Brien and Opie (2006) favor connectionism\nover CCTM.\n\nTheorists who reject FSC must reject Fodor\u2019s explanation of\nsemantic coherence. What alternative explanation might they offer? So\nfar, the question has received relatively little attention. Rescorla\n(2017a) argues that semantic computationalists can explain semantic\ncoherence and simultaneously avoid epiphenomenalist worries by\ninvoking neural implementation of semantically-sensitive mental\ncomputations.\n\nFodor\u2019s exposition sometimes suggests that CTM, CCTM, or\nCCTM+FSC is definitive of LOTH (1981: 26). Yet not everyone who\nendorses RTT+COMP endorses CTM, CCTM, or FSC. One can postulate a\nmental language without agreeing that mental activity is\ncomputational, and one can postulate mental computations over a mental\nlanguage without agreeing that the computations are sensitive only to\nsyntactic properties. For most purposes, it is not important whether\nwe regard CTM, CCTM, or CCTM+FSC as definitive of LOTH. More important\nis that we track the distinctions among the doctrines.\n4. Arguments for LOTH\n\nThe literature offers many arguments for LOTH. This section introduces\nfour influential arguments, each of which supports LOTH abductively by\nciting its explanatory benefits.\n Section 5\n discusses some prominent objections to the four arguments.\n4.1 Argument from Cognitive Science Practice\n\nFodor (1975) defends RTT+COMP+CCTM by appealing to scientific\npractice: our best cognitive science postulates Turing-style mental\ncomputations over Mentalese expressions; therefore, we should accept\nthat mental computation operates over Mentalese expressions. Fodor\ndevelops his argument by examining detailed case studies, including\nperception, decision-making, and linguistic comprehension. He argues\nthat, in each case, computation over mental representations plays a\ncentral explanatory role. Fodor\u2019s argument was widely heralded\nas a compelling analysis of then-current cognitive science. The\nargument from cognitive science practice has subsequently been\ndeveloped and updated both by Fodor and by other authors, such as\nQuilty-Dunn, Porot, and Mandelbaum (forthcoming).\n\nWhen evaluating cognitive science support for LOTH, it is crucial to\nspecify what version of LOTH one has in mind. Specifically,\nestablishing that certain mental processes operate over mental\nrepresentations is not enough to establish RTT. For example, one might\naccept that mental representations figure in perception and animal\nnavigation but not in high-level human cognition. Gallistel and King\n(2009) defend COMP+CCTM+FSC through a number of (mainly non-human)\nempirical case studies, but they do not endorse RTT. They focus on\nrelatively low-level phenomena, such as animal navigation, without\ndiscussing human decision-making, deductive inference, problem\nsolving, or other high-level cognitive phenomena.\n4.2 Argument from the Productivity of Thought\n\nDuring your lifetime, you will only entertain a finite number of\nthoughts. In principle, though, there are infinitely many thoughts you\nmight entertain. Consider:\n\n\nMary gave the test tube to John\u2019s daughter.\n\nMary gave the test tube to John\u2019s daughter\u2019s daughter.\n\nMary gave the test tube to John\u2019s daughter\u2019s\ndaughter\u2019s daughter.\n\n\u22ee\n\n\nThe moral usually drawn is that you have the competence to\nentertain a potential infinity of thoughts, even though your\nperformance is bounded by biological limits upon memory,\nattention, processing capacity, and so on. In a slogan: thought is\nproductive.\n\nRTT+COMP straightforwardly explains productivity. We postulate a\nfinite base of primitive Mentalese symbols, along with operations for\ncombining simple expressions into complex expressions. Iterative\napplication of the compounding operations generates an infinite array\nof mental sentences, each in principle within your cognitive\nrepertoire. By tokening a mental sentence, you entertain the thought\nexpressed by it. This explanation leverages the recursive nature of\ncompositional mechanisms to generate infinitely many expressions from\na finite base. It thereby illuminates how finite creatures such as\nourselves are able to entertain a potential infinity of thoughts.\n\nFodor and Pylyshyn (1988) argue that, since RTT+COMP provides a\nsatisfying explanation for productivity, we have good reason to accept\nRTT+COMP. A potential worry about this argument is that it rests upon\nan infinitary competence never manifested within actual performance.\nOne might dismiss the supposed infinitary competence as an\nidealization that, while perhaps convenient for certain purposes, does\nnot stand in need of explanation.\n4.3 Argument from the Systematicity of Thought\n\nThere are systematic interrelations among the thoughts a thinker can\nentertain. For example, if you can entertain the thought that John\nloves Mary, then you can also entertain the thought that Mary loves\nJohn. Systematicity looks like a crucial property of human thought and\nso demands a principled explanation.\n\nRTT+COMP gives a compelling explanation. According to RTT+COMP, your\nability to entertain the thought that p hinges upon your\nability to bear appropriate psychological relations to a Mentalese\nsentence S whose meaning is that p. If you are able to\nthink that John loves Mary, then your internal system of mental\nrepresentations includes a mental sentence John loves\nMary, composed of mental words John,\nloves, and Mary\ncombined in the right way. If you have the capacity to stand in\npsychological relation A* to John loves\nMary, then you also have the capacity to stand in relation\nA* to a distinct mental sentence Mary loves\nJohn. The constituent words John, \nloves,\nand Mary make the\nsame semantic contribution to both mental sentences (John\ndenotes John, loves\ndenotes the loving relation, and Mary denotes\nMary), but the words are arranged in different constituency structures\nso that the sentences have different meanings. Whereas John\nloves Mary means that John loves Mary, Mary\nloves John means that Mary loves John. By\nstanding in relation A* to the sentence Mary\nloves John, you entertain the thought that Mary loves John.\nThus, an ability to think that John loves Mary entails an ability to\nthink that John loves Mary. By comparison, an ability to think that\nJohn loves Mary does not entail an ability to think that whales are\nmammals or an ability to think that \\(56 + 138 = 194\\).\n\nFodor (1987: 148\u2013153) supports RTT+COMP by citing its ability to\nexplain systematicity. In contrast with the productivity argument, the\nsystematicity argument does not depend upon infinitary idealizations\nthat outstrip finite performance. Note that neither argument provides\nany direct support for CTM. Neither argument even mentions\ncomputation.\n4.4 Argument from the Systematicity of Thinking\n\nThere are systematic interrelations among which inferences a thinker\ncan draw. For example, if you can infer p from p\nand q, then you can also infer m from m and\nn. The systematicity of thinking requires explanation. Why is it\nthat thinkers who can infer p from p and\nq can also infer m from m and\nn?\n\nRTT+COMP+CCTM gives a compelling explanation. During an inference from\np and q to p, you transit from believing* mental\nsentence \\(S_1 \\amp S_2\\) (which means that p and q) to\nbelieving* mental sentence \\(S_{1}\\) (which means that p).\nAccording to CCTM, the transition involves symbol manipulation. A\nmechanical operation detaches the conjunct \\(S_{1}\\) from the\nconjunction \\(S_1 \\amp S_2\\). The same mechanical operation is\napplicable to a conjunction \\(S_{3} \\amp S_{4}\\) (which means that\nm and n), corresponding to the inference from m and\nn to n. An ability to execute the first inference entails\nan ability to execute the second, because drawing the inference in\neither case corresponds to executing a single uniform mechanical\noperation. More generally, logical inference deploys mechanical\noperations over structured symbols, and the mechanical operation\ncorresponding to a given inference pattern (e.g., conjunction\nintroduction, disjunction elimination, etc.) is applicable to any\npremises with the right logical structure. The uniform applicability\nof a single mechanical operation across diverse symbols explains\ninferential systematicity. Fodor and Pylyshyn (1988) conclude that\ninferential systematicity provides reason to accept RTT+COMP+CCTM.\n\nFodor and Pylyshyn (1988) endorse an additional thesis about the\nmechanical operations corresponding to logical transitions. In keeping\nwith FSC, they claim that the operations are sensitive to formal\nsyntactic properties but not semantic properties. For example,\nconjunction elimination responds to Mentalese conjunction as a piece\nof pure formal syntax, much as a computer manipulates items in a\nformal language without considering what those items mean.\n\nSemantic computationalists reject FSC. They claim that mental\ncomputation is sometimes sensitive to semantic properties. Semantic\ncomputationalists can agree that drawing an inference involves\nexecuting a mechanical operation over structured symbols, and they can\nagree that the same mechanical operation uniformly applies to any\npremises with appropriate logical structure. So they can still explain\ninferential systematicity. However, they can also say that the\npostulated mechanical operation is sensitive to semantic properties.\nFor example, they can say that conjunction elimination is sensitive to\nthe meaning of Mentalese conjunction.\n\nIn assessing the debate between FSC and semantic computationalism, one\nmust distinguish between logical versus non-logical\nsymbols. For present purposes, it is common ground that the meanings\nof non-logical symbols do not inform logical inference. The\ninference from \\(S_1 \\amp S_2\\) to \\(S_{1}\\) features the same\nmechanical operation as the inference from \\(S_{3} \\amp S_{4}\\) to\n\\(S_{4}\\), and this mechanical operation is not sensitive to the\nmeanings of the conjuncts \\(S_{1}\\), \\(S_{2}\\), \\(S_{3}\\), or\n\\(S_{4}\\). It does not follow that the mechanical operation is\ninsensitive to the meaning of Mentalese conjunction. The meaning of\nconjunction might influence how the logical inference proceeds, even\nthough the meanings of the conjuncts do not.\n5. The Connectionist Challenge\n\nIn the 1960s and 1970s, cognitive scientists almost universally\nmodeled mental activity as rule-governed symbol manipulation. In the\n1980s, connectionism gained currency as an alternative computational\nframework. Connectionists employ computational models, called\nneural networks, that differ quite significantly from\nTuring-style models. There is no central processor. There are no\nmemory locations for symbols to be inscribed. Instead, there is a\nnetwork of nodes bearing weighted connections to one another.\nDuring computation, waves of activation spread through the network. A\nnode\u2019s activation level depends upon the weighted activations of\nthe nodes to which it is connected. Nodes function somewhat\nanalogously to neurons, and connections between nodes function\nsomewhat analogously to synapses. One should receive the\nneurophysiological analogy cautiously, as there are numerous important\ndifferences between neural networks and actual neural configurations\nin the brain (Bechtel & Abramson 2002: 341\u2013343;\nBerm\u00fadez 2010: 237\u2013239; Clark 2014: 87\u201389; Harnish\n2002: 359\u2013362).\n\nConnectionists raise many objections to the classical computational\nparadigm (Rumelhart, McClelland, & the PDP Research Group 1986;\nHorgan & Tienson 1996; McLaughlin & Warfield 1994; Bechtel\n& Abrahamsen 2002), such as that classical systems are not\nbiologically realistic or that they are unable to model certain\npsychological tasks. Classicists in turn launch various arguments\nagainst connectionism. The most famous arguments showcase\nproductivity, systematicity of thought, and systematicity of thinking.\nFodor and Pylyshyn (1988) argue that these phenomena support classical\nCTM over connectionist CTM.\n\nFodor and Pylyshyn\u2019s argument hinges on the distinction between\neliminative connectionism and implementationist\nconnectionism (cf. Pinker & Prince 1988). Eliminative\nconnectionists advance neural networks as a replacement for\nthe Turing-style formalism. They deny that mental computation consists\nin rule-governed symbol manipulation. Implementationist connectionists\nallow that, in some cases, mental computation may instantiate\nrule-governed symbol manipulation. They advance neural networks not to\nreplace classical computations but rather to model how classical\ncomputations are implemented in the brain. The hope is that, because\nneural network computation more closely resembles actual brain\nactivity, it can illuminate the physical realization of rule-governed\nsymbol manipulation.\n\nBuilding on Aydede\u2019s (2015) discussion, we may reconstruct Fodor\nand Pylyshyn\u2019s argument like so:\n\n Representational mental states and processes exist. An\nexplanatorily adequate account of cognition should acknowledge these\nstates and processes.\n The representational states and processes that figure in\nhigh-level cognition have certain fundamental properties: thought is\nproductive and systematic; inferential thinking is\nsystematic. The states and processes have these properties as\na matter of nomic necessity: it is a psychological law that\nthey have the properties.\n A theory of mental computation is explanatorily adequate only if\nit explains the nomic necessity of systematicity and\nproductivity.\n The only way to explain the nomic necessity of systematicity and\nproductivity is to postulate that high-level cognition instantiates\ncomputation over mental symbols with a compositional semantics.\nSpecifically, we must accept RTT+COMP.\n Either a connectionist theory endorses RTT+COMP or it does\nnot.\n If it does, then it is a version of implementationist\nconnectionism.\n If it does not, then it is a version of eliminative\nconnectionism. As per (iv), it does not explain productivity and\nsystematicity. As per (iii), it is not explanatorily adequate.\n Conclusion: Eliminative connectionist theories\nare not explanatorily adequate.\n\n\nThe argument does not say that neural networks are unable to\nmodel systematicity. One can certainly build a neural network that is\nsystematic. For example, one might build a neural network that can\nrepresent that John loves Mary only if it can represent that Mary\nloves John. The problem is that one might just as well build a neural\nnetwork that can represent that John loves Mary but cannot represent\nthat Mary loves John. Hence, nothing about the connectionist framework\nper se guarantees systematicity. For that reason, the\nframework does not explain the nomic necessity of systematicity. It\ndoes not explain why all the minds we find are systematic. In\ncontrast, the classical framework mandates systematicity, and so it\nexplains the nomic necessity of systematicity. The only apparent\nrecourse for connectionists is to adopt the classical explanation,\nthereby becoming implementationist rather than eliminative\nconnectionists.\n\nFodor and Pylyshyn\u2019s argument has spawned a massive literature,\nincluding too many rebuttals to survey here. The most popular\nresponses fall into five categories:\n\n Deny (i). Some connectionists deny that cognitive\nscience should posit representational mental states. They believe that\nmature scientific theorizing about the mind will delineate\nconnectionist models specified in non-representational terms (P.S.\nChurchland 1986; P.S. Churchland & Sejnowski 1989; P.M. Churchland\n1990; P.M. Churchland & P.S. Churchland 1990; Ramsey 2007). If so,\nthen Fodor and Pylyshyn\u2019s argument falters at its first step.\nThere is no need to explain why representational mental states are\nsystematic and productive if one rejects all talk about\nrepresentational mental states.\n Accept (viii). Some authors, such as Marcus (2001), feel\nthat neural networks are best deployed to illuminate the\nimplementation of Turing-style models, rather than as replacements for\nTuring-style models.\n Deny (ii). Some authors claim that Fodor and Pylyshyn\ngreatly exaggerate the extent to which thought is productive\n(Rumelhart & McClelland 1986) or systematic (Dennett 1991; Johnson\n2004). Horgan and Tienson (1996: 91\u201394) question the\nsystematicity of thinking. They contend that we deviate from norms of\ndeductive inference more than one would expect if we were following\nthe rigid mechanical rules postulated by CCTM.\n Deny (iv). Braddon-Mitchell and Fitzpatrick (1990) offer\nan evolutionary explanation for the systematicity of thought,\nbypassing any appeal to structured mental representations. In a\nsimilar vein, Horgan and Tienson (1996: 90) seek to explain\nsystematicity by emphasizing how our survival depends upon our ability\nto keep track of objects in the environment and their ever-changing\nproperties. Clark (1991) argues that systematicity follows from the\nholistic nature of thought ascription.\n Deny (vi). Chalmers (1990, 1993), Smolensky (1991), and\nvan Gelder (1991) claim that one can reject Turing-style models while\nstill postulating mental representations with compositionally and\ncomputationally relevant internal structure.\n\n\nWe focus here on (vi).\n\nAs discussed in\n section 1.2,\n Fodor elucidates constituency structure in terms of part/whole\nrelations. A complex representation\u2019s constituents are literal\nparts of it. One consequence is that, whenever the first\nrepresentation is tokened, so are its constituents. Fodor takes this\nconsequence to be definitive of classical computation. As Fodor and\nMcLaughlin (1990: 186) put it: \n\n\nfor a pair of expression types E1, E2, the first is a\nClassical constituent of the second only if the\nfirst is tokened whenever the second is tokened. \n\n\nThus, structured representations have a concatenative\nstructure: each token of a structured representation involves a\nconcatenation of tokens of the constituent representations.\nConnectionists who deny (vi) espouse a non-concatenative\nconception of constituency structure, according to which structure is\nencoded by a suitable distributed representation.\nDevelopments of the non-concatenative conception are usually quite\ntechnical (Elman 1989; Hinton 1990; Pollack 1990; Smolensky 1990,\n1991, 1995; Touretzky 1990). Most models use vector or\ntensor algebra to define operations over connectionist\nrepresentations, which are codified by activity vectors across nodes\nin a neural network. The representations are said to have\nimplicit constituency structure: the constituents are not\nliteral parts of the complex representation, but they can be extracted\nfrom the complex representation through suitable computational\noperations over it.\n\nFodor and McLaughlin (1990) grant that distributed representations may\nhave constituency structure \u201cin an extended sense\u201d. But\nthey insist that distributed representations are ill-suited to explain\nsystematicity. They focus especially on the systematicity of thinking,\nthe classical explanation for which postulates mechanical operations\nthat respond to constituency structure. Fodor and McLaughlin argue\nthat the non-concatenative conception cannot replicate the classical\nexplanation and offers no satisfactory substitute for it. Chalmers\n(1993) and Niklasson and van Gelder (1994) disagree. They contend that\na neural network can execute structure-sensitive computations over\nrepresentations that have non-concatenative constituency structure.\nThey conclude that connectionists can explain productivity and\nsystematicity without retreating to implementationist\nconnectionism.\n\nAydede (1995, 1997a) agrees that there is a legitimate notion of\nnon-concatenative constituency structure, but he questions whether the\nresulting models are non-classical. He denies that we should regard\nconcatenative structure as integral to LOTH. According to Aydede,\nconcatenative structure is just one possible physical realization of\nconstituency structure. Non-concatenative structure is another\npossible realization. We can accept RTT+COMP without glossing\nconstituency structure in concatenative terms. On this view, a neural\nnetwork whose operations are sensitive to non-concatenative\nconstituency structure may still count as broadly classical and in\nparticular as manipulating Mentalese expressions.\n\nThe debate between classical and connectionist CTM is still active,\nalthough not as active as during the 1990s. Recent anti-connectionist\narguments tend to have a more empirical flavor. For example, Gallistel\nand King (2009) defend CCTM by canvassing a range of non-human\nempirical case studies. According to Gallistel and King, the case\nstudies manifest a kind of productivity that CCTM can easily explain\nbut eliminative connectionism cannot.\n6. Regress Objections to LOTH\n\nLOTH has elicited too many objections to cover in a single\nencyclopedia entry. We will discuss two objections, both alleging that\nLOTH generates a vicious regress. The first objection emphasizes\nlanguage learning. The second emphasizes\nlanguage understanding.\n6.1 Learning a Language\n\nLike many cognitive scientists, Fodor holds that children learn a\nnatural language via hypothesis formation and testing.\nChildren formulate, test, and confirm hypotheses about the denotations\nof words. For example, a child learning English will confirm the\nhypothesis that \u201ccat\u201d denotes cats. According to Fodor,\ndenotations are represented in Mentalese. To formulate the hypothesis\nthat \u201ccat\u201d denotes cats, the child uses a Mentalese word\ncat that denotes cats. It may seem that a\nregress is now in the offing, sparked by the question: How does the\nchild learn Mentalese? Suppose we extend the hypothesis formation and\ntesting model (henceforth HF) to Mentalese. Then we must posit a\nmeta-language to express hypotheses about denotations of Mentalese\nwords, a meta-meta-language to express hypotheses about denotations of\nmeta-language words, and so on ad infinitum (Atherton and\nSchwartz 1974: 163).\n\nFodor responds to the threatened regress by denying we should apply HF\nto Mentalese (1975: 65). Children do not test hypotheses about the\ndenotations of Mentalese words. They do not learn Mentalese at all.\nThe mental language is innate.\n\nThe doctrine that some concepts are innate was a focal point\nin the clash between rationalism versus empiricism. Rationalists\ndefended the innateness of certain fundamental ideas, such as \ngod\nand cause, while\nempiricists held that all ideas derive from sensory experience. A\nmajor theme in the 1960s cognitive science revolution was revival of a\nnativist picture, inspired by the rationalists, on which many\nkey elements of cognition are innate. Most famously, Chomsky (1965)\nexplained language acquisition by positing innate knowledge about\npossible human languages. Fodor\u2019s innateness thesis was widely\nperceived as going way beyond all precedent, verging on the\npreposterous (P.S. Churchland 1986; Putnam 1988). How could we have an\ninnate ability to represent all the denotations we mentally represent?\nFor example, how could we innately possess a Mentalese word \ncarburetor\nthat represents carburetors?\n\nIn evaluating these issues, it is vital to distinguish between\nlearning a concept versus acquiring a concept. When\nFodor says that a concept is innate, he does not mean to deny that we\nacquire the concept or even that certain kinds of experience are\nneeded to acquire it. Fodor fully grants that we cannot mentally\nrepresent carburetors at birth and that we come to represent them only\nby undergoing appropriate experiences. He agrees that most concepts\nare acquired. He denies that they are learned. In\neffect, he uses \u201cinnate\u201d as a synonym for\n\u201cunlearned\u201d (1975: 96). One might reasonably challenge\nFodor\u2019s usage. One might resist classifying a concept as innate\nsimply because it is unlearned. However, that is how Fodor\nuses the word \u201cinnate\u201d. Properly understood, then,\nFodor\u2019s position is not as far-fetched as it may\n sound.[7]\n\nFodor gives a simple but striking argument that concepts are\nunlearned. The argument begins from the premise that HF is the only\npotentially viable model of concept learning. Fodor then\nargues that HF is not a viable model of concept learning,\nfrom which he concludes that concepts are unlearned. He offers various\nformulations and refinements of the argument over his career. Here is\na relatively recent rendition (2008: 139):\n\n\nNow, according to HF, the process by which one learns C must\ninclude the inductive evaluation of some such hypothesis as \u201cThe\nC things are the ones that are green or triangular\u201d. But\nthe inductive evaluation of that hypothesis itself requires (inter\nalia) bringing the property green or triangular before\nthe mind as such\u2026 Quite generally, you can\u2019t represent\nanything as such and such unless you already have the concept\nsuch and such. All that being so, it follows, on pain of\ncircularity, that \u201cconcept learning\u201d as HF understands it\ncan\u2019t be a way of acquiring concept C\u2026\nConclusion: If concept learning is as HF understands it, there can\nbe no such thing. This conclusion is entirely general; it\ndoesn\u2019t matter whether the target concept is primitive (like\ngreen) or complex (like green\nor triangular).\n\n\nFodor\u2019s argument does not presuppose RTT, COMP, or CTM. To the\nextent that the argument works, it applies to any view on which people\nhave concepts.\n\nIf concepts are not learned, then how are they acquired? Fodor offers\nsome preliminary remarks (2008: 144\u2013168), but by his own\nadmission the remarks are sketchy and leave numerous questions\nunanswered (2008: 144\u2013145). Prinz (2011) critiques Fodor\u2019s\npositive treatment of concept acquisition.\n\nThe most common rejoinder to Fodor\u2019s innateness argument is to\ndeny that HF is the only viable model of concept learning. The\nrejoinder acknowledges that concepts are not learned through\nhypothesis testing but insists they are learned through other\nmeans. Three examples:\n\n Margolis (1998) proposes an acquisition model that differs from\nHF but that allegedly yields concept learning. Fodor (2008:\n140\u2013144) retorts that Margolis\u2019s model does not yield\ngenuine concept learning. Margolis and Laurence (2011) insist that it\ndoes.\n Carey (2009) maintains that children can \u201cbootstrap\u201d\ntheir way to new concepts using induction, analogical reasoning, and\nother techniques. She develops her view in great detail, supporting it\npartly through her groundbreaking experimental work with young\nchildren. Fodor (2010) and Rey (2014) object that Carey\u2019s\nbootstrapping theory is circular: it surreptitiously presupposes that\nchildren already possess the very concepts whose acquisition it\npurports to explain. Beck (2017) and Carey (2014) respond to the\ncircularity objection.\n Shea (2016) argues that connectionist modeling can explain\nconcept acquisition in non-HF terms and that the resulting models\ninstantiate genuine learning.\n\n\nA lot depends here upon what counts as \u201clearning\u201d and what\ndoes not, a question that seems difficult to adjudicate. A closely\nconnected question is whether concept acquisition is a\nrational process or a mere causal process. To the\nextent that acquiring some concept is a rational achievement, we will\nwant to say that one learned the concept. To the extent that acquiring\nthe concept is a mere causal process (more like catching a cold than\nconfirming a hypothesis), we will feel less inclined to say that\ngenuine learning took place (Fodor 1981: 275).\n\nThese issues lie at the frontier of psychological and philosophical\nresearch. The key point for present purposes is that there are two\noptions for halting the regress of language learning: we can say that\nthinkers acquire concepts but do not learn them; or we can say that\nthinkers learn concepts through some means other than hypothesis\ntesting. Of course, it is not enough just to note that the two options\nexist. Ultimately, one must develop one\u2019s favored option into a\ncompelling theory. But there is no reason to think that doing so would\nreinitiate the regress. In any event, explaining concept acquisition\nis an important task facing any theorist who accepts that we have\nconcepts, whether or not the theorist accepts LOTH. Thus, the learning\nregress objection is best regarded not as posing a challenge specific\nto LOTH but rather as highlighting a more widely shared theoretical\nobligation: the obligation to explain how we acquire concepts.\n\nFor further discussion, see the entry on innateness. See also the\nexchange between Cowie (1999) and Fodor (2001).\n6.2 Understanding a Language\n\nWhat is it to understand a natural language word? On a popular\npicture, understanding a word requires that you mentally represent the\nword\u2019s denotation. For example, understanding the word\n\u201ccat\u201d requires representing that it denotes cats. LOT\ntheorists will say that you use Mentalese words to represent\ndenotations. The question now arises what it is to understand a\nMentalese word. If understanding the Mentalese word requires\nrepresenting that it has a certain denotation, then we face an\ninfinite regress of meta-languages (Blackburn 1984: 43\u201344).\n\nThe standard response is to deny that ordinary thinkers represent\nMentalese words as having denotations (Bach 1987; Fodor 1975:\n66\u201379). Mentalese is not an instrument of communication.\nThinking is not \u201ctalking to oneself\u201d in Mentalese. A\ntypical thinker does not represent, perceive, interpret, or reflect\nupon Mentalese expressions. Mentalese serves as a medium within which\nher thought occurs, not an object of interpretation. We should not say\nthat she \u201cunderstands\u201d Mentalese in the same way that she\nunderstands a natural language.\n\nThere is perhaps another sense in which the thinker\n\u201cunderstands\u201d Mentalese: her mental activity coheres with\nthe meanings of Mentalese words. For example, her deductive reasoning\ncoheres with the truth-tables expressed by Mentalese logical\nconnectives. More generally, her mental activity is semantically\ncoherent. To say that the thinker \u201cunderstands\u201d Mentalese\nin this sense is not to say that she represents Mentalese\ndenotations. Nor is there any evident reason to suspect that\nexplaining semantic coherence will ultimately require us to posit\nmental representation of Mentalese denotations. So there is no regress\nof understanding.\n\nFor further criticism of this regress argument, see the discussions of\nKnowles (1998) and Laurence and Margolis\n (1997).[8]\n7. Naturalizing the Mind\n\nNaturalism\n is a movement that seeks to ground philosophical theorizing in the\nscientific enterprise. As so often in philosophy, different authors\nuse the term \u201cnaturalism\u201d in different ways. Usage within\nphilosophy of mind typically connotes an effort to depict mental\nstates and processes as denizens of the physical world, with no\nirreducibly mental entities or properties allowed. In the modern era,\nphilosophers have often recruited LOTH to advance naturalism. Indeed,\nLOTH\u2019s supposed contribution to naturalism is frequently cited\nas a significant consideration in its favor. One example is\nFodor\u2019s use of CCTM+FSC to explain semantic coherence. The other\nmain example turns upon the problem of intentionality.\n\nHow does intentionality arise? How do mental states come to be\nabout anything, or to have semantic properties? Brentano\n(1874 [1973: 97]) maintained that intentionality is a hallmark of the\nmental as opposed to the physical: \u201cThe reference to something\nas an object is a distinguishing characteristic of all mental\nphenomena. No physical phenomenon exhibits anything similar\u201d. In\nresponse, contemporary naturalists seek to naturalize\nintentionality. They want to explain in naturalistically\nacceptable terms what makes it the case that mental states have\nsemantic properties. In effect, the goal is to reduce the intentional\nto the non-intentional. Beginning in the 1980s, philosophers have\noffered various proposals about how to naturalize intentionality. Most\nproposals emphasize causal or nomic links between mind and world\n(Aydede & G\u00fczeldere 2005; Dretske 1981; Fodor 1987, 1990;\nStalnaker 1984), sometimes also invoking teleological factors\n(Millikan 1984, 1993; Neander 2017l; Papineau 1987; Dretske 1988) or\nhistorical lineages of mental states (Devitt 1995; Field 2001).\nAnother approach, functional role semantics, emphasizes the\nfunctional role of a mental state: the cluster of causal or\ninferential relations that the state bears to other mental states. The\nidea is that meaning emerges at least partly through these causal and\ninferential relations. Some functional role theories cite causal\nrelations to the external world (Block 1987; Loar 1982), and others do\nnot (Cummins 1989).\n\nEven the best developed attempts at naturalizing intentionality, such\nas Fodor\u2019s (1990) version of the nomic strategy, face serious\nproblems that no one knows how to solve (M. Greenberg 2014; Loewer\n1997). Partly for that reason, the flurry of naturalizing attempts\nabated in the 2000s. Burge (2010: 298) reckons that the naturalizing\nproject is not promising and that current proposals are\n\u201chopeless\u201d. He agrees that we should try to illuminate\nrepresentationality by limning its connections to the physical, the\ncausal, the biological, and the teleological. But he insists that\nillumination need not yield a reduction of the intentional to the\nnon-intentional.\n\nLOTH is neutral as to the naturalization of intentionality. An LOT\ntheorist might attempt to reduce the intentional to the\nnon-intentional. Alternatively, she might dismiss the reductive\nproject as impossible or pointless. Assuming she chooses the reductive\nroute, LOTH provides guidance regarding how she might proceed.\nAccording to RTT,\n\n\nX A\u2019s that p iff there is a mental\nrepresentation S such that X bears A* to S\nand S means that p.\n\n\nThe task of elucidating \u201cX A\u2019s that\np\u201d in naturalistically acceptable terms factors into two\nsub-tasks (Field 2001: 33):\n\n Explain in naturalistically acceptable terms what it is to bear\npsychological relation A* to mental representation\nS.\n Explain in naturalistically acceptable terms what it is for\nmental representation S to mean that p.\n\n\nAs we have seen, functionalism helps with (a). Moreover, COMP provides\na blueprint for tackling (b). We can first delineate a compositional\nsemantics describing how S\u2019s meaning depends upon\nsemantic properties of its component words and upon the compositional\nimport of the constituency structure into which those words are\narranged. We can then explain in naturalistically acceptable terms why\nthe component words have the semantic properties that they have and\nwhy the constituency structure has the compositional import that it\nhas.\n\nHow much does LOTH advance the naturalization of intentionality? Our\ncompositional semantics for Mentalese may illuminate how the semantic\nproperties of a complex expression depend upon the semantic properties\nof primitive expressions, but it says nothing about how primitive\nexpressions get their semantic properties in the first place.\nBrentano\u2019s challenge (How could intentionality arise from\npurely physical entities and processes?) remains unanswered. To\nmeet the challenge, we must invoke naturalizing strategies that go\nwell beyond LOTH itself, such as the causal or nomic strategies\nmentioned above. Those naturalizing strategies are not specifically\nlinked to LOTH and can usually be tailored to semantic properties of\nneural states rather than semantic properties of Mentalese\nexpressions. Thus, it is debatable how much LOTH ultimately helps us\nnaturalize intentionality. Naturalizing strategies orthogonal to LOTH\nseem to do the heavy lifting.\n8. Individuation of Mentalese Expressions\n\nHow are Mentalese expressions individuated? Since Mentalese\nexpressions are types, answering this question requires us to consider\nthe type/token relation for Mentalese. We want to fill in the\nschema\n\n\ne and e* are tokens of the same Mentalese type iff\nR(e, e*).\n\n\nWhat should we substitute for R(e, e*)? The\nliterature typically focuses on primitive symbol types, and\nwe will follow suit here.\n\nIt is almost universally agreed among contemporary LOT theorists that\nMentalese tokens are neurophysiological entities of some sort. One\nmight therefore hope to individuate Mentalese types by citing neural\nproperties of the tokens. Drawing R(e, e*) from\nthe language of neuroscience induces a theory along the following\nlines:\n\n\nNeural individuation: e and e*\nare tokens of the same primitive Mentalese type iff e and\ne* are tokens of the same neural type.\n\n\nThis schema leaves open how neural types are individuated. We may\nbypass that question here, because neural individuation of Mentalese\ntypes finds no proponents in the contemporary literature. The main\nreason is that it conflicts with\n multiple realizability:\n the doctrine that a single mental state type can be realized by\nphysical systems that are wildly heterogeneous when described in\nphysical, biological, or neuroscientific terms. Putnam (1967)\nintroduced multiple realizability as evidence against the\n mind/brain identity theory,\n which asserts that mental state types are brain state types. Fodor\n(1975: 13\u201325) further developed the multiple realizability\nargument, presenting it as foundational to LOTH. Although the multiple\nrealizability argument has subsequently been challenged (Polger 2004),\nLOT theorists widely agree that we should not individuate Mentalese\ntypes in neural terms.\n\nThe most popular strategy is to individuate Mentalese types\nfunctionally:\n\n\nFunctional individuation: e and\ne* are tokens of the same primitive Mentalese type iff e\nand e* have the same functional role.\n\n\nField (2001: 56\u201367), Fodor (1994: 105\u2013109), and Stich\n(1983: 149\u2013151) pursue functional individuation. They specify\nfunctional roles using a Turing-style computationalism formalism, so\nthat \u201cfunctional role\u201d becomes something like\n\u201ccomputational role\u201d, i.e., role within mental\ncomputation.\n\nFunctional roles theories divide into two categories:\nmolecular and holist. Molecular theories isolate\nprivileged canonical relations that a symbol bears to other symbols.\nCanonical relations individuate the symbol, but non-canonical\nrelations do not. For example, one might individuate Mentalese\nconjunction solely through the introduction and elimination rules\ngoverning conjunction while ignoring any other computational rules. If\nwe say that a symbol\u2019s \u201ccanonical functional role\u201d\nis constituted by its canonical relations to other symbols, then we\ncan offer the following theory:\n\n\nMolecular functional individuation: e\nand e* are tokens of the same primitive Mentalese type iff\ne and e* have the same canonical functional role.\n\n\nOne problem facing molecular individuation is that, aside from logical\nconnectives and a few other special cases, it is difficult to draw any\nprincipled demarcation between canonical and non-canonical relations\n(Schneider 2011: 106). Which relations are canonical for\n SOFA?[9]\n Citing the demarcation problem, Schneider espouses a holist approach\nthat individuates mental symbols through total functional\nrole, i.e., every single aspect of the role that a symbol plays\nwithin mental activity:\n\n\nHolist functional individuation: e\nand e* are tokens of the same primitive Mentalese type iff\ne and e* have the same total functional role.\n\n\nHolist individuation is very fine-grained: the slightest difference in\ntotal functional role entails that different types are tokened. Since\ndifferent thinkers will always differ somewhat in their mental\ncomputations, it now looks like two thinkers will never share the same\nmental language. This consequence is worrisome, for two reasons\nemphasized by Aydede (1998). First, it violates the plausible\npublicity constraint that propositional attitudes are in\nprinciple shareable. Second, it apparently precludes interpersonal\npsychological explanations that cite Mentalese expressions. Schneider\n(2011: 111\u2013158) addresses both concerns, arguing that they are\nmisdirected.\n\nA crucial consideration when individuating mental symbols is what role\nto assign to semantic properties. Here we may usefully compare\nMentalese with natural language. It is widely agreed that natural\nlanguage words do not have their denotations essentially. The English\nword \u201ccat\u201d denotes cats, but it could just as well have\ndenoted dogs, or the number 27, or anything else, or nothing at all,\nif our linguistic conventions had been different. Virtually all\ncontemporary LOT theorists hold that a Mentalese word likewise does\nnot have its denotation essentially. The Mentalese word cat\ndenotes cats, but it could have had a different\ndenotation had it born different causal relations to the external\nworld or had it occupied a different role in mental activity. In that\nsense, cat is a piece of formal syntax.\nFodor\u2019s early view (1981: 225\u2013253) was that a Mentalese\nword could have had a different denotation but not an\narbitrarily different denotation: cat\ncould not have denoted just anything\u2014it could not have denoted\nthe number 27\u2014but it could have denoted some other animal\nspecies had the thinker suitably interacted with that species rather\nthan with cats. Fodor eventually (1994, 2008) embraces the stronger\nthesis that a Mentalese word bears an arbitrary relation to\nits denotation: cat could have had any\narbitrarily different denotation. Most contemporary theorists agree\n(Egan 1992: 446; Field 2001: 58; Harnad 1994: 386; Haugeland 1985: 91:\n117\u2013123; Pylyshyn 1984: 50).\n\nThe historical literature on LOTH suggests an alternative\nsemantically permeated view: Mentalese words are individuated\npartly through their denotations. The Mentalese word cat\nis not a piece of formal syntax subject to\nreinterpretation. It could not have denoted another species, or the\nnumber 27, or anything else. It denotes cats by its inherent\nnature. From a semantically permeated viewpoint, a Mentalese word\nhas its denotation essentially. Thus, there is a profound difference\nbetween natural language and mental language. Mental words, unlike\nnatural language words, bring with them one fixed semantic\ninterpretation. The semantically permeated approach is present in\nOckham, among other medieval LOT theorists (Normore 2003, 2009). In\nlight of the problems facing neural and functional individuation,\nAydede (2005) recommends that we consider taking semantics into\naccount when individuating Mentalese expressions. Rescorla (2012b)\nconcurs, defending a semantically permeated approach as applied to at\nleast some mental representations. He proposes that certain mental\ncomputations operate over mental symbols with essential semantic\nproperties, and he argues that the proposal fits well with many\nsectors of cognitive\n science.[10]\n\nA recurring complaint about the semantically permeated approach is\nthat inherently meaningful mental representations seem like highly\nsuspect entities (Putnam 1988: 21). How could a mental word have one\nfixed denotation by its inherent nature? What magic ensures\nthe necessary connection between the word and the denotation? These\nworries diminish in force if one keeps firmly in mind that Mentalese\nwords are types. Types are abstract entities corresponding to a scheme\nfor classifying, or type-identifying, tokens. To ascribe a\ntype to a token is to type-identify the token as belonging to some\ncategory. Semantically permeated types correspond to a classificatory\nscheme that takes semantics into account when categorizing tokens. As\nBurge emphasizes (2007: 302), there is nothing magical about\nsemantically-based classification. On the contrary, both folk\npsychology and cognitive science routinely classify mental events\nbased at least partly upon their semantic properties.\n\nA simplistic implementation of the semantically permeated approach\nindividuates symbol tokens solely through their\ndenotations:\n\n\nDenotational individuation: e and\ne* are tokens of the same primitive Mentalese type iff e\nand e* have the same denotation.\n\n\nAs Aydede (2000) and Schneider (2011) emphasize, denotational\nindividuation is unsatisfying. Co-referring words may play\nsignificantly different roles in mental activity. Frege\u2019s (1892\n[1997]) famous Hesperus-Phosphorus example illustrates: one can\nbelieve that Hesperus is Hesperus without believing that Hesperus is\nPhosphorus. As Frege put it, one can think about the same denotation\n\u201cin different ways\u201d, or \u201cunder different modes of\npresentation\u201d. Different modes of presentation have different\nroles within mental activity, implicating different psychological\nexplanations. Thus, a semantically permeated individuative scheme\nadequate for psychological explanation must be finer-grained than\ndenotational individuation allows. It must take mode of presentation\ninto account. But what it is to think about a denotation \u201cunder\nthe same mode of presentation\u201d? How are \u201cmodes of\npresentation\u201d individuated? Ultimately, semantically permeated\ntheorists must grapple with these questions. Rescorla (2020)\noffers some suggestions about how to\n proceed.[11]\n\nChalmers (2012) complains that semantically permeated individuation\nsacrifices significant virtues that made LOTH attractive in the first\nplace. LOTH promised to advance naturalism by grounding cognitive\nscience in non-representational computational models.\nRepresentationally-specified computational models seem like a\nsignificant retrenchment from these naturalistic ambitions. For\nexample, semantically permeated theorists cannot accept the FSC\nexplanation of semantic coherence, because they do not postulate\nformal syntactic types manipulated during mental computation.\n\nHow compelling one finds naturalistic worries about semantically\npermeated individuation will depend on how impressive one finds the\nnaturalistic contributions made by formal mental syntax. We saw\nearlier that FSC arguably engenders a worrisome epiphenomenalism.\nMoreover, the semantically permeated approach in no way precludes a\nnaturalistic reduction of intentionality. It merely precludes invoking\nformal syntactic Mentalese types while executing such a reduction. For\nexample, proponents of the semantically permeated approach can still\npursue the causal or nomic naturalizing strategies discussed in\n section 7.\n Nothing about either strategy presupposes formal syntactic Mentalese\ntypes. Thus, it is not clear that replacing a formal syntactic\nindividuative scheme with a semantically permeated scheme\nsignificantly impedes the naturalistic endeavor.\n\nNo one has yet provided an individuative scheme for Mentalese that\ncommands widespread assent. The topic demands continued investigation,\nbecause LOTH remains highly schematic until its proponents clarify\nsameness and difference of Mentalese types.\n",
    "bibliography": {
        "categories": [],
        "cat_ref_text": {
            "ref_list": [
                "Arjo, Dennis, 1996, \u201cSticking Up for Oedipus: Fodor on\nIntentional Generalizations and Broad Content\u201d, <em>Mind &amp;\nLanguage</em>, 11(3): 231\u2013245.\ndoi:10.1111/j.1468-0017.1996.tb00044.x",
                "Armstrong, D. M., 1973, <em>Belief Truth and Knowledge</em>,\nCambridge: Cambridge University Press.\ndoi:10.1017/CBO9780511570827",
                "Atherton, Margaret and Robert Schwartz, 1974, \u201cLinguistic\nInnateness and Its Evidence\u201d:, <em>Journal of Philosophy</em>,\n71(6): 155\u2013168. doi:10.2307/2024657",
                "Aydede, Murat, 1995, \u201cConnectionism and Language of\nThought\u201d, CSLI Technical Report 195, Stanford: Center for the\nStudy of Language and Information Publications. ",
                "\u2013\u2013\u2013, 1997a, \u201cLanguage of Thought: The\nConnectionist Contribution\u201d, <em>Minds and Machines</em>, 7(1):\n57\u2013101. doi:10.1023/A:1008203301671",
                "\u2013\u2013\u2013, 1997b, \u201cHas Fodor Really Changed His\nMind on Narrow Content?\u201d, <em>Mind &amp; Language</em>,\n12(3\u20134): 422\u2013458.\ndoi:10.1111/j.1468-0017.1997.tb00082.x",
                "\u2013\u2013\u2013, 1998, \u201cFodor on Concepts and Frege\nPuzzles\u201d, <em>Pacific Philosophical Quarterly</em>, 79(4):\n289\u2013294. doi:10.1111/1468-0114.00063",
                "\u2013\u2013\u2013, 2000, \u201cOn the Type/Token Relation of\nMental Representations\u201d, <em>Facta Philosophica</em>, 2:\n23\u201349.",
                "\u2013\u2013\u2013, 2005, \u201cComputation and Functionalism:\nSyntactic Theory of Mind Revisited\u201d, in <em>Turkish Studies in\nthe History and Philosophy of Science</em>, G\u00fcrol Irzik and\nG\u00fcven G\u00fczeldere (eds.), (Boston Studies in the History and\nPhilosophy of Science 244), Berlin/Heidelberg: Springer-Verlag,\n177\u2013204. doi:10.1007/1-4020-3333-8_13",
                "\u2013\u2013\u2013, 2015, \u201cThe Language of Thought\nHypothesis\u201d, <em>The Stanford Encyclopedia of Philosophy</em>\n(Fall 2015 Edition), Edward Zalta (ed.). URL =\n &lt;<a href=\"https://plato.stanford.edu/archives/fall2015/entries/language-thought/\" target=\"other\">https://plato.stanford.edu/archives/fall2015/entries/language-thought/</a>&gt;.",
                "Aydede, Murat and G\u00fcven G\u00fczeldere, 2005,\n\u201cCognitive Architecture, Concepts, and Introspection: An\nInformation-Theoretic Solution to the Problem of Phenomenal\nConsciousness\u201d, <em>No\u00fbs</em>, 39(2): 197\u2013255.\ndoi:10.1111/j.0029-4624.2005.00500.x",
                "Aydede, Murat and Philip Robbins, 2001, \u201cAre Frege Cases\nExceptions to Intentional Generalizations?\u201d, <em>Canadian\nJournal of Philosophy</em>, 31(1): 1\u201322.\ndoi:10.1080/00455091.2001.10717558",
                "Bach, Kent, 1987, \u201cReview: <em>Spreading the\nWord</em>\u201d, <em>The Philosophical Review</em>, 96(1):\n120\u2013123. doi:10.2307/2185336",
                "Barsalou, Lawrence W., 1999, \u201cPerceptual Symbol\nSystems\u201d, <em>Behavioral and Brain Sciences</em>, 22(4):\n577\u2013660. doi:10.1017/S0140525X99002149",
                "Bechtel, William and Adele Abrahamsen, 2002, <em>Connectionism and\nthe Mind: Parallel Processing, Dynamics and Evolution in\nNetworks</em>, second edition, Malden, MA: Blackwell.",
                "Beck, Jacob, 2017, \u201cCan Bootstrapping Explain Concept\nLearning?\u201d, <em>Cognition</em>, 158: 110\u2013121.\ndoi:10.1016/j.cognition.2016.10.017",
                "Berm\u00fadez, Jos\u00e9 Luis, 2010, <em>Cognitive Science: An\nIntroduction to the Science of the Mind</em>, Cambridge: Cambridge\nUniversity Press.",
                "Blackburn, Simon, 1984, <em>Spreading the Word</em>, Oxford:\nOxford University Press.",
                "Block, Ned, 1983, \u201cMental Pictures and Cognitive\nScience\u201d, <em>The Philosophical Review</em>, 92(4):\n499\u2013451. doi:10.2307/2184879",
                "\u2013\u2013\u2013, 1987, \u201cAdvertisement for a Semantics\nfor Psychology\u201d, in <em>Midwest Studies in Philosophy</em>, 10:\n615\u2013678. doi:10.1111/j.1475-4975.1987.tb00558.x",
                "\u2013\u2013\u2013, 1990, \u201cCan the Mind Change the\nWorld?\u201d, in <em>Meaning and Method: Essays in Honor of Hilary\nPutnam</em>, George Boolos (ed.), Cambridge: Cambridge University\nPress.",
                "\u2013\u2013\u2013, 2023, <em>The Border Between Seeing and\nThinking</em>, Oxford: Oxford University Press.",
                "Blumson, Ben, 2012, \u201cMental Maps\u201d, <em>Philosophy and\nPhenomenological Research</em>, 85(2): 413\u2013434.\ndoi:10.1111/j.1933-1592.2011.00499.x",
                "Braddon-Mitchell, David and John Fitzpatrick, 1990,\n\u201cExplanation and the Language of Thought\u201d,\n<em>Synthese</em>, 83(1): 3\u201329. doi: 10.1007/BF00413686",
                "Braddon-Mitchell, David and Frank Jackson, 2007, <em>Philosophy of\nMind and Cognition</em>, second edition, Cambridge: Blackwell.",
                "Brentano, Franz, 1874 [1973], <em>Psychology from an Empirical\nStandpoint</em> (<em>Psychologie vom empirischen Standpunkt</em>, 1924\nedition), Antos C. Rancurello, D.B. Terrell, and Linda McAlister\n(trans.), London: Routledge and Kegan Paul.",
                "Burge, Tyler, 2007, <em>Foundations of Mind</em>, (Philosophical\nEssays, 2), Oxford: Oxford University Press.",
                "\u2013\u2013\u2013, 2010, <em>Origins of Objectivity</em>,\nOxford: Oxford University Press.\ndoi:10.1093/acprof:oso/9780199581405.001.0001",
                "\u2013\u2013\u2013, 2018, \u201cIconic Representation: Maps,\nPictures, and Perception\u201d, in <em>The Map and the Territory:\nExploring the Foundations of Science, Thought, and Reality</em>, Shyam\nWuppuluri and Francisco Antonio Doria (eds.), Cham: Springer\nInternational Publishing, 79\u2013100.\ndoi:10.1007/978-3-319-72478-2_5",
                "\u2013\u2013\u2013, 2022, <em>Perception: First Form of\nMind</em>, Oxford: Oxford University Press.",
                "Camp, Elisabeth, 2009, \u201cA Language of Baboon\nThought?\u201d, in Lurz 2009: 108\u2013127.\ndoi:10.1017/CBO9780511819001.007",
                "\u2013\u2013\u2013, 2018, \u201cWhy Maps Are Not\nPropositional\u201d, in <em>Non-Propositional Intentionality</em>,\nAlex Grzankowski and Michelle Montague (eds.), Oxford: Oxford\nUniversity Press. doi:10.1093/oso/9780198732570.003.0002",
                "Carey, Susan, 2009, <em>The Origin of Concepts</em>, Oxford:\nOxford University Press.\ndoi:10.1093/acprof:oso/9780195367638.001.0001",
                "\u2013\u2013\u2013, 2014, \u201cOn Learning New Primitives in\nthe Language of Thought: Reply to Rey\u201d, <em>Mind and\nLanguage</em>, 29(2): 133\u2013166. doi:10.1111/mila.12045",
                "Casati, Roberto and Achille C. Varzi, 1999, <em>Parts and Places:\nThe Structures of Spatial Representation</em>, Cambridge, MA: MIT\nPress.",
                "Chalmers, David J., 1990, \u201cSyntactic Transformations on\nDistributed Representations\u201d, <em>Connection Science</em>,\n2(1\u20132): 53\u201362. doi:10.1080/09540099008915662",
                "\u2013\u2013\u2013, 1993, \u201cConnectionism and\nCompositionality: Why Fodor and Pylyshyn Were Wrong\u201d,\n<em>Philosophical Psychology</em>, 6(3): 305\u2013319.\ndoi:10.1080/09515089308573094",
                "\u2013\u2013\u2013, 2012, \u201cThe Varieties of Computation:\nA Reply\u201d, <em>Journal of Cognitive Science</em>, 13(3):\n211\u2013248. doi:10.17791/jcs.2012.13.3.211",
                "Chomsky, Noam, 1965, <em>Aspects of the Theory of Syntax</em>.\nCambridge, MA: MIT Press.",
                "Churchland, Patricia S., 1986, <em>Neurophilosophy: Toward a\nUnified Science of Mind-Brain</em>, Cambridge, MA: MIT Press.",
                "Churchland, Patricia S. and Terrence J. Sejnowski, 1989,\n\u201cNeural Representation and Neural Computation\u201d, in\n<em>Neural Connections, Neural Computation</em>, Lynn Nadel, Lynn A.\nCooper, Peter W. Culicover, and Robert M. Harnish, Cambridge, MA: MIT\nPress.",
                "Churchland, Paul M., 1990, <em>A Neurocomputational Perspective:\nThe Nature of Mind and the Structure of Science</em>, Cambridge, MA:\nMIT Press.",
                "Churchland, Paul M., and Patricia S. Churchland, 1990,\n\u201cCould a Machine Think?\u201d, <em>Scientific American</em>,\n262(1): 32\u201337. doi:10.1038/scientificamerican0190-32",
                "Clark, Andy, 1991, \u201cSystematicity, Structured\nRepresentations and Cognitive Architecture: A Reply to Fodor and\nPylyshyn\u201d, in Horgan and Tienson 1991: 198\u2013218.\ndoi:10.1007/978-94-011-3524-5_9",
                "\u2013\u2013\u2013, 2014, <em>Mindware: An Introduction to the\nPhilosophy of Cognitive Science</em>, second edition, Oxford: Oxford\nUniversity Press.",
                "Cowie, Fiona, 1999, <em>What\u2019s Within? Nativism\nReconsidered</em>, Oxford: Oxford University Press.\ndoi:10.1093/acprof:oso/9780195159783.001.0001",
                "Cummins, Robert, 1989, <em>Meaning and Mental Representation</em>,\nCambridge, MA: MIT Press.",
                "Dennett, Daniel C., 1977 [1981], \u201cCritical Noticw: Review of\nThe Language of Thought by Jerry Fodor\u201d, <em>Mind</em>, 86(342):\n265\u2013280. Reprinted as \u201cA Cure for the Common Code\u201d,\nin <em>Brainstorms: Philosophical Essays on Mind and Psychology</em>,\nCambridge, MA: MIT Press, 1981. doi:10.1093/mind/LXXXVI.342.265",
                "\u2013\u2013\u2013, 1991, \u201cMother Nature Versus the\nWalking Encyclopedia: A Western Drama\u201d, in <em>Philosophy and\nConnectionist Theory</em>, W. Ramsey, S. Stich, and D. Rumelhart,\nHillsdale, NJ: Lawrence Erlbaum Associates.\n [<a href=\"https://ase.tufts.edu/cogstud/dennett/papers/motherna.htm\" target=\"other\">available online</a>]",
                "Devitt, Michael, 1995, <em>Coming to Our Senses: A Naturalistic\nProgram for Semantic Localism</em>, Cambridge: Cambridge University\nPress. doi:10.1017/CBO9780511609190",
                "Dretske, Fred, 1981, <em>Knowledge and the Flow of\nInformation</em>, Cambridge, MA: MIT Press.",
                "\u2013\u2013\u2013, 1988. <em>Explaining Behavior</em>,\nCambridge, MA: MIT Press.",
                "Egan, Frances, 1992, \u201cIndividualism, Computation, and\nPerceptual Content\u201d, <em>Mind</em>, 101(403): 443\u2013459.\ndoi:10.1093/mind/101.403.443",
                "Elman, Jeffrey L., 1989, \u201cStructured Representations and\nConnectionist Models\u201d, in <em>Proceedings of the Eleventh Annual\nMeeting of the Cognitive Science Society</em>, Mahwah: Laurence\nErlbaum Associates.",
                "Field, Hartry, 2001, <em>Truth and the Absence of Fact</em>,\nOxford: Oxford University Press. doi:10.1093/0199242895.001.0001",
                "Figdor, Carrie, 2009, \u201cSemantic Externalism and the\nMechanics of Thought\u201d, <em>Minds and Machines</em>, 19(1):\n1\u201324. doi:10.1007/s11023-008-9114-6",
                "Fodor, Jerry A., 1975, <em>The Language of Thought</em>, New York:\nThomas Y. Crowell.",
                "\u2013\u2013\u2013, 1981, <em>Representations</em>, Cambridge,\nMA: MIT Press.",
                "\u2013\u2013\u2013, 1987, <em>Psychosemantics</em>, Cambridge,\nMA: MIT Press.",
                "\u2013\u2013\u2013, 1990, <em>A Theory of Content and Other\nEssays</em>, Cambridge, MA: MIT Press.",
                "\u2013\u2013\u2013, 1991, \u201cReplies\u201d, in <em>Meaning\nin Mind: Fodor and His Critics</em>, Barry M. Loewer and Georges Rey\n(eds.), Cambridge, MA: MIT Press.",
                "\u2013\u2013\u2013, 1994, <em>The Elm and the Expert</em>,\nCambridge, MA: MIT Press.",
                "\u2013\u2013\u2013, 1998, <em>Concepts: Where Cognitive Science\nWent Wrong</em>, Oxford: Oxford University Press.\ndoi:10.1093/0198236360.001.0001",
                "\u2013\u2013\u2013, 2001, \u201cDoing without What\u2019s\nwithin: Fiona Cowie\u2019s Critique of Nativism\u201d,\n<em>Mind</em>, 110(437): 99\u2013148.\ndoi:10.1093/mind/110.437.99",
                "\u2013\u2013\u2013, 2003, <em>Hume Variations</em>, Oxford:\nOxford University Press.\ndoi:10.1093/acprof:oso/9780199287338.001.0001",
                "\u2013\u2013\u2013, 2008, <em>LOT 2: The Language of Thought\nRevisited</em>, Oxford: Oxford University Press.\ndoi:10.1093/acprof:oso/9780199548774.001.0001",
                "\u2013\u2013\u2013, 2010, \u201cWoof, Woof. Review of <em>The\nOrigin of Concepts</em> by Susan Carey\u201d, <em>The Times Literary\nSupplement</em>, October 8: pp. 7\u20138.",
                "Fodor, Jerry and Brian P. McLaughlin, 1990, \u201cConnectionism\nand the Problem of Systematicity: Why Smolensky\u2019s Solution\nDoesn\u2019t Work\u201d, <em>Cognition</em>, 35(2): 183\u2013204.\ndoi:10.1016/0010-0277(90)90014-B",
                "Fodor, Jerry A. and Zenon W. Pylyshyn, 1981, \u201cHow Direct Is\nVisual Perception?: Some Reflections on Gibson\u2019s\n\u2018Ecological Approach\u2019\u201d, <em>Cognition</em>, 9(2):\n139\u2013196. doi:10.1016/0010-0277(81)90009-3",
                "\u2013\u2013\u2013, 1988, \u201cConnectionism and Cognitive\nArchitecture: A Critical Analysis\u201d, <em>Cognition</em>,\n28(1\u20132): 3\u201371. doi:10.1016/0010-0277(88)90031-5",
                "\u2013\u2013\u2013, 2015, <em>Minds Without Meanings</em>,\nCambridge, MA: MIT Press.",
                "Frege, Gottlob, 1879 [1967], <em>Begriffsschrift, eine der\nArithmetischen Nachgebildete Formelsprache des Reinen Denkens</em>.\nTranslated as <em>Concept Script, a Formal Language of Pure Thought\nModeled upon that of Arithmetic</em> in <em>From Frege to G\u00f6del:\nA Source Book in Mathematical Logic, 1879\u20131931</em>, J. van\nHeijenoort (ed.), S. Bauer-Mengelberg (trans.), Cambridge, MA: Harvard\nUniversity Press.",
                "\u2013\u2013\u2013, 1892 [1997], \u201cOn <em>Sinn</em> and\n<em>Bedeutung</em>\u201d. Reprinted in the <em>The Frege Reader</em>,\nM. Beaney (ed.), M. Black (trans.), Malden, MA: Blackwell.",
                "\u2013\u2013\u2013, 1918 [1997], \u201cThought\u201d.\nReprinted in <em>The Frege Reader</em>, M. Beaney (ed.), P. Geach and\nR. Stoothof (trans.), Malden, MA: Blackwell.",
                "Gallistel, Charles R., 1990, <em>The Organization of\nLearning</em>, Cambridge, MA: MIT Press.",
                "Gallistel, Charles R. and Adam Philip King, 2009, <em>Memory and\nthe Computational Brain</em>, Malden, MA: Wiley- Blackwell.",
                "Gallistel, C.R. and Louis D. Matzel, 2013, \u201cThe Neuroscience\nof Learning: Beyond the Hebbian Synapse\u201d, <em>Annual Review of\nPsychology</em>, 64(1): 169\u2013200.\ndoi:10.1146/annurev-psych-113011-143807",
                "Gibson, James J., 1979, <em>The Ecological Approach to Visual\nPerception</em>, Boston, MA: Houghton Mifflin.",
                "Greenberg, Gabriel, 2013, \u201cBeyond Resemblance\u201d,\n<em>Philosophical Review</em>, 122(2): 215\u2013287.\ndoi:10.1215/00318108-1963716",
                "Greenberg, Mark, 2014, \u201cTroubles for Content I\u201d, in\n<em>Metasemantics: New Essays on the Foundations of Meaning</em>,\nAlexis Burgess and Brett Sherman (eds.), Oxford: Oxford University\nPress, 147\u2013168.\ndoi:10.1093/acprof:oso/9780199669592.003.0006",
                "Harman, Gilbert, 1973, <em>Thought</em>, Princeton, NJ: Princeton\nUniversity Press.",
                "Harnad, Stevan, 1994, \u201cComputation Is Just Interpretable\nSymbol Manipulation; Cognition Isn\u2019t\u201d, <em>Minds and\nMachines</em>, 4(4): 379\u2013390. doi:10.1007/BF00974165",
                "Harnish, Robert M., 2002, <em>Minds, Brains, Computers: An\nHistorical Introduction to the Foundations of Cognitive Science</em>,\nMalden, MA: Blackwell.",
                "Haugeland, John, 1985, <em>Artificial Intelligence: The Very\nIdea</em>, Cambridge, MA: MIT Press",
                "Helmholtz, Hermann von, 1867 [1925], <em>Treatise on Physiological\nOptics</em> (<em>Handbuch der physiologischen Optik</em>), James P.C.\nSouthall, Manasha, WI: George Banta Publishing Company.",
                "Hinton, G. 1990. \u201cMapping Part-Whole Hierarchies into\nConnectionist Networks\u201d. <em>Artificial Intelligence</em> 46:\npp. 47\u201375.",
                "Horgan, Terence and John Tienson (eds.), 1991, <em>Connectionism\nand the Philosophy of Mind</em>, (Studies in Cognitive Systems 9),\nDordrecht: Springer Netherlands. doi:10.1007/978-94-011-3524-5",
                "\u2013\u2013\u2013, 1996, <em>Connectionism and the Philosophy\nof Psychology</em>, Cambridge, MA: MIT Press.",
                "Hume, David, 1739 [1978], <em>A Treatise on Human Nature</em>,\nsecond edition, P. H. Nidditch (ed.). Oxford: Clarendon Press.",
                "Jacobs, Lucia F and Randolf Menzel, 2014, \u201cNavigation\nOutside of the Box: What the Lab Can Learn from the Field and What the\nField Can Learn from the Lab\u201d, <em>Movement Ecology</em>, 2(1):\n3. doi:10.1186/2051-3933-2-3",
                "Johnson, Kent, 2004, \u201cOn the Systematicity of Language and\nThought\u201d:, <em>Journal of Philosophy</em>, 101(3):\n111\u2013139. doi:10.5840/jphil2004101321",
                "Johnson-Laird, Philip N., 2004, \u201cThe History of Mental\nModels\u201d, in <em>Psychology of Reasoning: Theoretical and\nHistorical Perspectives</em>, Ken Manktelow and Man Cheung Chung, New\nYork: Psychology Press.",
                "Kant, Immanuel, 1781 [1998], <em>The Critique of Pure Reason</em>,\nP. Guyer and A. Wood (eds), Cambridge: Cambridge University\nPress.",
                "Kaplan, David, 1989, \u201cDemonstratives\u201d, in <em>Themes\nfrom Kaplan</em>, Joseph Almog, John Perry, and Howard Wettstein\n(eds.), New York: Oxford University Press.",
                "Kazez, Jean R., 1994, \u201cComputationalism and the Causal Role\nof Content\u201d, <em>Philosophical Studies</em>, 75(3):\n231\u2013260. doi:10.1007/BF00989583",
                "King, Peter, 2005, \u201cWilliam of Ockham: <em>Summa\nLogicae</em>\u201d, in <em>Central Works of Philosophy: Ancient and\nMedieval, volume 1: Ancient and Medieval Philosophy</em>, John Shand\n(ed.), Montreal: McGill-Queen\u2019s University Press,\n242\u2013270.",
                "Knill, David C. and Whitman Richards (eds.), 1996, <em>Perception\nas Bayesian Inference</em>, Cambridge: Cambridge University Press.\ndoi:10.1017/CBO9780511984037",
                "Knowles, Jonathan, 1998, \u201cThe Language of Thought and\nNatural Language Understanding\u201d, <em>Analysis</em>, 58(4):\n264\u2013272. doi: 10.1093/analys/58.4.264",
                "Kosslyn, Stephen, 1980, <em>Image and Mind</em>, Cambridge, MA:\nHarvard University Press.",
                "Kulvicki, John, 2015, \u201cMaps, Pictures, and\nPredication\u201d, <em>Ergo: An Open Access Journal of\nPhilosophy</em>, 2(7): 149\u2013174.",
                "Laurence, Stephen and Eric Margolis, 1997, \u201cRegress\nArguments Against the Language of Thought\u201d, <em>Analysis</em>,\n57(1): 60\u201366.",
                "Loar, Brian, 1982, <em>Mind and Meaning</em>, Cambridge: Cambridge\nUniversity Press.",
                "Loewer, Barry, 1997, \u201cA Guide to Naturalizing\nSemantics\u201d, in <em>A Companion to the Philosophy of\nLanguage</em>, Bob Hale and Crispin Wright (eds.), Oxford:\nBlackwell.",
                "Lurz, Robert W. (ed.), 2009, <em>The Philosophy of Animal\nMinds</em>, Cambridge: Cambridge University Press.\ndoi:10.1017/CBO9780511819001",
                "Mackintosh, Nicholas John, 2002, \u201cDo Not Ask Whether They\nHave a Cognitive Map, but How They Find Their Way about\u201d,\n<em>Psicol\u00f3gica</em>, 23(1): 165\u2013185.\n [<a href=\"https://www.uv.es/psicologica/articulos1.02/M8MacKin.pdf\" target=\"other\">Mackintosh 2002 available online</a>]",
                "Mandelbaum, Eric, Yarrow Dunham, Roman Freiman, Chaz Firestone, E.\nJ. Green, Daniel Harris, Melissa Kibbe, Benedek Kurdi, Myrto\nMylopoulos, Joshua Sheperd, Alexis Wellwood, Nicholas Porot, and Jake\nQuilty-Dunn, 2022, \u201cProblems and Mysteries of the Many Languages\nof Thought\u201d, <em>Cognitive Science</em>, 46(12): e13225.",
                "Margolis, Eric, 1998, \u201cHow to Acquire a Concept\u201d,\n<em>Mind &amp; Language</em>, 13(3): 347\u2013369.\ndoi:10.1111/1468-0017.00081",
                "Margolis, Eric and Stephen Laurence, 2011, \u201cLearning\nMatters: The Role of Learning in Concept Acquisition\u201d, <em>Mind\n&amp; Language</em>, 26(5): 507\u2013539.\ndoi:10.1111/j.1468-0017.2011.01429.x",
                "McDermott, Drew V., 2001, <em>Mind and Mechanism</em>, Cambridge,\nMA: MIT Press.",
                "McLaughlin, B. P. and T. A. Warfield, 1994, \u201cThe Allure of\nConnectionism Reexamined\u201d, <em>Synthese</em>, 101(3):\n365\u2013400. doi:10.1007/BF01063895",
                "Marcus, G., 2001, <em>The Algebraic Mind</em>, Cambridge: MIT\nPress.",
                "Millikan, Ruth Garrett, 1984, <em>Language, Thought, and Other\nBiological Categories: New Foundations for Realism</em>, Cambridge,\nMA: MIT Press.",
                "\u2013\u2013\u2013, 1993, <em>White Queen Psychology and Other\nEssays for Alice</em>, Cambridge, MA: MIT Press.",
                "Neander, Karen, 2017, <em>A Mark of the Mental: In Defense of\nInformational Teleosemantics</em>, Cambridge, MA: MIT Press.",
                "Niklasson, Lars F. and Tim Gelder, 1994, \u201cOn Being\nSystematically Connectionist\u201d, <em>Mind &amp; Language</em>,\n9(3): 288\u2013302. doi:10.1111/j.1468-0017.1994.tb00227.x",
                "Normore, Calvin, 1990, \u201cOckham on Mental Language\u201d, in\n<em>The Historical Foundations of Cognitive Science</em>, J. Smith\n(ed.), Dordrecht: Kluwer.",
                "\u2013\u2013\u2013, 2003, \u201cBurge, Descartes, and\nUs\u201d, in <em>Reflections and Replies: Essays on the Philosophy of\nTyler Burge</em>, Martin Hahn and Bj\u00f8rn Ramberg, Cambridge, MA:\nMIT Press.",
                "\u2013\u2013\u2013, 2009, \u201cThe End of Mental\nLanguage\u201d, in <em>Le Langage Mental du Moyen \u00c2ge \u00e0\nl\u2019\u00c2ge Classique</em>, J. Biard (ed.), Leuven:\nPeeters.",
                "O\u2019Brien, Gerard and Jon Opie, 2006, \u201cHow Do\nConnectionist Networks Compute?\u201d, <em>Cognitive Processing</em>,\n7(1): 30\u201341. doi:10.1007/s10339-005-0017-7",
                "O\u2019Keefe, John and Lynn Nadel, 1978, <em>The Hippocampus as a\nCognitive Map</em>, Oxford: Clarendon Press.",
                "Ockham, William of, c. 1323 [1957], <em>Summa Logicae</em>,\nTranslated in his <em>Philosophical Writings, A Selection</em>,\nPhilotheus Boehner (ed. and trans.), London: Nelson, 1957.",
                "Panaccio, Claude, 1999 [2017], <em>Mental Language: From Plato to\nWilliam of Ockham</em> (<em>Discours int\u00e9rieur</em>), Joshua P.\nHochschild and Meredith K. Ziebart (trans.), New York: Fordham\nUniversity Press.",
                "Papineau, David, 1987, <em>Reality and Representation</em>,\nOxford: Basil Blackwell.",
                "Peacocke, Christopher, 1992, <em>A Study of Concepts</em>,\nCambridge, MA: MIT Press.",
                "\u2013\u2013\u2013, 1994, \u201cContent, Computation and\nExternalism\u201d, <em>Mind &amp; Language</em>, 9(3): 303\u2013335.\ndoi:10.1111/j.1468-0017.1994.tb00228.x",
                "\u2013\u2013\u2013, 1999, \u201cComputation as Involving\nContent: A Response to Egan\u201d, <em>Mind &amp; Language</em>,\n14(2): 195\u2013202. doi:10.1111/1468-0017.00109",
                "Perry, John, 1998, \u201cBroadening the Mind\u201d,\n<em>Philosophy and Phenomenological Research</em>, 58(1):\n223\u2013231. doi:10.2307/2653644",
                "Piccinini, Gualtiero, 2008, \u201cComputation without\nRepresentation\u201d, <em>Philosophical Studies</em>, 137(2):\n205\u2013241. doi:10.1007/s11098-005-5385-4",
                "Pinker, Steven, 2005, \u201cSo How Does the Mind Work?\u201d,\n<em>Mind &amp; Language</em>, 20(1): 1\u201324.\ndoi:10.1111/j.0268-1064.2005.00274.x",
                "Pinker, Steven and Alan Prince, 1988, \u201cOn Language and\nConnectionism: Analysis of a Parallel Distributed Processing Model of\nLanguage Acquisition\u201d, <em>Cognition</em>, 28(1\u20132):\n73\u2013193. doi:10.1016/0010-0277(88)90032-7",
                "Polger, Thomas W., 2004, <em>Natural Minds</em>, Cambridge, MA:\nMIT Press.",
                "Pollack, Jordan B., 1990, \u201cRecursive Distributed\nRepresentations\u201d, <em>Artificial Intelligence</em>,\n46(1\u20132): 77\u2013105. doi:10.1016/0004-3702(90)90005-K",
                "Prinz, Jesse, 2002, <em>Furnishing the Mind: Concepts and Their\nPerceptual Basis</em>, Cambridge, MA: MIT Press.",
                "\u2013\u2013\u2013, 2011, \u201cHas Mentalese Earned Its Keep?\nOn Jerry Fodor\u2019s LOT 2\u201d, <em>Mind</em>, 120(478):\n485\u2013501. doi:10.1093/mind/fzr025",
                "Putnam, Hilary, 1967, \u201cPsychophysical Predicates\u201d, In\n<em>Art, Mind, and Religion: Proceedings of the 1965 Oberlin\nColloquium in Philosophy</em>, W.H. Capitan and D.D. Merrill (eds),\nPittsburgh, PA: University of Pittsburgh Press, 37\u201348.",
                "\u2013\u2013\u2013, 1988, <em>Representation and Reality</em>,\nCambridge, MA: MIT Press.",
                "Pylyshyn, Zenon W., 1984, <em>Computation and Cognition: Toward a\nFoundation for Cognitive Science</em>, Cambridge, MA: MIT Press.",
                "\u2013\u2013\u2013, 2003, <em>Seeing and Visualizing:\nIt\u2019s Not What You Think</em>, Cambridge, MA: MIT Press.",
                "Quilty-Dunn, Jake, Nicholas Porot, and Eric Mandelbaum,\nforthcoming, \u201cThe Best Game in Town: The Re-Emergence of the\nLanguage of Thought Hypothesis Across the Cognitive Sciences\u201d,\n<em>Behavioral and Brain Sciences</em>.",
                "Quine, W. V., 1951 [1980], \u201cTwo Dogmas of Empiricism\u201d,\n<em>The Philosophical Review</em>, 60(1): 20\u201343. Reprinted in\nhis <em>From a Logical Point of View</em>, second edition, Cambridge,\nMA: Harvard University Press, 1980, 20\u201346.\ndoi:10.2307/2181906",
                "Ramsey, William M., 2007, <em>Representation Reconsidered</em>,\nCambridge: Cambridge University Press.\ndoi:10.1017/CBO9780511597954",
                "Rescorla, Michael, 2009a, \u201cChrysippus\u2019 Dog as a Case\nStudy in Non-Linguistic Cognition\u201d, in Lurz 2009: 52\u201371.\ndoi:10.1017/CBO9780511819001.004",
                "\u2013\u2013\u2013, 2009b, \u201cCognitive Maps and the\nLanguage of Thought\u201d, <em>The British Journal for the Philosophy\nof Science</em>, 60(2): 377\u2013407. doi:10.1093/bjps/axp012",
                "\u2013\u2013\u2013, 2009c, \u201cPredication and Cartographic\nRepresentation\u201d, <em>Synthese</em>, 169(1): 175\u2013200.\ndoi:10.1007/s11229-008-9343-5",
                "\u2013\u2013\u2013, 2012a, \u201cAre Computational Transitions\nSensitive to Semantics?\u201d, <em>Australasian Journal of\nPhilosophy</em>, 90(4): 703\u2013721.\ndoi:10.1080/00048402.2011.615333",
                "\u2013\u2013\u2013, 2012b, \u201cHow to Integrate\nRepresentation into Computational Modeling, and Why We Should\u201d,\n<em>Journal of Cognitive Science</em>, 13(1): 1\u201337.\ndoi:10.17791/jcs.2012.13.1.1",
                "\u2013\u2013\u2013, 2014a, \u201cThe Causal Relevance of\nContent to Computation\u201d, <em>Philosophy and Phenomenological\nResearch</em>, 88(1): 173\u2013208.\ndoi:10.1111/j.1933-1592.2012.00619.x",
                "\u2013\u2013\u2013, 2014b, \u201cA Theory of Computational\nImplementation\u201d, <em>Synthese</em>, 191(6): 1277\u20131307.\ndoi:10.1007/s11229-013-0324-y",
                "\u2013\u2013\u2013, 2015, \u201cBayesian Perceptual\nPsychology\u201d, in <em>The Oxford Handbook of Philosophy of\nPerception</em>, Mohan Matthen (ed.), Oxford: Oxford University Press.\ndoi:10.1093/oxfordhb/9780199600472.013.010",
                "\u2013\u2013\u2013, 2017a, \u201cFrom Ockham to\nTuring\u2014and Back Again\u201d, in <em>Philosophical Explorations\nof the Legacy of Alan Turing</em>, Juliet Floyd and Alisa Bokulich\n(eds.), (Boston Studies in the Philosophy and History of Science 324),\nCham: Springer International Publishing, 279\u2013304.\ndoi:10.1007/978-3-319-53280-6_12",
                "\u2013\u2013\u2013, 2017b, \u201cMaps in the Head?\u201d,\n<em>The Routledge Handbook of Philosophy of Animal Minds</em>, Kristin\nAndrews and Jacob Beck (eds.), New York: Routledge.",
                "\u2013\u2013\u2013, 2020, \u201cReifying\nRepresentations\u201d, in <em>What Are Mental Representations?</em>,\nTobias Schlicht, Krzysztof Doulega, and Joulia Smortchkova (eds.),\nOxford: Oxford University Press.",
                "Rey, Georges, 2014, \u201cInnate and Learned: Carey, Mad Dog\nNativism, and the Poverty of Stimuli and Analogies (Yet Again): Innate\nand Learned\u201d, <em>Mind &amp; Language</em>, 29(2):\n109\u2013132. doi:10.1111/mila.12044",
                "Rumelhart, David and James L. McClelland, 1986, \u201cPDP Models\nand General Issues in Cognitive Science\u201d, in Rumelhart, et al.\n1986: 110\u2013146.",
                "Rumelhart, David E., James L. McClelland, and the PDP Research\nGroup, 1986, <em>Parallel Distributed Processing, volume 1:\nExplorations in the Microstructure of Cognition: Foundations</em>,\nCambridge, MA: MIT Press.",
                "Russell, Bertrand, 1918\u20131919 [1985], \u201cThe Philosophy\nof Logical Atomism: Lectures 1\u20132\u201d, <em>Monist</em>, 28(4):\n495\u2013527, doi:10.5840/monist19182843, 29(1): 32\u201363,\ndoi:10.5840/monist191929120, 29(2): 190\u2013222,\ndoi:10.5840/monist19192922, 29(3): 345\u2013380,\ndoi:10.5840/monist19192937. Reprinted in <em>The Philosophy of Logical\nAtomism</em>, David F. Pears (ed.), La Salle, IL: Open Court. ",
                "Rupert, Robert D., 2008, \u201cFrege\u2019s Puzzle and Frege\nCases: Defending a Quasi-Syntactic Solution\u201d, <em>Cognitive\nSystems Research</em>, 9(1\u20132): 76\u201391.\ndoi:10.1016/j.cogsys.2007.07.003",
                "Schiffer, Stephen, 1981, \u201cTruth and the Theory of\nContent\u201d, in <em>Meaning and Understanding</em>, Herman Parret\nand Jacques Bouveresse, Berlin: Walter de Gruyter, 204\u2013222.",
                "Schneider, Susan, 2005, \u201cDirect Reference, Psychological\nExplanation, and Frege Cases\u201d, <em>Mind &amp; Language</em>,\n20(4): 423\u2013447. doi:10.1111/j.0268-1064.2005.00294.x",
                "\u2013\u2013\u2013, 2011, <em>The Language of Thought: A New\nPhilosophical Direction</em>, Cambridge, MA: MIT Press.",
                "Sellars, Wilfrid, 1975, \u201cThe Structure of Knowledge\u201d,\nin <em>Action, Knowledge and Reality: Studies in Honor of Wilfrid\nSellars</em>, Hector-Neri Casta\u00f1eda (ed.), Indianapolis, IN:\nBobbs-Merrill, 295\u2013347.",
                "Shagrir, Oron, 2020, \u201cIn Defense of the Semantic View of\nComputation\u201d, <em>Synthese</em>, 197(9): 4083\u20134108.\ndoi:10.1007/s11229-018-01921-z",
                "Shea, Nicholas, 2016, \u201cRepresentational Development Need Not\nBe Explicable-By-Content\u201d, in <em>Fundamental Issues of\nArtificial Intelligence</em>, Vincent C. M\u00fcller (ed.), Cham:\nSpringer International Publishing, 223\u2013240.\ndoi:10.1007/978-3-319-26485-1_14",
                "Sloman, Aaron, 1978, <em>The Computer Revolution in Philosophy:\nPhilosophy, Science and Models of the Mind</em>, Hassocks: The\nHarvester Press.",
                "Smolensky, Paul, 1990, \u201cTensor Product Variable Binding and\nthe Representation of Symbolic Structures in Connectionist\nSystems\u201d, <em>Artificial Intelligence</em>, 46(1\u20132):\n159\u2013216. doi:10.1016/0004-3702(90)90007-M",
                "\u2013\u2013\u2013, 1991, \u201cConnectionism, Constituency,\nand the Language of Thought\u201d, in <em>Meaning in Mind: Fodor and\nHis Critics</em>, Barry M. Loewer and Georges Rey (eds), Cambridge,\nMA: Blackwell.",
                "\u2013\u2013\u2013, 1995, \u201cConstituent Structure and\nExplanation in an Integrated Connectionist/Symbolic Cognitive\nArchitecture\u201d, in <em>Connectionism: Debates on Psychological\nExplanation</em>, Cynthia Macdonald and Graham Macdonald (eds),\nOxford: Basil Blackwell.",
                "Stalnaker, Robert C., 1984, <em>Inquiry</em>, Cambridge, MA: MIT\nPress.",
                "Stich, Stephen P., 1983, <em>From Folk Psychology to Cognitive\nScience</em>, Cambridge, MA: MIT Press.",
                "Stojni\u0107, Una and Ernie Lepore, 2020, \u201cFodor and\nDemonstratives in LOT\u201d, <em>Theoria</em>, 35(1):\n75\u201392.",
                "Tarski, Alfred, 1933 [1983], \u201cPoj\u0119cie prawdy w\nj\u0119zykach nauk dedukcyjnych\u201d, Warsaw: Nak\u0142adem\nTowarzystwa Naukowego Warszawskiego. Translated into German (1935) by\nL. Blaustein as \u201cDer Wahrheitsbegriff in den formalisierten\nSprachen\u201d, <em>Studia Philosophica</em>, 1: 261\u2013405.\nTranslated into English (1983) as \u201cThe Concept of Truth in\nFormalized Languages\u201d, in <em>Logic, Semantics, Metamathematics:\nPapers from 1923 to 1938</em>, second edition, J.H. Woodger (trans.),\nJohn Corcoran (ed.), Indianapolis, IN: Hackett.",
                "Tolman, Edward C., 1948, \u201cCognitive Maps in Rats and\nMen.\u201d, <em>Psychological Review</em>, 55(4): 189\u2013208.\ndoi:10.1037/h0061626",
                "Touretzky, David S., 1990, \u201cBoltzCONS: Dynamic Symbol\nStructures in a Connectionist Network\u201d, <em>Artificial\nIntelligence</em>, 46(1\u20132): 5\u201346.\ndoi:10.1016/0004-3702(90)90003-I",
                "Turing, Alan M., 1936, \u201cOn Computable Numbers, with an\nApplication to the Entscheidungsproblem\u201d, <em>Proceedings of the\nLondon Mathematical Society</em>, s2\u201342(1): 230\u2013265.\ndoi:10.1112/plms/s2\u201342.1.230",
                "van Gelder, Timothy, 1991, \u201cClassical Questions, Radical\nAnswers: Connectionism and the Structure of Mental\nRepresentations\u201d. In Horgan and Tienson 1991: 355\u2013381,\ndoi:10.1007/978-94-011-3524-5_16",
                "Wakefield, Jerome C., 2002, \u201cBroad versus Narrow Content in\nthe Explanation of Action: Fodor on Frege Cases\u201d,\n<em>Philosophical Psychology</em>, 15(2): 119\u2013133.\ndoi:10.1080/09515080220127099",
                "Weiner, Jan, Sara Shettleworth, Verner P. Bingman, Ken Cheng,\nSusan Healy, Lucia F. Jacobs, Kathryn J. Jeffery, Hanspeter A. Mallot,\nRandolf Menzel, and Nora S. Newcombe, 2011, \u201cAnimal Navigation:\nA Synthesis\u201d, in <em>Animal Thinking</em>, Randolf Menzel and\nJulia Fischer (eds), Cambridge, MA: MIT Press.",
                "Wittgenstein, Ludwig, 1921 [1922], <em>Logisch-Philosophische\nAbhandlung</em>, in W. Ostwald (ed.), <em>Annalen der\nNaturphilosophie</em>, 14. Translated as <em>Tractatus\nLogico-Philosophicus</em>, C.K. Ogden (trans.), London: Kegan Paul,\n1922.",
                "\u2013\u2013\u2013, 1953, <em>Philosophical\nInvestigations</em>, G.E.M. Anscombe (trans.), Oxford: Blackwell."
            ]
        },
        "raw_text": "<div id=\"bibliography\">\n<h2 id=\"Bib\">Bibliography</h2>\n<ul class=\"hanging\">\n<li>Arjo, Dennis, 1996, \u201cSticking Up for Oedipus: Fodor on\nIntentional Generalizations and Broad Content\u201d, <em>Mind &amp;\nLanguage</em>, 11(3): 231\u2013245.\ndoi:10.1111/j.1468-0017.1996.tb00044.x</li>\n<li>Armstrong, D. M., 1973, <em>Belief Truth and Knowledge</em>,\nCambridge: Cambridge University Press.\ndoi:10.1017/CBO9780511570827</li>\n<li>Atherton, Margaret and Robert Schwartz, 1974, \u201cLinguistic\nInnateness and Its Evidence\u201d:, <em>Journal of Philosophy</em>,\n71(6): 155\u2013168. doi:10.2307/2024657</li>\n<li>Aydede, Murat, 1995, \u201cConnectionism and Language of\nThought\u201d, CSLI Technical Report 195, Stanford: Center for the\nStudy of Language and Information Publications. </li>\n<li>\u2013\u2013\u2013, 1997a, \u201cLanguage of Thought: The\nConnectionist Contribution\u201d, <em>Minds and Machines</em>, 7(1):\n57\u2013101. doi:10.1023/A:1008203301671</li>\n<li>\u2013\u2013\u2013, 1997b, \u201cHas Fodor Really Changed His\nMind on Narrow Content?\u201d, <em>Mind &amp; Language</em>,\n12(3\u20134): 422\u2013458.\ndoi:10.1111/j.1468-0017.1997.tb00082.x</li>\n<li>\u2013\u2013\u2013, 1998, \u201cFodor on Concepts and Frege\nPuzzles\u201d, <em>Pacific Philosophical Quarterly</em>, 79(4):\n289\u2013294. doi:10.1111/1468-0114.00063</li>\n<li>\u2013\u2013\u2013, 2000, \u201cOn the Type/Token Relation of\nMental Representations\u201d, <em>Facta Philosophica</em>, 2:\n23\u201349.</li>\n<li>\u2013\u2013\u2013, 2005, \u201cComputation and Functionalism:\nSyntactic Theory of Mind Revisited\u201d, in <em>Turkish Studies in\nthe History and Philosophy of Science</em>, G\u00fcrol Irzik and\nG\u00fcven G\u00fczeldere (eds.), (Boston Studies in the History and\nPhilosophy of Science 244), Berlin/Heidelberg: Springer-Verlag,\n177\u2013204. doi:10.1007/1-4020-3333-8_13</li>\n<li>\u2013\u2013\u2013, 2015, \u201cThe Language of Thought\nHypothesis\u201d, <em>The Stanford Encyclopedia of Philosophy</em>\n(Fall 2015 Edition), Edward Zalta (ed.). URL =\n &lt;<a href=\"https://plato.stanford.edu/archives/fall2015/entries/language-thought/\" target=\"other\">https://plato.stanford.edu/archives/fall2015/entries/language-thought/</a>&gt;.</li>\n<li>Aydede, Murat and G\u00fcven G\u00fczeldere, 2005,\n\u201cCognitive Architecture, Concepts, and Introspection: An\nInformation-Theoretic Solution to the Problem of Phenomenal\nConsciousness\u201d, <em>No\u00fbs</em>, 39(2): 197\u2013255.\ndoi:10.1111/j.0029-4624.2005.00500.x</li>\n<li>Aydede, Murat and Philip Robbins, 2001, \u201cAre Frege Cases\nExceptions to Intentional Generalizations?\u201d, <em>Canadian\nJournal of Philosophy</em>, 31(1): 1\u201322.\ndoi:10.1080/00455091.2001.10717558</li>\n<li>Bach, Kent, 1987, \u201cReview: <em>Spreading the\nWord</em>\u201d, <em>The Philosophical Review</em>, 96(1):\n120\u2013123. doi:10.2307/2185336</li>\n<li>Barsalou, Lawrence W., 1999, \u201cPerceptual Symbol\nSystems\u201d, <em>Behavioral and Brain Sciences</em>, 22(4):\n577\u2013660. doi:10.1017/S0140525X99002149</li>\n<li>Bechtel, William and Adele Abrahamsen, 2002, <em>Connectionism and\nthe Mind: Parallel Processing, Dynamics and Evolution in\nNetworks</em>, second edition, Malden, MA: Blackwell.</li>\n<li>Beck, Jacob, 2017, \u201cCan Bootstrapping Explain Concept\nLearning?\u201d, <em>Cognition</em>, 158: 110\u2013121.\ndoi:10.1016/j.cognition.2016.10.017</li>\n<li>Berm\u00fadez, Jos\u00e9 Luis, 2010, <em>Cognitive Science: An\nIntroduction to the Science of the Mind</em>, Cambridge: Cambridge\nUniversity Press.</li>\n<li>Blackburn, Simon, 1984, <em>Spreading the Word</em>, Oxford:\nOxford University Press.</li>\n<li>Block, Ned, 1983, \u201cMental Pictures and Cognitive\nScience\u201d, <em>The Philosophical Review</em>, 92(4):\n499\u2013451. doi:10.2307/2184879</li>\n<li>\u2013\u2013\u2013, 1987, \u201cAdvertisement for a Semantics\nfor Psychology\u201d, in <em>Midwest Studies in Philosophy</em>, 10:\n615\u2013678. doi:10.1111/j.1475-4975.1987.tb00558.x</li>\n<li>\u2013\u2013\u2013, 1990, \u201cCan the Mind Change the\nWorld?\u201d, in <em>Meaning and Method: Essays in Honor of Hilary\nPutnam</em>, George Boolos (ed.), Cambridge: Cambridge University\nPress.</li>\n<li>\u2013\u2013\u2013, 2023, <em>The Border Between Seeing and\nThinking</em>, Oxford: Oxford University Press.</li>\n<li>Blumson, Ben, 2012, \u201cMental Maps\u201d, <em>Philosophy and\nPhenomenological Research</em>, 85(2): 413\u2013434.\ndoi:10.1111/j.1933-1592.2011.00499.x</li>\n<li>Braddon-Mitchell, David and John Fitzpatrick, 1990,\n\u201cExplanation and the Language of Thought\u201d,\n<em>Synthese</em>, 83(1): 3\u201329. doi: 10.1007/BF00413686</li>\n<li>Braddon-Mitchell, David and Frank Jackson, 2007, <em>Philosophy of\nMind and Cognition</em>, second edition, Cambridge: Blackwell.</li>\n<li>Brentano, Franz, 1874 [1973], <em>Psychology from an Empirical\nStandpoint</em> (<em>Psychologie vom empirischen Standpunkt</em>, 1924\nedition), Antos C. Rancurello, D.B. Terrell, and Linda McAlister\n(trans.), London: Routledge and Kegan Paul.</li>\n<li>Burge, Tyler, 2007, <em>Foundations of Mind</em>, (Philosophical\nEssays, 2), Oxford: Oxford University Press.</li>\n<li>\u2013\u2013\u2013, 2010, <em>Origins of Objectivity</em>,\nOxford: Oxford University Press.\ndoi:10.1093/acprof:oso/9780199581405.001.0001</li>\n<li>\u2013\u2013\u2013, 2018, \u201cIconic Representation: Maps,\nPictures, and Perception\u201d, in <em>The Map and the Territory:\nExploring the Foundations of Science, Thought, and Reality</em>, Shyam\nWuppuluri and Francisco Antonio Doria (eds.), Cham: Springer\nInternational Publishing, 79\u2013100.\ndoi:10.1007/978-3-319-72478-2_5</li>\n<li>\u2013\u2013\u2013, 2022, <em>Perception: First Form of\nMind</em>, Oxford: Oxford University Press.</li>\n<li>Camp, Elisabeth, 2009, \u201cA Language of Baboon\nThought?\u201d, in Lurz 2009: 108\u2013127.\ndoi:10.1017/CBO9780511819001.007</li>\n<li>\u2013\u2013\u2013, 2018, \u201cWhy Maps Are Not\nPropositional\u201d, in <em>Non-Propositional Intentionality</em>,\nAlex Grzankowski and Michelle Montague (eds.), Oxford: Oxford\nUniversity Press. doi:10.1093/oso/9780198732570.003.0002</li>\n<li>Carey, Susan, 2009, <em>The Origin of Concepts</em>, Oxford:\nOxford University Press.\ndoi:10.1093/acprof:oso/9780195367638.001.0001</li>\n<li>\u2013\u2013\u2013, 2014, \u201cOn Learning New Primitives in\nthe Language of Thought: Reply to Rey\u201d, <em>Mind and\nLanguage</em>, 29(2): 133\u2013166. doi:10.1111/mila.12045</li>\n<li>Casati, Roberto and Achille C. Varzi, 1999, <em>Parts and Places:\nThe Structures of Spatial Representation</em>, Cambridge, MA: MIT\nPress.</li>\n<li>Chalmers, David J., 1990, \u201cSyntactic Transformations on\nDistributed Representations\u201d, <em>Connection Science</em>,\n2(1\u20132): 53\u201362. doi:10.1080/09540099008915662</li>\n<li>\u2013\u2013\u2013, 1993, \u201cConnectionism and\nCompositionality: Why Fodor and Pylyshyn Were Wrong\u201d,\n<em>Philosophical Psychology</em>, 6(3): 305\u2013319.\ndoi:10.1080/09515089308573094</li>\n<li>\u2013\u2013\u2013, 2012, \u201cThe Varieties of Computation:\nA Reply\u201d, <em>Journal of Cognitive Science</em>, 13(3):\n211\u2013248. doi:10.17791/jcs.2012.13.3.211</li>\n<li>Chomsky, Noam, 1965, <em>Aspects of the Theory of Syntax</em>.\nCambridge, MA: MIT Press.</li>\n<li>Churchland, Patricia S., 1986, <em>Neurophilosophy: Toward a\nUnified Science of Mind-Brain</em>, Cambridge, MA: MIT Press.</li>\n<li>Churchland, Patricia S. and Terrence J. Sejnowski, 1989,\n\u201cNeural Representation and Neural Computation\u201d, in\n<em>Neural Connections, Neural Computation</em>, Lynn Nadel, Lynn A.\nCooper, Peter W. Culicover, and Robert M. Harnish, Cambridge, MA: MIT\nPress.</li>\n<li>Churchland, Paul M., 1990, <em>A Neurocomputational Perspective:\nThe Nature of Mind and the Structure of Science</em>, Cambridge, MA:\nMIT Press.</li>\n<li>Churchland, Paul M., and Patricia S. Churchland, 1990,\n\u201cCould a Machine Think?\u201d, <em>Scientific American</em>,\n262(1): 32\u201337. doi:10.1038/scientificamerican0190-32</li>\n<li>Clark, Andy, 1991, \u201cSystematicity, Structured\nRepresentations and Cognitive Architecture: A Reply to Fodor and\nPylyshyn\u201d, in Horgan and Tienson 1991: 198\u2013218.\ndoi:10.1007/978-94-011-3524-5_9</li>\n<li>\u2013\u2013\u2013, 2014, <em>Mindware: An Introduction to the\nPhilosophy of Cognitive Science</em>, second edition, Oxford: Oxford\nUniversity Press.</li>\n<li>Cowie, Fiona, 1999, <em>What\u2019s Within? Nativism\nReconsidered</em>, Oxford: Oxford University Press.\ndoi:10.1093/acprof:oso/9780195159783.001.0001</li>\n<li>Cummins, Robert, 1989, <em>Meaning and Mental Representation</em>,\nCambridge, MA: MIT Press.</li>\n<li>Dennett, Daniel C., 1977 [1981], \u201cCritical Noticw: Review of\nThe Language of Thought by Jerry Fodor\u201d, <em>Mind</em>, 86(342):\n265\u2013280. Reprinted as \u201cA Cure for the Common Code\u201d,\nin <em>Brainstorms: Philosophical Essays on Mind and Psychology</em>,\nCambridge, MA: MIT Press, 1981. doi:10.1093/mind/LXXXVI.342.265</li>\n<li>\u2013\u2013\u2013, 1991, \u201cMother Nature Versus the\nWalking Encyclopedia: A Western Drama\u201d, in <em>Philosophy and\nConnectionist Theory</em>, W. Ramsey, S. Stich, and D. Rumelhart,\nHillsdale, NJ: Lawrence Erlbaum Associates.\n [<a href=\"https://ase.tufts.edu/cogstud/dennett/papers/motherna.htm\" target=\"other\">available online</a>]</li>\n<li>Devitt, Michael, 1995, <em>Coming to Our Senses: A Naturalistic\nProgram for Semantic Localism</em>, Cambridge: Cambridge University\nPress. doi:10.1017/CBO9780511609190</li>\n<li>Dretske, Fred, 1981, <em>Knowledge and the Flow of\nInformation</em>, Cambridge, MA: MIT Press.</li>\n<li>\u2013\u2013\u2013, 1988. <em>Explaining Behavior</em>,\nCambridge, MA: MIT Press.</li>\n<li>Egan, Frances, 1992, \u201cIndividualism, Computation, and\nPerceptual Content\u201d, <em>Mind</em>, 101(403): 443\u2013459.\ndoi:10.1093/mind/101.403.443</li>\n<li>Elman, Jeffrey L., 1989, \u201cStructured Representations and\nConnectionist Models\u201d, in <em>Proceedings of the Eleventh Annual\nMeeting of the Cognitive Science Society</em>, Mahwah: Laurence\nErlbaum Associates.</li>\n<li>Field, Hartry, 2001, <em>Truth and the Absence of Fact</em>,\nOxford: Oxford University Press. doi:10.1093/0199242895.001.0001</li>\n<li>Figdor, Carrie, 2009, \u201cSemantic Externalism and the\nMechanics of Thought\u201d, <em>Minds and Machines</em>, 19(1):\n1\u201324. doi:10.1007/s11023-008-9114-6</li>\n<li>Fodor, Jerry A., 1975, <em>The Language of Thought</em>, New York:\nThomas Y. Crowell.</li>\n<li>\u2013\u2013\u2013, 1981, <em>Representations</em>, Cambridge,\nMA: MIT Press.</li>\n<li>\u2013\u2013\u2013, 1987, <em>Psychosemantics</em>, Cambridge,\nMA: MIT Press.</li>\n<li>\u2013\u2013\u2013, 1990, <em>A Theory of Content and Other\nEssays</em>, Cambridge, MA: MIT Press.</li>\n<li>\u2013\u2013\u2013, 1991, \u201cReplies\u201d, in <em>Meaning\nin Mind: Fodor and His Critics</em>, Barry M. Loewer and Georges Rey\n(eds.), Cambridge, MA: MIT Press.</li>\n<li>\u2013\u2013\u2013, 1994, <em>The Elm and the Expert</em>,\nCambridge, MA: MIT Press.</li>\n<li>\u2013\u2013\u2013, 1998, <em>Concepts: Where Cognitive Science\nWent Wrong</em>, Oxford: Oxford University Press.\ndoi:10.1093/0198236360.001.0001</li>\n<li>\u2013\u2013\u2013, 2001, \u201cDoing without What\u2019s\nwithin: Fiona Cowie\u2019s Critique of Nativism\u201d,\n<em>Mind</em>, 110(437): 99\u2013148.\ndoi:10.1093/mind/110.437.99</li>\n<li>\u2013\u2013\u2013, 2003, <em>Hume Variations</em>, Oxford:\nOxford University Press.\ndoi:10.1093/acprof:oso/9780199287338.001.0001</li>\n<li>\u2013\u2013\u2013, 2008, <em>LOT 2: The Language of Thought\nRevisited</em>, Oxford: Oxford University Press.\ndoi:10.1093/acprof:oso/9780199548774.001.0001</li>\n<li>\u2013\u2013\u2013, 2010, \u201cWoof, Woof. Review of <em>The\nOrigin of Concepts</em> by Susan Carey\u201d, <em>The Times Literary\nSupplement</em>, October 8: pp. 7\u20138.</li>\n<li>Fodor, Jerry and Brian P. McLaughlin, 1990, \u201cConnectionism\nand the Problem of Systematicity: Why Smolensky\u2019s Solution\nDoesn\u2019t Work\u201d, <em>Cognition</em>, 35(2): 183\u2013204.\ndoi:10.1016/0010-0277(90)90014-B</li>\n<li>Fodor, Jerry A. and Zenon W. Pylyshyn, 1981, \u201cHow Direct Is\nVisual Perception?: Some Reflections on Gibson\u2019s\n\u2018Ecological Approach\u2019\u201d, <em>Cognition</em>, 9(2):\n139\u2013196. doi:10.1016/0010-0277(81)90009-3</li>\n<li>\u2013\u2013\u2013, 1988, \u201cConnectionism and Cognitive\nArchitecture: A Critical Analysis\u201d, <em>Cognition</em>,\n28(1\u20132): 3\u201371. doi:10.1016/0010-0277(88)90031-5</li>\n<li>\u2013\u2013\u2013, 2015, <em>Minds Without Meanings</em>,\nCambridge, MA: MIT Press.</li>\n<li>Frege, Gottlob, 1879 [1967], <em>Begriffsschrift, eine der\nArithmetischen Nachgebildete Formelsprache des Reinen Denkens</em>.\nTranslated as <em>Concept Script, a Formal Language of Pure Thought\nModeled upon that of Arithmetic</em> in <em>From Frege to G\u00f6del:\nA Source Book in Mathematical Logic, 1879\u20131931</em>, J. van\nHeijenoort (ed.), S. Bauer-Mengelberg (trans.), Cambridge, MA: Harvard\nUniversity Press.</li>\n<li>\u2013\u2013\u2013, 1892 [1997], \u201cOn <em>Sinn</em> and\n<em>Bedeutung</em>\u201d. Reprinted in the <em>The Frege Reader</em>,\nM. Beaney (ed.), M. Black (trans.), Malden, MA: Blackwell.</li>\n<li>\u2013\u2013\u2013, 1918 [1997], \u201cThought\u201d.\nReprinted in <em>The Frege Reader</em>, M. Beaney (ed.), P. Geach and\nR. Stoothof (trans.), Malden, MA: Blackwell.</li>\n<li>Gallistel, Charles R., 1990, <em>The Organization of\nLearning</em>, Cambridge, MA: MIT Press.</li>\n<li>Gallistel, Charles R. and Adam Philip King, 2009, <em>Memory and\nthe Computational Brain</em>, Malden, MA: Wiley- Blackwell.</li>\n<li>Gallistel, C.R. and Louis D. Matzel, 2013, \u201cThe Neuroscience\nof Learning: Beyond the Hebbian Synapse\u201d, <em>Annual Review of\nPsychology</em>, 64(1): 169\u2013200.\ndoi:10.1146/annurev-psych-113011-143807</li>\n<li>Gibson, James J., 1979, <em>The Ecological Approach to Visual\nPerception</em>, Boston, MA: Houghton Mifflin.</li>\n<li>Greenberg, Gabriel, 2013, \u201cBeyond Resemblance\u201d,\n<em>Philosophical Review</em>, 122(2): 215\u2013287.\ndoi:10.1215/00318108-1963716</li>\n<li>Greenberg, Mark, 2014, \u201cTroubles for Content I\u201d, in\n<em>Metasemantics: New Essays on the Foundations of Meaning</em>,\nAlexis Burgess and Brett Sherman (eds.), Oxford: Oxford University\nPress, 147\u2013168.\ndoi:10.1093/acprof:oso/9780199669592.003.0006</li>\n<li>Harman, Gilbert, 1973, <em>Thought</em>, Princeton, NJ: Princeton\nUniversity Press.</li>\n<li>Harnad, Stevan, 1994, \u201cComputation Is Just Interpretable\nSymbol Manipulation; Cognition Isn\u2019t\u201d, <em>Minds and\nMachines</em>, 4(4): 379\u2013390. doi:10.1007/BF00974165</li>\n<li>Harnish, Robert M., 2002, <em>Minds, Brains, Computers: An\nHistorical Introduction to the Foundations of Cognitive Science</em>,\nMalden, MA: Blackwell.</li>\n<li>Haugeland, John, 1985, <em>Artificial Intelligence: The Very\nIdea</em>, Cambridge, MA: MIT Press</li>\n<li>Helmholtz, Hermann von, 1867 [1925], <em>Treatise on Physiological\nOptics</em> (<em>Handbuch der physiologischen Optik</em>), James P.C.\nSouthall, Manasha, WI: George Banta Publishing Company.</li>\n<li>Hinton, G. 1990. \u201cMapping Part-Whole Hierarchies into\nConnectionist Networks\u201d. <em>Artificial Intelligence</em> 46:\npp. 47\u201375.</li>\n<li>Horgan, Terence and John Tienson (eds.), 1991, <em>Connectionism\nand the Philosophy of Mind</em>, (Studies in Cognitive Systems 9),\nDordrecht: Springer Netherlands. doi:10.1007/978-94-011-3524-5</li>\n<li>\u2013\u2013\u2013, 1996, <em>Connectionism and the Philosophy\nof Psychology</em>, Cambridge, MA: MIT Press.</li>\n<li>Hume, David, 1739 [1978], <em>A Treatise on Human Nature</em>,\nsecond edition, P. H. Nidditch (ed.). Oxford: Clarendon Press.</li>\n<li>Jacobs, Lucia F and Randolf Menzel, 2014, \u201cNavigation\nOutside of the Box: What the Lab Can Learn from the Field and What the\nField Can Learn from the Lab\u201d, <em>Movement Ecology</em>, 2(1):\n3. doi:10.1186/2051-3933-2-3</li>\n<li>Johnson, Kent, 2004, \u201cOn the Systematicity of Language and\nThought\u201d:, <em>Journal of Philosophy</em>, 101(3):\n111\u2013139. doi:10.5840/jphil2004101321</li>\n<li>Johnson-Laird, Philip N., 2004, \u201cThe History of Mental\nModels\u201d, in <em>Psychology of Reasoning: Theoretical and\nHistorical Perspectives</em>, Ken Manktelow and Man Cheung Chung, New\nYork: Psychology Press.</li>\n<li>Kant, Immanuel, 1781 [1998], <em>The Critique of Pure Reason</em>,\nP. Guyer and A. Wood (eds), Cambridge: Cambridge University\nPress.</li>\n<li>Kaplan, David, 1989, \u201cDemonstratives\u201d, in <em>Themes\nfrom Kaplan</em>, Joseph Almog, John Perry, and Howard Wettstein\n(eds.), New York: Oxford University Press.</li>\n<li>Kazez, Jean R., 1994, \u201cComputationalism and the Causal Role\nof Content\u201d, <em>Philosophical Studies</em>, 75(3):\n231\u2013260. doi:10.1007/BF00989583</li>\n<li>King, Peter, 2005, \u201cWilliam of Ockham: <em>Summa\nLogicae</em>\u201d, in <em>Central Works of Philosophy: Ancient and\nMedieval, volume 1: Ancient and Medieval Philosophy</em>, John Shand\n(ed.), Montreal: McGill-Queen\u2019s University Press,\n242\u2013270.</li>\n<li>Knill, David C. and Whitman Richards (eds.), 1996, <em>Perception\nas Bayesian Inference</em>, Cambridge: Cambridge University Press.\ndoi:10.1017/CBO9780511984037</li>\n<li>Knowles, Jonathan, 1998, \u201cThe Language of Thought and\nNatural Language Understanding\u201d, <em>Analysis</em>, 58(4):\n264\u2013272. doi: 10.1093/analys/58.4.264</li>\n<li>Kosslyn, Stephen, 1980, <em>Image and Mind</em>, Cambridge, MA:\nHarvard University Press.</li>\n<li>Kulvicki, John, 2015, \u201cMaps, Pictures, and\nPredication\u201d, <em>Ergo: An Open Access Journal of\nPhilosophy</em>, 2(7): 149\u2013174.</li>\n<li>Laurence, Stephen and Eric Margolis, 1997, \u201cRegress\nArguments Against the Language of Thought\u201d, <em>Analysis</em>,\n57(1): 60\u201366.</li>\n<li>Loar, Brian, 1982, <em>Mind and Meaning</em>, Cambridge: Cambridge\nUniversity Press.</li>\n<li>Loewer, Barry, 1997, \u201cA Guide to Naturalizing\nSemantics\u201d, in <em>A Companion to the Philosophy of\nLanguage</em>, Bob Hale and Crispin Wright (eds.), Oxford:\nBlackwell.</li>\n<li>Lurz, Robert W. (ed.), 2009, <em>The Philosophy of Animal\nMinds</em>, Cambridge: Cambridge University Press.\ndoi:10.1017/CBO9780511819001</li>\n<li>Mackintosh, Nicholas John, 2002, \u201cDo Not Ask Whether They\nHave a Cognitive Map, but How They Find Their Way about\u201d,\n<em>Psicol\u00f3gica</em>, 23(1): 165\u2013185.\n [<a href=\"https://www.uv.es/psicologica/articulos1.02/M8MacKin.pdf\" target=\"other\">Mackintosh 2002 available online</a>]</li>\n<li>Mandelbaum, Eric, Yarrow Dunham, Roman Freiman, Chaz Firestone, E.\nJ. Green, Daniel Harris, Melissa Kibbe, Benedek Kurdi, Myrto\nMylopoulos, Joshua Sheperd, Alexis Wellwood, Nicholas Porot, and Jake\nQuilty-Dunn, 2022, \u201cProblems and Mysteries of the Many Languages\nof Thought\u201d, <em>Cognitive Science</em>, 46(12): e13225.</li>\n<li>Margolis, Eric, 1998, \u201cHow to Acquire a Concept\u201d,\n<em>Mind &amp; Language</em>, 13(3): 347\u2013369.\ndoi:10.1111/1468-0017.00081</li>\n<li>Margolis, Eric and Stephen Laurence, 2011, \u201cLearning\nMatters: The Role of Learning in Concept Acquisition\u201d, <em>Mind\n&amp; Language</em>, 26(5): 507\u2013539.\ndoi:10.1111/j.1468-0017.2011.01429.x</li>\n<li>McDermott, Drew V., 2001, <em>Mind and Mechanism</em>, Cambridge,\nMA: MIT Press.</li>\n<li>McLaughlin, B. P. and T. A. Warfield, 1994, \u201cThe Allure of\nConnectionism Reexamined\u201d, <em>Synthese</em>, 101(3):\n365\u2013400. doi:10.1007/BF01063895</li>\n<li>Marcus, G., 2001, <em>The Algebraic Mind</em>, Cambridge: MIT\nPress.</li>\n<li>Millikan, Ruth Garrett, 1984, <em>Language, Thought, and Other\nBiological Categories: New Foundations for Realism</em>, Cambridge,\nMA: MIT Press.</li>\n<li>\u2013\u2013\u2013, 1993, <em>White Queen Psychology and Other\nEssays for Alice</em>, Cambridge, MA: MIT Press.</li>\n<li>Neander, Karen, 2017, <em>A Mark of the Mental: In Defense of\nInformational Teleosemantics</em>, Cambridge, MA: MIT Press.</li>\n<li>Niklasson, Lars F. and Tim Gelder, 1994, \u201cOn Being\nSystematically Connectionist\u201d, <em>Mind &amp; Language</em>,\n9(3): 288\u2013302. doi:10.1111/j.1468-0017.1994.tb00227.x</li>\n<li>Normore, Calvin, 1990, \u201cOckham on Mental Language\u201d, in\n<em>The Historical Foundations of Cognitive Science</em>, J. Smith\n(ed.), Dordrecht: Kluwer.</li>\n<li>\u2013\u2013\u2013, 2003, \u201cBurge, Descartes, and\nUs\u201d, in <em>Reflections and Replies: Essays on the Philosophy of\nTyler Burge</em>, Martin Hahn and Bj\u00f8rn Ramberg, Cambridge, MA:\nMIT Press.</li>\n<li>\u2013\u2013\u2013, 2009, \u201cThe End of Mental\nLanguage\u201d, in <em>Le Langage Mental du Moyen \u00c2ge \u00e0\nl\u2019\u00c2ge Classique</em>, J. Biard (ed.), Leuven:\nPeeters.</li>\n<li>O\u2019Brien, Gerard and Jon Opie, 2006, \u201cHow Do\nConnectionist Networks Compute?\u201d, <em>Cognitive Processing</em>,\n7(1): 30\u201341. doi:10.1007/s10339-005-0017-7</li>\n<li>O\u2019Keefe, John and Lynn Nadel, 1978, <em>The Hippocampus as a\nCognitive Map</em>, Oxford: Clarendon Press.</li>\n<li>Ockham, William of, c. 1323 [1957], <em>Summa Logicae</em>,\nTranslated in his <em>Philosophical Writings, A Selection</em>,\nPhilotheus Boehner (ed. and trans.), London: Nelson, 1957.</li>\n<li>Panaccio, Claude, 1999 [2017], <em>Mental Language: From Plato to\nWilliam of Ockham</em> (<em>Discours int\u00e9rieur</em>), Joshua P.\nHochschild and Meredith K. Ziebart (trans.), New York: Fordham\nUniversity Press.</li>\n<li>Papineau, David, 1987, <em>Reality and Representation</em>,\nOxford: Basil Blackwell.</li>\n<li>Peacocke, Christopher, 1992, <em>A Study of Concepts</em>,\nCambridge, MA: MIT Press.</li>\n<li>\u2013\u2013\u2013, 1994, \u201cContent, Computation and\nExternalism\u201d, <em>Mind &amp; Language</em>, 9(3): 303\u2013335.\ndoi:10.1111/j.1468-0017.1994.tb00228.x</li>\n<li>\u2013\u2013\u2013, 1999, \u201cComputation as Involving\nContent: A Response to Egan\u201d, <em>Mind &amp; Language</em>,\n14(2): 195\u2013202. doi:10.1111/1468-0017.00109</li>\n<li>Perry, John, 1998, \u201cBroadening the Mind\u201d,\n<em>Philosophy and Phenomenological Research</em>, 58(1):\n223\u2013231. doi:10.2307/2653644</li>\n<li>Piccinini, Gualtiero, 2008, \u201cComputation without\nRepresentation\u201d, <em>Philosophical Studies</em>, 137(2):\n205\u2013241. doi:10.1007/s11098-005-5385-4</li>\n<li>Pinker, Steven, 2005, \u201cSo How Does the Mind Work?\u201d,\n<em>Mind &amp; Language</em>, 20(1): 1\u201324.\ndoi:10.1111/j.0268-1064.2005.00274.x</li>\n<li>Pinker, Steven and Alan Prince, 1988, \u201cOn Language and\nConnectionism: Analysis of a Parallel Distributed Processing Model of\nLanguage Acquisition\u201d, <em>Cognition</em>, 28(1\u20132):\n73\u2013193. doi:10.1016/0010-0277(88)90032-7</li>\n<li>Polger, Thomas W., 2004, <em>Natural Minds</em>, Cambridge, MA:\nMIT Press.</li>\n<li>Pollack, Jordan B., 1990, \u201cRecursive Distributed\nRepresentations\u201d, <em>Artificial Intelligence</em>,\n46(1\u20132): 77\u2013105. doi:10.1016/0004-3702(90)90005-K</li>\n<li>Prinz, Jesse, 2002, <em>Furnishing the Mind: Concepts and Their\nPerceptual Basis</em>, Cambridge, MA: MIT Press.</li>\n<li>\u2013\u2013\u2013, 2011, \u201cHas Mentalese Earned Its Keep?\nOn Jerry Fodor\u2019s LOT 2\u201d, <em>Mind</em>, 120(478):\n485\u2013501. doi:10.1093/mind/fzr025</li>\n<li>Putnam, Hilary, 1967, \u201cPsychophysical Predicates\u201d, In\n<em>Art, Mind, and Religion: Proceedings of the 1965 Oberlin\nColloquium in Philosophy</em>, W.H. Capitan and D.D. Merrill (eds),\nPittsburgh, PA: University of Pittsburgh Press, 37\u201348.</li>\n<li>\u2013\u2013\u2013, 1988, <em>Representation and Reality</em>,\nCambridge, MA: MIT Press.</li>\n<li>Pylyshyn, Zenon W., 1984, <em>Computation and Cognition: Toward a\nFoundation for Cognitive Science</em>, Cambridge, MA: MIT Press.</li>\n<li>\u2013\u2013\u2013, 2003, <em>Seeing and Visualizing:\nIt\u2019s Not What You Think</em>, Cambridge, MA: MIT Press.</li>\n<li>Quilty-Dunn, Jake, Nicholas Porot, and Eric Mandelbaum,\nforthcoming, \u201cThe Best Game in Town: The Re-Emergence of the\nLanguage of Thought Hypothesis Across the Cognitive Sciences\u201d,\n<em>Behavioral and Brain Sciences</em>.</li>\n<li>Quine, W. V., 1951 [1980], \u201cTwo Dogmas of Empiricism\u201d,\n<em>The Philosophical Review</em>, 60(1): 20\u201343. Reprinted in\nhis <em>From a Logical Point of View</em>, second edition, Cambridge,\nMA: Harvard University Press, 1980, 20\u201346.\ndoi:10.2307/2181906</li>\n<li>Ramsey, William M., 2007, <em>Representation Reconsidered</em>,\nCambridge: Cambridge University Press.\ndoi:10.1017/CBO9780511597954</li>\n<li>Rescorla, Michael, 2009a, \u201cChrysippus\u2019 Dog as a Case\nStudy in Non-Linguistic Cognition\u201d, in Lurz 2009: 52\u201371.\ndoi:10.1017/CBO9780511819001.004</li>\n<li>\u2013\u2013\u2013, 2009b, \u201cCognitive Maps and the\nLanguage of Thought\u201d, <em>The British Journal for the Philosophy\nof Science</em>, 60(2): 377\u2013407. doi:10.1093/bjps/axp012</li>\n<li>\u2013\u2013\u2013, 2009c, \u201cPredication and Cartographic\nRepresentation\u201d, <em>Synthese</em>, 169(1): 175\u2013200.\ndoi:10.1007/s11229-008-9343-5</li>\n<li>\u2013\u2013\u2013, 2012a, \u201cAre Computational Transitions\nSensitive to Semantics?\u201d, <em>Australasian Journal of\nPhilosophy</em>, 90(4): 703\u2013721.\ndoi:10.1080/00048402.2011.615333</li>\n<li>\u2013\u2013\u2013, 2012b, \u201cHow to Integrate\nRepresentation into Computational Modeling, and Why We Should\u201d,\n<em>Journal of Cognitive Science</em>, 13(1): 1\u201337.\ndoi:10.17791/jcs.2012.13.1.1</li>\n<li>\u2013\u2013\u2013, 2014a, \u201cThe Causal Relevance of\nContent to Computation\u201d, <em>Philosophy and Phenomenological\nResearch</em>, 88(1): 173\u2013208.\ndoi:10.1111/j.1933-1592.2012.00619.x</li>\n<li>\u2013\u2013\u2013, 2014b, \u201cA Theory of Computational\nImplementation\u201d, <em>Synthese</em>, 191(6): 1277\u20131307.\ndoi:10.1007/s11229-013-0324-y</li>\n<li>\u2013\u2013\u2013, 2015, \u201cBayesian Perceptual\nPsychology\u201d, in <em>The Oxford Handbook of Philosophy of\nPerception</em>, Mohan Matthen (ed.), Oxford: Oxford University Press.\ndoi:10.1093/oxfordhb/9780199600472.013.010</li>\n<li>\u2013\u2013\u2013, 2017a, \u201cFrom Ockham to\nTuring\u2014and Back Again\u201d, in <em>Philosophical Explorations\nof the Legacy of Alan Turing</em>, Juliet Floyd and Alisa Bokulich\n(eds.), (Boston Studies in the Philosophy and History of Science 324),\nCham: Springer International Publishing, 279\u2013304.\ndoi:10.1007/978-3-319-53280-6_12</li>\n<li>\u2013\u2013\u2013, 2017b, \u201cMaps in the Head?\u201d,\n<em>The Routledge Handbook of Philosophy of Animal Minds</em>, Kristin\nAndrews and Jacob Beck (eds.), New York: Routledge.</li>\n<li>\u2013\u2013\u2013, 2020, \u201cReifying\nRepresentations\u201d, in <em>What Are Mental Representations?</em>,\nTobias Schlicht, Krzysztof Doulega, and Joulia Smortchkova (eds.),\nOxford: Oxford University Press.</li>\n<li>Rey, Georges, 2014, \u201cInnate and Learned: Carey, Mad Dog\nNativism, and the Poverty of Stimuli and Analogies (Yet Again): Innate\nand Learned\u201d, <em>Mind &amp; Language</em>, 29(2):\n109\u2013132. doi:10.1111/mila.12044</li>\n<li>Rumelhart, David and James L. McClelland, 1986, \u201cPDP Models\nand General Issues in Cognitive Science\u201d, in Rumelhart, et al.\n1986: 110\u2013146.</li>\n<li>Rumelhart, David E., James L. McClelland, and the PDP Research\nGroup, 1986, <em>Parallel Distributed Processing, volume 1:\nExplorations in the Microstructure of Cognition: Foundations</em>,\nCambridge, MA: MIT Press.</li>\n<li>Russell, Bertrand, 1918\u20131919 [1985], \u201cThe Philosophy\nof Logical Atomism: Lectures 1\u20132\u201d, <em>Monist</em>, 28(4):\n495\u2013527, doi:10.5840/monist19182843, 29(1): 32\u201363,\ndoi:10.5840/monist191929120, 29(2): 190\u2013222,\ndoi:10.5840/monist19192922, 29(3): 345\u2013380,\ndoi:10.5840/monist19192937. Reprinted in <em>The Philosophy of Logical\nAtomism</em>, David F. Pears (ed.), La Salle, IL: Open Court. </li>\n<li>Rupert, Robert D., 2008, \u201cFrege\u2019s Puzzle and Frege\nCases: Defending a Quasi-Syntactic Solution\u201d, <em>Cognitive\nSystems Research</em>, 9(1\u20132): 76\u201391.\ndoi:10.1016/j.cogsys.2007.07.003</li>\n<li>Schiffer, Stephen, 1981, \u201cTruth and the Theory of\nContent\u201d, in <em>Meaning and Understanding</em>, Herman Parret\nand Jacques Bouveresse, Berlin: Walter de Gruyter, 204\u2013222.</li>\n<li>Schneider, Susan, 2005, \u201cDirect Reference, Psychological\nExplanation, and Frege Cases\u201d, <em>Mind &amp; Language</em>,\n20(4): 423\u2013447. doi:10.1111/j.0268-1064.2005.00294.x</li>\n<li>\u2013\u2013\u2013, 2011, <em>The Language of Thought: A New\nPhilosophical Direction</em>, Cambridge, MA: MIT Press.</li>\n<li>Sellars, Wilfrid, 1975, \u201cThe Structure of Knowledge\u201d,\nin <em>Action, Knowledge and Reality: Studies in Honor of Wilfrid\nSellars</em>, Hector-Neri Casta\u00f1eda (ed.), Indianapolis, IN:\nBobbs-Merrill, 295\u2013347.</li>\n<li>Shagrir, Oron, 2020, \u201cIn Defense of the Semantic View of\nComputation\u201d, <em>Synthese</em>, 197(9): 4083\u20134108.\ndoi:10.1007/s11229-018-01921-z</li>\n<li>Shea, Nicholas, 2016, \u201cRepresentational Development Need Not\nBe Explicable-By-Content\u201d, in <em>Fundamental Issues of\nArtificial Intelligence</em>, Vincent C. M\u00fcller (ed.), Cham:\nSpringer International Publishing, 223\u2013240.\ndoi:10.1007/978-3-319-26485-1_14</li>\n<li>Sloman, Aaron, 1978, <em>The Computer Revolution in Philosophy:\nPhilosophy, Science and Models of the Mind</em>, Hassocks: The\nHarvester Press.</li>\n<li>Smolensky, Paul, 1990, \u201cTensor Product Variable Binding and\nthe Representation of Symbolic Structures in Connectionist\nSystems\u201d, <em>Artificial Intelligence</em>, 46(1\u20132):\n159\u2013216. doi:10.1016/0004-3702(90)90007-M</li>\n<li>\u2013\u2013\u2013, 1991, \u201cConnectionism, Constituency,\nand the Language of Thought\u201d, in <em>Meaning in Mind: Fodor and\nHis Critics</em>, Barry M. Loewer and Georges Rey (eds), Cambridge,\nMA: Blackwell.</li>\n<li>\u2013\u2013\u2013, 1995, \u201cConstituent Structure and\nExplanation in an Integrated Connectionist/Symbolic Cognitive\nArchitecture\u201d, in <em>Connectionism: Debates on Psychological\nExplanation</em>, Cynthia Macdonald and Graham Macdonald (eds),\nOxford: Basil Blackwell.</li>\n<li>Stalnaker, Robert C., 1984, <em>Inquiry</em>, Cambridge, MA: MIT\nPress.</li>\n<li>Stich, Stephen P., 1983, <em>From Folk Psychology to Cognitive\nScience</em>, Cambridge, MA: MIT Press.</li>\n<li>Stojni\u0107, Una and Ernie Lepore, 2020, \u201cFodor and\nDemonstratives in LOT\u201d, <em>Theoria</em>, 35(1):\n75\u201392.</li>\n<li>Tarski, Alfred, 1933 [1983], \u201cPoj\u0119cie prawdy w\nj\u0119zykach nauk dedukcyjnych\u201d, Warsaw: Nak\u0142adem\nTowarzystwa Naukowego Warszawskiego. Translated into German (1935) by\nL. Blaustein as \u201cDer Wahrheitsbegriff in den formalisierten\nSprachen\u201d, <em>Studia Philosophica</em>, 1: 261\u2013405.\nTranslated into English (1983) as \u201cThe Concept of Truth in\nFormalized Languages\u201d, in <em>Logic, Semantics, Metamathematics:\nPapers from 1923 to 1938</em>, second edition, J.H. Woodger (trans.),\nJohn Corcoran (ed.), Indianapolis, IN: Hackett.</li>\n<li>Tolman, Edward C., 1948, \u201cCognitive Maps in Rats and\nMen.\u201d, <em>Psychological Review</em>, 55(4): 189\u2013208.\ndoi:10.1037/h0061626</li>\n<li>Touretzky, David S., 1990, \u201cBoltzCONS: Dynamic Symbol\nStructures in a Connectionist Network\u201d, <em>Artificial\nIntelligence</em>, 46(1\u20132): 5\u201346.\ndoi:10.1016/0004-3702(90)90003-I</li>\n<li>Turing, Alan M., 1936, \u201cOn Computable Numbers, with an\nApplication to the Entscheidungsproblem\u201d, <em>Proceedings of the\nLondon Mathematical Society</em>, s2\u201342(1): 230\u2013265.\ndoi:10.1112/plms/s2\u201342.1.230</li>\n<li>van Gelder, Timothy, 1991, \u201cClassical Questions, Radical\nAnswers: Connectionism and the Structure of Mental\nRepresentations\u201d. In Horgan and Tienson 1991: 355\u2013381,\ndoi:10.1007/978-94-011-3524-5_16</li>\n<li>Wakefield, Jerome C., 2002, \u201cBroad versus Narrow Content in\nthe Explanation of Action: Fodor on Frege Cases\u201d,\n<em>Philosophical Psychology</em>, 15(2): 119\u2013133.\ndoi:10.1080/09515080220127099</li>\n<li>Weiner, Jan, Sara Shettleworth, Verner P. Bingman, Ken Cheng,\nSusan Healy, Lucia F. Jacobs, Kathryn J. Jeffery, Hanspeter A. Mallot,\nRandolf Menzel, and Nora S. Newcombe, 2011, \u201cAnimal Navigation:\nA Synthesis\u201d, in <em>Animal Thinking</em>, Randolf Menzel and\nJulia Fischer (eds), Cambridge, MA: MIT Press.</li>\n<li>Wittgenstein, Ludwig, 1921 [1922], <em>Logisch-Philosophische\nAbhandlung</em>, in W. Ostwald (ed.), <em>Annalen der\nNaturphilosophie</em>, 14. Translated as <em>Tractatus\nLogico-Philosophicus</em>, C.K. Ogden (trans.), London: Kegan Paul,\n1922.</li>\n<li>\u2013\u2013\u2013, 1953, <em>Philosophical\nInvestigations</em>, G.E.M. Anscombe (trans.), Oxford: Blackwell.</li>\n</ul>\n</div>"
    },
    "related_entries": {
        "entry_list": [
            "artificial intelligence",
            "belief",
            "Church-Turing Thesis",
            "cognitive science",
            "computation: in physical systems",
            "concepts",
            "connectionism",
            "consciousness: representational theories of",
            "folk psychology: as a theory",
            "functionalism",
            "intentionality",
            "mental content: causal theories of",
            "mental imagery",
            "mental representation",
            "mind: computational theory of",
            "naturalism",
            "physicalism",
            "propositional attitude reports",
            "qualia",
            "reasoning: automated",
            "Turing, Alan",
            "Turing machines"
        ],
        "entry_link": [
            {
                "../artificial-intelligence/": "artificial intelligence"
            },
            {
                "../belief/": "belief"
            },
            {
                "../church-turing/": "Church-Turing Thesis"
            },
            {
                "../cognitive-science/": "cognitive science"
            },
            {
                "../computation-physicalsystems/": "computation: in physical systems"
            },
            {
                "../concepts/": "concepts"
            },
            {
                "../connectionism/": "connectionism"
            },
            {
                "../consciousness-representational/": "consciousness: representational theories of"
            },
            {
                "../folkpsych-theory/": "folk psychology: as a theory"
            },
            {
                "../functionalism/": "functionalism"
            },
            {
                "../intentionality/": "intentionality"
            },
            {
                "../content-causal/": "mental content: causal theories of"
            },
            {
                "../mental-imagery/": "mental imagery"
            },
            {
                "../mental-representation/": "mental representation"
            },
            {
                "../computational-mind/": "mind: computational theory of"
            },
            {
                "../naturalism/": "naturalism"
            },
            {
                "../physicalism/": "physicalism"
            },
            {
                "../prop-attitude-reports/": "propositional attitude reports"
            },
            {
                "../qualia/": "qualia"
            },
            {
                "../reasoning-automated/": "reasoning: automated"
            },
            {
                "../turing/": "Turing, Alan"
            },
            {
                "../turing-machine/": "Turing machines"
            }
        ]
    },
    "academic_tools": {
        "listed_text": [
            "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=language-thought\" target=\"other\">How to cite this entry</a>.",
            "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://leibniz.stanford.edu/friends/preview/language-thought/\" target=\"other\">Preview the PDF version of this entry</a> at the\n <a href=\"https://leibniz.stanford.edu/friends/\" target=\"other\">Friends of the SEP Society</a>.",
            "<img alt=\"inpho icon\" src=\"../../symbols/inpho.png\"/>",
            "<a href=\"https://www.inphoproject.org/entity?sep=language-thought&amp;redirect=True\" target=\"other\">Look up topics and thinkers related to this entry</a>\n at the Internet Philosophy Ontology Project (InPhO).",
            "<img alt=\"phil papers icon\" src=\"../../symbols/pp.gif\"/>",
            "<a href=\"https://philpapers.org/sep/language-thought/\" target=\"other\">Enhanced bibliography for this entry</a>\nat <a href=\"https://philpapers.org/\" target=\"other\">PhilPapers</a>, with links to its database."
        ],
        "listed_links": [
            {
                "https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=language-thought": "How to cite this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/preview/language-thought/": "Preview the PDF version of this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/": "Friends of the SEP Society"
            },
            {
                "https://www.inphoproject.org/entity?sep=language-thought&redirect=True": "Look up topics and thinkers related to this entry"
            },
            {
                "https://philpapers.org/sep/language-thought/": "Enhanced bibliography for this entry"
            },
            {
                "https://philpapers.org/": "PhilPapers"
            }
        ]
    },
    "other_internet_resources": {
        "listed_text": [
            "Aydede, Murat, \u201cThe Language of Thought Hypothesis,\u201d\n<em>Stanford Encyclopedia of Philosophy</em> (Spring 2019 Edition),\nEdward N. Zalta (ed.), URL =\n &lt;<a href=\"https://plato.stanford.edu/archives/spr2019/entries/language-thought/\" target=\"other\">https://plato.stanford.edu/archives/spr2019/entries/language-thought/</a>&gt;.\n [This was the previous entry on the language of thought hypothesis\nin the <em>Stanford Encyclopedia of Philosophy</em> \u2014 see the\n <a class=\"plain\" href=\"https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=language-thought\" target=\"other\">version history</a>.]",
            "<a href=\"https://philpapers.org/browse/the-language-of-thought/\" target=\"other\">Bibliography on the language of thought</a>,\n in PhilPapers.org.",
            "<a href=\"https://philpapers.org/browse/philosophy-of-artificial-intelligence/\" target=\"other\">Bibliography on the philosophy of artificial intelligence</a>,\n curated by Eric Dietrich, in PhilPapers.org."
        ],
        "listed_links": [
            {
                "https://plato.stanford.edu/archives/spr2019/entries/language-thought/": "https://plato.stanford.edu/archives/spr2019/entries/language-thought/"
            },
            {
                "https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=language-thought": "version history"
            },
            {
                "https://philpapers.org/browse/the-language-of-thought/": "Bibliography on the language of thought"
            },
            {
                "https://philpapers.org/browse/philosophy-of-artificial-intelligence/": "Bibliography on the philosophy of artificial intelligence"
            }
        ]
    }
}