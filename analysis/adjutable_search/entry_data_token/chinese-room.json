{"url": "chinese-room", "title": "The Chinese Room Argument", "authorship": {"year": "Copyright \u00a9 2020", "author_text": "David Cole\n<dcole@d.umn.edu>", "author_links": [{"http://www.d.umn.edu/~dcole/": "David Cole"}, {"mailto:dcole%40d%2eumn%2eedu": "dcole@d.umn.edu"}], "raw_html": "<div id=\"article-copyright\">\n<p>\n<a href=\"../../info.html#c\">Copyright \u00a9 2020</a> by\n\n<br/>\n<a href=\"http://www.d.umn.edu/~dcole/\" target=\"other\">David Cole</a>\n&lt;<a href=\"mailto:dcole%40d%2eumn%2eedu\"><em>dcole<abbr title=\" at \">@</abbr>d<abbr title=\" dot \">.</abbr>umn<abbr title=\" dot \">.</abbr>edu</em></a>&gt;\n    </p>\n</div>"}, "pubinfo": ["First published Fri Mar 19, 2004", "substantive revision Thu Feb 20, 2020"], "preamble": "\n\nThe argument and thought-experiment now generally known as the Chinese\nRoom Argument was first published in a 1980 article by American\nphilosopher John Searle (1932\u2013 ). It has become one of the best-known\narguments in recent philosophy. Searle imagines himself alone in a\nroom following a computer program for responding to Chinese characters\nslipped under the door. Searle understands nothing of Chinese, and\nyet, by following the program for manipulating symbols and numerals\njust as a computer does, he sends appropriate strings of Chinese\ncharacters back out under the door, and this leads those outside to\nmistakenly suppose there is a Chinese speaker in the room. \n\nThe narrow conclusion of the argument is that programming a digital\ncomputer may make it appear to understand language but could not\nproduce real understanding. Hence the \u201cTuring Test\u201d is\ninadequate. Searle argues that the thought experiment underscores the\nfact that computers merely use syntactic rules to manipulate symbol\nstrings, but have no understanding of meaning or semantics. The\nbroader conclusion of the argument is that the theory that human minds\nare computer-like computational or information processing systems is\nrefuted. Instead minds must result from biological processes;\ncomputers can at best simulate these biological processes. Thus the\nargument has large implications for semantics, philosophy of language\nand mind, theories of consciousness, computer science and cognitive\nscience generally. As a result, there have been many critical replies\nto the argument.\n", "toc": [{"#Over": "1. Overview"}, {"#HistBack": "2. Historical Background"}, {"#LeibMill": "2.1 Leibniz\u2019 Mill"}, {"#TuriPapeMach": "2.2 Turing\u2019s Paper Machine"}, {"#ChinNati": "2.3 The Chinese Nation"}, {"#ChinRoomArgu": "3. The Chinese Room Argument"}, {"#ReplChinRoomArgu": "4. Replies to the Chinese Room Argument"}, {"#SystRepl": "4.1 The Systems Reply"}, {"#RoboRepl": "4.2 The Robot Reply"}, {"#BraiSimuRepl": "4.3 The Brain Simulator Reply"}, {"#OtheMindRepl": "4.4 The Other Minds Reply"}, {"#IntuRepl": "4.5 The Intuition Reply"}, {"#LargPhilIssu": "5. The Larger Philosophical Issues"}, {"#SyntSema": "5.1 Syntax and Semantics"}, {"#Inte": "5.2 Intentionality"}, {"#MindBody": "5.3 Mind and Body"}, {"#SimuDuplEvol": "5.4 Simulation, duplication and evolution"}, {"#Conc": "Conclusion"}, {"#Bib": "Bibliography"}, {"#Aca": "Academic Tools"}, {"#Oth": "Other Internet Resources"}, {"#Rel": "Related Entries"}], "main_text": "\n1. Overview\n\nWork in Artificial Intelligence (AI) has produced computer programs\nthat can beat the world chess champion, control autonomous vehicles,\ncomplete our email sentences, and defeat the best human players on the\ntelevision quiz show Jeopardy. AI has also produced programs\nwith which one can converse in natural language, including customer\nservice \u201cvirtual agents\u201d, and Amazon\u2019s Alexa and\nApple\u2019s Siri. Our experience shows that playing chess or\nJeopardy, and carrying on a conversation, are activities that\nrequire understanding and intelligence. Does computer prowess at\nconversation and challenging games then show that computers can\nunderstand language and be intelligent? Will further development\nresult in digital computers that fully match or even exceed human\nintelligence?\n Alan Turing\n (1950), one of the pioneer theoreticians of computing, believed the\nanswer to these questions was \u201cyes\u201d. Turing proposed what\nis now known as\n \u2018The Turing Test\u2019:\n if a computer can pass for human in online chat, we should grant that\nit is intelligent. By the late 1970s some AI researchers claimed that\ncomputers already understood at least some natural language. In 1980\nU.C. Berkeley philosopher John Searle introduced a short and\nwidely-discussed argument intended to show conclusively that it is\nimpossible for digital computers to understand language or think.\n\nSearle argues that a good way to test a theory of mind, say a theory\nthat holds that understanding can be created by doing such and such,\nis to imagine what it would be like to actually do what the theory\nsays will create understanding. Searle (1999) summarized his Chinese\nRoom Argument (herinafter, CRA) concisely:\n\nImagine a native English speaker who knows no Chinese locked in a room\nfull of boxes of Chinese symbols (a data base) together with a book of\ninstructions for manipulating the symbols (the program). Imagine that\npeople outside the room send in other Chinese symbols which, unknown\nto the person in the room, are questions in Chinese (the input). And\nimagine that by following the instructions in the program the man in\nthe room is able to pass out Chinese symbols which are correct answers\nto the questions (the output). The program enables the person in the\nroom to pass the Turing Test for understanding Chinese but he does not\nunderstand a word of Chinese.\n\n\nSearle goes on to say, \u201cThe point of the argument is this: if\nthe man in the room does not understand Chinese on the basis of\nimplementing the appropriate program for understanding Chinese then\nneither does any other digital computer solely on that basis because\nno computer, qua computer, has anything the man does not\nhave.\u201d\n\nThirty years after introducing the CRA Searle 2010 describes the\nconclusion in terms of consciousness and\n intentionality:\n\nI demonstrated years ago with the so-called Chinese Room Argument that\nthe implementation of the computer program is not by itself sufficient\nfor consciousness or intentionality (Searle 1980). Computation is\ndefined purely formally or syntactically, whereas minds have actual\nmental or semantic contents, and we cannot get from syntactical to the\nsemantic just by having the syntactical operations and nothing else.\nTo put this point slightly more technically, the notion \u201csame\nimplemented program\u201d defines an equivalence class that is\nspecified independently of any specific physical realization. But such\na specification necessarily leaves out the biologically specific\npowers of the brain to cause cognitive processes. A system, me, for\nexample, would not acquire an understanding of Chinese just by going\nthrough the steps of a computer program that simulated the behavior of\na Chinese speaker (p.17).\n\n\n\u201cIntentionality\u201d is a technical term for a feature of\nmental and certain other things, namely being about something. Thus a\ndesire for a piece of chocolate and thoughts about real Manhattan or\nfictional Harry Potter all display intentionality, as will be\ndiscussed in more detail in section 5.2 below.\n\nSearle\u2019s shift from machine understanding to consciousness and\nintentionality is not directly supported by the original 1980\nargument. However the re-description of the conclusion indicates the\nclose connection between understanding and consciousness in\nSearle\u2019s later accounts of meaning and intentionality. Those who\ndon\u2019t accept Searle\u2019s linking account might hold that\nrunning a program can create understanding without necessarily\ncreating consciousness, and conversely a fancy robot might have dog\nlevel consciousness, desires, and beliefs, without necessarily\nunderstanding natural language.\n\nIn moving to discussion of intentionality Searle seeks to develop the\nbroader implications of his argument. It aims to refute the\n functionalist\n approach to understanding minds, that is, the approach that holds\nthat mental states are defined by their causal roles, not by the stuff\n(neurons, transistors) that plays those roles. The argument counts\nespecially against that form of functionalism known as\n  the Computational Theory of Mind\n that treats minds as information processing systems. As a result of\nits scope, as well as Searle\u2019s clear and forceful writing style,\nthe Chinese Room argument has probably been the most widely discussed\nphilosophical argument in cognitive science to appear since the Turing\nTest. By 1991 computer scientist Pat Hayes had defined Cognitive\nScience as the ongoing research project of refuting Searle\u2019s\nargument. Cognitive psychologist Steven Pinker (1997) pointed out that\nby the mid-1990s well over 100 articles had been published on\nSearle\u2019s thought experiment \u2013 and that discussion of it\nwas so pervasive on the Internet that Pinker found it a compelling\nreason to remove his name from all Internet discussion lists. \n\nThis interest has not subsided, and the range of connections with the\nargument has broadened. A search on Google Scholar for \u201cSearle\nChinese Room\u201d limited to the period from 2010 through 2019\nproduced over 2000 results, including papers making connections\nbetween the argument and topics ranging from embodied cognition to\ntheater to talk psychotherapy to postmodern views of truth and\n\u201cour post-human future\u201d \u2013 as well as discussions of\ngroup or collective minds and discussions of the role of intuitions in\nphilosophy. In 2007 a game company took the name \u201cThe Chinese\nRoom\u201d in joking honor of \u201c...Searle\u2019s critique of AI\n\u2013 that you could create a system that gave the impression of\nintelligence without any actual internal smarts.\u201d This\nwide-range of discussion and implications is a tribute to the\nargument\u2019s simple clarity and centrality. \n2. Historical Background\n2.1 Leibniz\u2019 Mill\n\nSearle\u2019s argument has four important antecedents. The first of\nthese is an argument set out by the philosopher and mathematician\nGottfried Leibniz (1646\u20131716). This argument, often known as\n\u201cLeibniz\u2019 Mill\u201d, appears as section 17 of\nLeibniz\u2019 Monadology. Like Searle\u2019s argument,\nLeibniz\u2019 argument takes the form of a thought experiment.\nLeibniz asks us to imagine a physical system, a machine, that behaves\nin such a way that it supposedly thinks and has experiences\n(\u201cperception\u201d).\n\n17. Moreover, it must be confessed that perception and that which\ndepends upon it are inexplicable on mechanical grounds, that is to\nsay, by means of figures and motions. And supposing there were a\nmachine, so constructed as to think, feel, and have perception, it\nmight be conceived as increased in size, while keeping the same\nproportions, so that one might go into it as into a mill. That being\nso, we should, on examining its interior, find only parts which work\none upon another, and never anything by which to explain a perception.\nThus it is in a simple substance, and not in a compound or in a\nmachine, that perception must be sought for. [Robert Latta\ntranslation]\n\n\nNotice that Leibniz\u2019s strategy here is to contrast the overt\nbehavior of the machine, which might appear to be the product of\nconscious thought, with the way the machine operates internally. He\npoints out that these internal mechanical operations are just parts\nmoving from point to point, hence there is nothing that is conscious\nor that can explain thinking, feeling or perceiving. For Leibniz\nphysical states are not sufficient for, nor constitutive of, mental\nstates.\n2.2 Turing\u2019s Paper Machine\n\nA second antecedent to the Chinese Room argument is the idea of a\npaper machine, a computer implemented by a human. This idea is found\nin the work of Alan Turing, for example in \u201cIntelligent\nMachinery\u201d (1948). Turing writes there that he wrote a program\nfor a \u201cpaper machine\u201d to play chess. A paper machine is a\nkind of program, a series of simple steps like a computer program, but\nwritten in natural language (e.g., English), and implemented by a\nhuman. The human operator of the paper chess-playing machine need not\n(otherwise) know how to play chess. All the operator does is follow\nthe instructions for generating moves on the chess board. In fact, the\noperator need not even know that he or she is involved in playing\nchess \u2013 the input and output strings, such as\n\u201cN\u2013QB7\u201d need mean nothing to the operator of the\npaper machine.\n\nAs part of the WWII project to decipher German military encryption,\nTuring had written English-language programs for human\n\u201ccomputers\u201d, as these specialized workers were then known,\nand these human computers did not need to know what the programs that\nthey implemented were doing.\n\nOne reason the idea of a human-plus-paper machine is important is that\nit already raises questions about agency and understanding similar to\nthose in the CRA. Suppose I am alone in a closed room and follow an\ninstruction book for manipulating strings of symbols. I thereby\nimplement a paper machine that generates symbol strings such as\n\u201cN-KB3\u201d that I write on pieces of paper and slip under the\ndoor to someone ouside the room. Suppose further that prior to going\ninto the room I don\u2019t know how to play chess, or even that there\nis such a game. However, unbeknownst to me, in the room I am running\nTuring\u2019s chess program and the symbol strings I generate are\nchess notation and are taken as chess moves by those outside the room.\nThey reply by sliding the symbols for their own moves back under the\ndoor into the room. If all you see is the resulting sequence of moves\ndisplayed on a chess board outside the room, you might think that\nsomeone in the room knows how to play chess very well. Do I now know\nhow to play chess? Or is it the system (consisting of me, the manuals,\nand the paper on which I manipulate strings of symbols) that is\nplaying chess? If I memorize the program and do the symbol\nmanipulations inside my head, do I then know how to play chess, albeit\nwith an odd phenomenology? Does someone\u2019s conscious states\nmatter for whether or not they know how to play chess? If a digital\ncomputer implements the same program, does the computer then play\nchess, or merely simulate this? \n\nBy mid-century Turing was optimistic that the newly developed\nelectronic computers themselves would soon be able to exhibit\napparently intelligent behavior, answering questions posed in English\nand carrying on conversations. Turing (1950) proposed what is now\nknown as the Turing Test: if a computer could pass for human in\non-line chat, it should be counted as intelligent. \n\nA third antecedent of Searle\u2019s argument was the work of\nSearle\u2019s colleague at Berkeley, Hubert Dreyfus. Dreyfus was an\nearly critic of the optimistic claims made by AI researchers. In 1965,\nwhen Dreyfus was at MIT, he published a circa hundred page report\ntitled \u201cAlchemy and Artificial Intelligence\u201d. Dreyfus\nargued that key features of human mental life could not be captured by\nformal rules for manipulating symbols. Dreyfus moved to Berkeley in\n1968 and in 1972 published his extended critique, \u201cWhat\nComputers Can\u2019t Do\u201d. Dreyfus\u2019 primary research\ninterests were in Continental philosophy, with its focus on\nconsciousness, intentionality, and the role of intuition and the\ninarticulated background in shaping our understandings. Dreyfus\nidentified several problematic assumptions in AI, including the view\nthat brains are like digital computers, and, again, the assumption\nthat understanding can be codified as explicit rules. \n\nHowever by the late 1970s, as computers became faster and less\nexpensive, some in the burgeoning AI community started to claim that\ntheir programs could understand English sentences, using a database of\nbackground information. The work of one of these, Yale researcher\nRoger Schank (Schank & Abelson 1977) came to Searle\u2019s\nattention.Schank developed a technique called \u201cconceptual\nrepresentation\u201d that used \u201cscripts\u201d to represent\nconceptual relations (related to Conceptual Role Semantics).\nSearle\u2019s argument was originally presented as a response to the\nclaim that AI programs such as Schank\u2019s literally understand the\nsentences that they respond to.\n2.3 The Chinese Nation\n\nA fourth antecedent to the Chinese Room argument are thought\nexperiments involving myriad humans acting as a computer. In 1961\nAnatoly Mickevich (pseudonym A. Dneprov) published \u201cThe\nGame\u201d, a story in which a stadium full of 1400 math students are\narranged to function as a digital computer (see Dneprov 1961 and the\nEnglish translation listed at Mickevich 1961, Other Internet\nResources). For 4 hours each repeatedly does a bit of calculation on\nbinary numbers received from someone near them, then passes the binary\nresult onto someone nearby. They learn the next day that they\ncollectively translated a sentence from Portuguese into their native\nRussian. Mickevich\u2019s protagonist concludes \u201cWe\u2019ve\nproven that even the most perfect simulation of machine thinking is\nnot the thinking process itself, which is a higher form of motion of\nliving matter.\u201d Apparently independently, a similar\nconsideration emerged in early discussion of functionalist theories of\nminds and cognition (see further discussion in section 5.3 below),\nFunctionalists hold that mental states are defined by the causal role\nthey play in a system (just as a door stop is defined by what it does,\nnot by what it is made out of). Critics of functionalism were quick to\nturn its proclaimed virtue of multiple realizability against it. While\nfunctionalism was consistent with a materialist or biological\nunderstanding of mental states (arguably a virtue), it did not\nidentify types of mental states (such as experiencing pain, or\nwondering about OZ) with particular types of neurophysiological\nstates, as \u201ctype-type identity theory\u201d did. In contrast\nwith type-type identity theory, functionalism allowed sentient beings\nwith different physiology to have the same types of mental states as\nhumans \u2013 pains, for example. But it was pointed out that if\nextraterrestrial aliens, with some other complex system in place of\nbrains, could realize the functional properties that constituted\nmental states, then, presumably so could systems even less like human\nbrains. The computational form of functionalism, which holds that the\ndefining role of each mental state is its role in information\nprocessing or computation, is particularly vulnerable to this\nmaneuver, since a wide variety of systems with simple components are\ncomputationally equivalent (see e.g., Maudlin 1989 for discussion of a\ncomputer built from buckets of water). Critics asked if it was really\nplausible that these inorganic systems could have mental states or\nfeel pain.\n\nDaniel Dennett (1978) reports that in 1974 Lawrence Davis gave a\ncolloquium at MIT in which he presented one such unorthodox\nimplementation. Dennett summarizes Davis\u2019 thought experiment as\nfollows:\n\nLet a functionalist theory of pain (whatever its details) be\ninstantiated by a system the subassemblies of which are not such\nthings as C-fibers and reticular systems but telephone lines and\noffices staffed by people. Perhaps it is a giant robot controlled by\nan army of human beings that inhabit it. When the theory\u2019s\nfunctionally characterized conditions for pain are now met we must\nsay, if the theory is true, that the robot is in pain. That is, real\npain, as real as our own, would exist in virtue of the perhaps\ndisinterested and businesslike activities of these bureaucratic teams,\nexecuting their proper functions.\n\n\nIn \u201cTroubles with Functionalism\u201d, also published in 1978,\nNed Block envisions the entire population of China implementing the\nfunctions of neurons in the brain. This scenario has subsequently been\ncalled \u201cThe Chinese Nation\u201d or \u201cThe Chinese\nGym\u201d. We can suppose that every Chinese citizen would be given a\ncall-list of phone numbers, and at a preset time on implementation\nday, designated \u201cinput\u201d citizens would initiate the\nprocess by calling those on their call-list. When any citizen\u2019s\nphone rang, he or she would then phone those on his or her list, who\nwould in turn contact yet others. No phone message need be exchanged;\nall that is required is the pattern of calling. The call-lists would\nbe constructed in such a way that the patterns of calls implemented\nthe same patterns of activation that occur between neurons in\nsomeone\u2019s brain when that person is in a mental state \u2013\npain, for example. The phone calls play the same functional role as\nneurons causing one another to fire. Block was primarily interested in\nqualia, and in particular, whether it is plausible to hold that the\npopulation of China might collectively be in pain, while no individual\nmember of the population experienced any pain, but the thought\nexperiment applies to any mental states and operations, including\nunderstanding language.\n\nThus Block\u2019s precursor thought experiment, as with those of\nDavis and Dennett, is a system of many humans rather than one. The\nfocus is on consciousness, but to the extent that Searle\u2019s\nargument also involves consciousness, the thought experiment is\nclosely related to Searle\u2019s. Cole (1984) tries to pump\nintuitions in the reverse direction by setting out a thought\nexperiment in which each of his neurons is itself conscious, and fully\naware of its actions including being doused with neurotransmitters,\nundergoing action potentials, and squirting neurotransmitters at its\nneighbors. Cole argues that his conscious neurons would find it\nimplausible that their collective activity produced a consciousness\nand other cognitive competences, including understanding English, that\nthe neurons lack. Cole suggests the intuitions of implementing systems\nare not to be trusted. \n3. The Chinese Room Argument\n\nIn 1980 John Searle published \u201cMinds, Brains and Programs\u201d\nin the journal The Behavioral and Brain Sciences. In this\narticle, Searle sets out the argument, and then replies to the\nhalf-dozen main objections that had been raised during his earlier\npresentations at various university campuses (see next section). In\naddition, Searle\u2019s article in BBS was published along\nwith comments and criticisms by 27 cognitive science researchers.\nThese 27 comments were followed by Searle\u2019s replies to his\ncritics.\n\nIn the decades following its publication, the Chinese Room argument\nwas the subject of very many discussions. By 1984, Searle presented\nthe Chinese Room argument in a book, Minds, Brains and\nScience. In January 1990, the popular periodical Scientific\nAmerican took the debate to a general scientific audience. Searle\nincluded the Chinese Room Argument in his contribution, \u201cIs the\nBrain\u2019s Mind a Computer Program?\u201d, and Searle\u2019s\npiece was followed by a responding article, \u201cCould a Machine\nThink?\u201d, written by philosophers Paul and Patricia Churchland.\nSoon thereafter Searle had a published exchange about the Chinese Room\nwith another leading philosopher, Jerry Fodor (in Rosenthal (ed.)\n1991).\n\nThe heart of the argument is Searle imagining himself following a\nsymbol-processing program written in English (which is what Turing\ncalled \u201ca paper machine\u201d). The English speaker (Searle)\nsitting in the room follows English instructions for manipulating\nChinese symbols, whereas a computer \u201cfollows\u201d (in some\nsense) a program written in a computing language. The human produces\nthe appearance of understanding Chinese by following the symbol\nmanipulating instructions, but does not thereby come to understand\nChinese. Since a computer just does what the human does \u2013\nmanipulate symbols on the basis of their syntax alone \u2013 no\ncomputer, merely by following a program, comes to genuinely understand\nChinese.\n\nThis narrow argument, based closely on the Chinese Room scenario, is\nspecifically directed at a position Searle calls \u201cStrong\nAI\u201d. Strong AI is the view that suitably programmed computers\n(or the programs themselves) can understand natural language and\nactually have other mental capabilities similar to the humans whose\nbehavior they mimic. According to Strong AI, these computers really\nplay chess intelligently, make clever moves, or understand language.\nBy contrast, \u201cweak AI\u201d is the much more modest claim that\ncomputers are merely useful in psychology, linguistics, and other\nareas, in part because they can simulate mental abilities. But weak AI\nmakes no claim that computers actually understand or are intelligent.\nThe Chinese Room argument is not directed at weak AI, nor does it\npurport to show that no machine can think \u2013 Searle says that\nbrains are machines, and brains think. The argument is directed at the\nview that formal computations on symbols can produce thought.\n\nWe might summarize the narrow argument as a reductio ad\nabsurdum against Strong AI as follows. Let L be a natural\nlanguage, and let us say that a \u201cprogram for L\u201d is a\nprogram for conversing fluently in L. A computing system is any\nsystem, human or otherwise, that can run a program.\n\nIf Strong AI is true, then there is a program for Chinese such\nthat if any computing system runs that program, that system thereby\ncomes to understand Chinese.\nI could run a program for Chinese without thereby coming to\nunderstand Chinese.\nTherefore Strong AI is false.\n\n\nThe first premise elucidates the claim of Strong AI. The second\npremise is supported by the Chinese Room thought experiment. The\nconclusion of this narrow argument is that running a program cannot\nendow the system with language understanding. (There are other ways of\nunderstanding the structure of the argument. It may be relevant to\nunderstand some of the claims as counterfactual: e.g. \u201cthere is\na program\u201d in premise 1 as meaning there could be a program,\netc. On this construal the argument involves modal logic, the logic of\npossibility and necessity (see Damper 2006 and Shaffer 2009)). \n\nIt is also worth noting that the first premise above attributes\nunderstanding to \u201cthe system\u201d. Exactly what Strong-AI\nsupposes will acquire understanding when the program runs is crucial\nto the success or failure of the CRA. Schank 1978 has a title that\nclaims their group\u2019s computer, a physical device, understands,\nbut in the body of the paper he claims that the program\n[\u201cSAM\u201d] is doing the understanding: SAM, Schank says\n\u201c...understands stories about domains about which it has\nknowledge\u201d (p. 133). As we will see in the next section (4),\nthese issues about the identity of the understander (the cpu? the\nprogram? the system? something else?) quickly came to the fore for\ncritics of the CRA. Searle\u2019s wider argument includes the claim\nthat the thought experiment shows more generally that one cannot get\nsemantics (meaning) from syntax (formal symbol manipulation). That and\nrelated issues are discussed in section 5: The Larger Philosophical\nIssues.\n4. Replies to the Chinese Room Argument\n\nCriticisms of the narrow Chinese Room argument against Strong AI have\noften followed three main lines, which can be distinguished by how\nmuch they concede:\n\n(1) Some critics concede that the man in the room doesn\u2019t\nunderstand Chinese, but hold that nevertheless running the program may\ncreate comprehension of Chinese by something other than the room\noperator. These critics object to the inference from the claim that\nthe man in the room does not understand Chinese to the\nconclusion that no understanding has been created. There might\nbe understanding by a larger, smaller, or different, entity. This is\nthe strategy of The Systems Reply and the Virtual Mind Reply. These\nreplies hold that the output of the room might reflect real\nunderstanding of Chinese, but the understanding would not be that of\nthe room operator. Thus Searle\u2019s claim that he doesn\u2019t\nunderstand Chinese while running the room is conceded, but his claim\nthat there is no understanding of the questions in Chinese, and that\ncomputationalism is false, is denied.\n\n(2) Other critics concede Searle\u2019s claim that just running a\nnatural language processing program as described in the CR scenario\ndoes not create any understanding, whether by a human or a computer\nsystem. But these critics hold that a variation on the\ncomputer system could understand. The variant might be a computer\nembedded in a robotic body, having interaction with the physical world\nvia sensors and motors (\u201cThe Robot Reply\u201d), or it might be\na system that simulated the detailed operation of an entire human\nbrain, neuron by neuron (\u201cthe Brain Simulator Reply\u201d).\n\n\n(3) Finally, some critics do not concede even the narrow point against\nAI. These critics hold that the man in the original Chinese Room\nscenario might understand Chinese, despite Searle\u2019s denials, or\nthat the scenario is impossible. For example, critics have argued that\nour intuitions in such cases are unreliable. Other critics have held\nthat it all depends on what one means by \u201cunderstand\u201d\n\u2013 points discussed in the section on The Intuition Reply. Others\n(e.g. Sprevak 2007) object to the assumption that any system (e.g.\nSearle in the room) can run any computer program. And finally some\nhave argued that if it is not reasonable to attribute understanding on\nthe basis of the behavior exhibited by the Chinese Room, then it would\nnot be reasonable to attribute understanding to humans on the basis of\nsimilar behavioral evidence (Searle calls this last the \u201cOther\nMinds Reply\u201d). The objection is that we should be willing to\nattribute understanding in the Chinese Room on the basis of the overt\nbehavior, just as we do with other humans (and some animals), and as\nwe would do with extra-terrestrial Aliens (or burning bushes or\nangels) that spoke our language. This position is close to\nTuring\u2019s own, when he proposed his behavioral test for machine\nintelligence.\n\nIn addition to these responses specifically to the Chinese Room\nscenario and the narrow argument to be discussed here, some critics\nalso independently argue against Searle\u2019s larger claim, and hold\nthat one can get semantics (that is, meaning) from syntactic symbol\nmanipulation, including the sort that takes place inside a digital\ncomputer, a question discussed in the section below on Syntax and\nSemantics.\n4.1 The Systems Reply\n\nIn the original BBS article, Searle identified and discussed several\nresponses to the argument that he had come across in giving the\nargument in talks at various places. As a result, these early\nresponses have received the most attention in subsequent discussion.\nWhat Searle 1980 calls \u201cperhaps the most common reply\u201d is\nthe Systems Reply.\n\nThe Systems Reply (which Searle says was originally associated with\nYale, the home of Schank\u2019s AI work) concedes that the man in the\nroom does not understand Chinese. But, the reply continues, the man is\nbut a part, a central processing unit (CPU), in a larger system. The\nlarger system includes the huge database, the memory (scratchpads)\ncontaining intermediate states, and the instructions \u2013 the\ncomplete system that is required for answering the Chinese questions.\nSo the Sytems Reply is that while the man running the program does not\nunderstand Chinese, the system as a whole does.\n\nNed Block was one of the first to press the Systems Reply, along with\nmany others including Jack Copeland, Daniel Dennett, Douglas\nHofstadter, Jerry Fodor, John Haugeland, Ray Kurzweil and Georges Rey.\nRey (1986) says the person in the room is just the CPU of the system.\nKurzweil (2002) says that the human being is just an implementer and\nof no significance (presumably meaning that the properties of the\nimplementer are not necessarily those of the system). Kurzweil hews to\nthe spirit of the Turing Test and holds that if the system displays\nthe apparent capacity to understand Chinese \u201cit would have to,\nindeed, understand Chinese\u201d \u2013 Searle is contradicting\nhimself in saying in effect, \u201cthe machine speaks Chinese but\ndoesn\u2019t understand Chinese\u201d.\n\nMargaret Boden (1988) raises levels considerations.\n\u201cComputational psychology does not credit the brain with seeing\nbean-sprouts or understanding English: intentional states such as\nthese are properties of people, not of brains\u201d (244). \u201cIn\nshort, Searle\u2019s description of the robot\u2019s pseudo-brain\n(that is, of Searle-in-the-robot) as understanding English involves a\ncategory-mistake comparable to treating the brain as the bearer, as\nopposed to the causal basis, of intelligence\u201d. Boden (1988)\npoints out that the room operator is a conscious agent, while the CPU\nin a computer is not \u2013 the Chinese Room scenario asks us to take\nthe perspective of the implementer, and not surprisingly fails to see\nthe larger picture.\n\nSearle\u2019s response to the Systems Reply is simple: in principle,\nhe could internalize the entire system, memorizing all the\ninstructions and the database, and doing all the calculations in his\nhead. He could then leave the room and wander outdoors, perhaps even\nconversing in Chinese. But he still would have no way to attach\n\u201cany meaning to the formal symbols\u201d. The man would now\nbe the entire system, yet he still would not understand\nChinese. For example, he would not know the meaning of the Chinese\nword for hamburger. He still cannot get semantics from syntax. \n\nIn some ways Searle\u2019s response here anticipates later extended\nmind views (e.g. Clark and Chalmers 1998): if Otto, who suffers loss\nof memory, can regain those recall abilities by externalizing some of\nthe information to his notebooks, then Searle arguably can do the\nreverse: by internalizing the instructions and notebooks he should\nacquire any abilities had by the extended system. And so Searle in\neffect concludes that since he doesn\u2019t acquire understanding of\nChinese by internalizing the external components of the entire system\n(e.g. he still doesn\u2019t know what the Chinese word for hamburger\nmeans), understanding was never there in the partially externalized\nsystem of the original Chinese Room.\n\nIn his 2002 paper \u201cThe Chinese Room from a Logical Point of\nView\u201d, Jack Copeland considers Searle\u2019s response to the\nSystems Reply and argues that a homunculus inside Searle\u2019s head\nmight understand even though the room operator himself does not, just\nas modules in minds solve tensor equations that enable us to catch\ncricket balls. Copeland then turns to consider the Chinese Gym, and\nagain appears to endorse the Systems Reply: \u201c\u2026the\nindividual players [do not] understand Chinese. But there is no\nentailment from this to the claim that the simulation as a whole does\nnot come to understand Chinese. The fallacy involved in moving from\npart to whole is even more glaring here than in the original version\nof the Chinese Room Argument\u201d. Copeland denies that\nconnectionism implies that a room of people can simulate the\nbrain.\n\nJohn Haugeland writes (2002) that Searle\u2019s response to the\nSystems Reply is flawed: \u201c\u2026what he now asks is what it\nwould be like if he, in his own mind, were consciously to implement\nthe underlying formal structures and operations that the theory says\nare sufficient to implement another mind\u201d. According to\nHaugeland, his failure to understand Chinese is irrelevant: he is just\nthe implementer. The larger system implemented would understand\n\u2013 there is a level-of-description fallacy.\n\nShaffer 2009 examines modal aspects of the logic of the CRA and argues\nthat familiar versions of the System Reply are question-begging. But,\nShaffer claims, a modalized version of the System Reply succeeds\nbecause there are possible worlds in which understanding is an\nemergent property of complex syntax manipulation. Nute 2011 is a reply\nto Shaffer.\n\nStevan Harnad has defended Searle\u2019s argument against Systems\nReply critics in two papers. In his 1989 paper, Harnad writes\n\u201cSearle formulates the problem as follows: Is the mind a\ncomputer program? Or, more specifically, if a computer program\nsimulates or imitates activities of ours that seem to require\nunderstanding (such as communicating in language), can the program\nitself be said to understand in so doing?\u201d (Note the specific\nclaim: the issue is taken to be whether the program itself\nunderstands.) Harnad concludes: \u201cOn the face of it, [the CR\nargument] looks valid. It certainly works against the most common\nrejoinder, the \u2018Systems Reply\u2019\u2026.\u201d Harnad\nappears to follow Searle in linking understanding and states of\nconsciousness: Harnad 2012 (Other Internet Resources) argues that\nSearle shows that the core problem of conscious \u201cfeeling\u201d\nrequires sensory connections to the real world. (See sections below\n\u201cThe Robot Reply\u201d and \u201cIntentionality\u201d for\ndiscussion.)\n\nFinally some have argued that even if the room operator memorizes the\nrules and does all the operations inside his head, the room operator\ndoes not become the system. Cole (1984) and Block (1998) both argue\nthat the result would not be identity of Searle with the system but\nmuch more like a case of multiple personality \u2013 distinct persons\nin a single head. The Chinese responding system would not be Searle,\nbut a sub-part of him. In the CR case, one person (Searle) is an\nEnglish monoglot and the other is a Chinese monoglot. The\nEnglish-speaking person\u2019s total unawareness of the meaning of\nthe Chinese responses does not show that they are not understood. This\nline, of distinct persons, leads to the Virtual Mind Reply. \n4.1.1 The Virtual Mind Reply\n\nThe Virtual Mind reply concedes, as does the System Reply, that the\noperator of the Chinese Room does not understand Chinese merely by\nrunning the paper machine. However the Virtual Mind reply holds that\nwhat is important is whether understanding is created, not whether the\nRoom operator is the agent that understands. Unlike the Systems Reply,\nthe Virtual Mind reply (VMR) holds that a running system may create\nnew, virtual, entities that are distinct from both the system as a\nwhole, as well as from the sub-systems such as the CPU or operator. In\nparticular, a running system might create a distinct agent that\nunderstands Chinese. This virtual agent would be distinct from both\nthe room operator and the entire system. The psychological traits,\nincluding linguistic abilities, of any mind created by artificial\nintelligence will depend entirely upon the program and the Chinese\ndatabase, and will not be identical with the psychological traits and\nabilities of a CPU or the operator of a paper machine, such as Searle\nin the Chinese Room scenario. According to the VMR the mistake in the\nChinese Room Argument is to make the claim of strong AI to be\n\u201cthe computer understands Chinese\u201d or \u201cthe System\nunderstands Chinese\u201d. The claim at issue for AI should simply be\nwhether \u201cthe running computer creates understanding of\nChinese\u201d.\n\nA familiar model of virtual agents are characters in computer or video\ngames, and personal digital assistants, such as Apple\u2019s Siri and\nMicrosoft\u2019s Cortana. These characters have various abilities and\npersonalities, and the characters are not identical with the system\nhardware or program that creates them. A single running system might\ncontrol two distinct agents, or physical robots, simultaneously, one\nof which converses only in Chinese and one of which can converse only\nin English, and which otherwise manifest very different personalities,\nmemories, and cognitive abilities. Thus the VM reply asks us to\ndistinguish between minds and their realizing systems. \n\nMinsky (1980) and Sloman and Croucher (1980) suggested a Virtual Mind\nreply when the Chinese Room argument first appeared. In his\nwidely-read 1989 paper \u201cComputation and Consciousness\u201d,\nTim Maudlin considers minimal physical systems that might implement a\ncomputational system running a program. His discussion revolves around\nhis imaginary Olympia machine, a system of buckets that transfers\nwater, implementing a Turing machine. Maudlin\u2019s main target is\nthe computationalists\u2019 claim that such a machine could have\nphenomenal consciousness. However in the course of his discussion,\nMaudlin considers the Chinese Room argument. Maudlin (citing Minsky,\nand Sloman and Croucher) points out a Virtual Mind reply that the\nagent that understands could be distinct from the physical system\n(414). Thus \u201cSearle has done nothing to discount the possibility\nof simultaneously existing disjoint mentalities\u201d\n(414\u20135).\n\nPerlis (1992), Chalmers (1996) and Block (2002) have apparently\nendorsed versions of a Virtual Mind reply as well, as has Richard\nHanley in The Metaphysics of Star Trek (1997). Penrose (2002)\nis a critic of this strategy, and Stevan Harnad scornfully dismisses\nsuch heroic resorts to metaphysics. Harnad defended Searle\u2019s\nposition in a \u201cVirtual Symposium on Virtual Minds\u201d (1992)\nagainst Patrick Hayes and Don Perlis. Perlis pressed a virtual minds\nargument derived, he says, from Maudlin. Chalmers (1996) notes that\nthe room operator is just a causal facilitator, a \u201cdemon\u201d,\nso that his states of consciousness are irrelevant to the properties\nof the system as a whole. Like Maudlin, Chalmers raises issues of\npersonal identity \u2013 we might regard the Chinese Room as\n\u201ctwo mental systems realized within the same physical space. The\norganization that gives rise to the Chinese experiences is quite\ndistinct from the organization that gives rise to the demon\u2019s [=\nroom operator\u2019s] experiences\u201d(326).\n\nCole (1991, 1994) develops the reply and argues as follows:\nSearle\u2019s argument requires that the agent of understanding be\nthe computer itself or, in the Chinese Room parallel, the person in\nthe room. However Searle\u2019s failure to understand Chinese in the\nroom does not show that there is no understanding being created. One\nof the key considerations is that in Searle\u2019s discussion the\nactual conversation with the Chinese Room is always seriously under\nspecified. Searle was considering Schank\u2019s programs, which can\nonly respond to a few questions about what happened in a restaurant,\nall in third person. But Searle wishes his conclusions to apply to any\nAI-produced responses, including those that would pass the toughest\nunrestricted Turing Test, i.e. they would be just the sort of\nconversations real people have with each other. If we flesh out the\nconversation in the original CR scenario to include questions in\nChinese such as \u201cHow tall are you?\u201d, \u201cWhere do you\nlive?\u201d, \u201cWhat did you have for breakfast?\u201d,\n\u201cWhat is your attitude toward Mao?\u201d, and so forth, it\nimmediately becomes clear that the answers in Chinese are not\nSearle\u2019s answers. Searle is not the author of the\nanswers, and his beliefs and desires, memories and personality traits\n(apart from his industriousness!) are not reflected in the answers and\nin general Searle\u2019s traits are causally inert in producing the\nanswers to the Chinese questions. This suggests the following\nconditional is true: if there is understanding of Chinese created by\nrunning the program, the mind understanding the Chinese would not be\nthe computer, whether the computer is human or electronic. The person\nunderstanding the Chinese would be a distinct person from the room\noperator, with beliefs and desires bestowed by the program and its\ndatabase. Hence Searle\u2019s failure to understand Chinese while\noperating the room does not show that understanding is not being\ncreated. \n\nCole (1991) offers an additional argument that the mind doing the\nunderstanding is neither the mind of the room operator nor the system\nconsisting of the operator and the program: running a suitably\nstructured computer program might produce answers submitted in Chinese\nand also answers to questions submitted in Korean. Yet the Chinese\nanswers might apparently display completely different knowledge and\nmemories, beliefs and desires than the answers to the Korean questions\n\u2013 along with a denial that the Chinese answerer knows any\nKorean, and vice versa. Thus the behavioral evidence would be that\nthere were two non-identical minds (one understanding Chinese only,\nand one understanding Korean only). Since these might have mutually\nexclusive properties, they cannot be identical, and ipso facto, cannot\nbe identical with the mind of the implementer in the room.\nAnalogously, a video game might include a character with one set of\ncognitive abilities (smart, understands Chinese) as well as another\ncharacter with an incompatible set (stupid, English monoglot). These\ninconsistent cognitive traits cannot be traits of the XBOX system that\nrealizes them. Cole argues that the implication is that minds\ngenerally are more abstract than the systems that realize them (see\nMind and Body in the Larger Philosophical Issues section).\n\nIn short, the Virtual Mind argument is that since the evidence that\nSearle provides that there is no understanding of Chinese was that\nhe wouldn\u2019t understand Chinese in the room, the Chinese\nRoom Argument cannot refute a differently formulated equally strong AI\nclaim, asserting the possibility of creating understanding using a\nprogrammed digital computer. Maudlin (1989) says that Searle has not\nadequately responded to this criticism.\n\nOthers however have replied to the VMR, including Stevan Harnad and\nmathematical physicist Roger Penrose. Penrose is generally sympathetic\nto the points Searle raises with the Chinese Room argument, and has\nargued against the Virtual Mind reply. Penrose does not believe that\ncomputational processes can account for consciousness, both on Chinese\nRoom grounds, as well as because of limitations on formal systems\nrevealed by Kurt G\u00f6del\u2019s incompleteness proof. (Penrose has\ntwo books on mind and consciousness; Chalmers and others have\nresponded to Penrose\u2019s appeals to G\u00f6del.) In his 2002\narticle \u201cConsciousness, Computation, and the Chinese Room\u201d\nthat specifically addresses the Chinese Room argument, Penrose argues\nthat the Chinese Gym variation \u2013 with a room expanded to the\nsize of India, with Indians doing the processing \u2013 shows it is\nvery implausible to hold there is \u201csome kind of disembodied\n\u2018understanding\u2019 associated with the person\u2019s\ncarrying out of that algorithm, and whose presence does not impinge in\nany way upon his own consciousness\u201d (230\u20131). Penrose\nconcludes the Chinese Room argument refutes Strong AI. Christian\nKaernbach (2005) reports that he subjected the virtual mind theory to\nan empirical test, with negative results.\n4.2 The Robot Reply\n\nThe Robot Reply concedes Searle is right about the Chinese Room\nscenario: it shows that a computer trapped in a computer room cannot\nunderstand language, or know what words mean. The Robot reply is\nresponsive to the problem of knowing the meaning of the Chinese word\nfor hamburger \u2013 Searle\u2019s example of something the room\noperator would not know. It seems reasonable to hold that most of us\nknow what a hamburger is because we have seen one, and perhaps even\nmade one, or tasted one, or at least heard people talk about\nhamburgers and understood what they are by relating them to things we\ndo know by seeing, making, and tasting. Given this is how one might\ncome to know what hamburgers are, the Robot Reply suggests that we put\na digital computer in a robot body, with sensors, such as video\ncameras and microphones, and add effectors, such as wheels to move\naround with, and arms with which to manipulate things in the world.\nSuch a robot \u2013 a computer with a body \u2013 might do what a\nchild does, learn by seeing and doing. The Robot Reply holds that such\na digital computer in a robot body, freed from the room, could attach\nmeanings to symbols and actually understand natural language. Margaret\nBoden, Tim Crane, Daniel Dennett, Jerry Fodor, Stevan Harnad, Hans\nMoravec and Georges Rey are among those who have endorsed versions of\nthis reply at one time or another. The Robot Reply in effect appeals\nto \u201cwide content\u201d or \u201cexternalist semantics\u201d.\nThis can agree with Searle that syntax and internal connections in\nisolation from the world are insufficient for semantics, while holding\nthat suitable causal connections with the world can provide content to\nthe internal symbols. \n\nAbout the time Searle was pressing the CRA, many in philosophy of\nlanguage and mind were recognizing the importance of causal\nconnections to the world as the source of meaning or reference for\nwords and concepts. Hilary Putnam 1981 argued that a Brain in a Vat,\nisolated from the world, might speak or think in a language that\nsounded like English, but it would not be English \u2013 hence a\nbrain in a vat could not wonder if it was a brain in a vat (because of\nits sensory isolation, its words \u201cbrain\u201d and\n\u201cvat\u201d do not refer to brains or vats). The view that\nmeaning was determined by connections with the world became\nwidespread. Searle resisted this turn outward and continued to think\nof meaning as subjective and connected with consciousness. \n\nA related view that minds are best understood as embodied or embedded\nin the world has gained many supporters since the 1990s, contra\nCartesian solipsistic intuitions. Organisms rely on environmental\nfeatures for the success of their behavior. So whether one takes a\nmind to be a symbol processing system, with the symbols getting their\ncontent from sensory connections with the world, or a non-symbolic\nsystem that succeeds by being embedded in a particular environment,\nthe important of things outside the head have come to the fore. Hence\nmany are sympathetic to some form of the Robot Reply: a computational\nsystem might understand, provided it is acting in the world. E.g\nCarter 2007 in a textbook on philosophy and AI concludes \u201cThe\nlesson to draw from the Chinese Room thought experiment is that\nembodied experience is necessary for the development of\nsemantics.\u201d \n\nHowever Searle does not think that the Robot Reply to the Chinese Room\nargument is any stronger than the Systems Reply. All the sensors can\ndo is provide additional input to the computer \u2013 and it will be\njust syntactic input. We can see this by making a parallel change to\nthe Chinese Room scenario. Suppose the man in the Chinese Room\nreceives, in addition to the Chinese characters slipped under the\ndoor, a stream of binary digits that appear, say, on a ticker tape in\na corner of the room. The instruction books are augmented to use the\nnumerals from the tape as input, along with the Chinese characters.\nUnbeknownst to the man in the room, the symbols on the tape are the\ndigitized output of a video camera (and possibly other sensors).\nSearle argues that additional syntactic inputs will do nothing to\nallow the man to associate meanings with the Chinese characters. It is\njust more work for the man in the room.\n\nJerry Fodor, Hilary Putnam, and David Lewis, were principle architects\nof the computational theory of mind that Searle\u2019s wider argument\nattacks. In his original 1980 reply to Searle, Fodor allows Searle is\ncertainly right that \u201cinstantiating the same program as the\nbrain does is not, in and of itself, sufficient for having those\npropositional attitudes characteristic of the organism that has the\nbrain.\u201d But Fodor holds that Searle is wrong about the robot\nreply. A computer might have propositional attitudes if it has the\nright causal connections to the world \u2013 but those are not ones\nmediated by a man sitting in the head of the robot. We don\u2019t\nknow what the right causal connections are. Searle commits the fallacy\nof inferring from \u201cthe little man is not the right causal\nconnection\u201d to conclude that no causal linkage would succeed.\nThere is considerable empirical evidence that mental processes involve\n\u201cmanipulation of symbols\u201d; Searle gives us no alternative\nexplanation (this is sometimes called Fodor\u2019s \u201cOnly Game\nin Town\u201d argument for computational approaches). In the 1980s\nand 1990s Fodor wrote extensively on what the connections must be\nbetween a brain state and the world for the state to have intentional\n(representational) properties, while also emphasizing that\ncomputationalism has limits because the computations are intrinsically\nlocal and so cannot account for abductive reasoning.\n\nIn a later piece, \u201cYin and Yang in the Chinese Room\u201d (in\nRosenthal 1991 pp.524\u2013525), Fodor substantially revises his 1980\nview. He distances himself from his earlier version of the robot\nreply, and holds instead that \u201cinstantiation\u201d should be\ndefined in such a way that the symbol must be the proximate cause of\nthe effect \u2013 no intervening guys in a room. So Searle in the\nroom is not an instantiation of a Turing Machine, and\n\u201cSearle\u2019s setup does not instantiate the machine that the\nbrain instantiates.\u201d He concludes: \u201c\u2026Searle\u2019s\nsetup is irrelevant to the claim that strong equivalence to a Chinese\nspeaker\u2019s brain is ipso facto sufficient for speaking\nChinese.\u201d Searle says of Fodor\u2019s move, \u201cOf all the\nzillions of criticisms of the Chinese Room argument, Fodor\u2019s is\nperhaps the most desperate. He claims that precisely because the man\nin the Chinese room sets out to implement the steps in the computer\nprogram, he is not implementing the steps in the computer program. He\noffers no argument for this extraordinary claim.\u201d (in Rosenthal\n1991, p. 525)\n\nIn a 1986 paper, Georges Rey advocated a combination of the system and\nrobot reply, after noting that the original Turing Test is\ninsufficient as a test of intelligence and understanding, and that the\nisolated system Searle describes in the room is certainly not\nfunctionally equivalent to a real Chinese speaker sensing and acting\nin the world. In a 2002 second look, \u201cSearle\u2019s\nMisunderstandings of Functionalism and Strong AI\u201d, Rey again\ndefends functionalism against Searle, and in the particular form Rey\ncalls the \u201ccomputational-representational theory of thought\n\u2013 CRTT\u201d. CRTT is not committed to attributing thought to\njust any system that passes the Turing Test (like the Chinese Room).\nNor is it committed to a conversation manual model of understanding\nnatural language. Rather, CRTT is concerned with intentionality,\nnatural and artificial (the representations in the system are\nsemantically evaluable \u2013 they are true or false, hence have\naboutness). Searle saddles functionalism with the\n\u201cblackbox\u201d character of behaviorism, but functionalism\ncares how things are done. Rey sketches \u201ca modest mind\u201d\n\u2013 a CRTT system that has perception, can make deductive and\ninductive inferences, makes decisions on basis of goals and\nrepresentations of how the world is, and can process natural language\nby converting to and from its native representations. To explain the\nbehavior of such a system we would need to use the same attributions\nneeded to explain the behavior of a normal Chinese speaker.\n\nIf we flesh out the Chinese conversation in the context of the Robot\nReply, we may again see evidence that the entity that understands is\nnot the operator inside the room. Suppose we ask the robot system\nChinese translations of \u201cwhat do you see?\u201d, we might get\nthe answer \u201cMy old friend Shakey\u201d, or \u201cI see\nyou!\u201d. Whereas if we phone Searle in the room and ask the same\nquestions in English we might get \u201cThese same four walls\u201d\nor \u201cthese damn endless instruction books and notebooks.\u201d\nAgain this is evidence that we have distinct responders here, an\nEnglish speaker and a Chinese speaker, who see and do quite different\nthings. If the giant robot goes on a rampage and smashes much of\nTokyo, and all the while oblivious Searle is just following the\nprogram in his notebooks in the room, Searle is not guilty of homicide\nand mayhem, because he is not the agent committing the acts. \n\nTim Crane discusses the Chinese Room argument in his 1991 book,\nThe Mechanical Mind. He cites the Churchlands\u2019 luminous\nroom analogy, but then goes on to argue that in the course of\noperating the room, Searle would learn the meaning of the Chinese:\n\u201c\u2026if Searle had not just memorized the rules and the\ndata, but also started acting in the world of Chinese people, then it\nis plausible that he would before too long come to realize what these\nsymbols mean.\u201d(127). (Rapaport 2006 presses an analogy between\nHelen Keller and the Chinese Room.) Crane appears to end with a\nversion of the Robot Reply: \u201cSearle\u2019s argument itself begs\nthe question by (in effect) just denying the central thesis of AI\n\u2013 that thinking is formal symbol manipulation. But\nSearle\u2019s assumption, none the less, seems to me correct \u2026\nthe proper response to Searle\u2019s argument is: sure,\nSearle-in-the-room, or the room alone, cannot understand Chinese. But\nif you let the outside world have some impact on the room, meaning or\n\u2018semantics\u2019 might begin to get a foothold. But of course,\nthis concedes that thinking cannot be simply symbol\nmanipulation.\u201d (129) The idea that learning grounds\nunderstanding has led to work in developmental robotics (a.k.a.\nepigenetic robotics). This AI research area seeks to replicate key\nhuman learning abilities, such as robots that are shown an object from\nseveral angles while being told in natural language the name of the\nobject. \n\nMargaret Boden 1988 also argues that Searle mistakenly supposes\nprograms are pure syntax. But programs bring about the activity of\ncertain machines: \u201cThe inherent procedural consequences of any\ncomputer program give it a toehold in semantics, where the semantics\nin question is not denotational, but causal.\u201d (250) Thus a robot\nmight have causal powers that enable it to refer to a hamburger.\n\nStevan Harnad also finds important our sensory and motor capabilities:\n\u201cWho is to say that the Turing Test, whether conducted in\nChinese or in any other language, could be successfully passed without\noperations that draw on our sensory, motor, and other higher cognitive\ncapacities as well? Where does the capacity to comprehend Chinese\nbegin and the rest of our mental competence leave off?\u201d Harnad\nbelieves that symbolic functions must be grounded in\n\u201crobotic\u201d functions that connect a system with the world.\nAnd he thinks this counts against symbolic accounts of mentality, such\nas Jerry Fodor\u2019s, and, one suspects, the approach of Roger\nSchank that was Searle\u2019s original target. Harnad 2012 (Other\nInternet Resources) argues that the CRA shows that even with a robot\nwith symbols grounded in the external world, there is still something\nmissing: feeling, such as the feeling of understanding.\n\nHowever Ziemke 2016 argues a robotic embodiment with layered systems\nof bodily regulation may ground emotion and meaning, and Seligman 2019\nargues that \u201cperceptually grounded\u201d approaches to natural\nlanguage processing (NLP) have the \u201cpotential to display\nintentionality, and thus after all to foster a truly meaningful\nsemantics that, in the view of Searle and other skeptics, is\nintrinsically beyond computers\u2019 capacity.\u201d \n4.3 The Brain Simulator Reply\n\nConsider a computer that operates in quite a different manner than the\nusual AI program with scripts and operations on sentence-like strings\nof symbols. The Brain Simulator reply asks us to suppose instead the\nprogram simulates the actual sequence of nerve firings that occur in\nthe brain of a native Chinese language speaker when that person\nunderstands Chinese \u2013 every nerve, every firing. Since the\ncomputer then works the very same way as the brain of a native Chinese\nspeaker, processing information in just the same way, it will\nunderstand Chinese. Paul and Patricia Churchland have set out a reply\nalong these lines, discussed below. \n\nIn response to this, Searle argues that it makes no difference. He\nsuggests a variation on the brain simulator scenario: suppose that in\nthe room the man has a huge set of valves and water pipes, in the same\narrangement as the neurons in a native Chinese speaker\u2019s brain.\nThe program now tells the man which valves to open in response to\ninput. Searle claims that it is obvious that there would be no\nunderstanding of Chinese. (Note however that the basis for this claim\nis no longer simply that Searle himself wouldn\u2019t understand\nChinese \u2013 it seems clear that now he is just facilitating the\ncausal operation of the system and so we rely on our Leibnizian\nintuition that water-works don\u2019t understand (see also Maudlin\n1989).) Searle concludes that a simulation of brain activity is not\nthe real thing. \n\nHowever, following Pylyshyn 1980, Cole and Foelber 1984, Chalmers\n1996, we might wonder about hybrid systems. Pylyshyn writes: \n\nIf more and more of the cells in your brain were to be replaced by\nintegrated circuit chips, programmed in such a way as to keep the\ninput-output function each unit identical to that of the unit being\nreplaced, you would in all likelihood just keep right on speaking\nexactly as you are doing now except that you would eventually stop\nmeaning anything by it. What we outside observers might take to be\nwords would become for you just certain noises that circuits caused\nyou to make.\n\n\nThese cyborgization thought experiments can be linked to the Chinese\nRoom. Suppose Otto has a neural disease that causes one of the neurons\nin my brain to fail, but surgeons install a tiny remotely controlled\nartificial neuron, a synron, along side his disabled neuron. The\ncontrol of Otto\u2019s neuron is by John Searle in the Chinese Room,\nunbeknownst to both Searle and Otto. Tiny wires connect the artificial\nneuron to the synapses on the cell-body of his disabled neuron. When\nhis artificial neuron is stimulated by neurons that synapse on his\ndisabled neuron, a light goes on in the Chinese Room. Searle then\nmanipulates some valves and switches in accord with a program. That,\nvia the radio link, causes Otto\u2019s artificial neuron to release\nneuro-transmitters from its tiny artificial vesicles. If\nSearle\u2019s programmed activity causes Otto\u2019s artificial\nneuron to behave just as his disabled natural neuron once did, the\nbehavior of the rest of his nervous system will be unchanged. Alas,\nOtto\u2019s disease progresses; more neurons are replaced by synrons\ncontrolled by Searle. Ex hypothesi the rest of the world will not\nnotice the difference; will Otto?  If so, when? And why?\n\nUnder the rubric \u201cThe Combination Reply\u201d, Searle also\nconsiders a system with the features of all three of the preceding: a\nrobot with a digital brain simulating computer in its cranium, such\nthat the system as a whole behaves indistinguishably from a human.\nSince the normal input to the brain is from sense organs, it is\nnatural to suppose that most advocates of the Brain Simulator Reply\nhave in mind such a combination of brain simulation, Robot, and\nSystems Reply. Some (e.g. Rey 1986) argue it is reasonable to\nattribute intentionality to such a system as a whole. Searle agrees\nthat it would indeed be reasonable to attribute understanding to such\nan android system \u2013 but only as long as you don\u2019t know how\nit works. As soon as you know the truth \u2013 it is a computer,\nuncomprehendingly manipulating symbols on the basis of syntax, not\nmeaning \u2013 you would cease to attribute intentionality to it.\n\n(One assumes this would be true even if it were one\u2019s spouse,\nwith whom one had built a life-long relationship, that was revealed to\nhide a silicon secret. Science fiction stories, including episodes of\nRod Serling\u2019s television series The Twilight Zone, have\nbeen based on such possibilities (the face of the beloved peels away\nto reveal the awful android truth); however, Steven Pinker (1997)\nmentions one episode in which the android\u2019s secret was known\nfrom the start, but the protagonist developed a romantic relationship\nwith the android.)\n\nOn its tenth anniversary the Chinese Room argument was featured in the\ngeneral science periodical Scientific American. Leading the\nopposition to Searle\u2019s lead article in that issue were\nphilosophers Paul and Patricia Churchland. The Churchlands agree with\nSearle that the Chinese Room does not understand Chinese, but hold\nthat the argument itself exploits our ignorance of cognitive and\nsemantic phenomena. They raise a parallel case of \u201cThe Luminous\nRoom\u201d where someone waves a magnet and argues that the absence\nof resulting visible light shows that Maxwell\u2019s electromagnetic\ntheory is false. The Churchlands advocate a view of the brain as a\nconnectionist system, a vector transformer, not a system manipulating\nsymbols according to structure-sensitive rules. The system in the\nChinese Room uses the wrong computational strategies. Thus they agree\nwith Searle against traditional AI, but they presumably would endorse\nwhat Searle calls \u201cthe Brain Simulator Reply\u201d, arguing\nthat, as with the Luminous Room, our intuitions fail us when\nconsidering such a complex system, and it is a fallacy to move from\npart to whole: \u201c\u2026 no neuron in my brain understands\nEnglish, although my whole brain does.\u201d\n\nIn his 1991 book, Microcognition. Andy Clark holds that\nSearle is right that a computer running Schank\u2019s program does\nnot know anything about restaurants, \u201cat least if by\n\u2018know\u2019 we mean anything like\n\u2018understand\u2019\u201d. But Searle thinks that this would\napply to any computational model, while Clark, like the Churchlands,\nholds that Searle is wrong about connectionist models. Clark\u2019s\ninterest is thus in the brain-simulator reply. The brain thinks in\nvirtue of its physical properties. What physical properties of the\nbrain are important? Clark answers that what is important about brains\nare \u201cvariable and flexible substructures\u201d which\nconventional AI systems lack. But that doesn\u2019t mean\ncomputationalism or functionalism is false. It depends on what level\nyou take the functional units to be. Clark defends\n\u201cmicrofunctionalism\u201d \u2013 one should look to a\nfine-grained functional description, e.g. neural net level. Clark\ncites William Lycan approvingly contra Block\u2019s absent qualia\nobjection \u2013 yes, there can be absent qualia, if the functional\nunits are made large. But that does not constitute a refutation of\nfunctionalism generally. So Clark\u2019s views are not unlike the\nChurchlands\u2019, conceding that Searle is right about Schank and\nsymbolic-level processing systems, but holding that he is mistaken\nabout connectionist systems.\n\nSimilarly Ray Kurzweil (2002) argues that Searle\u2019s argument\ncould be turned around to show that human brains cannot understand\n\u2013 the brain succeeds by manipulating neurotransmitter\nconcentrations and other mechanisms that are in themselves\nmeaningless. In criticism of Searle\u2019s response to the Brain\nSimulator Reply, Kurzweil says: \u201cSo if we scale up\nSearle\u2019s Chinese Room to be the rather massive\n\u2018room\u2019 it needs to be, who\u2019s to say that the entire\nsystem of a hundred trillion people simulating a Chinese Brain that\nknows Chinese isn\u2019t conscious? Certainly, it would be correct to\nsay that such a system knows Chinese. And we can\u2019t say that it\nis not conscious anymore than we can say that about any other process.\nWe can\u2019t know the subjective experience of another\nentity\u2026.\u201d \n4.4 The Other Minds Reply\n\nRelated to the preceding is The Other Minds Reply: \u201cHow do you\nknow that other people understand Chinese or anything else? Only by\ntheir behavior. Now the computer can pass the behavioral tests as well\nas they can (in principle), so if you are going to attribute cognition\nto other people you must in principle also attribute it to\ncomputers.\u201d\n\nSearle\u2019s (1980) reply to this is very short:\n\nThe problem in this discussion is not about how I know that other\npeople have cognitive states, but rather what it is that I am\nattributing to them when I attribute cognitive states to them. The\nthrust of the argument is that it couldn\u2019t be just computational\nprocesses and their output because the computational processes and\ntheir output can exist without the cognitive state. It is no answer to\nthis argument to feign anesthesia. In \u2018cognitive sciences\u2019\none presupposes the reality and knowability of the mental in the same\nway that in physical sciences one has to presuppose the reality and\nknowability of physical objects.\n\n\nCritics hold that if the evidence we have that humans understand is\nthe same as the evidence we might have that a visiting\nextra-terrestrial alien understands, which is the same as the evidence\nthat a robot understands, the presuppositions we may make in the case\nof our own species are not relevant, for presuppositions are sometimes\nfalse. For similar reasons, Turing, in proposing the Turing Test, is\nspecifically worried about our presuppositions and chauvinism. If the\nreasons for the presuppositions regarding humans are pragmatic, in\nthat they enable us to predict the behavior of humans and to interact\neffectively with them, perhaps the presupposition could apply equally\nto computers (similar considerations are pressed by Dennett, in his\ndiscussions of what he calls the Intentional Stance).\n\nSearle raises the question of just what we are attributing in\nattributing understanding to other minds, saying that it is more than\ncomplex behavioral dispositions. For Searle the additional seems to be\ncertain states of consciousness, as is seen in his 2010 summary of the\nCRA conclusions. Terry Horgan (2013) endorses this claim: \u201cthe\nreal moral of Searle\u2019s Chinese room thought experiment is that\ngenuine original intentionality requires the presence of internal\nstates with intrinsic phenomenal character that is inherently\nintentional\u2026\u201d But this tying of understanding to\nphenomenal consciousness raises a host of issues. \n\nWe attribute limited understanding of language to toddlers, dogs, and\nother animals, but it is not clear that we are ipso facto attributing\nunseen states of subjective consciousness \u2013 what do we know of\nthe hidden states of exotic creatures? Ludwig Wittgenstein (the\nPrivate Language Argument) and his followers pressed similar points.\nAltered qualia possibilities, analogous to the inverted spectrum,\narise: suppose I ask \u201cwhat\u2019s the sum of 5 and 7\u201d and\nyou respond \u201cthe sum of 5 and 7 is 12\u201d, but as you heard\nmy question you had the conscious experience of hearing and\nunderstanding \u201cwhat is the sum of 10 and 14\u201d, though you\nwere in the computational states appropriate for producing the correct\nsum and so said \u201c12\u201d. Are there certain conscious states\nthat are \u201ccorrect\u201d for certain functional states?\nWittgenstein\u2019s considerations appear to be that the subjective\nstate is irrelevant, at best epiphenomenal, if a language user\ndisplays appropriate linguistic behavior. Afterall, we are taught\nlanguage on the basis of our overt responses, not our qualia. The\nmathematical savant Daniel Tammet reports that when he generates the\ndecimal expansion of pi to thousands of digits he experiences colors\nthat reveal the next digit, but even here it may be that\nTennant\u2019s performance is likely not produced by the colors he\nexperiences, but rather by unconscious neural computation. The\npossible importance of subjective states is further considered in the\nsection on Intentionality, below. \n\nIn the 30 years since the CRA there has been philosophical interest in\nzombies \u2013 creatures that look like and behave just as normal\nhumans, including linguistic behavior, yet have no subjective\nconsciousness. A difficulty for claiming that subjective states of\nconsciousness are crucial for understanding meaning will arise in\nthese cases of absent qualia: we can\u2019t tell the difference\nbetween zombies and non-zombies, and so on Searle\u2019s account we\ncan\u2019t tell the difference between those that really understand\nEnglish and those that don\u2019t. And if you and I can\u2019t tell\nthe difference between those who understand language and Zombies who\nbehave like they do but don\u2019t really, than neither can any\nselection factor in the history of human evolution \u2013 to\npredators, prey, and mates, zombies and true understanders, with the\n\u201cright\u201d conscious experience, have been indistinguishable.\nBut then there appears to be a distinction without a difference. In\nany case, Searle\u2019s short reply to the Other Minds Reply may be\ntoo short. \n\nDescartes famously argued that speech was sufficient for attributing\nminds and consciousness to others, and infamously argued that it was\nnecessary. Turing was in effect endorsing Descartes\u2019 sufficiency\ncondition, at least for intelligence, while substituting written for\noral linguistic behavior. Since most of us use dialog as a sufficient\ncondition for attributing understanding, Searle\u2019s argument,\nwhich holds that speech is a sufficient condition for attributing\nunderstanding to humans but not for anything that doesn\u2019t share\nour biology, an account would appear to be required of what\nadditionally is being attributed, and what can justify the additional\nattribution. Further, if being con-specific is key on Searle\u2019s\naccount, a natural question arises as to what circumstances would\njustify us in attributing understanding (or consciousness) to\nextra-terrestrial aliens who do not share our biology? Offending\nET\u2019s by withholding attributions of understanding until after\ndoing a post-mortem may be risky.\n\nHans Moravec, director of the Robotics laboratory at Carnegie Mellon\nUniversity, and author of Robot: Mere Machine to Transcendent\nMind, argues that Searle\u2019s position merely reflects\nintuitions from traditional philosophy of mind that are out of step\nwith the new cognitive science. Moravec endorses a version of the\nOther Minds reply. It makes sense to attribute intentionality to\nmachines for the same reasons it makes sense to attribute them to\nhumans; his \u201cinterpretative position\u201d is similar to the\nviews of Daniel Dennett. Moravec goes on to note that one of the\nthings we attribute to others is the ability to make attributions of\nintentionality, and then we make such attributions to ourselves. It is\nsuch self-representation that is at the heart of consciousness. These\ncapacities appear to be implementation independent, and hence possible\nfor aliens and suitably programmed computers.\n\nAs we have seen, the reason that Searle thinks we can disregard the\nevidence in the case of robots and computers is that we know that\ntheir processing is syntactic, and this fact trumps all other\nconsiderations. Indeed, Searle believes this is the larger point that\nthe Chinese Room merely illustrates. This larger point is addressed in\nthe Syntax and Semantics section below.\n4.5 The Intuition Reply\n\nMany responses to the Chinese Room argument have noted that, as with\nLeibniz\u2019 Mill, the argument appears to be based on intuition:\nthe intuition that a computer (or the man in the room) cannot think or\nhave understanding. For example, Ned Block (1980) in his original BBS\ncommentary says \u201cSearle\u2019s argument depends for its force\non intuitions that certain entities do not think.\u201d But, Block\nargues, (1) intuitions sometimes can and should be trumped and (2)\nperhaps we need to bring our concept of understanding in line with a\nreality in which certain computer robots belong to the same natural\nkind as humans. Similarly Margaret Boden (1988) points out that we\ncan\u2019t trust our untutored intuitions about how mind depends on\nmatter; developments in science may change our intuitions. Indeed,\nelimination of bias in our intuitions was precisely what motivated\nTuring (1950) to propose the Turing Test, a test that was blind to the\nphysical character of the system replying to questions. Some of\nSearle\u2019s critics in effect argue that he has merely pushed the\nreliance on intuition back, into the room.\n\nFor example, one can hold that despite Searle\u2019s intuition that\nhe would not understand Chinese while in the room, perhaps he is\nmistaken and does, albeit unconsciously. Hauser (2002) accuses Searle\nof Cartesian bias in his inference from \u201cit seems to me quite\nobvious that I understand nothing\u201d to the conclusion that I\nreally understand nothing. Normally, if one understands English or\nChinese, one knows that one does \u2013 but not necessarily. Searle\nlacks the normal introspective awareness of understanding \u2013 but\nthis, while abnormal, is not conclusive. \n\nCritics of the CRA note that our intuitions about intelligence,\nunderstanding and meaning may all be unreliable. With regard to\nmeaning, Wakefield 2003, following Block 1998, defends what Wakefield\ncalls \u201cthe essentialist objection\u201d to the CRA, namely that\na computational account of meaning is not analysis of ordinary\nconcepts and their related intuitions. Rather we are building a\nscientific theory of meaning that may require revising our intuitions.\nAs a theory, it gets its evidence from its explanatory power, not its\naccord with pre-theoretic intuitions (however Wakefield himself argues\nthat computational accounts of meaning are afflicted by a pernicious\nindeterminacy (pp. 308ff)). \n\nOther critics focusing on the role of intuitions in the CRA argue that\nour intuitions regarding both intelligence and understanding may also\nbe unreliable, and perhaps incompatible even with current science.\nWith regard to understanding, Steven Pinker, in How the Mind\nWorks (1997), holds that \u201c\u2026 Searle is merely\nexploring facts about the English word understand\u2026.\nPeople are reluctant to use the word unless certain stereotypical\nconditions apply\u2026\u201d But, Pinker claims, nothing\nscientifically speaking is at stake. Pinker objects to Searle\u2019s\nappeal to the \u201ccausal powers of the brain\u201d by noting that\nthe apparent locus of the causal powers is the \u201cpatterns of\ninterconnectivity that carry out the right information\nprocessing\u201d. Pinker ends his discussion by citing a science\nfiction story in which Aliens, anatomically quite unlike humans,\ncannot believe that humans think when they discover that our heads are\nfilled with meat. The Aliens\u2019 intuitions are unreliable \u2013\npresumably ours may be so as well.\n\nClearly the CRA turns on what is required to understand language.\nSchank 1978 clarifies his claim about what he thinks his programs can\ndo: \u201cBy \u2018understand\u2019, we mean SAM [one of his\nprograms] can create a linked causal chain of conceptualizations that\nrepresent what took place in each story.\u201d This is a nuanced\nunderstanding of \u201cunderstanding\u201d, whereas the Chinese Room\nthought experiment does not turn on a technical understanding of\n\u201cunderstanding\u201d, but rather intuitions about our ordinary\ncompetence when we understand a word like \u201chamburger\u201d.\nIndeed by 2015 Schank distances himself from weak senses of\n\u201cunderstand\u201d, holding that no computer can\n\u201cunderstand when you tell it something\u201d, and that\nIBM\u2019s WATSON \u201cdoesn\u2019t know what it is saying\u201d.\nSchank\u2019s program may get links right, but arguably does not know\nwhat the linked entities are. Whether it does or not depends on what\nconcepts are, see section 5.1. Furthermore it is possible that when it\ncomes to attributing understanding of language we have different\nstandards for different things \u2013 more relaxed for dogs and\ntoddlers. Some things understand a language \u201cun poco\u201d.\nSearle (1980)concedes that there are degrees of understanding, but\nsays that all that matters that there are clear cases of no\nunderstanding, and AI programs are an example: \u201cThe computer\nunderstanding is not just (like my understanding of German) partial or\nincomplete; it is zero.\u201d \n\nSome defenders of AI are also concerned with how our understanding of\nunderstanding bears on the Chinese Room argument. In their paper\n\u201cA Chinese Room that Understands\u201d AI researchers Simon and\nEisenstadt (2002) argue that whereas Searle refutes \u201clogical\nstrong AI\u201d, the thesis that a program that passes the Turing\nTest will necessarily understand, Searle\u2019s argument\ndoes not impugn \u201cEmpirical Strong AI\u201d \u2013 the thesis\nthat it is possible to program a computer that convincingly satisfies\nordinary criteria of understanding. They hold however that it is\nimpossible to settle these questions \u201cwithout employing a\ndefinition of the term \u2018understand\u2019 that can provide a\ntest for judging whether the hypothesis is true or false\u201d. They\ncite W.V.O. Quine\u2019s Word and Object as showing that\nthere is always empirical uncertainty in attributing understanding to\nhumans. The Chinese Room is a Clever Hans trick (Clever Hans was a\nhorse who appeared to clomp out the answers to simple arithmetic\nquestions, but it was discovered that Hans could detect unconscious\ncues from his trainer). Similarly, the man in the room doesn\u2019t\nunderstand Chinese, and could be exposed by watching him closely.\n(Simon and Eisenstadt do not explain just how this would be done, or\nhow it would affect the argument.) Citing the work of Rudolf Carnap,\nSimon and Eisenstadt argue that to understand is not just to exhibit\ncertain behavior, but to use \u201cintensions\u201d that determine\nextensions, and that one can see in actual programs that they do use\nappropriate intensions. They discuss three actual AI programs, and\ndefend various attributions of mentality to them, including\nunderstanding, and conclude that computers understand; they learn\n\u201cintensions by associating words and other linguistic structure\nwith their denotations, as detected through sensory stimuli\u201d.\nAnd since we can see exactly how the machines work, \u201cit is, in\nfact, easier to establish that a machine exhibits understanding that\nto establish that a human exhibits understanding\u2026.\u201d Thus,\nthey conclude, the evidence for empirical strong AI is\noverwhelming.\n\nSimilarly, Daniel Dennett in his original 1980 response to\nSearle\u2019s argument called it \u201can intuition pump\u201d, a\nterm he came up with in discussing the CRA with Hofstader. Sharvy 1983\nechoes the complaint. Dennett\u2019s considered view (2013) is that\nthe CRA is \u201cclearly a fallacious and misleading argument\n\u2026.\u201d (p. 320). Paul Thagard (2013) proposes that for every\nthought experiment in philosophy there is an equal and opposite\nthought experiment. Thagard holds that intuitions are unreliable, and\nthe CRA is an example (and that in fact the CRA has now been refuted\nby the technology of autonomous robotic cars). Dennett has elaborated\non concerns about our intuitions regarding intelligence. Dennett 1987\n(\u201cFast Thinking\u201d) expressed concerns about the slow speed\nat which the Chinese Room would operate, and he has been joined by\nseveral other commentators, including Tim Maudlin, David Chalmers, and\nSteven Pinker. The operator of the Chinese Room may eventually produce\nappropriate answers to Chinese questions. But slow thinkers are\nstupid, not intelligent \u2013 and in the wild, they may well end up\ndead. Dennett argues that \u201cspeed \u2026 is \u2018of the\nessence\u2019 for intelligence. If you can\u2019t figure out the\nrelevant portions of the changing environment fast enough to fend for\nyourself, you are not practically intelligent, however complex you\nare\u201d (326). Thus Dennett relativizes intelligence to processing\nspeed relative to current environment. \n\nTim Maudlin (1989) disagrees. Maudlin considers the time-scale problem\npointed to by other writers, and concludes, contra Dennett, that the\nextreme slowness of a computational system does not violate any\nnecessary conditions on thinking or consciousness. Furthermore,\nSearle\u2019s main claim is about understanding, not intelligence or\nbeing quick-witted. If we were to encounter extra-terrestrials that\ncould process information a thousand times more quickly than we do, it\nseems that would show nothing about our own slow-poke ability to\nunderstand the languages we speak.\n\nSteven Pinker (1997) also holds that Searle relies on untutored\nintuitions. Pinker endorses the Churchlands\u2019 (1990)\ncounterexample of an analogous thought experiment of waving a magnet\nand not generating light, noting that this outcome would not disprove\nMaxwell\u2019s theory that light consists of electromagnetic waves.\nPinker holds that the key issue is speed: \u201cThe thought\nexperiment slows down the waves to a range to which we humans no\nlonger see them as light. By trusting our intuitions in the thought\nexperiment, we falsely conclude that rapid waves cannot be light\neither. Similarly, Searle has slowed down the mental computations to a\nrange in which we humans no longer think of it as understanding (since\nunderstanding is ordinarily much faster)\u201d (94\u201395). Howard\nGardiner, a supporter of Searle\u2019s conclusions regarding the\nroom, makes a similar point about understanding. Gardiner addresses\nthe Chinese Room argument in his book The Mind\u2019s New\nScience (1985, 171\u2013177). Gardiner considers all the\nstandard replies to the Chinese Room argument and concludes that\nSearle is correct about the room: \u201c\u2026the word understand\nhas been unduly stretched in the case of the Chinese room\n\u2026.\u201d (175).\n\nThus several in this group of critics argue that speed affects our\nwillingness to attribute intelligence and understanding to a slow\nsystem, such as that in the Chinese Room. The result may simply be\nthat our intuitions regarding the Chinese Room are unreliable, and\nthus the man in the room, in implementing the program, may understand\nChinese despite intuitions to the contrary (Maudlin and Pinker). Or it\nmay be that the slowness marks a crucial difference between the\nsimulation in the room and what a fast computer does, such that the\nman is not intelligent while the computer system is (Dennett).\n5. The Larger Philosophical Issues\n5.1 Syntax and Semantics\n\nSearle believes the Chinese Room argument supports a larger point,\nwhich explains the failure of the Chinese Room to produce\nunderstanding. Searle argued that programs implemented by computers\nare just syntactical. Computer operations are \u201cformal\u201d in\nthat they respond only to the physical form of the strings of symbols,\nnot to the meaning of the symbols. Minds on the other hand have states\nwith meaning, mental contents. We associate meanings with the words or\nsigns in language. We respond to signs because of their meaning, not\njust their physical appearance. In short, we understand. But, and\naccording to Searle this is the key point, \u201cSyntax is not by\nitself sufficient for, nor constitutive of, semantics.\u201d So\nalthough computers may be able to manipulate syntax to produce\nappropriate responses to natural language input, they do not\nunderstand the sentences they receive or output, for they cannot\nassociate meanings with the words.\n\nSearle (1984) presents a three premise argument that because syntax is\nnot sufficient for semantics, programs cannot produce minds.\n\nPrograms are purely formal (syntactic).\nHuman minds have mental contents (semantics).\nSyntax by itself is neither constitutive of, nor sufficient for,\nsemantic content.\nTherefore, programs by themselves are not constitutive of nor\nsufficient for minds.\n\n\nThe Chinese Room thought experiment itself is the support for the\nthird premise. The claim that syntactic manipulation is not sufficient\nfor meaning or thought is a significant issue, with wider implications\nthan AI, or attributions of understanding. Prominent theories of mind\nhold that human cognition generally is computational. In one form, it\nis held that thought involves operations on symbols in virtue of their\nphysical properties. On an alternative connectionist account, the\ncomputations are on \u201csubsymbolic\u201d states. If Searle is\nright, not only Strong AI but also these main approaches to\nunderstanding human cognition are misguided.\n\nAs we have seen, Searle holds that the Chinese Room scenario shows\nthat one cannot get semantics from syntax alone. In a symbolic logic\nsystem, a kind of artificial language, rules are given for syntax. A\nsemantics, if any, comes later. The logician specifies the basic\nsymbol set and some rules for manipulating strings to produce new\nones. These rules are purely syntactic \u2013 they are applied to\nstrings of symbols solely in virtue of their syntax or form. A\nsemantics, if any, for the symbol system must be provided separately.\nAnd if one wishes to show that interesting additional relationships\nhold between the syntactic operations and semantics, such as that the\nsymbol manipulations preserve truth, one must provide sometimes\ncomplex meta-proofs to show this. So on the face of it, semantics is\nquite independent of syntax for artificial languages, and one cannot\nget semantics from syntax alone. \u201cFormal symbols by themselves\ncan never be enough for mental contents, because the symbols, by\ndefinition, have no meaning (or interpretation, or semantics) except\ninsofar as someone outside the system gives it to them\u201d (Searle\n1989, 45).\n\nSearle\u2019s identification of meaning with interpretation in this\npassage is important. Searle\u2019s point is clearly true of the\ncausally inert formal systems of logicians. A semantic interpretation\nhas to be given to those symbols by a logician. When we move from\nformal systems to computational systems, the situation is more\ncomplex. As many of Searle\u2019s critics (e.g. Cole 1984, Dennett\n1987, Boden 1988, and Chalmers 1996) have noted, a computer running a\nprogram is not the same as \u201csyntax alone\u201d. A computer is\nan enormously complex electronic causal system. State changes in the\nsystem are physical. One can interpret the physical states,\ne.g. voltages, as syntactic 1\u2019s and 0\u2019s, but the intrinsic\nreality is electronic and the syntax is \u201cderived\u201d, a\nproduct of interpretation. The states are syntactically specified by\nprogrammers, but when implemented in a running machine they are\nelectronic states of a complex causal system embedded in the real\nworld. This is quite different from the abstract formal systems that\nlogicians study. Dennett notes that no \u201ccomputer program by\nitself\u201d (Searle\u2019s language) \u2013 e.g. a program lying\non a shelf \u2013 can cause anything, even simple addition, let alone\nmental states. The program must be running. Chalmers (1996) offers a\nparody in which it is reasoned that recipes are syntactic, syntax is\nnot sufficient for crumbliness, cakes are crumbly, so implementation\nof a recipe is not sufficient for making a cake. Implementation makes\nall the difference; an abstract entity (recipe, program) determines\nthe causal powers of a physical system embedded in the larger causal\nnexus of the world. \n\nDennett (1987) sums up the issue: \u201cSearle\u2019s view, then,\ncomes to this: take a material object (any material object) that does\nnot have the power of causing mental phenomena; you cannot turn it in\nto an object that does have the power of producing mental phenomena\nsimply by programming it \u2013 reorganizing the conditional\ndependencies of transitions between its states.\u201d Dennett\u2019s\nview is the opposite: programming \u201cis precisely what could give\nsomething a mind\u201d. But Dennett claims that in fact it is\n\u201cempirically unlikely that the right sorts of programs can be\nrun on anything but organic, human brains\u201d (325\u20136).\n\nA further related complication is that it is not clear that computers\nperform syntactic operations in quite the same sense that a human does\n\u2013 it is not clear that a computer understands syntax or\nsyntactic operations. A computer does not know that it is manipulating\n1\u2019s and 0\u2019s. A computer does not recognize that its binary\ndata strings have a certain form, and thus that certain syntactic\nrules may be applied to them, unlike the man inside the Chinese Room.\nInside a computer, there is nothing that literally reads input data,\nor that \u201cknows\u201d what symbols are. Instead, there are\nmillions of transistors that change states. A sequence of voltages\ncauses operations to be performed. We humans may choose to interpret\nthese voltages as binary numerals and the voltage changes as syntactic\noperations, but a computer does not interpret its operations as\nsyntactic or any other way. So perhaps a computer does not need to\nmake the move from syntax to semantics that Searle objects to; it\nneeds to move from complex causal connections to semantics.\nFurthermore, perhaps any causal system is describable as\nperforming syntactic operations \u2013 if we interpret a light square\nas logical \u201c0\u201d and a dark square as logical\n\u201c1\u201d, then a kitchen toaster may be described as a\ndevice that rewrites logical \u201c0\u201ds as logical\n\u201c1\u201ds. But there is no philosophical problem about getting\nfrom syntax to breakfast. \n\nIn the 1990s, Searle began to use considerations related to these to\nargue that computational views are not just false, but lack a clear\nsense. Computation, or syntax, is \u201cobserver-relative\u201d, not\nan intrinsic feature of reality: \u201c\u2026you can assign a\ncomputational interpretation to anything\u201d (Searle 2002b, p. 17),\neven the molecules in the paint on the wall. Since nothing is\nintrinsically computational, one cannot have a scientific theory that\nreduces the mental, which is not observer-relative, to computation,\nwhich is. \u201cComputation exists only relative to some agent or\nobserver who imposes a computational interpretation on some\nphenomenon. This is an obvious point. I should have seen it ten years\nago, but I did not.\u201d (Searle 2002b, p.17, originally published\n1993).\n\nCritics note that walls are not computers; unlike a wall, a computer\ngoes through state-transitions that are counterfactually described by\na program (Chalmers 1996, Block 2002, Haugeland 2002). In his 2002\npaper, Block addresses the question of whether a wall is a computer\n(in reply to Searle\u2019s charge that anything that maps onto a\nformal system is a formal system, whereas minds are quite different).\nBlock denies that whether or not something is a computer depends\nentirely on our interpretation. Block notes that Searle ignores the\ncounterfactuals that must be true of an implementing system. Haugeland\n(2002) makes the similar point that an implementation will be a causal\nprocess that reliably carries out the operations \u2013 and they must\nbe the right causal powers. Block concludes that Searle\u2019s\narguments fail, but he concedes that they \u201cdo succeed in\nsharpening our understanding of the nature of intentionality and its\nrelation to computation and representation\u201d (78).\n\nRey (2002) also addresses Searle\u2019s arguments that syntax and\nsymbols are observer-relative properties, not physical. Searle infers\nthis from the fact that syntactic properties (e.g. being a logical\n\u201c1\u201d)are not defined in physics; however Rey holds that it\ndoes not follow that they are observer-relative. Rey argues that\nSearle also misunderstands what it is to realize a program. Rey\nendorses Chalmers\u2019 reply to Putnam: a realization is not just a\nstructural mapping, but involves causation, supporting\ncounterfactuals. \u201cThis point is missed so often, it bears\nrepeating: the syntactically specifiable objects over which\ncomputations are defined can and standardly do possess a semantics;\nit\u2019s just that the semantics is not involved in the\nspecification.\u201d States of a person have their semantics in\nvirtue of computational organization and their causal relations to the\nworld. Rey concludes: Searle \u201csimply does not consider the\nsubstantial resources of functionalism and Strong AI.\u201d (222) A\nplausibly detailed story would defuse negative conclusions drawn from\nthe superficial sketch of the system in the Chinese Room.\n\nJohn Haugeland (2002) argues that there is a sense in which a\nprocessor must intrinsically understand the commands in the programs\nit runs: it executes them in accord with the specifications.\n\u201cThe only way that we can make sense of a computer as executing\na program is by understanding its processor as responding to the\nprogram prescriptions as meaningful\u201d (385). Thus operation\nsymbols have meaning to a system. Haugeland goes on to draw a\ndistinction between narrow and wide system. He argues that data can\nhave semantics in the wide system that includes representations of\nexternal objects produced by transducers. In passing, Haugeland makes\nthe unusual claim, argued for elsewhere, that genuine intelligence and\nsemantics presuppose \u201cthe capacity for a kind of commitment in\nhow one lives\u201d which is non-propositional \u2013 that is, love\n(cp. Steven Spielberg\u2019s 2001 film Artificial Intelligence:\nAI). \n\nTo Searle\u2019s claim that syntax is observer-relative, that the\nmolecules in a wall might be interpreted as implementing the Wordstar\nprogram (an early word processing program) because \u201cthere is\nsome pattern in the molecule movements which is isomorphic with the\nformal structure of Wordstar\u201d (Searle 1990b, p. 27), Haugeland\ncounters that \u201cthe very idea of a complex syntactical token\n\u2026 presupposes specified processes of writing and\nreading\u2026.\u201d The tokens must be systematically producible\nand retrievable. So no random isomorphism or pattern somewhere (e.g.\non some wall) is going to count, and hence syntax is not\nobserver-relative.\n\nWith regard to the question of whether one can get semantics from\nsyntax, William Rapaport has for many years argued for\n\u201csyntactic semantics\u201d, a view in which understanding is a\nspecial form of syntactic structure in which symbols (such as Chinese\nwords) are linked to concepts, themselves represented syntactically.\nOthers believe we are not there yet. AI futurist (The Age of\nSpiritual Machines) Ray Kurzweil holds in a 2002 follow-up book\nthat it is red herring to focus on traditional symbol-manipulating\ncomputers. Kurzweil agrees with Searle that existent computers do not\nunderstand language \u2013 as evidenced by the fact that they\ncan\u2019t engage in convincing dialog. But that failure does not\nbear on the capacity of future computers based on different\ntechnology. Kurzweil claims that Searle fails to understand that\nfuture machines will use \u201cchaotic emergent methods that are\nmassively parallel\u201d. This claim appears to be similar to that of\nconnectionists, such as Andy Clark, and the position taken by the\nChurchlands in their 1990 Scientific American article.\n\nApart from Haugeland\u2019s claim that processors understand program\ninstructions, Searle\u2019s critics can agree that computers no more\nunderstand syntax than they understand semantics, although, like all\ncausal engines, a computer has syntactic descriptions. And while it is\noften useful to programmers to treat the machine as if it performed\nsyntactic operations, it is not always so: sometimes the characters\nprogrammers use are just switches that make the machine do something,\nfor example, make a given pixel on the computer display turn red, or\nmake a car transmission shift gears. Thus it is not clear that Searle\nis correct when he says a digital computer is just \u201ca device\nwhich manipulates symbols\u201d. Computers are complex causal\nengines, and syntactic descriptions are useful in order to structure\nthe causal interconnections in the machine. AI programmers face many\ntough problems, but one can hold that they do not have to get\nsemantics from syntax. If they are to get semantics, they must get it\nfrom causality.\n\nTwo main approaches have developed that explain meaning in terms of\ncausal connections. The internalist approaches, such as Schank\u2019s\nand Rapaport\u2019s conceptual representation approaches, and also\nConceptual Role Semantics, hold that a state of a physical system gets\nits semantics from causal connections to other states of the same\nsystem. Thus a state of a computer might represent \u201ckiwi\u201d\nbecause it is connected to \u201cbird\u201d and\n\u201cflightless\u201d nodes, and perhaps also to images of\nprototypical kiwis. The state that represents the property of being\n\u201cflightless\u201d might get its content from a\nNegation-operator modifying a representation of \u201ccapable of\nairborne self-propulsion\u201d, and so forth, to form a vast\nconnected conceptual network, a kind of mental dictionary.\n\nExternalist\n approaches developed by Dennis Stampe, Fred Dretske, Hilary Putnam,\nJerry Fodor, Ruth Millikan, and others, hold that states of a physical\nsystem get their content through causal connections to the external\nreality they represent. Thus, roughly, a system with a KIWI concept is\none that has a state it uses to represent the presence of kiwis in the\nexternal environment. This kiwi-representing state can be any state\nthat is appropriately causally connected to the presence of kiwis.\nDepending on the system, the kiwi representing state could be a state\nof a brain, or of an electrical device such as a computer, or even of\na hydraulic system. The internal representing state can then in turn\nplay a causal role in the determining the behavior of the system. For\nexample, Rey (1986) endorses an indicator semantics along the lines of\nthe work of Dennis Stampe (1977) and Fodor\u2019s\nPsychosemantics. These semantic theories that locate content\nor meaning in appropriate causal relations to the world fit well with\nthe Robot Reply. A computer in a robot body might have just the causal\nconnections that could allow its inner syntactic states to have the\nsemantic property of representing states of things in its\nenvironment.\n\nThus there are at least two families of theories (and marriages of the\ntwo, as in Block 1986) about how semantics might depend upon causal\nconnections. Both of these attempt to provide accounts that are\nsubstance neutral: states of suitably organized causal systems can\nhave content, no matter what the systems are made of. On these\ntheories a computer could have states that have meaning. It is not\nnecessary that the computer be aware of its own states and know that\nthey have meaning, nor that any outsider appreciate the meaning of the\nstates. On either of these accounts meaning depends upon the (possibly\ncomplex) causal connections, and digital computers are systems\ndesigned to have states that have just such complex causal\ndependencies. It should be noted that Searle does not subscribe to\nthese theories of semantics. Instead, Searle\u2019s discussions of\nlinguistic meaning have often centered on the notion of\nintentionality.\n5.2 Intentionality\n\nIntentionality\n is the property of being about something, having content. In the 19th\nCentury, psychologist Franz Brentano re-introduced this term from\nMedieval philosophy and held that intentionality was the \u201cmark\nof the mental\u201d. Beliefs and desires are intentional states: they\nhave propositional content (one believes that p, one desires\nthat p, where sentences that represent propositions substitute\nfor \u201cp\u201d). Searle\u2019s views regarding\nintentionality are complex; of relevance here is that he makes a\ndistinction between the original or intrinsic intentionality of\ngenuine mental states, and the derived intentionality of language. A\nwritten or spoken sentence only has derivative intentionality insofar\nas it is interpreted by someone. It appears that on Searle\u2019s\nview, original intentionality can at least potentially be conscious.\nSearle then argues that the distinction between original and derived\nintentionality applies to computers. We can interpret the states of a\ncomputer as having content, but the states themselves do not have\noriginal intentionality. Many philosophers endorse this intentionality\ndualism, including Sayre (1986) and even Fodor (2009), despite\nFodor\u2019s many differences with Searle.\n\nIn a section of her 1988 book, Computer Models of the Mind,\nMargaret Boden notes that intentionality is not well-understood\n\u2013 reason to not put too much weight on arguments that turn on\nintentionality. Furthermore, insofar as we understand the brain, we\nfocus on informational functions, not unspecified causal powers of the\nbrain: \u201c\u2026from the psychological point of view, it is not\nthe biochemistry as such which matters but the information-bearing\nfunctions grounded in it.\u201d (241) Searle sees intentionality as a\ncausal power of the brain, uniquely produced by biological processes.\nDale Jacquette 1989 argues against a reduction of intentionality\n\u2013 intentionality, he says, is an \u201cineliminable,\nirreducible primitive concept.\u201d However most AI sympathizers\nhave seen intentionality, aboutness, as bound up with information, and\nnon-biological states can bear information as well as can brain\nstates. Hence many responders to Searle have argued that he displays\nsubstance chauvinism, in holding that brains understand but systems\nmade of silicon with comparable information processing capabilities\ncannot, even in principle. Papers on both sides of the issue appeared,\nsuch as J. Maloney\u2019s 1987 paper \u201cThe Right Stuff\u201d,\ndefending Searle, and R. Sharvy\u2019s 1983 critique, \u201cIt\nAin\u2019t the Meat, it\u2019s the Motion\u201d. AI proponents such\nas Kurzweil (1999, see also Richards 2002) have continued to hold that\nAI systems can potentially have such mental properties as\nunderstanding, intelligence, consciousness and intentionality, and\nwill exceed human abilities in these areas.\n\nOther critics of Searle\u2019s position take intentionality more\nseriously than Boden does, but deny his dualistic distinction between\noriginal and derived intentionality. Dennett (1987, e.g.) argues that\nall intentionality is derived, in that attributions of intentionality\n\u2013 to animals, other people, and even ourselves \u2013 are\ninstrumental and allow us to predict behavior, but they are not\ndescriptions of intrinsic properties. As we have seen, Dennett is\nconcerned about the slow speed of things in the Chinese Room, but he\nargues that once a system is working up to speed, it has all that is\nneeded for intelligence and derived intentionality \u2013 and derived\nintentionality is the only kind that there is, according to Dennett. A\nmachine can be an intentional system because intentional explanations\nwork in predicting the machine\u2019s behavior. Dennett also suggests\nthat Searle conflates intentionality with awareness of intentionality.\nIn his syntax-semantic arguments, \u201cSearle has apparently\nconfused a claim about the underivability of semantics from syntax\nwith a claim about the underivability of the consciousness of\nsemantics from syntax\u201d (336). The emphasis on consciousness\nforces us to think about things from a first-person point of view, but\nDennett 2017 continues to press the claim that this is a fundamental\nmistake if we want to understand the mental. \n\nWe might also worry that Searle conflates meaning and interpretation,\nand that Searle\u2019s original or underived intentionality is just\nsecond-order intentionality, a representation of what an intentional\nobject represents or means. Dretske and others have seen\nintentionality as information-based. One state of the world, including\na state in a computer, may carry information about other states in the\nworld, and this informational aboutness is a mind-independent feature\nof states. Hence it is a mistake to hold that conscious attributions\nof meaning are the source of intentionality. \n\nOthers have noted that Searle\u2019s discussion has shown a shift\nover time from issues of intentionality and understanding to issues of\nconsciousness. Searle links intentionality to awareness of\nintentionality, in holding that intentional states are at least\npotentially conscious. In his 1996 book, The Conscious Mind,\nDavid Chalmers notes that although Searle originally directs his\nargument against machine intentionality, it is clear from later\nwritings that the real issue is consciousness, which Searle holds is a\nnecessary condition of intentionality. It is consciousness that is\nlacking in digital computers. Chalmers uses thought experiments to\nargue that it is implausible that one system has some basic mental\nproperty (such as having qualia) that another system lacks, if it is\npossible to imagine transforming one system into the other, either\ngradually (as replacing neurons one at a time by digital circuits), or\nall at once, switching back and forth between flesh and silicon.\n\nA second strategy regarding the attribution of intentionality is taken\nby critics who in effect argue that intentionality is an intrinsic\nfeature of states of physical systems that are causally connected with\nthe world in the right way, independently of interpretation (see the\npreceding Syntax and Semantics section). Fodor\u2019s semantic\nexternalism is influenced by Fred Dretske, but they come to different\nconclusions with regard to the semantics of states of computers. Over\na period of years, Dretske developed an historical account of meaning\nor mental content that would preclude attributing beliefs and\nunderstanding to most machines. Dretske (1985) agrees with Searle that\nadding machines don\u2019t literally add; we do the adding,\nusing the machines. Dretske emphasizes the crucial role of natural\nselection and learning in producing states that have genuine content.\nHuman built systems will be, at best, like Swampmen (beings that\nresult from a lightning strike in a swamp and by chance happen to be a\nmolecule by molecule copy of some human being, say, you) \u2013 they\nappear to have intentionality or mental states, but do not, because\nsuch states require the right history. AI states will generally be\ncounterfeits of real mental states; like counterfeit money, they may\nappear perfectly identical but lack the right pedigree. But\nDretske\u2019s account of belief appears to make it distinct from\nconscious awareness of the belief or intentional state (if that is\ntaken to require a higher order thought), and so would apparently\nallow attribution of intentionality to artificial systems that can get\nthe right history by learning.\n\nHoward Gardiner endorses Zenon Pylyshyn\u2019s criticisms of\nSearle\u2019s view of the relation of brain and intentionality, as\nsupposing that intentionality is somehow a stuff \u201csecreted by\nthe brain\u201d, and Pylyshyn\u2019s own counter-thought experiment\nin which one\u2019s neurons are replaced one by one with integrated\ncircuit workalikes (see also Cole and Foelber (1984) and Chalmers\n(1996) for exploration of neuron replacement scenarios). Gardiner\nholds that Searle owes us a more precise account of intentionality\nthan Searle has given so far, and until then it is an open question\nwhether AI can produce it, or whether it is beyond its scope. Gardiner\nconcludes with the possibility that the dispute between Searle and his\ncritics is not scientific, but (quasi?) religious.\n5.3 Mind and Body\n\nSeveral critics have noted that there are metaphysical issues at stake\nin the original argument. The Systems Reply draws attention to the\nmetaphysical problem of the relation of mind to body. It does this in\nholding that understanding is a property of the system as a whole, not\nthe physical implementer. The Virtual Mind Reply holds that minds or\npersons \u2013 the entities that understand and are conscious \u2013\nare more abstract than any physical system, and that there could be a\nmany-to-one relation between minds and physical systems. (Even if\neverything is physical, in principle a single body could be shared by\nmultiple minds, and a single mind could have a sequence of bodies over\ntime.) Thus larger issues about personal identity and the relation of\nmind and body are in play in the debate between Searle and some of his\ncritics.\n\nSearle\u2019s view is that the problem the relation of mind and body\n\u201chas a rather simple solution. Here it is: Conscious states are\ncaused by lower level neurobiological processes in the brain and are\nthemselves higher level features of the brain\u201d (Searle 2002b, p.\n9). In his early discussion of the CRA, Searle spoke of the causal\npowers of the brain. Thus his view appears to be that brain states\ncause consciousness and understanding, and \u201cconsciousness is\njust a feature of the brain\u201d (ibid). However, as we have seen,\neven if this is true it begs the question of just whose consciousness\na brain creates. Roger Sperry\u2019s split-brain experiments suggest\nthat perhaps there can be two centers of consciousness, and so in that\nsense two minds, implemented by a single brain. While both display at\nleast some language comprehension, only one (typically created by the\nleft hemisphere) controls language production. Thus many current\napproaches to understanding the relation of brain and consciousness\nemphasize connectedness and information flow (see e.g. Dehaene 2014).\n\n\nConsciousness and understanding are features of persons, so it appears\nthat Searle accepts a metaphysics in which I, my conscious self, am\nidentical with my brain \u2013 a form of mind-brain identity theory.\nThis very concrete metaphysics is reflected in Searle\u2019s original\npresentation of the CR argument, in which Strong AI was described by\nhim as the claim that \u201cthe appropriately programmed computer\nreally is a mind\u201d (Searle 1980). This is an identity claim, and\nhas odd consequences. If A and B are identical, any property of A is a\nproperty of B. Computers are physical objects. Some computers weigh 6\nlbs and have stereo speakers. So the claim that Searle called Strong\nAI would entail that some minds weigh 6 lbs and have stereo speakers.\nHowever it seems to be clear that while humans may weigh 150 pounds;\nhuman minds do not weigh 150 pounds. This suggests that neither bodies\nnor machines can literally be minds. Such considerations support the\nview that minds are more abstract that brains, and if so that at least\none version of the claim that Searle calls Strong AI, the version that\nsays that computers literally are minds, is metaphysically untenable\non the face of it, apart from any thought-experiments.\n\nSearle\u2019s CR argument was thus directed against the claim that a\ncomputer is a mind, that a suitably programmed digital computer\nunderstands language, or that its program does. Searle\u2019s thought\nexperiment appeals to our strong intuition that someone who did\nexactly what the computer does would not thereby come to understand\nChinese. As noted above, many critics have held that Searle is quite\nright on this point \u2013 no matter how you program a computer, the\ncomputer will not literally be a mind and the computer will not\nunderstand natural language. But if minds are not physical objects\nthis inability of a computer to be a mind does not show that running\nan AI program cannot produce understanding of natural\nlanguage, by something other than the computer (See section 4.1\nabove.) \n\nFunctionalism\n is a theory of the relation of minds to bodies that was developed in\nthe two decades prior to Searle\u2019s CRA. Functionalism is an\nalternative to the identity theory that is implicit in much of\nSearle\u2019s discussion, as well as to the dominant behaviorism of\nthe mid-Twentieth Century. If functionalism is correct, there appears\nto be no intrinsic reason why a computer couldn\u2019t have mental\nstates. Hence the CRA\u2019s conclusion that a computer is\nintrinsically incapable of mental states is an important consideration\nagainst functionalism. Julian Baggini (2009, 37) writes that Searle\n\u201ccame up with perhaps the most famous counter-example in history\n\u2013 the Chinese room argument \u2013 and in one intellectual\npunch inflicted so much damage on the then dominant theory of\nfunctionalism that many would argue it has never recovered.\u201d\n\nFunctionalists hold that a mental state is what a mental\nstate does \u2013 the causal (or \u201cfunctional\u201d)\nrole that the state plays determines what state it is. A functionalist\nmight hold that pain, for example, is a state that is typically caused\nby damage to the body, is located in a body-image, and is aversive.\nFunctionalists distance themselves both from behaviorists and identity\ntheorists. In contrast with the former, functionalists hold that the\ninternal causal processes are important for the possession of\nmental states. Thus functionalists may agree with Searle in rejecting\nthe Turing Test as too behavioristic. In contrast with identity\ntheorists (who might e.g. hold \u201cpain is identical with C-fiber\nfiring\u201d), functionalists hold that mental states might be had by\na variety of physical systems (or non-physical, as in Cole and Foelber\n1984, in which a mind changes from a material to an immaterial\nimplementation, neuron by neuron). Thus while an identity theorist\nwill identify pain with certain neuron firings, a functionalist will\nidentify pain with something more abstract and higher level, a\nfunctional role that might be had by many different types of\nunderlying system. \n\nFunctionalists accuse identity theorists of substance chauvinism.\nHowever, functionalism remains controversial: functionalism is\nvulnerable to the Chinese Nation type objections discussed above, and\nfunctionalists notoriously have trouble explaining qualia, a problem\nhighlighted by the apparent possibility of an inverted spectrum, where\nqualitatively different states might have the same functional role\n(e.g. Block 1978, Maudlin 1989, Cole 1990).\n\nComputationalism\n is the sub-species of functionalism that holds that the important\ncausal role of brain processes is information processing. Milkowski\n2017 notes that computational approaches have been fruitful in\ncognitive science; he surveys objections to computationalism and\nconcludes that the majority target a strawman version. However Jerry\nFodor, an early proponent of computational approaches, argues in Fodor\n2005 that key mental processes, such as inference to the best\nexplanation, which depend on non-local properties of representations,\ncannot be explained by computational modules in the brain. If Fodor is\nright, understanding language and interpretation appear to involve\nglobal considerations such as linguistic and non-linguistic context\nand theory of mind and so might resist computational explanation. If\nso, we reach Searle\u2019s conclusion on the basis of different\nconsiderations. \n\nSearle\u2019s 2010 statement of the conclusion of the CRA has it\nshowing that computational accounts cannot explain consciousness.\nThere has been considerable interest in the decades since 1980 in\ndetermining what does explain consciousness, and this has been an\nextremely active research area across disciplines. One interest has\nbeen in the neural correlates of consciousness. This bears directly on\nSearle\u2019s claim that consciousness is intrinsically biological\nand not computational or information processing. There is no\ndefinitive answer yet, though some recent work on anesthesia suggests\nthat consciousness is lost when cortical (and cortico-thalamic)\nconnections and information flow are disrupted (e.g.Hudetz 2012, a\nreview article). \n\nIn general, if the basis of consciousness is confirmed to be at the\nrelatively abstract level of information flow through neural networks,\nit will be friendly to functionalism, and if it is turns out to be\nlower and more biological (or sub-neuronal), it will be friendly to\nSearle\u2019s account.\n\nThese controversial biological and metaphysical issues bear on the\ncentral inference in the Chinese Room argument. From the intuition\nthat in the CR thought experiment he would not understand Chinese by\nrunning a program, Searle infers that there is no understanding\ncreated by running a program. Clearly, whether that inference is valid\nor not turns on a metaphysical question about the identity of persons\nand minds. If the person understanding is not identical with the room\noperator, then the inference is unsound.\n5.4 Simulation, duplication and evolution\n\nIn discussing the CRA, Searle argues that there is an important\ndistinction between simulation and duplication. No one would mistake a\ncomputer simulation of the weather for weather, or a computer\nsimulation of digestion for real digestion. Searle concludes that it\nis just as serious a mistake to confuse a computer simulation of\nunderstanding with understanding.\n\nOn the face of it, there is generally an important distinction between\na simulation and the real thing. But two problems emerge. It is not\nclear that the distinction can always be made. Hearts are biological\nif anything is. Are artificial hearts simulations of hearts? Or are\nthey functional duplicates of hearts, hearts made from different\nmaterials? Walking is normally a biological phenomenon performed using\nlimbs. Do those with artificial limbs walk? Or do they simulate\nwalking? Do robots walk? If the properties that are needed to be\ncertain kind of thing are high-level properties, anything sharing\nthose properties will be a thing of that kind, even if it differs in\nits lower level properties. Chalmers (1996) offers a principle\ngoverning when simulation is replication. Chalmers suggests that,\ncontra Searle and Harnad (1989), a simulation of X can be an\nX, namely when the property of being an X is an\norganizational invariant, a property that depends only on the\nfunctional organization of the underlying system, and not on any other\ndetails.\n\nCopeland (2002) argues that the Church-Turing thesis does not entail\nthat the brain (or every machine) can be simulated by a universal\nTuring machine, for the brain (or other machine) might have primitive\noperations that are not simple clerical routines that can be carried\nout by hand. (An example might be that human brains likely display\ngenuine low-level randomness, whereas computers are carefully designed\nnot to do that, and so computers resort to pseudo-random numbers when\napparent randomness is needed.) Sprevak 2007 raises a related point.\nTuring\u2019s 1938 Princeton thesis described such machines\n(\u201cO-machines\u201d). O-machines are machines that include\nfunctions of natural numbers that are not Turing-machine computable.\nIf the brain is such a machine, then, says Sprevak,: \u201cThere is\nno possibility of Searle\u2019s Chinese Room Argument being\nsuccessfully deployed against the functionalist hypothesis that the\nbrain instantiates an O-machine\u2026.\u201d (120). \n\nCopeland discusses the simulation / duplication distinction in\nconnection with the Brain Simulator Reply. He argues that Searle\ncorrectly notes that one cannot infer from X simulates\nY, and Y has property P, to the conclusion\nthat therefore X has Y\u2019s property P\nfor arbitrary P. But Copeland claims that Searle himself\ncommits the simulation fallacy in extending the CR argument from\ntraditional AI to apply against computationalism. The contrapositive\nof the inference is logically equivalent \u2013 X simulates\nY, X does not have P therefore Y\ndoes not \u2013 where P is understands Chinese. The faulty\nstep is: the CR operator S simulates a neural net N,\nit is not the case that S understands Chinese, therefore it\nis not the case that N understands Chinese. Copeland also\nnotes results by Siegelmann and Sontag (1994) showing that some\nconnectionist networks cannot be simulated by a universal Turing\nMachine (in particular, where connection weights are real\nnumbers).\n\nThere is another problem with the simulation-duplication distinction,\narising from the process of evolution. Searle wishes to see original\nintentionality and genuine understanding as properties only of certain\nbiological systems, presumably the product of evolution. Computers\nmerely simulate these properties. At the same time, in the Chinese\nRoom scenario, Searle maintains that a system can exhibit behavior\njust as complex as human behavior, simulating any degree of\nintelligence and language comprehension that one can imagine, and\nsimulating any ability to deal with the world, yet not understand a\nthing. He also says that such behaviorally complex systems might be\nimplemented with very ordinary materials, for example with tubes of\nwater and valves.\n\nThis creates a biological problem, beyond the Other Minds problem\nnoted by early critics of the CR argument. While we may\npresuppose that others have minds, evolution makes no such\npresuppositions. The selection forces that drive biological evolution\nselect on the basis of behavior. Evolution can select for the ability\nto use information about the environment creatively and intelligently,\nas long as this is manifest in the behavior of the organism. If there\nis no overt difference in behavior in any set of circumstances between\na system that understands and one that does not, evolution cannot\nselect for genuine understanding. And so it seems that on\nSearle\u2019s account, minds that genuinely understand meaning have\nno advantage over creatures that merely process information, using\npurely computational processes. Thus a position that implies that\nsimulations of understanding can be just as biologically adaptive as\nthe real thing, leaves us with a puzzle about how and why systems with\n\u201cgenuine\u201d understanding could evolve. Original\nintentionality and genuine understanding become epiphenomenal.\nConclusion\n\nAs we have seen, since its appearance in 1980 the Chinese Room\nargument has sparked discussion across disciplines. Despite the\nextensive discussion there is still no consensus as to whether the\nargument is sound. At one end we have Julian Baggini\u2019s (2009)\nassessment that Searle \u201ccame up with perhaps the most famous\ncounter-example in history \u2013 the Chinese room argument \u2013\nand in one intellectual punch inflicted so much damage on the then\ndominant theory of functionalism that many would argue it has never\nrecovered.\u201d Whereas philosopher Daniel Dennett (2013, p. 320)\nconcludes that the Chinese Room argument is \u201cclearly a\nfallacious and misleading argument\u201d. Hence there is no consensus\nas to whether the argument is a proof that limits the aspirations of\nArtificial Intelligence or computational accounts of mind. \n\nMeanwhile work in artificial intelligence and natural language\nprocessing has continued. The CRA led Stevan Harnad and others on a\nquest for \u201csymbol grounding\u201d in AI. Many in philosophy\n(Dretske, Fodor, Millikan) worked on naturalistic theories of mental\ncontent. Speculation about the nature of consciousness continues in\nmany disciplines. And computers have moved from the lab to the pocket\nand the wrist.\n\nAt the time of Searle\u2019s construction of the argument, personal\ncomputers were very limited hobbyist devices. Weizenbaum\u2019s\n\u2018Eliza\u2019 and a few text \u2018adventure\u2019 games were\nplayed on DEC computers; these included limited parsers. More advanced\nparsing of language was limited to computer researchers such as\nSchank. Much changed in the next quarter century; billions now use\nnatural language to interrogate and command virtual agents via\ncomputers they carry in their pockets. Has the Chinese Room argument\nmoderated claims by those who produce AI and natural language systems?\nSome manufacturers linking devices to the \u201cinternet of\nthings\u201d make modest claims: appliance manufacturer LG says the\nsecond decade of the 21st century brings the \u201cexperience of\nconversing\u201d with major appliances. That may or may not be the\nsame as conversing. Apple is less cautious than LG in describing the\ncapabilities of its \u201cvirtual personal assistant\u201d\napplication called \u2018Siri\u2019: Apple says of Siri that\n\u201cIt understands what you say. It knows what you mean.\u201d IBM\nis quick to claim its much larger \u2018Watson\u2019 system is\nsuperior in language abilities to Siri. In 2011 Watson beat human\nchampions on the television game show \u2018Jeopardy\u2019, a feat\nthat relies heavily on language abilities and inference. IBM goes on\nto claim that what distinguishes Watson is that it \u201cknows what\nit knows, and knows what it does not know.\u201d This appears to be\nclaiming a form of reflexive self-awareness or consciousness for the\nWatson computer system. Thus the claims of strong AI now are hardly\nchastened, and if anything some are stronger and more exuberant. At\nthe same time, as we have seen, many others believe that the Chinese\nRoom Argument showed once and for all that at best computers can\nsimulate human cognition.\n\nThough separated by three centuries, Leibniz and Searle had similar\nintuitions about the systems they consider in their respective thought\nexperiments, Leibniz\u2019 Mill and the Chinese Room. In both cases\nthey consider a complex system composed of relatively simple\noperations, and note that it is impossible to see how understanding or\nconsciousness could result. These simple arguments do us the service\nof highlighting the serious problems we face in understanding meaning\nand minds. The many issues raised by the Chinese Room argument may not\nbe settled until there is a consensus about the nature of meaning, its\nrelation to syntax, and about the biological basis of consciousness.\nThere continues to be significant disagreement about what processes\ncreate meaning, understanding, and consciousness, as well as what can\nbe proven a priori by thought experiments.\n", "bibliography": {"categories": [], "cat_ref_text": {"ref_list": ["Apple Inc., 2014,\n \u2018<a href=\"http://www.apple.com/ios/siri/\" target=\"other\">IOS 7 Siri</a>\u2019,\n accessed 1/10/2014. ", "Baggini, J., 2009, \u2018Painting the bigger picture\u2019,\n<em>The Philosopher\u2019s Magazine</em>, 8: 37\u201339.", "Block, N., 1978, \u2018Troubles with Functionalism\u2019, in C.\nW. Savage (ed.), <em>Perception and Cognition: Issues in the\nFoundations of Psychology</em>, Minneapolis: University of Minnesota\nPress. (Reprinted in many anthologies on philosophy of mind and\npsychology.)", "\u2013\u2013\u2013, 1986, \u2018Advertisement for a Semantics\nfor Psychology\u2019, <em>Midwest Studies in Philosophy</em> (Volume\nX), P.A. French, <em>et al</em>. (eds.), Minneapolis: University of\nMinnesota Press, 615\u2013678.", "\u2013\u2013\u2013, 2002, \u2018Searle\u2019s Arguments\nAgainst Cognitive Science\u2019, in Preston and Bishop (eds.)\n2002.", "Boden, M., 1988, <em>Computer Models of the Mind</em>, Cambridge:\nCambridge University Press; pp. 238\u2013251 were excerpted and\npublished as \u2018Escaping from the Chinese Room\u2019, in <em>The\nPhilosophy of Artificial Intelligence</em>, ed M. A. Boden, New York:\nOxford University Press, 1990.", "Cam, P., 1990, \u2018Searle on Strong AI\u2019, <em>Australasian\nJournal of Philosophy</em>, 68: 103\u20138.", "Chalmers, D., 1992, \u2018Subsymbolic Computation and the Chinese\nRoom\u2019, in J. Dinsmore (ed.), <em>The Symbolic and Connectionist\nParadigms: Closing the Gap</em>, Hillsdale, NJ: Lawrence Erlbaum.", "\u2013\u2013\u2013, 1996, <em>The Conscious Mind</em>, Oxford:\nOxford University Press.", "\u2013\u2013\u2013, 1996a, \u201cDoes a Rock Implement Every\nFinite-State Automaton\u2019, <em>Synthese</em> 108:\n309\u201333.", "\u2013\u2013\u2013, 1996b, \u2018Minds, machines, and\nmathematics\u2019, <em>Psyche</em>, 2: 11\u201320.", "Churchland, P., 1985, \u2018Reductionism, Qualia, and the Direct\nIntrospection of Brain States\u2019, <em>The Journal of\nPhilosophy</em>, LXXXII: 8\u201328.", "Churchland, P. and Churchland, P., 1990, \u2018Could a machine\nthink?\u2019, <em>Scientific American</em>, 262(1): 32\u201337.", "Clark, A., 1991, <em>Microcognition: Philosophy, Cognitive\nScience, and Parallel Distributed Processing</em>, Cambridge, MA: MIT\nPress.", "Cole, D., 1984, \u2018Thought and Thought Experiments\u2019,\n<em>Philosophical Studies</em>, 45: 431\u201344.", "\u2013\u2013\u2013, 1990, \u2018Functionalism and Inverted\nSpectra\u2019, <em>Synthese</em>, 82: 202\u2013222.", "\u2013\u2013\u2013, 1991a, \u2018Artificial Intelligence and\nPersonal Identity\u2019, <em>Synthese</em>, 88: 399\u2013417.", "\u2013\u2013\u2013, 1991b, \u2018Artificial Minds: Cam on\nSearle\u2019, <em>Australasian Journal of Philosophy</em>, 69:\n329\u201333.", "\u2013\u2013\u2013, 1994, \u2018The Causal Powers of\nCPUs\u2019, in E. Dietrich (ed.), <em>Thinking Computers and Virtual\nPersons</em>, New York: Academic Press", "Cole, D. and Foelber, R., 1984, Contingent Materialism\u2019,\n<em>Pacific Philosophical Quarterly</em>, 65(1): 74\u201385.", "Copeland, J., 2002, \u2018The Chinese Room from a Logical Point\nof View\u2019, in Preston and Bishop (eds.) 2002, 104\u2013122.", "Crane, Tim., 1996, <em>The Mechanical Mind</em>: <em>A\nPhilosophical Introduction to Minds, Machines and Mental\nRepresentation</em>, London: Penguin.", "Davis, Lawrence, 2001, \u2018Functionalism, the Brain, and\nPersonal Identity\u2019, <em>Philosophical Studies</em>, 102(3):\n259\u2013279.", "Dehaene, S., 2014, <em>Consciousness and the Brain: Deciphering\nHow the Brain Codes Our Thoughts</em>, New York: Viking Penquin.", "Dennett, D., 1978, \u2018Toward a Cognitive Theory of\nConsciousness\u2019, in <em>Brainstorms: Philosophical Essays on Mind\nand Psychology</em>, Cambridge, MA: MIT Press.", "\u2013\u2013\u2013, 1981, \u2018Where am I?\u2019 in\n<em>Brainstorms: Philosophical Essays on Mind and Psychology</em>,\nCambridge, MA: MIT Press, pp. 310\u2013323.", "\u2013\u2013\u2013, 1987, \u2018Fast Thinking\u2019, in\n<em>The Intentional Stance</em>, Cambridge, MA: MIT Press,\n324\u2013337.", "\u2013\u2013\u2013, 1997, \u2018Consciousness in Humans and\nRobot Minds,\u2019 in M. Ito, Y. Miyashita and E.T. Rolls (eds.),\n<em>Cognition, computation, and consciousness</em>, New York: Oxford\nUniversity Press, pp. 17\u201329.", "\u2013\u2013\u2013, 2013, <em>Intuition Pumps and Other Tools\nfor Thinking</em>, New York: W.W. Norton and Co.", "Dneprov, A., 1961, \u2018\u0418\u0433\u0440\u0430\u2019\n(\u2018The Game\u2019),\n<em>\u0417\u043d\u0430\u043d\u0438\u0435-\u0441\u0438\u043b\u0430</em>\n(<em>Knowledge is Power</em>), 5: 39\u201342; for a link to the\ntranslation, see Mickevich 1961, Other Internet Resources.", "Double, R., 1983, \u2018Searle, Programs and\nFunctionalism\u2019, <em>Nature and System</em>, 5:\n107\u201314.", "Dretske, F. 1985, \u2018Presidential Address\u2019 (Central\nDivision Meetings of the American Philosophical Association),\n<em>Proceedings and Addresses of the American Philosophical\nAssociation</em>, 59(1): 23\u201333.", "Dreyfus, H. 1965, \u2018Alchemy and Artificial\nIntelligence\u2019, Boston, MA: Rand Corporation. ", "\u2013\u2013\u2013, 1972, <em>What Computers Can\u2019t\nDo</em>, New York: Harper &amp; Row. ", "Fodor, J., 1987, <em>Psychosemantics</em>, Cambridge, MA: MIT\nPress.", "\u2013\u2013\u2013, 1991, \u2018Yin and Yang in the Chinese\nRoom\u2019, in D. Rosenthal (ed.), <em>The Nature of Mind</em>, New\nYork: Oxford University Press.", "\u2013\u2013\u2013, 1992, <em>A Theory of Content and other\nessays</em>, Cambridge, MA: MIT Press.", "\u2013\u2013\u2013, 2009, \u2018Where is my Mind?\u2019,\n<em>London Review of Books</em>, (31)3: 13\u201315. ", "Ford, J., 2010, \u2018Helen Keller was never in a Chinese\nRoom\u2019, <em>Minds and Machines</em>, VOLUME: PAGES. ", "Gardiner, H., 1987, <em>The Mind\u2019s New Science: A History of\nthe Cognitive Revolution</em>, New York: Basic Books.", "Hanley, R., 1997, <em>The Metaphysics of Star Trek</em>, New York:\nBasic Books.", "Harnad, S., 1989, \u2018Minds, Machines and Searle\u2019,\n<em>Journal of Experimental and Theoretical Artificial\nIntelligence</em>, 1: 5\u201325.", "\u2013\u2013\u2013, 2002, \u2018Minds, Machines, and Searle2:\nWhat\u2019s Right and Wrong about the Chinese Room Argument\u2019,\nin Preston and Bishop (eds.) 2002, 294\u2013307.", "Haugeland, J., 2002, \u2018Syntax, Semantics, Physics\u2019, in\nPreston and Bishop (eds.) 2002, 379\u2013392.", "Hauser, L., 1997, \u2018Searle\u2019s Chinese Box: Debunking the\nChinese Room Argument\u2019, <em>Minds and Machines</em>, 7:\n199\u2013226.", "\u2013\u2013\u2013, 2002, \u2018Nixin\u2019 Goes to\nChina\u2019, in Preston and Bishop (eds.) 2002, 123\u2013143.", "Hayes, P., Harnad, S., Perlis, D. &amp; Block, N., 1992,\n\u2018Virtual Symposium on Virtual Mind\u2019, <em>Minds and\nMachines</em>, 2(3): 217\u2013238.", "Hofstadter, D., 1981, \u2018Reflections on Searle\u2019, in\nHofstadter and Dennett (eds.), <em>The Mind\u2019s I</em>, New York:\nBasic Books, pp. 373\u2013382.", "Horgan, T., 2013, \u2018Original Intentionality is Phenomenal\nIntentionality\u2019, <em>The Monist</em> 96: 232\u2013251.", "Hudetz, A., 2012, \u2018General Anesthesia and Human Brain\nConnectivity\u2019, <em>Brain Connect</em>, 2(6): 291\u2013302.\n", "Jackson, F., 1986, \u2018What Mary Didn\u2019t Know\u2019,\n<em>Journal of Philosophy</em>, LXXXIII: 291\u20135.", "Kaernbach, C., 2005, \u2018No Virtual Mind in the Chinese\nRoom\u2019, <em>Journal of Consciousness Studies</em>, 12(11):\n31\u201342. ", "Kim, J., 2010, <em>The Philosophy of Mind</em>, (3rd edition),\nBoulder, CO: Westview Press.", "Kurzweil, R., 2000, <em>The Age of Spiritual Machines: When\nComputers Exceed Human Intelligence</em>, New York: Penguin.", "\u2013\u2013\u2013, 2002, \u2018Locked in his Chinese\nRoom\u2019, in Richards 2002, 128\u2013171.", "Maloney, J., 1987, \u2018The Right Stuff\u2019,\n<em>Synthese</em>, 70: 349\u201372.", "Maudlin, T., 1989, \u2018Computation and Consciousness\u2019,\n<em>Journal of Philosophy</em>, LXXXVI: 407\u2013432.", "Milkowski, M. 2017, \u2018Why think that the brain is not a\ncomputer?\u2019, <em>APA Newsletter on Philosophy and Computers</em>,\n16(2), 22\u201328.", "Millikan, R., 1984, <em>Language, Thought, and other Biological\nCategories</em>, Cambridge, MA: MIT Press.", "Moravec, H., 1999, <em>Robot: Mere Machine to Transcendent\nMind</em>, New York: Oxford University Press.", "Nute, D., 2011, \u2018A Logical Hole the Chinese Room\nAvoids\u2019, <em>Minds and Machines</em>, 21: 431\u20133; this is a\nreply to Shaffer 2009.", "Penrose, R., 2002, \u2018Consciousness, Computation, and the\nChinese Room\u2019 in Preston and Bishop (eds.) 2002,\n226\u2013249.", "Pinker, S., 1997, <em>How the Mind Works</em>, New York:\nNorton.", "Preston, J. and M. Bishop (eds.), 2002, <em>Views into the Chinese\nRoom: New Essays on Searle and Artificial Intelligence</em>, New York:\nOxford University Press.", "Pylyshyn, Z., 1980, Reply to Searle,<em>Behavioral and Brain\nSciences</em>, 3. ", "Rapaport, W., 1984, \u2018Searle\u2019s Experiments with\nThought\u2019, <em>Philosophy of Science</em>, 53: 271\u20139.", "\u2013\u2013\u2013 2006, \u2018How Helen Keller Used Syntactic\nSemantics to Escape from a Chinese Room\u2019, <em>Minds and\nMachines</em>, 16(4): 381\u2013436.", "Rey, G., 1986, \u2018What\u2019s Really Going on in\nSearle\u2019s \u201cChinese Room\u201d\u2009\u2019, <em>Philosophical\nStudies</em>, 50: 169\u201385.", "\u2013\u2013\u2013, 2002, \u2018Searle\u2019s\nMisunderstandings of Functionalism and Strong AI\u2019, in Preston\nand Bishop (eds.) 2002, 201\u2013225.", "Richards, J. W. (ed.), 2002, <em>Are We Spiritual Machines: Ray\nKurzweil vs. the Critics of Strong AI</em>, Seattle: Discovery\nInstitute.", "Rosenthal, D. (ed), 1991, <em>The Nature of Mind</em>, Oxford and\nNY: Oxford University Press.", "Schank, R., 2015, \u2018Machines that Think are in the\nMovies\u2019, in Brockman, J. (ed.), <em>What to Think About Machines\nthat Think</em>, New York: Harper Collins", "Schank, R. and Abelson, R., 1977, <em>Scripts, Plans, Goals, and\nUnderstanding</em>, Hillsdale, NJ: Lawrence Erlbaum.", "Schank, R. and P. Childers, 1985, <em>The Cognitive Computer: On\nLanguage, Learning, and Artificial Intelligence</em>, New York:\nAddison-Wesley.", "Schweizer, P., 2012, \u2018The Externalist Foundations of a Truly\nTotal Turing Test\u2019, <em>Minds and Machines</em>, 22:\n191\u2013212.", "Searle, J., 1980, \u2018Minds, Brains and Programs\u2019,\n<em>Behavioral and Brain Sciences</em>, 3: 417\u201357\n [<a href=\"http://cogprints.org/7150/1/10.1.1.83.5248.pdf\" target=\"other\">Preprint available online</a>]", "\u2013\u2013\u2013, 1984, <em>Minds, Brains and Science</em>,\nCambridge, MA: Harvard University Press.", "\u2013\u2013\u2013, 1989, \u2018Artificial Intelligence and\nthe Chinese Room: An Exchange\u2019, <em>New York Review of\nBooks</em>, 36: 2 (February 16, 1989). ", "\u2013\u2013\u2013, 1990a, \u2018Is the Brain\u2019s Mind a\nComputer Program?\u2019, <em>Scientific American</em>, 262(1):\n26\u201331.", "\u2013\u2013\u2013, 1990b, \u2018Presidential Address\u2019,\n<em>Proceedings and Addresses of the American Philosophical\nAssociation</em>, 64: 21\u201337.", "\u2013\u2013\u2013, 1998, \u2018Do We Understand\nConsciousness?\u2019 (Interview with Walter Freeman), <em>Journal of\nConsciousness Studies</em>, 6: 5\u20136.", "\u2013\u2013\u2013, 1999, \u2018The Chinese Room\u2019, in\nR.A. Wilson and F. Keil (eds.), <em>The MIT Encyclopedia of the\nCognitive Sciences</em>, Cambridge, MA: MIT Press.", "\u2013\u2013\u2013, 2002a, \u2018Twenty-one Years in the\nChinese Room\u2019, in Preston and Bishop (eds.) 2002,\n51\u201369.", "\u2013\u2013\u2013, 2002b, \u2018The Problem of\nConsciousness\u2019, in <em>Consciousness and Language</em>,\nCambridge: Cambridge University Press, 7\u201317.", "\u2013\u2013\u2013, 2004, <em>Mind: a Brief Introduction</em>,\nOxford: Oxford University Press.", "\u2013\u2013\u2013, 2010, \u2018Why Dualism (and Materialism)\nFail to Account for Consciousness\u2019, in Richard E. Lee (ed.),\n<em>Questioning Nineteenth Century Assumptions about Knowledge</em>\n(III: Dualism), New York: SUNY Press. ", "Seligman, M., 2019, \u2018The Evolving Treatment of Semantics in\nMachine Translation\u2019, in M. Ji and M. Oakes (eds.),\n<em>Advances in Empirical Translation Studies: Developing Translation\nResources and Technologies</em>, Cambridge: Cambridge University\nPress.", "Shaffer, M., 2009, \u2018A Logical Hole in the Chinese\nRoom\u2019, <em>Minds and Machines</em>, 19(2): 229\u2013235.", "Sharvy, R., 1983, \u2018It Ain\u2019t the Meat It\u2019s the\nMotion\u2019, <em>Inquiry</em>, 26: 125\u2013134.", "Simon, H. and Eisenstadt, S., 2002, \u2018A Chinese Room that\nUnderstands\u2019, in Preston and Bishop (eds.) 2002,\n95\u2013108.", "Sloman, A. and Croucher, M., 1980, \u2018How to turn an\ninformation processor into an understanding\u2019, <em>Brain and\nBehavioral Sciences</em>, 3: 447\u20138.", "Sprevak, M., 2007, \u2018Chinese Rooms and Program\nPortability\u2019, <em>British Journal for the Philosophy of\nScience</em>, 58(4): 755\u2013776.", "Stampe, Dennis, 1977, \u2018Towards a Causal Theory of Linguistic\nRepresentation\u2019, in P. French, T. Uehling, H. Wettstein, (eds.)\n<em>Contemporary Perspectives in the Philosophy of Language</em>,\n(Midwest Studies in Philosophy, Volume 2), Minneapolis: University of\nMinnesota Press, pp. 42\u201363.", "Thagard, P., 1986, \u2018The Emergence of Meaning: An Escape from\nSearle\u2019s Chinese Room\u2019, <em>Behaviorism</em>, 14:\n139\u201346.", "\u2013\u2013\u2013, 2013, \u2018Thought Experiments Considered\nHarmful\u2018, <em>Perspectives on Science</em>, 21:\n122\u2013139.", "Turing, A., 1948, \u2018Intelligent Machinery: A Report\u2019,\nLondon: National Physical Laboratory.", "\u2013\u2013\u2013, 1950, \u2018Computing Machinery and\nIntelligence\u2019, <em>Mind</em>, 59: 433\u2013460.", "Weiss, T., 1990, \u2018Closing the Chinese Room\u2019,\n<em>Ratio</em>, 3: 165\u201381.", "Ziemke, T., 2016, \u2018The Body of Knowledge: on the role of the\nliving body in grounding embodied cognition\u2019,\n<em>Biosystems</em>, 148: 4\u201311."]}, "raw_text": "<div id=\"bibliography\">\n<h2 id=\"Bib\">Bibliography</h2>\n<ul class=\"hanging\">\n<li>Apple Inc., 2014,\n \u2018<a href=\"http://www.apple.com/ios/siri/\" target=\"other\">IOS 7 Siri</a>\u2019,\n accessed 1/10/2014. </li>\n<li>Baggini, J., 2009, \u2018Painting the bigger picture\u2019,\n<em>The Philosopher\u2019s Magazine</em>, 8: 37\u201339.</li>\n<li>Block, N., 1978, \u2018Troubles with Functionalism\u2019, in C.\nW. Savage (ed.), <em>Perception and Cognition: Issues in the\nFoundations of Psychology</em>, Minneapolis: University of Minnesota\nPress. (Reprinted in many anthologies on philosophy of mind and\npsychology.)</li>\n<li>\u2013\u2013\u2013, 1986, \u2018Advertisement for a Semantics\nfor Psychology\u2019, <em>Midwest Studies in Philosophy</em> (Volume\nX), P.A. French, <em>et al</em>. (eds.), Minneapolis: University of\nMinnesota Press, 615\u2013678.</li>\n<li>\u2013\u2013\u2013, 2002, \u2018Searle\u2019s Arguments\nAgainst Cognitive Science\u2019, in Preston and Bishop (eds.)\n2002.</li>\n<li>Boden, M., 1988, <em>Computer Models of the Mind</em>, Cambridge:\nCambridge University Press; pp. 238\u2013251 were excerpted and\npublished as \u2018Escaping from the Chinese Room\u2019, in <em>The\nPhilosophy of Artificial Intelligence</em>, ed M. A. Boden, New York:\nOxford University Press, 1990.</li>\n<li>Cam, P., 1990, \u2018Searle on Strong AI\u2019, <em>Australasian\nJournal of Philosophy</em>, 68: 103\u20138.</li>\n<li>Chalmers, D., 1992, \u2018Subsymbolic Computation and the Chinese\nRoom\u2019, in J. Dinsmore (ed.), <em>The Symbolic and Connectionist\nParadigms: Closing the Gap</em>, Hillsdale, NJ: Lawrence Erlbaum.</li>\n<li>\u2013\u2013\u2013, 1996, <em>The Conscious Mind</em>, Oxford:\nOxford University Press.</li>\n<li>\u2013\u2013\u2013, 1996a, \u201cDoes a Rock Implement Every\nFinite-State Automaton\u2019, <em>Synthese</em> 108:\n309\u201333.</li>\n<li>\u2013\u2013\u2013, 1996b, \u2018Minds, machines, and\nmathematics\u2019, <em>Psyche</em>, 2: 11\u201320.</li>\n<li>Churchland, P., 1985, \u2018Reductionism, Qualia, and the Direct\nIntrospection of Brain States\u2019, <em>The Journal of\nPhilosophy</em>, LXXXII: 8\u201328.</li>\n<li>Churchland, P. and Churchland, P., 1990, \u2018Could a machine\nthink?\u2019, <em>Scientific American</em>, 262(1): 32\u201337.</li>\n<li>Clark, A., 1991, <em>Microcognition: Philosophy, Cognitive\nScience, and Parallel Distributed Processing</em>, Cambridge, MA: MIT\nPress.</li>\n<li>Cole, D., 1984, \u2018Thought and Thought Experiments\u2019,\n<em>Philosophical Studies</em>, 45: 431\u201344.</li>\n<li>\u2013\u2013\u2013, 1990, \u2018Functionalism and Inverted\nSpectra\u2019, <em>Synthese</em>, 82: 202\u2013222.</li>\n<li>\u2013\u2013\u2013, 1991a, \u2018Artificial Intelligence and\nPersonal Identity\u2019, <em>Synthese</em>, 88: 399\u2013417.</li>\n<li>\u2013\u2013\u2013, 1991b, \u2018Artificial Minds: Cam on\nSearle\u2019, <em>Australasian Journal of Philosophy</em>, 69:\n329\u201333.</li>\n<li>\u2013\u2013\u2013, 1994, \u2018The Causal Powers of\nCPUs\u2019, in E. Dietrich (ed.), <em>Thinking Computers and Virtual\nPersons</em>, New York: Academic Press</li>\n<li>Cole, D. and Foelber, R., 1984, Contingent Materialism\u2019,\n<em>Pacific Philosophical Quarterly</em>, 65(1): 74\u201385.</li>\n<li>Copeland, J., 2002, \u2018The Chinese Room from a Logical Point\nof View\u2019, in Preston and Bishop (eds.) 2002, 104\u2013122.</li>\n<li>Crane, Tim., 1996, <em>The Mechanical Mind</em>: <em>A\nPhilosophical Introduction to Minds, Machines and Mental\nRepresentation</em>, London: Penguin.</li>\n<li>Davis, Lawrence, 2001, \u2018Functionalism, the Brain, and\nPersonal Identity\u2019, <em>Philosophical Studies</em>, 102(3):\n259\u2013279.</li>\n<li>Dehaene, S., 2014, <em>Consciousness and the Brain: Deciphering\nHow the Brain Codes Our Thoughts</em>, New York: Viking Penquin.</li>\n<li>Dennett, D., 1978, \u2018Toward a Cognitive Theory of\nConsciousness\u2019, in <em>Brainstorms: Philosophical Essays on Mind\nand Psychology</em>, Cambridge, MA: MIT Press.</li>\n<li>\u2013\u2013\u2013, 1981, \u2018Where am I?\u2019 in\n<em>Brainstorms: Philosophical Essays on Mind and Psychology</em>,\nCambridge, MA: MIT Press, pp. 310\u2013323.</li>\n<li>\u2013\u2013\u2013, 1987, \u2018Fast Thinking\u2019, in\n<em>The Intentional Stance</em>, Cambridge, MA: MIT Press,\n324\u2013337.</li>\n<li>\u2013\u2013\u2013, 1997, \u2018Consciousness in Humans and\nRobot Minds,\u2019 in M. Ito, Y. Miyashita and E.T. Rolls (eds.),\n<em>Cognition, computation, and consciousness</em>, New York: Oxford\nUniversity Press, pp. 17\u201329.</li>\n<li>\u2013\u2013\u2013, 2013, <em>Intuition Pumps and Other Tools\nfor Thinking</em>, New York: W.W. Norton and Co.</li>\n<li>Dneprov, A., 1961, \u2018\u0418\u0433\u0440\u0430\u2019\n(\u2018The Game\u2019),\n<em>\u0417\u043d\u0430\u043d\u0438\u0435-\u0441\u0438\u043b\u0430</em>\n(<em>Knowledge is Power</em>), 5: 39\u201342; for a link to the\ntranslation, see Mickevich 1961, Other Internet Resources.</li>\n<li>Double, R., 1983, \u2018Searle, Programs and\nFunctionalism\u2019, <em>Nature and System</em>, 5:\n107\u201314.</li>\n<li>Dretske, F. 1985, \u2018Presidential Address\u2019 (Central\nDivision Meetings of the American Philosophical Association),\n<em>Proceedings and Addresses of the American Philosophical\nAssociation</em>, 59(1): 23\u201333.</li>\n<li>Dreyfus, H. 1965, \u2018Alchemy and Artificial\nIntelligence\u2019, Boston, MA: Rand Corporation. </li>\n<li>\u2013\u2013\u2013, 1972, <em>What Computers Can\u2019t\nDo</em>, New York: Harper &amp; Row. </li>\n<li>Fodor, J., 1987, <em>Psychosemantics</em>, Cambridge, MA: MIT\nPress.</li>\n<li>\u2013\u2013\u2013, 1991, \u2018Yin and Yang in the Chinese\nRoom\u2019, in D. Rosenthal (ed.), <em>The Nature of Mind</em>, New\nYork: Oxford University Press.</li>\n<li>\u2013\u2013\u2013, 1992, <em>A Theory of Content and other\nessays</em>, Cambridge, MA: MIT Press.</li>\n<li>\u2013\u2013\u2013, 2009, \u2018Where is my Mind?\u2019,\n<em>London Review of Books</em>, (31)3: 13\u201315. </li>\n<li>Ford, J., 2010, \u2018Helen Keller was never in a Chinese\nRoom\u2019, <em>Minds and Machines</em>, VOLUME: PAGES. </li>\n<li>Gardiner, H., 1987, <em>The Mind\u2019s New Science: A History of\nthe Cognitive Revolution</em>, New York: Basic Books.</li>\n<li>Hanley, R., 1997, <em>The Metaphysics of Star Trek</em>, New York:\nBasic Books.</li>\n<li>Harnad, S., 1989, \u2018Minds, Machines and Searle\u2019,\n<em>Journal of Experimental and Theoretical Artificial\nIntelligence</em>, 1: 5\u201325.</li>\n<li>\u2013\u2013\u2013, 2002, \u2018Minds, Machines, and Searle2:\nWhat\u2019s Right and Wrong about the Chinese Room Argument\u2019,\nin Preston and Bishop (eds.) 2002, 294\u2013307.</li>\n<li>Haugeland, J., 2002, \u2018Syntax, Semantics, Physics\u2019, in\nPreston and Bishop (eds.) 2002, 379\u2013392.</li>\n<li>Hauser, L., 1997, \u2018Searle\u2019s Chinese Box: Debunking the\nChinese Room Argument\u2019, <em>Minds and Machines</em>, 7:\n199\u2013226.</li>\n<li>\u2013\u2013\u2013, 2002, \u2018Nixin\u2019 Goes to\nChina\u2019, in Preston and Bishop (eds.) 2002, 123\u2013143.</li>\n<li>Hayes, P., Harnad, S., Perlis, D. &amp; Block, N., 1992,\n\u2018Virtual Symposium on Virtual Mind\u2019, <em>Minds and\nMachines</em>, 2(3): 217\u2013238.</li>\n<li>Hofstadter, D., 1981, \u2018Reflections on Searle\u2019, in\nHofstadter and Dennett (eds.), <em>The Mind\u2019s I</em>, New York:\nBasic Books, pp. 373\u2013382.</li>\n<li>Horgan, T., 2013, \u2018Original Intentionality is Phenomenal\nIntentionality\u2019, <em>The Monist</em> 96: 232\u2013251.</li>\n<li>Hudetz, A., 2012, \u2018General Anesthesia and Human Brain\nConnectivity\u2019, <em>Brain Connect</em>, 2(6): 291\u2013302.\n</li>\n<li>Jackson, F., 1986, \u2018What Mary Didn\u2019t Know\u2019,\n<em>Journal of Philosophy</em>, LXXXIII: 291\u20135.</li>\n<li>Kaernbach, C., 2005, \u2018No Virtual Mind in the Chinese\nRoom\u2019, <em>Journal of Consciousness Studies</em>, 12(11):\n31\u201342. </li>\n<li>Kim, J., 2010, <em>The Philosophy of Mind</em>, (3rd edition),\nBoulder, CO: Westview Press.</li>\n<li>Kurzweil, R., 2000, <em>The Age of Spiritual Machines: When\nComputers Exceed Human Intelligence</em>, New York: Penguin.</li>\n<li>\u2013\u2013\u2013, 2002, \u2018Locked in his Chinese\nRoom\u2019, in Richards 2002, 128\u2013171.</li>\n<li>Maloney, J., 1987, \u2018The Right Stuff\u2019,\n<em>Synthese</em>, 70: 349\u201372.</li>\n<li>Maudlin, T., 1989, \u2018Computation and Consciousness\u2019,\n<em>Journal of Philosophy</em>, LXXXVI: 407\u2013432.</li>\n<li>Milkowski, M. 2017, \u2018Why think that the brain is not a\ncomputer?\u2019, <em>APA Newsletter on Philosophy and Computers</em>,\n16(2), 22\u201328.</li>\n<li>Millikan, R., 1984, <em>Language, Thought, and other Biological\nCategories</em>, Cambridge, MA: MIT Press.</li>\n<li>Moravec, H., 1999, <em>Robot: Mere Machine to Transcendent\nMind</em>, New York: Oxford University Press.</li>\n<li>Nute, D., 2011, \u2018A Logical Hole the Chinese Room\nAvoids\u2019, <em>Minds and Machines</em>, 21: 431\u20133; this is a\nreply to Shaffer 2009.</li>\n<li>Penrose, R., 2002, \u2018Consciousness, Computation, and the\nChinese Room\u2019 in Preston and Bishop (eds.) 2002,\n226\u2013249.</li>\n<li>Pinker, S., 1997, <em>How the Mind Works</em>, New York:\nNorton.</li>\n<li>Preston, J. and M. Bishop (eds.), 2002, <em>Views into the Chinese\nRoom: New Essays on Searle and Artificial Intelligence</em>, New York:\nOxford University Press.</li>\n<li>Pylyshyn, Z., 1980, Reply to Searle,<em>Behavioral and Brain\nSciences</em>, 3. </li>\n<li>Rapaport, W., 1984, \u2018Searle\u2019s Experiments with\nThought\u2019, <em>Philosophy of Science</em>, 53: 271\u20139.</li>\n<li>\u2013\u2013\u2013 2006, \u2018How Helen Keller Used Syntactic\nSemantics to Escape from a Chinese Room\u2019, <em>Minds and\nMachines</em>, 16(4): 381\u2013436.</li>\n<li>Rey, G., 1986, \u2018What\u2019s Really Going on in\nSearle\u2019s \u201cChinese Room\u201d\u2009\u2019, <em>Philosophical\nStudies</em>, 50: 169\u201385.</li>\n<li>\u2013\u2013\u2013, 2002, \u2018Searle\u2019s\nMisunderstandings of Functionalism and Strong AI\u2019, in Preston\nand Bishop (eds.) 2002, 201\u2013225.</li>\n<li>Richards, J. W. (ed.), 2002, <em>Are We Spiritual Machines: Ray\nKurzweil vs. the Critics of Strong AI</em>, Seattle: Discovery\nInstitute.</li>\n<li>Rosenthal, D. (ed), 1991, <em>The Nature of Mind</em>, Oxford and\nNY: Oxford University Press.</li>\n<li>Schank, R., 2015, \u2018Machines that Think are in the\nMovies\u2019, in Brockman, J. (ed.), <em>What to Think About Machines\nthat Think</em>, New York: Harper Collins</li>\n<li>Schank, R. and Abelson, R., 1977, <em>Scripts, Plans, Goals, and\nUnderstanding</em>, Hillsdale, NJ: Lawrence Erlbaum.</li>\n<li>Schank, R. and P. Childers, 1985, <em>The Cognitive Computer: On\nLanguage, Learning, and Artificial Intelligence</em>, New York:\nAddison-Wesley.</li>\n<li>Schweizer, P., 2012, \u2018The Externalist Foundations of a Truly\nTotal Turing Test\u2019, <em>Minds and Machines</em>, 22:\n191\u2013212.</li>\n<li>Searle, J., 1980, \u2018Minds, Brains and Programs\u2019,\n<em>Behavioral and Brain Sciences</em>, 3: 417\u201357\n [<a href=\"http://cogprints.org/7150/1/10.1.1.83.5248.pdf\" target=\"other\">Preprint available online</a>]</li>\n<li>\u2013\u2013\u2013, 1984, <em>Minds, Brains and Science</em>,\nCambridge, MA: Harvard University Press.</li>\n<li>\u2013\u2013\u2013, 1989, \u2018Artificial Intelligence and\nthe Chinese Room: An Exchange\u2019, <em>New York Review of\nBooks</em>, 36: 2 (February 16, 1989). </li>\n<li>\u2013\u2013\u2013, 1990a, \u2018Is the Brain\u2019s Mind a\nComputer Program?\u2019, <em>Scientific American</em>, 262(1):\n26\u201331.</li>\n<li>\u2013\u2013\u2013, 1990b, \u2018Presidential Address\u2019,\n<em>Proceedings and Addresses of the American Philosophical\nAssociation</em>, 64: 21\u201337.</li>\n<li>\u2013\u2013\u2013, 1998, \u2018Do We Understand\nConsciousness?\u2019 (Interview with Walter Freeman), <em>Journal of\nConsciousness Studies</em>, 6: 5\u20136.</li>\n<li>\u2013\u2013\u2013, 1999, \u2018The Chinese Room\u2019, in\nR.A. Wilson and F. Keil (eds.), <em>The MIT Encyclopedia of the\nCognitive Sciences</em>, Cambridge, MA: MIT Press.</li>\n<li>\u2013\u2013\u2013, 2002a, \u2018Twenty-one Years in the\nChinese Room\u2019, in Preston and Bishop (eds.) 2002,\n51\u201369.</li>\n<li>\u2013\u2013\u2013, 2002b, \u2018The Problem of\nConsciousness\u2019, in <em>Consciousness and Language</em>,\nCambridge: Cambridge University Press, 7\u201317.</li>\n<li>\u2013\u2013\u2013, 2004, <em>Mind: a Brief Introduction</em>,\nOxford: Oxford University Press.</li>\n<li>\u2013\u2013\u2013, 2010, \u2018Why Dualism (and Materialism)\nFail to Account for Consciousness\u2019, in Richard E. Lee (ed.),\n<em>Questioning Nineteenth Century Assumptions about Knowledge</em>\n(III: Dualism), New York: SUNY Press. </li>\n<li>Seligman, M., 2019, \u2018The Evolving Treatment of Semantics in\nMachine Translation\u2019, in M. Ji and M. Oakes (eds.),\n<em>Advances in Empirical Translation Studies: Developing Translation\nResources and Technologies</em>, Cambridge: Cambridge University\nPress.</li>\n<li>Shaffer, M., 2009, \u2018A Logical Hole in the Chinese\nRoom\u2019, <em>Minds and Machines</em>, 19(2): 229\u2013235.</li>\n<li>Sharvy, R., 1983, \u2018It Ain\u2019t the Meat It\u2019s the\nMotion\u2019, <em>Inquiry</em>, 26: 125\u2013134.</li>\n<li>Simon, H. and Eisenstadt, S., 2002, \u2018A Chinese Room that\nUnderstands\u2019, in Preston and Bishop (eds.) 2002,\n95\u2013108.</li>\n<li>Sloman, A. and Croucher, M., 1980, \u2018How to turn an\ninformation processor into an understanding\u2019, <em>Brain and\nBehavioral Sciences</em>, 3: 447\u20138.</li>\n<li>Sprevak, M., 2007, \u2018Chinese Rooms and Program\nPortability\u2019, <em>British Journal for the Philosophy of\nScience</em>, 58(4): 755\u2013776.</li>\n<li>Stampe, Dennis, 1977, \u2018Towards a Causal Theory of Linguistic\nRepresentation\u2019, in P. French, T. Uehling, H. Wettstein, (eds.)\n<em>Contemporary Perspectives in the Philosophy of Language</em>,\n(Midwest Studies in Philosophy, Volume 2), Minneapolis: University of\nMinnesota Press, pp. 42\u201363.</li>\n<li>Thagard, P., 1986, \u2018The Emergence of Meaning: An Escape from\nSearle\u2019s Chinese Room\u2019, <em>Behaviorism</em>, 14:\n139\u201346.</li>\n<li>\u2013\u2013\u2013, 2013, \u2018Thought Experiments Considered\nHarmful\u2018, <em>Perspectives on Science</em>, 21:\n122\u2013139.</li>\n<li>Turing, A., 1948, \u2018Intelligent Machinery: A Report\u2019,\nLondon: National Physical Laboratory.</li>\n<li>\u2013\u2013\u2013, 1950, \u2018Computing Machinery and\nIntelligence\u2019, <em>Mind</em>, 59: 433\u2013460.</li>\n<li>Weiss, T., 1990, \u2018Closing the Chinese Room\u2019,\n<em>Ratio</em>, 3: 165\u201381.</li>\n<li>Ziemke, T., 2016, \u2018The Body of Knowledge: on the role of the\nliving body in grounding embodied cognition\u2019,\n<em>Biosystems</em>, 148: 4\u201311.</li>\n</ul>\n</div>"}, "related_entries": {"entry_list": ["computation: in physical systems", "consciousness: and intentionality", "consciousness: representational theories of", "emergent properties", "epiphenomenalism", "externalism about the mind", "functionalism", "information: biological", "information: semantic conceptions of", "intentionality", "mental content: causal theories of", "mental content: teleological theories of", "mental representation", "mind: computational theory of", "multiple realizability", "neuroscience, philosophy of", "other minds", "thought experiments", "Turing, Alan", "Turing test", "zombies"], "entry_link": [{"../computation-physicalsystems/": "computation: in physical systems"}, {"../consciousness-intentionality/": "consciousness: and intentionality"}, {"../consciousness-representational/": "consciousness: representational theories of"}, {"../properties-emergent/": "emergent properties"}, {"../epiphenomenalism/": "epiphenomenalism"}, {"../content-externalism/": "externalism about the mind"}, {"../functionalism/": "functionalism"}, {"../information-biological/": "information: biological"}, {"../information-semantic/": "information: semantic conceptions of"}, {"../intentionality/": "intentionality"}, {"../content-causal/": "mental content: causal theories of"}, {"../content-teleological/": "mental content: teleological theories of"}, {"../mental-representation/": "mental representation"}, {"../computational-mind/": "mind: computational theory of"}, {"../multiple-realizability/": "multiple realizability"}, {"../neuroscience/": "neuroscience, philosophy of"}, {"../other-minds/": "other minds"}, {"../thought-experiment/": "thought experiments"}, {"../turing/": "Turing, Alan"}, {"../turing-test/": "Turing test"}, {"../zombies/": "zombies"}]}, "academic_tools": {"listed_text": ["<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>", "<a href=\"https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=chinese-room\" target=\"other\">How to cite this entry</a>.", "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>", "<a href=\"https://leibniz.stanford.edu/friends/preview/chinese-room/\" target=\"other\">Preview the PDF version of this entry</a> at the\n <a href=\"https://leibniz.stanford.edu/friends/\" target=\"other\">Friends of the SEP Society</a>.", "<img alt=\"inpho icon\" src=\"../../symbols/inpho.png\"/>", "<a href=\"https://www.inphoproject.org/entity?sep=chinese-room&amp;redirect=True\" target=\"other\">Look up topics and thinkers related to this entry</a>\n at the Internet Philosophy Ontology Project (InPhO).", "<img alt=\"phil papers icon\" src=\"../../symbols/pp.gif\"/>", "<a href=\"https://philpapers.org/sep/chinese-room/\" target=\"other\">Enhanced bibliography for this entry</a>\nat <a href=\"https://philpapers.org/\" target=\"other\">PhilPapers</a>, with links to its database."], "listed_links": [{"https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=chinese-room": "How to cite this entry"}, {"https://leibniz.stanford.edu/friends/preview/chinese-room/": "Preview the PDF version of this entry"}, {"https://leibniz.stanford.edu/friends/": "Friends of the SEP Society"}, {"https://www.inphoproject.org/entity?sep=chinese-room&redirect=True": "Look up topics and thinkers related to this entry"}, {"https://philpapers.org/sep/chinese-room/": "Enhanced bibliography for this entry"}, {"https://philpapers.org/": "PhilPapers"}]}, "other_internet_resources": {"listed_text": ["Harnad, S., 2012,\n \u2018<a href=\"http://eprints.soton.ac.uk/340293/1/harnad-huma-turingessay.pdf\" target=\"other\">Alan Turing and the \u2018Hard\u2019 and \u2018Easy\u2019 Problem of Cognition: Doing and Feeling</a>,\u201d\n <em>Turing100: Essays in Honour of Centenary Turing Year 2012</em>,\navailable online. ", "Mickevich, A., 1961,\n \u2018<a href=\"http://www.hardproblem.ru/en/posts/Events/a-russian-chinese-room-story-antedating-searle-s-1980-discussion/\" target=\"other\">The Game</a>\u2019,\n translation of Dneprov 1961, at Center for Consciousness Studies\n(Philosophy Department, Moscow State University).", "Searle, J.,\n <a href=\"http://www.ecs.soton.ac.uk/~harnad/Papers/Harnad/harnad93.symb.anal.net.searle.html\" target=\"other\">Failures of Computationalism</a>\n (Searle\u2019s reply to Harnad, and Harnad\u2019s response)", "<a href=\"http://philpapers.org/browse/the-chinese-room\" target=\"other\">Papers on the Chinese Room Argument</a>,\n at PhilPapers.org. ", "<a href=\"https://web.archive.org/web/20200426064652/http://host.uniroma3.it/progetti/kant/field/chinesebiblio.html\" target=\"other\">Annotated Chinese Room Bibliography,</a>\n by L. Hauser."], "listed_links": [{"http://eprints.soton.ac.uk/340293/1/harnad-huma-turingessay.pdf": "Alan Turing and the \u2018Hard\u2019 and \u2018Easy\u2019 Problem of Cognition: Doing and Feeling"}, {"http://www.hardproblem.ru/en/posts/Events/a-russian-chinese-room-story-antedating-searle-s-1980-discussion/": "The Game"}, {"http://www.ecs.soton.ac.uk/~harnad/Papers/Harnad/harnad93.symb.anal.net.searle.html": "Failures of Computationalism"}, {"http://philpapers.org/browse/the-chinese-room": "Papers on the Chinese Room Argument"}, {"https://web.archive.org/web/20200426064652/http://host.uniroma3.it/progetti/kant/field/chinesebiblio.html": "Annotated Chinese Room Bibliography,"}]}, "tokenized_text": ["1", "overview", "work", "artificial", "intelligence", "ai", "produced", "computer", "program", "beat", "world", "chess", "champion", "control", "autonomous", "vehicle", "complete", "email", "sentence", "defeat", "best", "human", "player", "television", "quiz", "show", "jeopardy", "ai", "also", "produced", "program", "one", "converse", "natural", "language", "including", "customer", "service", "virtual", "agent", "amazon", "alexa", "apple", "siri", "experience", "show", "playing", "chess", "jeopardy", "carrying", "conversation", "activity", "require", "understanding", "intelligence", "computer", "prowess", "conversation", "challenging", "game", "show", "computer", "understand", "language", "intelligent", "development", "result", "digital", "computer", "fully", "match", "even", "exceed", "human", "intelligence", "alan", "turing", "1950", "one", "pioneer", "theoretician", "computing", "believed", "answer", "question", "yes", "turing", "proposed", "known", "turing", "test", "computer", "pas", "human", "online", "chat", "grant", "intelligent", "late", "1970s", "ai", "researcher", "claimed", "computer", "already", "understood", "least", "natural", "language", "1980", "uc", "berkeley", "philosopher", "john", "searle", "introduced", "short", "widelydiscussed", "argument", "intended", "show", "conclusively", "impossible", "digital", "computer", "understand", "language", "think", "searle", "argues", "good", "way", "test", "theory", "mind", "say", "theory", "hold", "understanding", "created", "imagine", "would", "like", "actually", "theory", "say", "create", "understanding", "searle", "1999", "summarized", "chinese", "room", "argument", "herinafter", "cra", "concisely", "imagine", "native", "english", "speaker", "know", "chinese", "locked", "room", "full", "box", "chinese", "symbol", "data", "base", "together", "book", "instruction", "manipulating", "symbol", "program", "imagine", "people", "outside", "room", "send", "chinese", "symbol", "unknown", "person", "room", "question", "chinese", "input", "imagine", "following", "instruction", "program", "man", "room", "able", "pas", "chinese", "symbol", "correct", "answer", "question", "output", "program", "enables", "person", "room", "pas", "turing", "test", "understanding", "chinese", "understand", "word", "chinese", "searle", "go", "say", "point", "argument", "man", "room", "understand", "chinese", "basis", "implementing", "appropriate", "program", "understanding", "chinese", "neither", "digital", "computer", "solely", "basis", "computer", "qua", "computer", "anything", "man", "have", "thirty", "year", "introducing", "cra", "searle", "2010", "describes", "conclusion", "term", "consciousness", "intentionality", "demonstrated", "year", "ago", "socalled", "chinese", "room", "argument", "implementation", "computer", "program", "sufficient", "consciousness", "intentionality", "searle", "1980", "computation", "defined", "purely", "formally", "syntactically", "whereas", "mind", "actual", "mental", "semantic", "content", "get", "syntactical", "semantic", "syntactical", "operation", "nothing", "else", "put", "point", "slightly", "technically", "notion", "implemented", "program", "defines", "equivalence", "class", "specified", "independently", "specific", "physical", "realization", "specification", "necessarily", "leaf", "biologically", "specific", "power", "brain", "cause", "cognitive", "process", "system", "example", "would", "acquire", "understanding", "chinese", "going", "step", "computer", "program", "simulated", "behavior", "chinese", "speaker", "p17", "intentionality", "technical", "term", "feature", "mental", "certain", "thing", "namely", "something", "thus", "desire", "piece", "chocolate", "thought", "real", "manhattan", "fictional", "harry", "potter", "display", "intentionality", "discussed", "detail", "section", "52", "searle", "shift", "machine", "understanding", "consciousness", "intentionality", "directly", "supported", "original", "1980", "argument", "however", "redescription", "conclusion", "indicates", "close", "connection", "understanding", "consciousness", "searle", "later", "account", "meaning", "intentionality", "accept", "searle", "linking", "account", "might", "hold", "running", "program", "create", "understanding", "without", "necessarily", "creating", "consciousness", "conversely", "fancy", "robot", "might", "dog", "level", "consciousness", "desire", "belief", "without", "necessarily", "understanding", "natural", "language", "moving", "discussion", "intentionality", "searle", "seek", "develop", "broader", "implication", "argument", "aim", "refute", "functionalist", "approach", "understanding", "mind", "approach", "hold", "mental", "state", "defined", "causal", "role", "stuff", "neuron", "transistor", "play", "role", "argument", "count", "especially", "form", "functionalism", "known", "computational", "theory", "mind", "treat", "mind", "information", "processing", "system", "result", "scope", "well", "searle", "clear", "forceful", "writing", "style", "chinese", "room", "argument", "probably", "widely", "discussed", "philosophical", "argument", "cognitive", "science", "appear", "since", "turing", "test", "1991", "computer", "scientist", "pat", "hayes", "defined", "cognitive", "science", "ongoing", "research", "project", "refuting", "searle", "argument", "cognitive", "psychologist", "steven", "pinker", "1997", "pointed", "mid1990s", "well", "100", "article", "published", "searle", "thought", "experiment", "discussion", "pervasive", "internet", "pinker", "found", "compelling", "reason", "remove", "name", "internet", "discussion", "list", "interest", "subsided", "range", "connection", "argument", "broadened", "search", "google", "scholar", "searle", "chinese", "room", "limited", "period", "2010", "2019", "produced", "2000", "result", "including", "paper", "making", "connection", "argument", "topic", "ranging", "embodied", "cognition", "theater", "talk", "psychotherapy", "postmodern", "view", "truth", "posthuman", "future", "well", "discussion", "group", "collective", "mind", "discussion", "role", "intuition", "philosophy", "2007", "game", "company", "took", "name", "chinese", "room", "joking", "honor", "searle", "critique", "ai", "could", "create", "system", "gave", "impression", "intelligence", "without", "actual", "internal", "smarts", "widerange", "discussion", "implication", "tribute", "argument", "simple", "clarity", "centrality", "2", "historical", "background", "21", "leibniz", "mill", "searle", "argument", "four", "important", "antecedent", "first", "argument", "set", "philosopher", "mathematician", "gottfried", "leibniz", "16461716", "argument", "often", "known", "leibniz", "mill", "appears", "section", "17", "leibniz", "monadology", "like", "searle", "argument", "leibniz", "argument", "take", "form", "thought", "experiment", "leibniz", "asks", "u", "imagine", "physical", "system", "machine", "behaves", "way", "supposedly", "think", "experience", "perception", "17", "moreover", "must", "confessed", "perception", "depends", "upon", "inexplicable", "mechanical", "ground", "say", "mean", "figure", "motion", "supposing", "machine", "constructed", "think", "feel", "perception", "might", "conceived", "increased", "size", "keeping", "proportion", "one", "might", "go", "mill", "examining", "interior", "find", "part", "work", "one", "upon", "another", "never", "anything", "explain", "perception", "thus", "simple", "substance", "compound", "machine", "perception", "must", "sought", "robert", "latta", "translation", "notice", "leibniz", "strategy", "contrast", "overt", "behavior", "machine", "might", "appear", "product", "conscious", "thought", "way", "machine", "operates", "internally", "point", "internal", "mechanical", "operation", "part", "moving", "point", "point", "hence", "nothing", "conscious", "explain", "thinking", "feeling", "perceiving", "leibniz", "physical", "state", "sufficient", "constitutive", "mental", "state", "22", "turing", "paper", "machine", "second", "antecedent", "chinese", "room", "argument", "idea", "paper", "machine", "computer", "implemented", "human", "idea", "found", "work", "alan", "turing", "example", "intelligent", "machinery", "1948", "turing", "writes", "wrote", "program", "paper", "machine", "play", "chess", "paper", "machine", "kind", "program", "series", "simple", "step", "like", "computer", "program", "written", "natural", "language", "eg", "english", "implemented", "human", "human", "operator", "paper", "chessplaying", "machine", "need", "otherwise", "know", "play", "chess", "operator", "follow", "instruction", "generating", "move", "chess", "board", "fact", "operator", "need", "even", "know", "involved", "playing", "chess", "input", "output", "string", "nqb7", "need", "mean", "nothing", "operator", "paper", "machine", "part", "wwii", "project", "decipher", "german", "military", "encryption", "turing", "written", "englishlanguage", "program", "human", "computer", "specialized", "worker", "known", "human", "computer", "need", "know", "program", "implemented", "one", "reason", "idea", "humanpluspaper", "machine", "important", "already", "raise", "question", "agency", "understanding", "similar", "cra", "suppose", "alone", "closed", "room", "follow", "instruction", "book", "manipulating", "string", "symbol", "thereby", "implement", "paper", "machine", "generates", "symbol", "string", "nkb3", "write", "piece", "paper", "slip", "door", "someone", "ouside", "room", "suppose", "prior", "going", "room", "know", "play", "chess", "even", "game", "however", "unbeknownst", "room", "running", "turing", "chess", "program", "symbol", "string", "generate", "chess", "notation", "taken", "chess", "move", "outside", "room", "reply", "sliding", "symbol", "move", "back", "door", "room", "see", "resulting", "sequence", "move", "displayed", "chess", "board", "outside", "room", "might", "think", "someone", "room", "know", "play", "chess", "well", "know", "play", "chess", "system", "consisting", "manual", "paper", "manipulate", "string", "symbol", "playing", "chess", "memorize", "program", "symbol", "manipulation", "inside", "head", "know", "play", "chess", "albeit", "odd", "phenomenology", "someone", "conscious", "state", "matter", "whether", "know", "play", "chess", "digital", "computer", "implement", "program", "computer", "play", "chess", "merely", "simulate", "midcentury", "turing", "optimistic", "newly", "developed", "electronic", "computer", "would", "soon", "able", "exhibit", "apparently", "intelligent", "behavior", "answering", "question", "posed", "english", "carrying", "conversation", "turing", "1950", "proposed", "known", "turing", "test", "computer", "could", "pas", "human", "online", "chat", "counted", "intelligent", "third", "antecedent", "searle", "argument", "work", "searle", "colleague", "berkeley", "hubert", "dreyfus", "dreyfus", "early", "critic", "optimistic", "claim", "made", "ai", "researcher", "1965", "dreyfus", "mit", "published", "circa", "hundred", "page", "report", "titled", "alchemy", "artificial", "intelligence", "dreyfus", "argued", "key", "feature", "human", "mental", "life", "could", "captured", "formal", "rule", "manipulating", "symbol", "dreyfus", "moved", "berkeley", "1968", "1972", "published", "extended", "critique", "computer", "dreyfus", "primary", "research", "interest", "continental", "philosophy", "focus", "consciousness", "intentionality", "role", "intuition", "inarticulated", "background", "shaping", "understanding", "dreyfus", "identified", "several", "problematic", "assumption", "ai", "including", "view", "brain", "like", "digital", "computer", "assumption", "understanding", "codified", "explicit", "rule", "however", "late", "1970s", "computer", "became", "faster", "le", "expensive", "burgeoning", "ai", "community", "started", "claim", "program", "could", "understand", "english", "sentence", "using", "database", "background", "information", "work", "one", "yale", "researcher", "roger", "schank", "schank", "abelson", "1977", "came", "searle", "attentionschank", "developed", "technique", "called", "conceptual", "representation", "used", "script", "represent", "conceptual", "relation", "related", "conceptual", "role", "semantics", "searle", "argument", "originally", "presented", "response", "claim", "ai", "program", "schank", "literally", "understand", "sentence", "respond", "23", "chinese", "nation", "fourth", "antecedent", "chinese", "room", "argument", "thought", "experiment", "involving", "myriad", "human", "acting", "computer", "1961", "anatoly", "mickevich", "pseudonym", "a", "dneprov", "published", "game", "story", "stadium", "full", "1400", "math", "student", "arranged", "function", "digital", "computer", "see", "dneprov", "1961", "english", "translation", "listed", "mickevich", "1961", "internet", "resource", "4", "hour", "repeatedly", "bit", "calculation", "binary", "number", "received", "someone", "near", "pass", "binary", "result", "onto", "someone", "nearby", "learn", "next", "day", "collectively", "translated", "sentence", "portuguese", "native", "russian", "mickevich", "protagonist", "concludes", "proven", "even", "perfect", "simulation", "machine", "thinking", "thinking", "process", "higher", "form", "motion", "living", "matter", "apparently", "independently", "similar", "consideration", "emerged", "early", "discussion", "functionalist", "theory", "mind", "cognition", "see", "discussion", "section", "53", "functionalist", "hold", "mental", "state", "defined", "causal", "role", "play", "system", "door", "stop", "defined", "made", "critic", "functionalism", "quick", "turn", "proclaimed", "virtue", "multiple", "realizability", "functionalism", "consistent", "materialist", "biological", "understanding", "mental", "state", "arguably", "virtue", "identify", "type", "mental", "state", "experiencing", "pain", "wondering", "oz", "particular", "type", "neurophysiological", "state", "typetype", "identity", "theory", "contrast", "typetype", "identity", "theory", "functionalism", "allowed", "sentient", "being", "different", "physiology", "type", "mental", "state", "human", "pain", "example", "pointed", "extraterrestrial", "alien", "complex", "system", "place", "brain", "could", "realize", "functional", "property", "constituted", "mental", "state", "presumably", "could", "system", "even", "le", "like", "human", "brain", "computational", "form", "functionalism", "hold", "defining", "role", "mental", "state", "role", "information", "processing", "computation", "particularly", "vulnerable", "maneuver", "since", "wide", "variety", "system", "simple", "component", "computationally", "equivalent", "see", "eg", "maudlin", "1989", "discussion", "computer", "built", "bucket", "water", "critic", "asked", "really", "plausible", "inorganic", "system", "could", "mental", "state", "feel", "pain", "daniel", "dennett", "1978", "report", "1974", "lawrence", "davis", "gave", "colloquium", "mit", "presented", "one", "unorthodox", "implementation", "dennett", "summarizes", "davis", "thought", "experiment", "follows", "let", "functionalist", "theory", "pain", "whatever", "detail", "instantiated", "system", "subassemblies", "thing", "cfibers", "reticular", "system", "telephone", "line", "office", "staffed", "people", "perhaps", "giant", "robot", "controlled", "army", "human", "being", "inhabit", "theory", "functionally", "characterized", "condition", "pain", "met", "must", "say", "theory", "true", "robot", "pain", "real", "pain", "real", "would", "exist", "virtue", "perhaps", "disinterested", "businesslike", "activity", "bureaucratic", "team", "executing", "proper", "function", "trouble", "functionalism", "also", "published", "1978", "ned", "block", "envisions", "entire", "population", "china", "implementing", "function", "neuron", "brain", "scenario", "subsequently", "called", "chinese", "nation", "chinese", "gym", "suppose", "every", "chinese", "citizen", "would", "given", "calllist", "phone", "number", "preset", "time", "implementation", "day", "designated", "input", "citizen", "would", "initiate", "process", "calling", "calllist", "citizen", "phone", "rang", "would", "phone", "list", "would", "turn", "contact", "yet", "others", "phone", "message", "need", "exchanged", "required", "pattern", "calling", "calllists", "would", "constructed", "way", "pattern", "call", "implemented", "pattern", "activation", "occur", "neuron", "someone", "brain", "person", "mental", "state", "pain", "example", "phone", "call", "play", "functional", "role", "neuron", "causing", "one", "another", "fire", "block", "primarily", "interested", "qualia", "particular", "whether", "plausible", "hold", "population", "china", "might", "collectively", "pain", "individual", "member", "population", "experienced", "pain", "thought", "experiment", "applies", "mental", "state", "operation", "including", "understanding", "language", "thus", "block", "precursor", "thought", "experiment", "davis", "dennett", "system", "many", "human", "rather", "one", "focus", "consciousness", "extent", "searle", "argument", "also", "involves", "consciousness", "thought", "experiment", "closely", "related", "searle", "cole", "1984", "try", "pump", "intuition", "reverse", "direction", "setting", "thought", "experiment", "neuron", "conscious", "fully", "aware", "action", "including", "doused", "neurotransmitter", "undergoing", "action", "potential", "squirting", "neurotransmitter", "neighbor", "cole", "argues", "conscious", "neuron", "would", "find", "implausible", "collective", "activity", "produced", "consciousness", "cognitive", "competence", "including", "understanding", "english", "neuron", "lack", "cole", "suggests", "intuition", "implementing", "system", "trusted", "3", "chinese", "room", "argument", "1980", "john", "searle", "published", "mind", "brain", "program", "journal", "behavioral", "brain", "science", "article", "searle", "set", "argument", "reply", "halfdozen", "main", "objection", "raised", "earlier", "presentation", "various", "university", "campus", "see", "next", "section", "addition", "searle", "article", "bb", "published", "along", "comment", "criticism", "27", "cognitive", "science", "researcher", "27", "comment", "followed", "searle", "reply", "critic", "decade", "following", "publication", "chinese", "room", "argument", "subject", "many", "discussion", "1984", "searle", "presented", "chinese", "room", "argument", "book", "mind", "brain", "science", "january", "1990", "popular", "periodical", "scientific", "american", "took", "debate", "general", "scientific", "audience", "searle", "included", "chinese", "room", "argument", "contribution", "brain", "mind", "computer", "program", "searle", "piece", "followed", "responding", "article", "could", "machine", "think", "written", "philosopher", "paul", "patricia", "churchland", "soon", "thereafter", "searle", "published", "exchange", "chinese", "room", "another", "leading", "philosopher", "jerry", "fodor", "rosenthal", "ed", "1991", "heart", "argument", "searle", "imagining", "following", "symbolprocessing", "program", "written", "english", "turing", "called", "paper", "machine", "english", "speaker", "searle", "sitting", "room", "follows", "english", "instruction", "manipulating", "chinese", "symbol", "whereas", "computer", "follows", "sense", "program", "written", "computing", "language", "human", "produce", "appearance", "understanding", "chinese", "following", "symbol", "manipulating", "instruction", "thereby", "come", "understand", "chinese", "since", "computer", "human", "manipulate", "symbol", "basis", "syntax", "alone", "computer", "merely", "following", "program", "come", "genuinely", "understand", "chinese", "narrow", "argument", "based", "closely", "chinese", "room", "scenario", "specifically", "directed", "position", "searle", "call", "strong", "ai", "strong", "ai", "view", "suitably", "programmed", "computer", "program", "understand", "natural", "language", "actually", "mental", "capability", "similar", "human", "whose", "behavior", "mimic", "according", "strong", "ai", "computer", "really", "play", "chess", "intelligently", "make", "clever", "move", "understand", "language", "contrast", "weak", "ai", "much", "modest", "claim", "computer", "merely", "useful", "psychology", "linguistics", "area", "part", "simulate", "mental", "ability", "weak", "ai", "make", "claim", "computer", "actually", "understand", "intelligent", "chinese", "room", "argument", "directed", "weak", "ai", "purport", "show", "machine", "think", "searle", "say", "brain", "machine", "brain", "think", "argument", "directed", "view", "formal", "computation", "symbol", "produce", "thought", "might", "summarize", "narrow", "argument", "reductio", "ad", "absurdum", "strong", "ai", "follows", "let", "l", "natural", "language", "let", "u", "say", "program", "l", "program", "conversing", "fluently", "l", "computing", "system", "system", "human", "otherwise", "run", "program", "strong", "ai", "true", "program", "chinese", "computing", "system", "run", "program", "system", "thereby", "come", "understand", "chinese", "could", "run", "program", "chinese", "without", "thereby", "coming", "understand", "chinese", "therefore", "strong", "ai", "false", "first", "premise", "elucidates", "claim", "strong", "ai", "second", "premise", "supported", "chinese", "room", "thought", "experiment", "conclusion", "narrow", "argument", "running", "program", "endow", "system", "language", "understanding", "way", "understanding", "structure", "argument", "may", "relevant", "understand", "claim", "counterfactual", "eg", "program", "premise", "1", "meaning", "could", "program", "etc", "construal", "argument", "involves", "modal", "logic", "logic", "possibility", "necessity", "see", "damper", "2006", "shaffer", "2009", "also", "worth", "noting", "first", "premise", "attribute", "understanding", "system", "exactly", "strongai", "supposes", "acquire", "understanding", "program", "run", "crucial", "success", "failure", "cra", "schank", "1978", "title", "claim", "group", "computer", "physical", "device", "understands", "body", "paper", "claim", "program", "sam", "understanding", "sam", "schank", "say", "understands", "story", "domain", "knowledge", "p", "133", "see", "next", "section", "4", "issue", "identity", "understander", "cpu", "program", "system", "something", "else", "quickly", "came", "fore", "critic", "cra", "searle", "wider", "argument", "includes", "claim", "thought", "experiment", "show", "generally", "one", "get", "semantics", "meaning", "syntax", "formal", "symbol", "manipulation", "related", "issue", "discussed", "section", "5", "larger", "philosophical", "issue", "4", "reply", "chinese", "room", "argument", "criticism", "narrow", "chinese", "room", "argument", "strong", "ai", "often", "followed", "three", "main", "line", "distinguished", "much", "concede", "1", "critic", "concede", "man", "room", "understand", "chinese", "hold", "nevertheless", "running", "program", "may", "create", "comprehension", "chinese", "something", "room", "operator", "critic", "object", "inference", "claim", "man", "room", "understand", "chinese", "conclusion", "understanding", "created", "might", "understanding", "larger", "smaller", "different", "entity", "strategy", "system", "reply", "virtual", "mind", "reply", "reply", "hold", "output", "room", "might", "reflect", "real", "understanding", "chinese", "understanding", "would", "room", "operator", "thus", "searle", "claim", "understand", "chinese", "running", "room", "conceded", "claim", "understanding", "question", "chinese", "computationalism", "false", "denied", "2", "critic", "concede", "searle", "claim", "running", "natural", "language", "processing", "program", "described", "cr", "scenario", "create", "understanding", "whether", "human", "computer", "system", "critic", "hold", "variation", "computer", "system", "could", "understand", "variant", "might", "computer", "embedded", "robotic", "body", "interaction", "physical", "world", "via", "sensor", "motor", "robot", "reply", "might", "system", "simulated", "detailed", "operation", "entire", "human", "brain", "neuron", "neuron", "brain", "simulator", "reply", "3", "finally", "critic", "concede", "even", "narrow", "point", "ai", "critic", "hold", "man", "original", "chinese", "room", "scenario", "might", "understand", "chinese", "despite", "searle", "denial", "scenario", "impossible", "example", "critic", "argued", "intuition", "case", "unreliable", "critic", "held", "depends", "one", "mean", "understand", "point", "discussed", "section", "intuition", "reply", "others", "eg", "sprevak", "2007", "object", "assumption", "system", "eg", "searle", "room", "run", "computer", "program", "finally", "argued", "reasonable", "attribute", "understanding", "basis", "behavior", "exhibited", "chinese", "room", "would", "reasonable", "attribute", "understanding", "human", "basis", "similar", "behavioral", "evidence", "searle", "call", "last", "mind", "reply", "objection", "willing", "attribute", "understanding", "chinese", "room", "basis", "overt", "behavior", "human", "animal", "would", "extraterrestrial", "alien", "burning", "bush", "angel", "spoke", "language", "position", "close", "turing", "proposed", "behavioral", "test", "machine", "intelligence", "addition", "response", "specifically", "chinese", "room", "scenario", "narrow", "argument", "discussed", "critic", "also", "independently", "argue", "searle", "larger", "claim", "hold", "one", "get", "semantics", "meaning", "syntactic", "symbol", "manipulation", "including", "sort", "take", "place", "inside", "digital", "computer", "question", "discussed", "section", "syntax", "semantics", "41", "system", "reply", "original", "bb", "article", "searle", "identified", "discussed", "several", "response", "argument", "come", "across", "giving", "argument", "talk", "various", "place", "result", "early", "response", "received", "attention", "subsequent", "discussion", "searle", "1980", "call", "perhaps", "common", "reply", "system", "reply", "system", "reply", "searle", "say", "originally", "associated", "yale", "home", "schank", "ai", "work", "concedes", "man", "room", "understand", "chinese", "reply", "continues", "man", "part", "central", "processing", "unit", "cpu", "larger", "system", "larger", "system", "includes", "huge", "database", "memory", "scratchpad", "containing", "intermediate", "state", "instruction", "complete", "system", "required", "answering", "chinese", "question", "sytems", "reply", "man", "running", "program", "understand", "chinese", "system", "whole", "ned", "block", "one", "first", "press", "system", "reply", "along", "many", "others", "including", "jack", "copeland", "daniel", "dennett", "douglas", "hofstadter", "jerry", "fodor", "john", "haugeland", "ray", "kurzweil", "george", "rey", "rey", "1986", "say", "person", "room", "cpu", "system", "kurzweil", "2002", "say", "human", "implementer", "significance", "presumably", "meaning", "property", "implementer", "necessarily", "system", "kurzweil", "hews", "spirit", "turing", "test", "hold", "system", "display", "apparent", "capacity", "understand", "chinese", "would", "indeed", "understand", "chinese", "searle", "contradicting", "saying", "effect", "machine", "speaks", "chinese", "understand", "chinese", "margaret", "boden", "1988", "raise", "level", "consideration", "computational", "psychology", "credit", "brain", "seeing", "beansprouts", "understanding", "english", "intentional", "state", "property", "people", "brain", "244", "short", "searle", "description", "robot", "pseudobrain", "searleintherobot", "understanding", "english", "involves", "categorymistake", "comparable", "treating", "brain", "bearer", "opposed", "causal", "basis", "intelligence", "boden", "1988", "point", "room", "operator", "conscious", "agent", "cpu", "computer", "chinese", "room", "scenario", "asks", "u", "take", "perspective", "implementer", "surprisingly", "fails", "see", "larger", "picture", "searle", "response", "system", "reply", "simple", "principle", "could", "internalize", "entire", "system", "memorizing", "instruction", "database", "calculation", "head", "could", "leave", "room", "wander", "outdoors", "perhaps", "even", "conversing", "chinese", "still", "would", "way", "attach", "meaning", "formal", "symbol", "man", "would", "entire", "system", "yet", "still", "would", "understand", "chinese", "example", "would", "know", "meaning", "chinese", "word", "hamburger", "still", "get", "semantics", "syntax", "way", "searle", "response", "anticipates", "later", "extended", "mind", "view", "eg", "clark", "chalmers", "1998", "otto", "suffers", "loss", "memory", "regain", "recall", "ability", "externalizing", "information", "notebook", "searle", "arguably", "reverse", "internalizing", "instruction", "notebook", "acquire", "ability", "extended", "system", "searle", "effect", "concludes", "since", "acquire", "understanding", "chinese", "internalizing", "external", "component", "entire", "system", "eg", "still", "know", "chinese", "word", "hamburger", "mean", "understanding", "never", "partially", "externalized", "system", "original", "chinese", "room", "2002", "paper", "chinese", "room", "logical", "point", "view", "jack", "copeland", "considers", "searle", "response", "system", "reply", "argues", "homunculus", "inside", "searle", "head", "might", "understand", "even", "though", "room", "operator", "module", "mind", "solve", "tensor", "equation", "enable", "u", "catch", "cricket", "ball", "copeland", "turn", "consider", "chinese", "gym", "appears", "endorse", "system", "reply", "the", "individual", "player", "understand", "chinese", "entailment", "claim", "simulation", "whole", "come", "understand", "chinese", "fallacy", "involved", "moving", "part", "whole", "even", "glaring", "original", "version", "chinese", "room", "argument", "copeland", "denies", "connectionism", "implies", "room", "people", "simulate", "brain", "john", "haugeland", "writes", "2002", "searle", "response", "system", "reply", "flawed", "what", "asks", "would", "like", "mind", "consciously", "implement", "underlying", "formal", "structure", "operation", "theory", "say", "sufficient", "implement", "another", "mind", "according", "haugeland", "failure", "understand", "chinese", "irrelevant", "implementer", "larger", "system", "implemented", "would", "understand", "levelofdescription", "fallacy", "shaffer", "2009", "examines", "modal", "aspect", "logic", "cra", "argues", "familiar", "version", "system", "reply", "questionbegging", "shaffer", "claim", "modalized", "version", "system", "reply", "succeeds", "possible", "world", "understanding", "emergent", "property", "complex", "syntax", "manipulation", "nute", "2011", "reply", "shaffer", "stevan", "harnad", "defended", "searle", "argument", "system", "reply", "critic", "two", "paper", "1989", "paper", "harnad", "writes", "searle", "formulates", "problem", "follows", "mind", "computer", "program", "specifically", "computer", "program", "simulates", "imitates", "activity", "seem", "require", "understanding", "communicating", "language", "program", "said", "understand", "note", "specific", "claim", "issue", "taken", "whether", "program", "understands", "harnad", "concludes", "face", "cr", "argument", "look", "valid", "certainly", "work", "common", "rejoinder", "system", "reply", "harnad", "appears", "follow", "searle", "linking", "understanding", "state", "consciousness", "harnad", "2012", "internet", "resource", "argues", "searle", "show", "core", "problem", "conscious", "feeling", "requires", "sensory", "connection", "real", "world", "see", "section", "robot", "reply", "intentionality", "discussion", "finally", "argued", "even", "room", "operator", "memorizes", "rule", "operation", "inside", "head", "room", "operator", "become", "system", "cole", "1984", "block", "1998", "argue", "result", "would", "identity", "searle", "system", "much", "like", "case", "multiple", "personality", "distinct", "person", "single", "head", "chinese", "responding", "system", "would", "searle", "subpart", "cr", "case", "one", "person", "searle", "english", "monoglot", "chinese", "monoglot", "englishspeaking", "person", "total", "unawareness", "meaning", "chinese", "response", "show", "understood", "line", "distinct", "person", "lead", "virtual", "mind", "reply", "411", "virtual", "mind", "reply", "virtual", "mind", "reply", "concedes", "system", "reply", "operator", "chinese", "room", "understand", "chinese", "merely", "running", "paper", "machine", "however", "virtual", "mind", "reply", "hold", "important", "whether", "understanding", "created", "whether", "room", "operator", "agent", "understands", "unlike", "system", "reply", "virtual", "mind", "reply", "vmr", "hold", "running", "system", "may", "create", "new", "virtual", "entity", "distinct", "system", "whole", "well", "subsystems", "cpu", "operator", "particular", "running", "system", "might", "create", "distinct", "agent", "understands", "chinese", "virtual", "agent", "would", "distinct", "room", "operator", "entire", "system", "psychological", "trait", "including", "linguistic", "ability", "mind", "created", "artificial", "intelligence", "depend", "entirely", "upon", "program", "chinese", "database", "identical", "psychological", "trait", "ability", "cpu", "operator", "paper", "machine", "searle", "chinese", "room", "scenario", "according", "vmr", "mistake", "chinese", "room", "argument", "make", "claim", "strong", "ai", "computer", "understands", "chinese", "system", "understands", "chinese", "claim", "issue", "ai", "simply", "whether", "running", "computer", "creates", "understanding", "chinese", "familiar", "model", "virtual", "agent", "character", "computer", "video", "game", "personal", "digital", "assistant", "apple", "siri", "microsoft", "cortana", "character", "various", "ability", "personality", "character", "identical", "system", "hardware", "program", "creates", "single", "running", "system", "might", "control", "two", "distinct", "agent", "physical", "robot", "simultaneously", "one", "converse", "chinese", "one", "converse", "english", "otherwise", "manifest", "different", "personality", "memory", "cognitive", "ability", "thus", "vm", "reply", "asks", "u", "distinguish", "mind", "realizing", "system", "minsky", "1980", "sloman", "croucher", "1980", "suggested", "virtual", "mind", "reply", "chinese", "room", "argument", "first", "appeared", "widelyread", "1989", "paper", "computation", "consciousness", "tim", "maudlin", "considers", "minimal", "physical", "system", "might", "implement", "computational", "system", "running", "program", "discussion", "revolves", "around", "imaginary", "olympia", "machine", "system", "bucket", "transfer", "water", "implementing", "turing", "machine", "maudlin", "main", "target", "computationalists", "claim", "machine", "could", "phenomenal", "consciousness", "however", "course", "discussion", "maudlin", "considers", "chinese", "room", "argument", "maudlin", "citing", "minsky", "sloman", "croucher", "point", "virtual", "mind", "reply", "agent", "understands", "could", "distinct", "physical", "system", "414", "thus", "searle", "done", "nothing", "discount", "possibility", "simultaneously", "existing", "disjoint", "mentality", "4145", "perlis", "1992", "chalmers", "1996", "block", "2002", "apparently", "endorsed", "version", "virtual", "mind", "reply", "well", "richard", "hanley", "metaphysics", "star", "trek", "1997", "penrose", "2002", "critic", "strategy", "stevan", "harnad", "scornfully", "dismisses", "heroic", "resort", "metaphysics", "harnad", "defended", "searle", "position", "virtual", "symposium", "virtual", "mind", "1992", "patrick", "hayes", "perlis", "perlis", "pressed", "virtual", "mind", "argument", "derived", "say", "maudlin", "chalmers", "1996", "note", "room", "operator", "causal", "facilitator", "demon", "state", "consciousness", "irrelevant", "property", "system", "whole", "like", "maudlin", "chalmers", "raise", "issue", "personal", "identity", "might", "regard", "chinese", "room", "two", "mental", "system", "realized", "within", "physical", "space", "organization", "give", "rise", "chinese", "experience", "quite", "distinct", "organization", "give", "rise", "demon", "room", "operator", "experience", "326", "cole", "1991", "1994", "develops", "reply", "argues", "follows", "searle", "argument", "requires", "agent", "understanding", "computer", "chinese", "room", "parallel", "person", "room", "however", "searle", "failure", "understand", "chinese", "room", "show", "understanding", "created", "one", "key", "consideration", "searle", "discussion", "actual", "conversation", "chinese", "room", "always", "seriously", "specified", "searle", "considering", "schank", "program", "respond", "question", "happened", "restaurant", "third", "person", "searle", "wish", "conclusion", "apply", "aiproduced", "response", "including", "would", "pas", "toughest", "unrestricted", "turing", "test", "ie", "would", "sort", "conversation", "real", "people", "flesh", "conversation", "original", "cr", "scenario", "include", "question", "chinese", "tall", "live", "breakfast", "attitude", "toward", "mao", "forth", "immediately", "becomes", "clear", "answer", "chinese", "searle", "answer", "searle", "author", "answer", "belief", "desire", "memory", "personality", "trait", "apart", "industriousness", "reflected", "answer", "general", "searle", "trait", "causally", "inert", "producing", "answer", "chinese", "question", "suggests", "following", "conditional", "true", "understanding", "chinese", "created", "running", "program", "mind", "understanding", "chinese", "would", "computer", "whether", "computer", "human", "electronic", "person", "understanding", "chinese", "would", "distinct", "person", "room", "operator", "belief", "desire", "bestowed", "program", "database", "hence", "searle", "failure", "understand", "chinese", "operating", "room", "show", "understanding", "created", "cole", "1991", "offer", "additional", "argument", "mind", "understanding", "neither", "mind", "room", "operator", "system", "consisting", "operator", "program", "running", "suitably", "structured", "computer", "program", "might", "produce", "answer", "submitted", "chinese", "also", "answer", "question", "submitted", "korean", "yet", "chinese", "answer", "might", "apparently", "display", "completely", "different", "knowledge", "memory", "belief", "desire", "answer", "korean", "question", "along", "denial", "chinese", "answerer", "know", "korean", "vice", "versa", "thus", "behavioral", "evidence", "would", "two", "nonidentical", "mind", "one", "understanding", "chinese", "one", "understanding", "korean", "since", "might", "mutually", "exclusive", "property", "identical", "ipso", "facto", "identical", "mind", "implementer", "room", "analogously", "video", "game", "might", "include", "character", "one", "set", "cognitive", "ability", "smart", "understands", "chinese", "well", "another", "character", "incompatible", "set", "stupid", "english", "monoglot", "inconsistent", "cognitive", "trait", "trait", "xbox", "system", "realizes", "cole", "argues", "implication", "mind", "generally", "abstract", "system", "realize", "see", "mind", "body", "larger", "philosophical", "issue", "section", "short", "virtual", "mind", "argument", "since", "evidence", "searle", "provides", "understanding", "chinese", "understand", "chinese", "room", "chinese", "room", "argument", "refute", "differently", "formulated", "equally", "strong", "ai", "claim", "asserting", "possibility", "creating", "understanding", "using", "programmed", "digital", "computer", "maudlin", "1989", "say", "searle", "adequately", "responded", "criticism", "others", "however", "replied", "vmr", "including", "stevan", "harnad", "mathematical", "physicist", "roger", "penrose", "penrose", "generally", "sympathetic", "point", "searle", "raise", "chinese", "room", "argument", "argued", "virtual", "mind", "reply", "penrose", "believe", "computational", "process", "account", "consciousness", "chinese", "room", "ground", "well", "limitation", "formal", "system", "revealed", "kurt", "g\u00f6del", "incompleteness", "proof", "penrose", "two", "book", "mind", "consciousness", "chalmers", "others", "responded", "penrose", "appeal", "g\u00f6del", "2002", "article", "consciousness", "computation", "chinese", "room", "specifically", "address", "chinese", "room", "argument", "penrose", "argues", "chinese", "gym", "variation", "room", "expanded", "size", "india", "indian", "processing", "show", "implausible", "hold", "kind", "disembodied", "understanding", "associated", "person", "carrying", "algorithm", "whose", "presence", "impinge", "way", "upon", "consciousness", "2301", "penrose", "concludes", "chinese", "room", "argument", "refutes", "strong", "ai", "christian", "kaernbach", "2005", "report", "subjected", "virtual", "mind", "theory", "empirical", "test", "negative", "result", "42", "robot", "reply", "robot", "reply", "concedes", "searle", "right", "chinese", "room", "scenario", "show", "computer", "trapped", "computer", "room", "understand", "language", "know", "word", "mean", "robot", "reply", "responsive", "problem", "knowing", "meaning", "chinese", "word", "hamburger", "searle", "example", "something", "room", "operator", "would", "know", "seems", "reasonable", "hold", "u", "know", "hamburger", "seen", "one", "perhaps", "even", "made", "one", "tasted", "one", "least", "heard", "people", "talk", "hamburger", "understood", "relating", "thing", "know", "seeing", "making", "tasting", "given", "one", "might", "come", "know", "hamburger", "robot", "reply", "suggests", "put", "digital", "computer", "robot", "body", "sensor", "video", "camera", "microphone", "add", "effector", "wheel", "move", "around", "arm", "manipulate", "thing", "world", "robot", "computer", "body", "might", "child", "learn", "seeing", "robot", "reply", "hold", "digital", "computer", "robot", "body", "freed", "room", "could", "attach", "meaning", "symbol", "actually", "understand", "natural", "language", "margaret", "boden", "tim", "crane", "daniel", "dennett", "jerry", "fodor", "stevan", "harnad", "han", "moravec", "george", "rey", "among", "endorsed", "version", "reply", "one", "time", "another", "robot", "reply", "effect", "appeal", "wide", "content", "externalist", "semantics", "agree", "searle", "syntax", "internal", "connection", "isolation", "world", "insufficient", "semantics", "holding", "suitable", "causal", "connection", "world", "provide", "content", "internal", "symbol", "time", "searle", "pressing", "cra", "many", "philosophy", "language", "mind", "recognizing", "importance", "causal", "connection", "world", "source", "meaning", "reference", "word", "concept", "hilary", "putnam", "1981", "argued", "brain", "vat", "isolated", "world", "might", "speak", "think", "language", "sounded", "like", "english", "would", "english", "hence", "brain", "vat", "could", "wonder", "brain", "vat", "sensory", "isolation", "word", "brain", "vat", "refer", "brain", "vat", "view", "meaning", "determined", "connection", "world", "became", "widespread", "searle", "resisted", "turn", "outward", "continued", "think", "meaning", "subjective", "connected", "consciousness", "related", "view", "mind", "best", "understood", "embodied", "embedded", "world", "gained", "many", "supporter", "since", "1990s", "contra", "cartesian", "solipsistic", "intuition", "organism", "rely", "environmental", "feature", "success", "behavior", "whether", "one", "take", "mind", "symbol", "processing", "system", "symbol", "getting", "content", "sensory", "connection", "world", "nonsymbolic", "system", "succeeds", "embedded", "particular", "environment", "important", "thing", "outside", "head", "come", "fore", "hence", "many", "sympathetic", "form", "robot", "reply", "computational", "system", "might", "understand", "provided", "acting", "world", "eg", "carter", "2007", "textbook", "philosophy", "ai", "concludes", "lesson", "draw", "chinese", "room", "thought", "experiment", "embodied", "experience", "necessary", "development", "semantics", "however", "searle", "think", "robot", "reply", "chinese", "room", "argument", "stronger", "system", "reply", "sensor", "provide", "additional", "input", "computer", "syntactic", "input", "see", "making", "parallel", "change", "chinese", "room", "scenario", "suppose", "man", "chinese", "room", "receives", "addition", "chinese", "character", "slipped", "door", "stream", "binary", "digit", "appear", "say", "ticker", "tape", "corner", "room", "instruction", "book", "augmented", "use", "numeral", "tape", "input", "along", "chinese", "character", "unbeknownst", "man", "room", "symbol", "tape", "digitized", "output", "video", "camera", "possibly", "sensor", "searle", "argues", "additional", "syntactic", "input", "nothing", "allow", "man", "associate", "meaning", "chinese", "character", "work", "man", "room", "jerry", "fodor", "hilary", "putnam", "david", "lewis", "principle", "architect", "computational", "theory", "mind", "searle", "wider", "argument", "attack", "original", "1980", "reply", "searle", "fodor", "allows", "searle", "certainly", "right", "instantiating", "program", "brain", "sufficient", "propositional", "attitude", "characteristic", "organism", "brain", "fodor", "hold", "searle", "wrong", "robot", "reply", "computer", "might", "propositional", "attitude", "right", "causal", "connection", "world", "one", "mediated", "man", "sitting", "head", "robot", "know", "right", "causal", "connection", "searle", "commits", "fallacy", "inferring", "little", "man", "right", "causal", "connection", "conclude", "causal", "linkage", "would", "succeed", "considerable", "empirical", "evidence", "mental", "process", "involve", "manipulation", "symbol", "searle", "give", "u", "alternative", "explanation", "sometimes", "called", "fodor", "game", "town", "argument", "computational", "approach", "1980s", "1990s", "fodor", "wrote", "extensively", "connection", "must", "brain", "state", "world", "state", "intentional", "representational", "property", "also", "emphasizing", "computationalism", "limit", "computation", "intrinsically", "local", "account", "abductive", "reasoning", "later", "piece", "yin", "yang", "chinese", "room", "rosenthal", "1991", "pp524525", "fodor", "substantially", "revise", "1980", "view", "distance", "earlier", "version", "robot", "reply", "hold", "instead", "instantiation", "defined", "way", "symbol", "must", "proximate", "cause", "effect", "intervening", "guy", "room", "searle", "room", "instantiation", "turing", "machine", "searle", "setup", "instantiate", "machine", "brain", "instantiates", "concludes", "searle", "setup", "irrelevant", "claim", "strong", "equivalence", "chinese", "speaker", "brain", "ipso", "facto", "sufficient", "speaking", "chinese", "searle", "say", "fodor", "move", "zillion", "criticism", "chinese", "room", "argument", "fodor", "perhaps", "desperate", "claim", "precisely", "man", "chinese", "room", "set", "implement", "step", "computer", "program", "implementing", "step", "computer", "program", "offer", "argument", "extraordinary", "claim", "rosenthal", "1991", "p", "525", "1986", "paper", "george", "rey", "advocated", "combination", "system", "robot", "reply", "noting", "original", "turing", "test", "insufficient", "test", "intelligence", "understanding", "isolated", "system", "searle", "describes", "room", "certainly", "functionally", "equivalent", "real", "chinese", "speaker", "sensing", "acting", "world", "2002", "second", "look", "searle", "misunderstanding", "functionalism", "strong", "ai", "rey", "defends", "functionalism", "searle", "particular", "form", "rey", "call", "computationalrepresentational", "theory", "thought", "crtt", "crtt", "committed", "attributing", "thought", "system", "pass", "turing", "test", "like", "chinese", "room", "committed", "conversation", "manual", "model", "understanding", "natural", "language", "rather", "crtt", "concerned", "intentionality", "natural", "artificial", "representation", "system", "semantically", "evaluable", "true", "false", "hence", "aboutness", "searle", "saddle", "functionalism", "blackbox", "character", "behaviorism", "functionalism", "care", "thing", "done", "rey", "sketch", "modest", "mind", "crtt", "system", "perception", "make", "deductive", "inductive", "inference", "make", "decision", "basis", "goal", "representation", "world", "process", "natural", "language", "converting", "native", "representation", "explain", "behavior", "system", "would", "need", "use", "attribution", "needed", "explain", "behavior", "normal", "chinese", "speaker", "flesh", "chinese", "conversation", "context", "robot", "reply", "may", "see", "evidence", "entity", "understands", "operator", "inside", "room", "suppose", "ask", "robot", "system", "chinese", "translation", "see", "might", "get", "answer", "old", "friend", "shakey", "see", "whereas", "phone", "searle", "room", "ask", "question", "english", "might", "get", "four", "wall", "damn", "endless", "instruction", "book", "notebooks", "evidence", "distinct", "responder", "english", "speaker", "chinese", "speaker", "see", "quite", "different", "thing", "giant", "robot", "go", "rampage", "smash", "much", "tokyo", "oblivious", "searle", "following", "program", "notebook", "room", "searle", "guilty", "homicide", "mayhem", "agent", "committing", "act", "tim", "crane", "discus", "chinese", "room", "argument", "1991", "book", "mechanical", "mind", "cite", "churchlands", "luminous", "room", "analogy", "go", "argue", "course", "operating", "room", "searle", "would", "learn", "meaning", "chinese", "if", "searle", "memorized", "rule", "data", "also", "started", "acting", "world", "chinese", "people", "plausible", "would", "long", "come", "realize", "symbol", "mean", "127", "rapaport", "2006", "press", "analogy", "helen", "keller", "chinese", "room", "crane", "appears", "end", "version", "robot", "reply", "searle", "argument", "begs", "question", "effect", "denying", "central", "thesis", "ai", "thinking", "formal", "symbol", "manipulation", "searle", "assumption", "none", "le", "seems", "correct", "proper", "response", "searle", "argument", "sure", "searleintheroom", "room", "alone", "understand", "chinese", "let", "outside", "world", "impact", "room", "meaning", "semantics", "might", "begin", "get", "foothold", "course", "concedes", "thinking", "simply", "symbol", "manipulation", "129", "idea", "learning", "ground", "understanding", "led", "work", "developmental", "robotics", "aka", "epigenetic", "robotics", "ai", "research", "area", "seek", "replicate", "key", "human", "learning", "ability", "robot", "shown", "object", "several", "angle", "told", "natural", "language", "name", "object", "margaret", "boden", "1988", "also", "argues", "searle", "mistakenly", "supposes", "program", "pure", "syntax", "program", "bring", "activity", "certain", "machine", "inherent", "procedural", "consequence", "computer", "program", "give", "toehold", "semantics", "semantics", "question", "denotational", "causal", "250", "thus", "robot", "might", "causal", "power", "enable", "refer", "hamburger", "stevan", "harnad", "also", "find", "important", "sensory", "motor", "capability", "say", "turing", "test", "whether", "conducted", "chinese", "language", "could", "successfully", "passed", "without", "operation", "draw", "sensory", "motor", "higher", "cognitive", "capacity", "well", "capacity", "comprehend", "chinese", "begin", "rest", "mental", "competence", "leave", "harnad", "belief", "symbolic", "function", "must", "grounded", "robotic", "function", "connect", "system", "world", "think", "count", "symbolic", "account", "mentality", "jerry", "fodor", "one", "suspect", "approach", "roger", "schank", "searle", "original", "target", "harnad", "2012", "internet", "resource", "argues", "cra", "show", "even", "robot", "symbol", "grounded", "external", "world", "still", "something", "missing", "feeling", "feeling", "understanding", "however", "ziemke", "2016", "argues", "robotic", "embodiment", "layered", "system", "bodily", "regulation", "may", "ground", "emotion", "meaning", "seligman", "2019", "argues", "perceptually", "grounded", "approach", "natural", "language", "processing", "nlp", "potential", "display", "intentionality", "thus", "foster", "truly", "meaningful", "semantics", "view", "searle", "skeptic", "intrinsically", "beyond", "computer", "capacity", "43", "brain", "simulator", "reply", "consider", "computer", "operates", "quite", "different", "manner", "usual", "ai", "program", "script", "operation", "sentencelike", "string", "symbol", "brain", "simulator", "reply", "asks", "u", "suppose", "instead", "program", "simulates", "actual", "sequence", "nerve", "firing", "occur", "brain", "native", "chinese", "language", "speaker", "person", "understands", "chinese", "every", "nerve", "every", "firing", "since", "computer", "work", "way", "brain", "native", "chinese", "speaker", "processing", "information", "way", "understand", "chinese", "paul", "patricia", "churchland", "set", "reply", "along", "line", "discussed", "response", "searle", "argues", "make", "difference", "suggests", "variation", "brain", "simulator", "scenario", "suppose", "room", "man", "huge", "set", "valve", "water", "pipe", "arrangement", "neuron", "native", "chinese", "speaker", "brain", "program", "tell", "man", "valve", "open", "response", "input", "searle", "claim", "obvious", "would", "understanding", "chinese", "note", "however", "basis", "claim", "longer", "simply", "searle", "understand", "chinese", "seems", "clear", "facilitating", "causal", "operation", "system", "rely", "leibnizian", "intuition", "waterworks", "understand", "see", "also", "maudlin", "1989", "searle", "concludes", "simulation", "brain", "activity", "real", "thing", "however", "following", "pylyshyn", "1980", "cole", "foelber", "1984", "chalmers", "1996", "might", "wonder", "hybrid", "system", "pylyshyn", "writes", "cell", "brain", "replaced", "integrated", "circuit", "chip", "programmed", "way", "keep", "inputoutput", "function", "unit", "identical", "unit", "replaced", "would", "likelihood", "keep", "right", "speaking", "exactly", "except", "would", "eventually", "stop", "meaning", "anything", "outside", "observer", "might", "take", "word", "would", "become", "certain", "noise", "circuit", "caused", "make", "cyborgization", "thought", "experiment", "linked", "chinese", "room", "suppose", "otto", "neural", "disease", "cause", "one", "neuron", "brain", "fail", "surgeon", "install", "tiny", "remotely", "controlled", "artificial", "neuron", "synron", "along", "side", "disabled", "neuron", "control", "otto", "neuron", "john", "searle", "chinese", "room", "unbeknownst", "searle", "otto", "tiny", "wire", "connect", "artificial", "neuron", "synapsis", "cellbody", "disabled", "neuron", "artificial", "neuron", "stimulated", "neuron", "synapse", "disabled", "neuron", "light", "go", "chinese", "room", "searle", "manipulates", "valve", "switch", "accord", "program", "via", "radio", "link", "cause", "otto", "artificial", "neuron", "release", "neurotransmitters", "tiny", "artificial", "vesicle", "searle", "programmed", "activity", "cause", "otto", "artificial", "neuron", "behave", "disabled", "natural", "neuron", "behavior", "rest", "nervous", "system", "unchanged", "ala", "otto", "disease", "progress", "neuron", "replaced", "synrons", "controlled", "searle", "ex", "hypothesi", "rest", "world", "notice", "difference", "otto", "rubric", "combination", "reply", "searle", "also", "considers", "system", "feature", "three", "preceding", "robot", "digital", "brain", "simulating", "computer", "cranium", "system", "whole", "behaves", "indistinguishably", "human", "since", "normal", "input", "brain", "sense", "organ", "natural", "suppose", "advocate", "brain", "simulator", "reply", "mind", "combination", "brain", "simulation", "robot", "system", "reply", "eg", "rey", "1986", "argue", "reasonable", "attribute", "intentionality", "system", "whole", "searle", "agrees", "would", "indeed", "reasonable", "attribute", "understanding", "android", "system", "long", "know", "work", "soon", "know", "truth", "computer", "uncomprehendingly", "manipulating", "symbol", "basis", "syntax", "meaning", "would", "cease", "attribute", "intentionality", "one", "assumes", "would", "true", "even", "one", "spouse", "one", "built", "lifelong", "relationship", "revealed", "hide", "silicon", "secret", "science", "fiction", "story", "including", "episode", "rod", "serling", "television", "series", "twilight", "zone", "based", "possibility", "face", "beloved", "peel", "away", "reveal", "awful", "android", "truth", "however", "steven", "pinker", "1997", "mention", "one", "episode", "android", "secret", "known", "start", "protagonist", "developed", "romantic", "relationship", "android", "tenth", "anniversary", "chinese", "room", "argument", "featured", "general", "science", "periodical", "scientific", "american", "leading", "opposition", "searle", "lead", "article", "issue", "philosopher", "paul", "patricia", "churchland", "churchlands", "agree", "searle", "chinese", "room", "understand", "chinese", "hold", "argument", "exploit", "ignorance", "cognitive", "semantic", "phenomenon", "raise", "parallel", "case", "luminous", "room", "someone", "wave", "magnet", "argues", "absence", "resulting", "visible", "light", "show", "maxwell", "electromagnetic", "theory", "false", "churchlands", "advocate", "view", "brain", "connectionist", "system", "vector", "transformer", "system", "manipulating", "symbol", "according", "structuresensitive", "rule", "system", "chinese", "room", "us", "wrong", "computational", "strategy", "thus", "agree", "searle", "traditional", "ai", "presumably", "would", "endorse", "searle", "call", "brain", "simulator", "reply", "arguing", "luminous", "room", "intuition", "fail", "u", "considering", "complex", "system", "fallacy", "move", "part", "whole", "neuron", "brain", "understands", "english", "although", "whole", "brain", "does", "1991", "book", "microcognition", "andy", "clark", "hold", "searle", "right", "computer", "running", "schank", "program", "know", "anything", "restaurant", "least", "know", "mean", "anything", "like", "understand", "searle", "think", "would", "apply", "computational", "model", "clark", "like", "churchlands", "hold", "searle", "wrong", "connectionist", "model", "clark", "interest", "thus", "brainsimulator", "reply", "brain", "think", "virtue", "physical", "property", "physical", "property", "brain", "important", "clark", "answer", "important", "brain", "variable", "flexible", "substructure", "conventional", "ai", "system", "lack", "mean", "computationalism", "functionalism", "false", "depends", "level", "take", "functional", "unit", "clark", "defends", "microfunctionalism", "one", "look", "finegrained", "functional", "description", "eg", "neural", "net", "level", "clark", "cite", "william", "lycan", "approvingly", "contra", "block", "absent", "qualia", "objection", "yes", "absent", "qualia", "functional", "unit", "made", "large", "constitute", "refutation", "functionalism", "generally", "clark", "view", "unlike", "churchlands", "conceding", "searle", "right", "schank", "symboliclevel", "processing", "system", "holding", "mistaken", "connectionist", "system", "similarly", "ray", "kurzweil", "2002", "argues", "searle", "argument", "could", "turned", "around", "show", "human", "brain", "understand", "brain", "succeeds", "manipulating", "neurotransmitter", "concentration", "mechanism", "meaningless", "criticism", "searle", "response", "brain", "simulator", "reply", "kurzweil", "say", "scale", "searle", "chinese", "room", "rather", "massive", "room", "need", "say", "entire", "system", "hundred", "trillion", "people", "simulating", "chinese", "brain", "know", "chinese", "conscious", "certainly", "would", "correct", "say", "system", "know", "chinese", "say", "conscious", "anymore", "say", "process", "know", "subjective", "experience", "another", "entity", "44", "mind", "reply", "related", "preceding", "mind", "reply", "know", "people", "understand", "chinese", "anything", "else", "behavior", "computer", "pas", "behavioral", "test", "well", "principle", "going", "attribute", "cognition", "people", "must", "principle", "also", "attribute", "computers", "searle", "1980", "reply", "short", "problem", "discussion", "know", "people", "cognitive", "state", "rather", "attributing", "attribute", "cognitive", "state", "thrust", "argument", "computational", "process", "output", "computational", "process", "output", "exist", "without", "cognitive", "state", "answer", "argument", "feign", "anesthesia", "cognitive", "science", "one", "presupposes", "reality", "knowability", "mental", "way", "physical", "science", "one", "presuppose", "reality", "knowability", "physical", "object", "critic", "hold", "evidence", "human", "understand", "evidence", "might", "visiting", "extraterrestrial", "alien", "understands", "evidence", "robot", "understands", "presupposition", "may", "make", "case", "specie", "relevant", "presupposition", "sometimes", "false", "similar", "reason", "turing", "proposing", "turing", "test", "specifically", "worried", "presupposition", "chauvinism", "reason", "presupposition", "regarding", "human", "pragmatic", "enable", "u", "predict", "behavior", "human", "interact", "effectively", "perhaps", "presupposition", "could", "apply", "equally", "computer", "similar", "consideration", "pressed", "dennett", "discussion", "call", "intentional", "stance", "searle", "raise", "question", "attributing", "attributing", "understanding", "mind", "saying", "complex", "behavioral", "disposition", "searle", "additional", "seems", "certain", "state", "consciousness", "seen", "2010", "summary", "cra", "conclusion", "terry", "horgan", "2013", "endorses", "claim", "real", "moral", "searle", "chinese", "room", "thought", "experiment", "genuine", "original", "intentionality", "requires", "presence", "internal", "state", "intrinsic", "phenomenal", "character", "inherently", "intentional", "tying", "understanding", "phenomenal", "consciousness", "raise", "host", "issue", "attribute", "limited", "understanding", "language", "toddler", "dog", "animal", "clear", "ipso", "facto", "attributing", "unseen", "state", "subjective", "consciousness", "know", "hidden", "state", "exotic", "creature", "ludwig", "wittgenstein", "private", "language", "argument", "follower", "pressed", "similar", "point", "altered", "qualia", "possibility", "analogous", "inverted", "spectrum", "arise", "suppose", "ask", "sum", "5", "7", "respond", "sum", "5", "7", "12", "heard", "question", "conscious", "experience", "hearing", "understanding", "sum", "10", "14", "though", "computational", "state", "appropriate", "producing", "correct", "sum", "said", "12", "certain", "conscious", "state", "correct", "certain", "functional", "state", "wittgenstein", "consideration", "appear", "subjective", "state", "irrelevant", "best", "epiphenomenal", "language", "user", "display", "appropriate", "linguistic", "behavior", "afterall", "taught", "language", "basis", "overt", "response", "qualia", "mathematical", "savant", "daniel", "tammet", "report", "generates", "decimal", "expansion", "pi", "thousand", "digit", "experience", "color", "reveal", "next", "digit", "even", "may", "tennant", "performance", "likely", "produced", "color", "experience", "rather", "unconscious", "neural", "computation", "possible", "importance", "subjective", "state", "considered", "section", "intentionality", "30", "year", "since", "cra", "philosophical", "interest", "zombie", "creature", "look", "like", "behave", "normal", "human", "including", "linguistic", "behavior", "yet", "subjective", "consciousness", "difficulty", "claiming", "subjective", "state", "consciousness", "crucial", "understanding", "meaning", "arise", "case", "absent", "qualia", "tell", "difference", "zombie", "nonzombies", "searle", "account", "tell", "difference", "really", "understand", "english", "tell", "difference", "understand", "language", "zombie", "behave", "like", "really", "neither", "selection", "factor", "history", "human", "evolution", "predator", "prey", "mate", "zombie", "true", "understanders", "right", "conscious", "experience", "indistinguishable", "appears", "distinction", "without", "difference", "case", "searle", "short", "reply", "mind", "reply", "may", "short", "descartes", "famously", "argued", "speech", "sufficient", "attributing", "mind", "consciousness", "others", "infamously", "argued", "necessary", "turing", "effect", "endorsing", "descartes", "sufficiency", "condition", "least", "intelligence", "substituting", "written", "oral", "linguistic", "behavior", "since", "u", "use", "dialog", "sufficient", "condition", "attributing", "understanding", "searle", "argument", "hold", "speech", "sufficient", "condition", "attributing", "understanding", "human", "anything", "share", "biology", "account", "would", "appear", "required", "additionally", "attributed", "justify", "additional", "attribution", "conspecific", "key", "searle", "account", "natural", "question", "arises", "circumstance", "would", "justify", "u", "attributing", "understanding", "consciousness", "extraterrestrial", "alien", "share", "biology", "offending", "et", "withholding", "attribution", "understanding", "postmortem", "may", "risky", "han", "moravec", "director", "robotics", "laboratory", "carnegie", "mellon", "university", "author", "robot", "mere", "machine", "transcendent", "mind", "argues", "searle", "position", "merely", "reflects", "intuition", "traditional", "philosophy", "mind", "step", "new", "cognitive", "science", "moravec", "endorses", "version", "mind", "reply", "make", "sense", "attribute", "intentionality", "machine", "reason", "make", "sense", "attribute", "human", "interpretative", "position", "similar", "view", "daniel", "dennett", "moravec", "go", "note", "one", "thing", "attribute", "others", "ability", "make", "attribution", "intentionality", "make", "attribution", "selfrepresentation", "heart", "consciousness", "capacity", "appear", "implementation", "independent", "hence", "possible", "alien", "suitably", "programmed", "computer", "seen", "reason", "searle", "think", "disregard", "evidence", "case", "robot", "computer", "know", "processing", "syntactic", "fact", "trump", "consideration", "indeed", "searle", "belief", "larger", "point", "chinese", "room", "merely", "illustrates", "larger", "point", "addressed", "syntax", "semantics", "section", "45", "intuition", "reply", "many", "response", "chinese", "room", "argument", "noted", "leibniz", "mill", "argument", "appears", "based", "intuition", "intuition", "computer", "man", "room", "think", "understanding", "example", "ned", "block", "1980", "original", "bb", "commentary", "say", "searle", "argument", "depends", "force", "intuition", "certain", "entity", "think", "block", "argues", "1", "intuition", "sometimes", "trumped", "2", "perhaps", "need", "bring", "concept", "understanding", "line", "reality", "certain", "computer", "robot", "belong", "natural", "kind", "human", "similarly", "margaret", "boden", "1988", "point", "trust", "untutored", "intuition", "mind", "depends", "matter", "development", "science", "may", "change", "intuition", "indeed", "elimination", "bias", "intuition", "precisely", "motivated", "turing", "1950", "propose", "turing", "test", "test", "blind", "physical", "character", "system", "replying", "question", "searle", "critic", "effect", "argue", "merely", "pushed", "reliance", "intuition", "back", "room", "example", "one", "hold", "despite", "searle", "intuition", "would", "understand", "chinese", "room", "perhaps", "mistaken", "albeit", "unconsciously", "hauser", "2002", "accuses", "searle", "cartesian", "bias", "inference", "seems", "quite", "obvious", "understand", "nothing", "conclusion", "really", "understand", "nothing", "normally", "one", "understands", "english", "chinese", "one", "know", "one", "necessarily", "searle", "lack", "normal", "introspective", "awareness", "understanding", "abnormal", "conclusive", "critic", "cra", "note", "intuition", "intelligence", "understanding", "meaning", "may", "unreliable", "regard", "meaning", "wakefield", "2003", "following", "block", "1998", "defends", "wakefield", "call", "essentialist", "objection", "cra", "namely", "computational", "account", "meaning", "analysis", "ordinary", "concept", "related", "intuition", "rather", "building", "scientific", "theory", "meaning", "may", "require", "revising", "intuition", "theory", "get", "evidence", "explanatory", "power", "accord", "pretheoretic", "intuition", "however", "wakefield", "argues", "computational", "account", "meaning", "afflicted", "pernicious", "indeterminacy", "pp", "308ff", "critic", "focusing", "role", "intuition", "cra", "argue", "intuition", "regarding", "intelligence", "understanding", "may", "also", "unreliable", "perhaps", "incompatible", "even", "current", "science", "regard", "understanding", "steven", "pinker", "mind", "work", "1997", "hold", "searle", "merely", "exploring", "fact", "english", "word", "understand", "people", "reluctant", "use", "word", "unless", "certain", "stereotypical", "condition", "apply", "pinker", "claim", "nothing", "scientifically", "speaking", "stake", "pinker", "object", "searle", "appeal", "causal", "power", "brain", "noting", "apparent", "locus", "causal", "power", "pattern", "interconnectivity", "carry", "right", "information", "processing", "pinker", "end", "discussion", "citing", "science", "fiction", "story", "alien", "anatomically", "quite", "unlike", "human", "believe", "human", "think", "discover", "head", "filled", "meat", "alien", "intuition", "unreliable", "presumably", "may", "well", "clearly", "cra", "turn", "required", "understand", "language", "schank", "1978", "clarifies", "claim", "think", "program", "understand", "mean", "sam", "one", "program", "create", "linked", "causal", "chain", "conceptualization", "represent", "took", "place", "story", "nuanced", "understanding", "understanding", "whereas", "chinese", "room", "thought", "experiment", "turn", "technical", "understanding", "understanding", "rather", "intuition", "ordinary", "competence", "understand", "word", "like", "hamburger", "indeed", "2015", "schank", "distance", "weak", "sens", "understand", "holding", "computer", "understand", "tell", "something", "ibm", "watson", "know", "saying", "schank", "program", "may", "get", "link", "right", "arguably", "know", "linked", "entity", "whether", "depends", "concept", "see", "section", "51", "furthermore", "possible", "come", "attributing", "understanding", "language", "different", "standard", "different", "thing", "relaxed", "dog", "toddler", "thing", "understand", "language", "un", "poco", "searle", "1980", "concedes", "degree", "understanding", "say", "matter", "clear", "case", "understanding", "ai", "program", "example", "computer", "understanding", "like", "understanding", "german", "partial", "incomplete", "zero", "defender", "ai", "also", "concerned", "understanding", "understanding", "bear", "chinese", "room", "argument", "paper", "chinese", "room", "understands", "ai", "researcher", "simon", "eisenstadt", "2002", "argue", "whereas", "searle", "refutes", "logical", "strong", "ai", "thesis", "program", "pass", "turing", "test", "necessarily", "understand", "searle", "argument", "impugn", "empirical", "strong", "ai", "thesis", "possible", "program", "computer", "convincingly", "satisfies", "ordinary", "criterion", "understanding", "hold", "however", "impossible", "settle", "question", "without", "employing", "definition", "term", "understand", "provide", "test", "judging", "whether", "hypothesis", "true", "false", "cite", "wvo", "quine", "word", "object", "showing", "always", "empirical", "uncertainty", "attributing", "understanding", "human", "chinese", "room", "clever", "han", "trick", "clever", "han", "horse", "appeared", "clomp", "answer", "simple", "arithmetic", "question", "discovered", "han", "could", "detect", "unconscious", "cue", "trainer", "similarly", "man", "room", "understand", "chinese", "could", "exposed", "watching", "closely", "simon", "eisenstadt", "explain", "would", "done", "would", "affect", "argument", "citing", "work", "rudolf", "carnap", "simon", "eisenstadt", "argue", "understand", "exhibit", "certain", "behavior", "use", "intension", "determine", "extension", "one", "see", "actual", "program", "use", "appropriate", "intension", "discus", "three", "actual", "ai", "program", "defend", "various", "attribution", "mentality", "including", "understanding", "conclude", "computer", "understand", "learn", "intension", "associating", "word", "linguistic", "structure", "denotation", "detected", "sensory", "stimulus", "since", "see", "exactly", "machine", "work", "fact", "easier", "establish", "machine", "exhibit", "understanding", "establish", "human", "exhibit", "understanding", "thus", "conclude", "evidence", "empirical", "strong", "ai", "overwhelming", "similarly", "daniel", "dennett", "original", "1980", "response", "searle", "argument", "called", "intuition", "pump", "term", "came", "discussing", "cra", "hofstader", "sharvy", "1983", "echo", "complaint", "dennett", "considered", "view", "2013", "cra", "clearly", "fallacious", "misleading", "argument", "p", "320", "paul", "thagard", "2013", "proposes", "every", "thought", "experiment", "philosophy", "equal", "opposite", "thought", "experiment", "thagard", "hold", "intuition", "unreliable", "cra", "example", "fact", "cra", "refuted", "technology", "autonomous", "robotic", "car", "dennett", "elaborated", "concern", "intuition", "regarding", "intelligence", "dennett", "1987", "fast", "thinking", "expressed", "concern", "slow", "speed", "chinese", "room", "would", "operate", "joined", "several", "commentator", "including", "tim", "maudlin", "david", "chalmers", "steven", "pinker", "operator", "chinese", "room", "may", "eventually", "produce", "appropriate", "answer", "chinese", "question", "slow", "thinker", "stupid", "intelligent", "wild", "may", "well", "end", "dead", "dennett", "argues", "speed", "essence", "intelligence", "figure", "relevant", "portion", "changing", "environment", "fast", "enough", "fend", "practically", "intelligent", "however", "complex", "326", "thus", "dennett", "relativizes", "intelligence", "processing", "speed", "relative", "current", "environment", "tim", "maudlin", "1989", "disagrees", "maudlin", "considers", "timescale", "problem", "pointed", "writer", "concludes", "contra", "dennett", "extreme", "slowness", "computational", "system", "violate", "necessary", "condition", "thinking", "consciousness", "furthermore", "searle", "main", "claim", "understanding", "intelligence", "quickwitted", "encounter", "extraterrestrials", "could", "process", "information", "thousand", "time", "quickly", "seems", "would", "show", "nothing", "slowpoke", "ability", "understand", "language", "speak", "steven", "pinker", "1997", "also", "hold", "searle", "relies", "untutored", "intuition", "pinker", "endorses", "churchlands", "1990", "counterexample", "analogous", "thought", "experiment", "waving", "magnet", "generating", "light", "noting", "outcome", "would", "disprove", "maxwell", "theory", "light", "consists", "electromagnetic", "wave", "pinker", "hold", "key", "issue", "speed", "thought", "experiment", "slows", "wave", "range", "human", "longer", "see", "light", "trusting", "intuition", "thought", "experiment", "falsely", "conclude", "rapid", "wave", "light", "either", "similarly", "searle", "slowed", "mental", "computation", "range", "human", "longer", "think", "understanding", "since", "understanding", "ordinarily", "much", "faster", "9495", "howard", "gardiner", "supporter", "searle", "conclusion", "regarding", "room", "make", "similar", "point", "understanding", "gardiner", "address", "chinese", "room", "argument", "book", "mind", "new", "science", "1985", "171177", "gardiner", "considers", "standard", "reply", "chinese", "room", "argument", "concludes", "searle", "correct", "room", "the", "word", "understand", "unduly", "stretched", "case", "chinese", "room", "175", "thus", "several", "group", "critic", "argue", "speed", "affect", "willingness", "attribute", "intelligence", "understanding", "slow", "system", "chinese", "room", "result", "may", "simply", "intuition", "regarding", "chinese", "room", "unreliable", "thus", "man", "room", "implementing", "program", "may", "understand", "chinese", "despite", "intuition", "contrary", "maudlin", "pinker", "may", "slowness", "mark", "crucial", "difference", "simulation", "room", "fast", "computer", "man", "intelligent", "computer", "system", "dennett", "5", "larger", "philosophical", "issue", "51", "syntax", "semantics", "searle", "belief", "chinese", "room", "argument", "support", "larger", "point", "explains", "failure", "chinese", "room", "produce", "understanding", "searle", "argued", "program", "implemented", "computer", "syntactical", "computer", "operation", "formal", "respond", "physical", "form", "string", "symbol", "meaning", "symbol", "mind", "hand", "state", "meaning", "mental", "content", "associate", "meaning", "word", "sign", "language", "respond", "sign", "meaning", "physical", "appearance", "short", "understand", "according", "searle", "key", "point", "syntax", "sufficient", "constitutive", "semantics", "although", "computer", "may", "able", "manipulate", "syntax", "produce", "appropriate", "response", "natural", "language", "input", "understand", "sentence", "receive", "output", "associate", "meaning", "word", "searle", "1984", "present", "three", "premise", "argument", "syntax", "sufficient", "semantics", "program", "produce", "mind", "program", "purely", "formal", "syntactic", "human", "mind", "mental", "content", "semantics", "syntax", "neither", "constitutive", "sufficient", "semantic", "content", "therefore", "program", "constitutive", "sufficient", "mind", "chinese", "room", "thought", "experiment", "support", "third", "premise", "claim", "syntactic", "manipulation", "sufficient", "meaning", "thought", "significant", "issue", "wider", "implication", "ai", "attribution", "understanding", "prominent", "theory", "mind", "hold", "human", "cognition", "generally", "computational", "one", "form", "held", "thought", "involves", "operation", "symbol", "virtue", "physical", "property", "alternative", "connectionist", "account", "computation", "subsymbolic", "state", "searle", "right", "strong", "ai", "also", "main", "approach", "understanding", "human", "cognition", "misguided", "seen", "searle", "hold", "chinese", "room", "scenario", "show", "one", "get", "semantics", "syntax", "alone", "symbolic", "logic", "system", "kind", "artificial", "language", "rule", "given", "syntax", "semantics", "come", "later", "logician", "specifies", "basic", "symbol", "set", "rule", "manipulating", "string", "produce", "new", "one", "rule", "purely", "syntactic", "applied", "string", "symbol", "solely", "virtue", "syntax", "form", "semantics", "symbol", "system", "must", "provided", "separately", "one", "wish", "show", "interesting", "additional", "relationship", "hold", "syntactic", "operation", "semantics", "symbol", "manipulation", "preserve", "truth", "one", "must", "provide", "sometimes", "complex", "metaproofs", "show", "face", "semantics", "quite", "independent", "syntax", "artificial", "language", "one", "get", "semantics", "syntax", "alone", "formal", "symbol", "never", "enough", "mental", "content", "symbol", "definition", "meaning", "interpretation", "semantics", "except", "insofar", "someone", "outside", "system", "give", "searle", "1989", "45", "searle", "identification", "meaning", "interpretation", "passage", "important", "searle", "point", "clearly", "true", "causally", "inert", "formal", "system", "logician", "semantic", "interpretation", "given", "symbol", "logician", "move", "formal", "system", "computational", "system", "situation", "complex", "many", "searle", "critic", "eg", "cole", "1984", "dennett", "1987", "boden", "1988", "chalmers", "1996", "noted", "computer", "running", "program", "syntax", "alone", "computer", "enormously", "complex", "electronic", "causal", "system", "state", "change", "system", "physical", "one", "interpret", "physical", "state", "eg", "voltage", "syntactic", "1", "0", "intrinsic", "reality", "electronic", "syntax", "derived", "product", "interpretation", "state", "syntactically", "specified", "programmer", "implemented", "running", "machine", "electronic", "state", "complex", "causal", "system", "embedded", "real", "world", "quite", "different", "abstract", "formal", "system", "logician", "study", "dennett", "note", "computer", "program", "searle", "language", "eg", "program", "lying", "shelf", "cause", "anything", "even", "simple", "addition", "let", "alone", "mental", "state", "program", "must", "running", "chalmers", "1996", "offer", "parody", "reasoned", "recipe", "syntactic", "syntax", "sufficient", "crumbliness", "cake", "crumbly", "implementation", "recipe", "sufficient", "making", "cake", "implementation", "make", "difference", "abstract", "entity", "recipe", "program", "determines", "causal", "power", "physical", "system", "embedded", "larger", "causal", "nexus", "world", "dennett", "1987", "sum", "issue", "searle", "view", "come", "take", "material", "object", "material", "object", "power", "causing", "mental", "phenomenon", "turn", "object", "power", "producing", "mental", "phenomenon", "simply", "programming", "reorganizing", "conditional", "dependency", "transition", "states", "dennett", "view", "opposite", "programming", "precisely", "could", "give", "something", "mind", "dennett", "claim", "fact", "empirically", "unlikely", "right", "sort", "program", "run", "anything", "organic", "human", "brain", "3256", "related", "complication", "clear", "computer", "perform", "syntactic", "operation", "quite", "sense", "human", "clear", "computer", "understands", "syntax", "syntactic", "operation", "computer", "know", "manipulating", "1", "0", "computer", "recognize", "binary", "data", "string", "certain", "form", "thus", "certain", "syntactic", "rule", "may", "applied", "unlike", "man", "inside", "chinese", "room", "inside", "computer", "nothing", "literally", "read", "input", "data", "know", "symbol", "instead", "million", "transistor", "change", "state", "sequence", "voltage", "cause", "operation", "performed", "human", "may", "choose", "interpret", "voltage", "binary", "numeral", "voltage", "change", "syntactic", "operation", "computer", "interpret", "operation", "syntactic", "way", "perhaps", "computer", "need", "make", "move", "syntax", "semantics", "searle", "object", "need", "move", "complex", "causal", "connection", "semantics", "furthermore", "perhaps", "causal", "system", "describable", "performing", "syntactic", "operation", "interpret", "light", "square", "logical", "0", "dark", "square", "logical", "1", "kitchen", "toaster", "may", "described", "device", "rewrite", "logical", "0", "logical", "1", "philosophical", "problem", "getting", "syntax", "breakfast", "1990s", "searle", "began", "use", "consideration", "related", "argue", "computational", "view", "false", "lack", "clear", "sense", "computation", "syntax", "observerrelative", "intrinsic", "feature", "reality", "you", "assign", "computational", "interpretation", "anything", "searle", "2002b", "p", "17", "even", "molecule", "paint", "wall", "since", "nothing", "intrinsically", "computational", "one", "scientific", "theory", "reduces", "mental", "observerrelative", "computation", "computation", "exists", "relative", "agent", "observer", "imposes", "computational", "interpretation", "phenomenon", "obvious", "point", "seen", "ten", "year", "ago", "not", "searle", "2002b", "p17", "originally", "published", "1993", "critic", "note", "wall", "computer", "unlike", "wall", "computer", "go", "statetransitions", "counterfactually", "described", "program", "chalmers", "1996", "block", "2002", "haugeland", "2002", "2002", "paper", "block", "address", "question", "whether", "wall", "computer", "reply", "searle", "charge", "anything", "map", "onto", "formal", "system", "formal", "system", "whereas", "mind", "quite", "different", "block", "denies", "whether", "something", "computer", "depends", "entirely", "interpretation", "block", "note", "searle", "ignores", "counterfactuals", "must", "true", "implementing", "system", "haugeland", "2002", "make", "similar", "point", "implementation", "causal", "process", "reliably", "carry", "operation", "must", "right", "causal", "power", "block", "concludes", "searle", "argument", "fail", "concedes", "succeed", "sharpening", "understanding", "nature", "intentionality", "relation", "computation", "representation", "78", "rey", "2002", "also", "address", "searle", "argument", "syntax", "symbol", "observerrelative", "property", "physical", "searle", "infers", "fact", "syntactic", "property", "eg", "logical", "1", "defined", "physic", "however", "rey", "hold", "follow", "observerrelative", "rey", "argues", "searle", "also", "misunderstands", "realize", "program", "rey", "endorses", "chalmers", "reply", "putnam", "realization", "structural", "mapping", "involves", "causation", "supporting", "counterfactuals", "point", "missed", "often", "bear", "repeating", "syntactically", "specifiable", "object", "computation", "defined", "standardly", "posse", "semantics", "semantics", "involved", "specification", "state", "person", "semantics", "virtue", "computational", "organization", "causal", "relation", "world", "rey", "concludes", "searle", "simply", "consider", "substantial", "resource", "functionalism", "strong", "ai", "222", "plausibly", "detailed", "story", "would", "defuse", "negative", "conclusion", "drawn", "superficial", "sketch", "system", "chinese", "room", "john", "haugeland", "2002", "argues", "sense", "processor", "must", "intrinsically", "understand", "command", "program", "run", "executes", "accord", "specification", "way", "make", "sense", "computer", "executing", "program", "understanding", "processor", "responding", "program", "prescription", "meaningful", "385", "thus", "operation", "symbol", "meaning", "system", "haugeland", "go", "draw", "distinction", "narrow", "wide", "system", "argues", "data", "semantics", "wide", "system", "includes", "representation", "external", "object", "produced", "transducer", "passing", "haugeland", "make", "unusual", "claim", "argued", "elsewhere", "genuine", "intelligence", "semantics", "presuppose", "capacity", "kind", "commitment", "one", "life", "nonpropositional", "love", "cp", "steven", "spielberg", "2001", "film", "artificial", "intelligence", "ai", "searle", "claim", "syntax", "observerrelative", "molecule", "wall", "might", "interpreted", "implementing", "wordstar", "program", "early", "word", "processing", "program", "pattern", "molecule", "movement", "isomorphic", "formal", "structure", "wordstar", "searle", "1990b", "p", "27", "haugeland", "counter", "idea", "complex", "syntactical", "token", "presupposes", "specified", "process", "writing", "reading", "token", "must", "systematically", "producible", "retrievable", "random", "isomorphism", "pattern", "somewhere", "eg", "wall", "going", "count", "hence", "syntax", "observerrelative", "regard", "question", "whether", "one", "get", "semantics", "syntax", "william", "rapaport", "many", "year", "argued", "syntactic", "semantics", "view", "understanding", "special", "form", "syntactic", "structure", "symbol", "chinese", "word", "linked", "concept", "represented", "syntactically", "others", "believe", "yet", "ai", "futurist", "age", "spiritual", "machine", "ray", "kurzweil", "hold", "2002", "followup", "book", "red", "herring", "focus", "traditional", "symbolmanipulating", "computer", "kurzweil", "agrees", "searle", "existent", "computer", "understand", "language", "evidenced", "fact", "engage", "convincing", "dialog", "failure", "bear", "capacity", "future", "computer", "based", "different", "technology", "kurzweil", "claim", "searle", "fails", "understand", "future", "machine", "use", "chaotic", "emergent", "method", "massively", "parallel", "claim", "appears", "similar", "connectionists", "andy", "clark", "position", "taken", "churchlands", "1990", "scientific", "american", "article", "apart", "haugeland", "claim", "processor", "understand", "program", "instruction", "searle", "critic", "agree", "computer", "understand", "syntax", "understand", "semantics", "although", "like", "causal", "engine", "computer", "syntactic", "description", "often", "useful", "programmer", "treat", "machine", "performed", "syntactic", "operation", "always", "sometimes", "character", "programmer", "use", "switch", "make", "machine", "something", "example", "make", "given", "pixel", "computer", "display", "turn", "red", "make", "car", "transmission", "shift", "gear", "thus", "clear", "searle", "correct", "say", "digital", "computer", "device", "manipulates", "symbol", "computer", "complex", "causal", "engine", "syntactic", "description", "useful", "order", "structure", "causal", "interconnection", "machine", "ai", "programmer", "face", "many", "tough", "problem", "one", "hold", "get", "semantics", "syntax", "get", "semantics", "must", "get", "causality", "two", "main", "approach", "developed", "explain", "meaning", "term", "causal", "connection", "internalist", "approach", "schank", "rapaport", "conceptual", "representation", "approach", "also", "conceptual", "role", "semantics", "hold", "state", "physical", "system", "get", "semantics", "causal", "connection", "state", "system", "thus", "state", "computer", "might", "represent", "kiwi", "connected", "bird", "flightless", "node", "perhaps", "also", "image", "prototypical", "kiwi", "state", "represents", "property", "flightless", "might", "get", "content", "negationoperator", "modifying", "representation", "capable", "airborne", "selfpropulsion", "forth", "form", "vast", "connected", "conceptual", "network", "kind", "mental", "dictionary", "externalist", "approach", "developed", "dennis", "stampe", "fred", "dretske", "hilary", "putnam", "jerry", "fodor", "ruth", "millikan", "others", "hold", "state", "physical", "system", "get", "content", "causal", "connection", "external", "reality", "represent", "thus", "roughly", "system", "kiwi", "concept", "one", "state", "us", "represent", "presence", "kiwi", "external", "environment", "kiwirepresenting", "state", "state", "appropriately", "causally", "connected", "presence", "kiwi", "depending", "system", "kiwi", "representing", "state", "could", "state", "brain", "electrical", "device", "computer", "even", "hydraulic", "system", "internal", "representing", "state", "turn", "play", "causal", "role", "determining", "behavior", "system", "example", "rey", "1986", "endorses", "indicator", "semantics", "along", "line", "work", "dennis", "stampe", "1977", "fodor", "psychosemantics", "semantic", "theory", "locate", "content", "meaning", "appropriate", "causal", "relation", "world", "fit", "well", "robot", "reply", "computer", "robot", "body", "might", "causal", "connection", "could", "allow", "inner", "syntactic", "state", "semantic", "property", "representing", "state", "thing", "environment", "thus", "least", "two", "family", "theory", "marriage", "two", "block", "1986", "semantics", "might", "depend", "upon", "causal", "connection", "attempt", "provide", "account", "substance", "neutral", "state", "suitably", "organized", "causal", "system", "content", "matter", "system", "made", "theory", "computer", "could", "state", "meaning", "necessary", "computer", "aware", "state", "know", "meaning", "outsider", "appreciate", "meaning", "state", "either", "account", "meaning", "depends", "upon", "possibly", "complex", "causal", "connection", "digital", "computer", "system", "designed", "state", "complex", "causal", "dependency", "noted", "searle", "subscribe", "theory", "semantics", "instead", "searle", "discussion", "linguistic", "meaning", "often", "centered", "notion", "intentionality", "52", "intentionality", "intentionality", "property", "something", "content", "19th", "century", "psychologist", "franz", "brentano", "reintroduced", "term", "medieval", "philosophy", "held", "intentionality", "mark", "mental", "belief", "desire", "intentional", "state", "propositional", "content", "one", "belief", "p", "one", "desire", "p", "sentence", "represent", "proposition", "substitute", "p", "searle", "view", "regarding", "intentionality", "complex", "relevance", "make", "distinction", "original", "intrinsic", "intentionality", "genuine", "mental", "state", "derived", "intentionality", "language", "written", "spoken", "sentence", "derivative", "intentionality", "insofar", "interpreted", "someone", "appears", "searle", "view", "original", "intentionality", "least", "potentially", "conscious", "searle", "argues", "distinction", "original", "derived", "intentionality", "applies", "computer", "interpret", "state", "computer", "content", "state", "original", "intentionality", "many", "philosopher", "endorse", "intentionality", "dualism", "including", "sayre", "1986", "even", "fodor", "2009", "despite", "fodor", "many", "difference", "searle", "section", "1988", "book", "computer", "model", "mind", "margaret", "boden", "note", "intentionality", "wellunderstood", "reason", "put", "much", "weight", "argument", "turn", "intentionality", "furthermore", "insofar", "understand", "brain", "focus", "informational", "function", "unspecified", "causal", "power", "brain", "from", "psychological", "point", "view", "biochemistry", "matter", "informationbearing", "function", "grounded", "it", "241", "searle", "see", "intentionality", "causal", "power", "brain", "uniquely", "produced", "biological", "process", "dale", "jacquette", "1989", "argues", "reduction", "intentionality", "intentionality", "say", "ineliminable", "irreducible", "primitive", "concept", "however", "ai", "sympathizer", "seen", "intentionality", "aboutness", "bound", "information", "nonbiological", "state", "bear", "information", "well", "brain", "state", "hence", "many", "responder", "searle", "argued", "display", "substance", "chauvinism", "holding", "brain", "understand", "system", "made", "silicon", "comparable", "information", "processing", "capability", "even", "principle", "paper", "side", "issue", "appeared", "j", "maloney", "1987", "paper", "right", "stuff", "defending", "searle", "r", "sharvy", "1983", "critique", "meat", "motion", "ai", "proponent", "kurzweil", "1999", "see", "also", "richards", "2002", "continued", "hold", "ai", "system", "potentially", "mental", "property", "understanding", "intelligence", "consciousness", "intentionality", "exceed", "human", "ability", "area", "critic", "searle", "position", "take", "intentionality", "seriously", "boden", "deny", "dualistic", "distinction", "original", "derived", "intentionality", "dennett", "1987", "eg", "argues", "intentionality", "derived", "attribution", "intentionality", "animal", "people", "even", "instrumental", "allow", "u", "predict", "behavior", "description", "intrinsic", "property", "seen", "dennett", "concerned", "slow", "speed", "thing", "chinese", "room", "argues", "system", "working", "speed", "needed", "intelligence", "derived", "intentionality", "derived", "intentionality", "kind", "according", "dennett", "machine", "intentional", "system", "intentional", "explanation", "work", "predicting", "machine", "behavior", "dennett", "also", "suggests", "searle", "conflates", "intentionality", "awareness", "intentionality", "syntaxsemantic", "argument", "searle", "apparently", "confused", "claim", "underivability", "semantics", "syntax", "claim", "underivability", "consciousness", "semantics", "syntax", "336", "emphasis", "consciousness", "force", "u", "think", "thing", "firstperson", "point", "view", "dennett", "2017", "continues", "press", "claim", "fundamental", "mistake", "want", "understand", "mental", "might", "also", "worry", "searle", "conflates", "meaning", "interpretation", "searle", "original", "underived", "intentionality", "secondorder", "intentionality", "representation", "intentional", "object", "represents", "mean", "dretske", "others", "seen", "intentionality", "informationbased", "one", "state", "world", "including", "state", "computer", "may", "carry", "information", "state", "world", "informational", "aboutness", "mindindependent", "feature", "state", "hence", "mistake", "hold", "conscious", "attribution", "meaning", "source", "intentionality", "others", "noted", "searle", "discussion", "shown", "shift", "time", "issue", "intentionality", "understanding", "issue", "consciousness", "searle", "link", "intentionality", "awareness", "intentionality", "holding", "intentional", "state", "least", "potentially", "conscious", "1996", "book", "conscious", "mind", "david", "chalmers", "note", "although", "searle", "originally", "directs", "argument", "machine", "intentionality", "clear", "later", "writing", "real", "issue", "consciousness", "searle", "hold", "necessary", "condition", "intentionality", "consciousness", "lacking", "digital", "computer", "chalmers", "us", "thought", "experiment", "argue", "implausible", "one", "system", "basic", "mental", "property", "qualia", "another", "system", "lack", "possible", "imagine", "transforming", "one", "system", "either", "gradually", "replacing", "neuron", "one", "time", "digital", "circuit", "switching", "back", "forth", "flesh", "silicon", "second", "strategy", "regarding", "attribution", "intentionality", "taken", "critic", "effect", "argue", "intentionality", "intrinsic", "feature", "state", "physical", "system", "causally", "connected", "world", "right", "way", "independently", "interpretation", "see", "preceding", "syntax", "semantics", "section", "fodor", "semantic", "externalism", "influenced", "fred", "dretske", "come", "different", "conclusion", "regard", "semantics", "state", "computer", "period", "year", "dretske", "developed", "historical", "account", "meaning", "mental", "content", "would", "preclude", "attributing", "belief", "understanding", "machine", "dretske", "1985", "agrees", "searle", "adding", "machine", "literally", "add", "adding", "using", "machine", "dretske", "emphasizes", "crucial", "role", "natural", "selection", "learning", "producing", "state", "genuine", "content", "human", "built", "system", "best", "like", "swampmen", "being", "result", "lightning", "strike", "swamp", "chance", "happen", "molecule", "molecule", "copy", "human", "say", "appear", "intentionality", "mental", "state", "state", "require", "right", "history", "ai", "state", "generally", "counterfeit", "real", "mental", "state", "like", "counterfeit", "money", "may", "appear", "perfectly", "identical", "lack", "right", "pedigree", "dretske", "account", "belief", "appears", "make", "distinct", "conscious", "awareness", "belief", "intentional", "state", "taken", "require", "higher", "order", "thought", "would", "apparently", "allow", "attribution", "intentionality", "artificial", "system", "get", "right", "history", "learning", "howard", "gardiner", "endorses", "zenon", "pylyshyn", "criticism", "searle", "view", "relation", "brain", "intentionality", "supposing", "intentionality", "somehow", "stuff", "secreted", "brain", "pylyshyn", "counterthought", "experiment", "one", "neuron", "replaced", "one", "one", "integrated", "circuit", "workalikes", "see", "also", "cole", "foelber", "1984", "chalmers", "1996", "exploration", "neuron", "replacement", "scenario", "gardiner", "hold", "searle", "owes", "u", "precise", "account", "intentionality", "searle", "given", "far", "open", "question", "whether", "ai", "produce", "whether", "beyond", "scope", "gardiner", "concludes", "possibility", "dispute", "searle", "critic", "scientific", "quasi", "religious", "53", "mind", "body", "several", "critic", "noted", "metaphysical", "issue", "stake", "original", "argument", "system", "reply", "draw", "attention", "metaphysical", "problem", "relation", "mind", "body", "holding", "understanding", "property", "system", "whole", "physical", "implementer", "virtual", "mind", "reply", "hold", "mind", "person", "entity", "understand", "conscious", "abstract", "physical", "system", "could", "manytoone", "relation", "mind", "physical", "system", "even", "everything", "physical", "principle", "single", "body", "could", "shared", "multiple", "mind", "single", "mind", "could", "sequence", "body", "time", "thus", "larger", "issue", "personal", "identity", "relation", "mind", "body", "play", "debate", "searle", "critic", "searle", "view", "problem", "relation", "mind", "body", "rather", "simple", "solution", "conscious", "state", "caused", "lower", "level", "neurobiological", "process", "brain", "higher", "level", "feature", "brain", "searle", "2002b", "p", "9", "early", "discussion", "cra", "searle", "spoke", "causal", "power", "brain", "thus", "view", "appears", "brain", "state", "cause", "consciousness", "understanding", "consciousness", "feature", "brain", "ibid", "however", "seen", "even", "true", "begs", "question", "whose", "consciousness", "brain", "creates", "roger", "sperry", "splitbrain", "experiment", "suggest", "perhaps", "two", "center", "consciousness", "sense", "two", "mind", "implemented", "single", "brain", "display", "least", "language", "comprehension", "one", "typically", "created", "left", "hemisphere", "control", "language", "production", "thus", "many", "current", "approach", "understanding", "relation", "brain", "consciousness", "emphasize", "connectedness", "information", "flow", "see", "eg", "dehaene", "2014", "consciousness", "understanding", "feature", "person", "appears", "searle", "accepts", "metaphysics", "conscious", "self", "identical", "brain", "form", "mindbrain", "identity", "theory", "concrete", "metaphysics", "reflected", "searle", "original", "presentation", "cr", "argument", "strong", "ai", "described", "claim", "appropriately", "programmed", "computer", "really", "mind", "searle", "1980", "identity", "claim", "odd", "consequence", "b", "identical", "property", "property", "b", "computer", "physical", "object", "computer", "weigh", "6", "lb", "stereo", "speaker", "claim", "searle", "called", "strong", "ai", "would", "entail", "mind", "weigh", "6", "lb", "stereo", "speaker", "however", "seems", "clear", "human", "may", "weigh", "150", "pound", "human", "mind", "weigh", "150", "pound", "suggests", "neither", "body", "machine", "literally", "mind", "consideration", "support", "view", "mind", "abstract", "brain", "least", "one", "version", "claim", "searle", "call", "strong", "ai", "version", "say", "computer", "literally", "mind", "metaphysically", "untenable", "face", "apart", "thoughtexperiments", "searle", "cr", "argument", "thus", "directed", "claim", "computer", "mind", "suitably", "programmed", "digital", "computer", "understands", "language", "program", "searle", "thought", "experiment", "appeal", "strong", "intuition", "someone", "exactly", "computer", "would", "thereby", "come", "understand", "chinese", "noted", "many", "critic", "held", "searle", "quite", "right", "point", "matter", "program", "computer", "computer", "literally", "mind", "computer", "understand", "natural", "language", "mind", "physical", "object", "inability", "computer", "mind", "show", "running", "ai", "program", "produce", "understanding", "natural", "language", "something", "computer", "see", "section", "41", "functionalism", "theory", "relation", "mind", "body", "developed", "two", "decade", "prior", "searle", "cra", "functionalism", "alternative", "identity", "theory", "implicit", "much", "searle", "discussion", "well", "dominant", "behaviorism", "midtwentieth", "century", "functionalism", "correct", "appears", "intrinsic", "reason", "computer", "mental", "state", "hence", "cra", "conclusion", "computer", "intrinsically", "incapable", "mental", "state", "important", "consideration", "functionalism", "julian", "baggini", "2009", "37", "writes", "searle", "came", "perhaps", "famous", "counterexample", "history", "chinese", "room", "argument", "one", "intellectual", "punch", "inflicted", "much", "damage", "dominant", "theory", "functionalism", "many", "would", "argue", "never", "recovered", "functionalist", "hold", "mental", "state", "mental", "state", "causal", "functional", "role", "state", "play", "determines", "state", "functionalist", "might", "hold", "pain", "example", "state", "typically", "caused", "damage", "body", "located", "bodyimage", "aversive", "functionalist", "distance", "behaviorist", "identity", "theorist", "contrast", "former", "functionalist", "hold", "internal", "causal", "process", "important", "possession", "mental", "state", "thus", "functionalist", "may", "agree", "searle", "rejecting", "turing", "test", "behavioristic", "contrast", "identity", "theorist", "might", "eg", "hold", "pain", "identical", "cfiber", "firing", "functionalist", "hold", "mental", "state", "might", "variety", "physical", "system", "nonphysical", "cole", "foelber", "1984", "mind", "change", "material", "immaterial", "implementation", "neuron", "neuron", "thus", "identity", "theorist", "identify", "pain", "certain", "neuron", "firing", "functionalist", "identify", "pain", "something", "abstract", "higher", "level", "functional", "role", "might", "many", "different", "type", "underlying", "system", "functionalist", "accuse", "identity", "theorist", "substance", "chauvinism", "however", "functionalism", "remains", "controversial", "functionalism", "vulnerable", "chinese", "nation", "type", "objection", "discussed", "functionalist", "notoriously", "trouble", "explaining", "qualia", "problem", "highlighted", "apparent", "possibility", "inverted", "spectrum", "qualitatively", "different", "state", "might", "functional", "role", "eg", "block", "1978", "maudlin", "1989", "cole", "1990", "computationalism", "subspecies", "functionalism", "hold", "important", "causal", "role", "brain", "process", "information", "processing", "milkowski", "2017", "note", "computational", "approach", "fruitful", "cognitive", "science", "survey", "objection", "computationalism", "concludes", "majority", "target", "strawman", "version", "however", "jerry", "fodor", "early", "proponent", "computational", "approach", "argues", "fodor", "2005", "key", "mental", "process", "inference", "best", "explanation", "depend", "nonlocal", "property", "representation", "explained", "computational", "module", "brain", "fodor", "right", "understanding", "language", "interpretation", "appear", "involve", "global", "consideration", "linguistic", "nonlinguistic", "context", "theory", "mind", "might", "resist", "computational", "explanation", "reach", "searle", "conclusion", "basis", "different", "consideration", "searle", "2010", "statement", "conclusion", "cra", "showing", "computational", "account", "explain", "consciousness", "considerable", "interest", "decade", "since", "1980", "determining", "explain", "consciousness", "extremely", "active", "research", "area", "across", "discipline", "one", "interest", "neural", "correlate", "consciousness", "bear", "directly", "searle", "claim", "consciousness", "intrinsically", "biological", "computational", "information", "processing", "definitive", "answer", "yet", "though", "recent", "work", "anesthesia", "suggests", "consciousness", "lost", "cortical", "corticothalamic", "connection", "information", "flow", "disrupted", "eghudetz", "2012", "review", "article", "general", "basis", "consciousness", "confirmed", "relatively", "abstract", "level", "information", "flow", "neural", "network", "friendly", "functionalism", "turn", "lower", "biological", "subneuronal", "friendly", "searle", "account", "controversial", "biological", "metaphysical", "issue", "bear", "central", "inference", "chinese", "room", "argument", "intuition", "cr", "thought", "experiment", "would", "understand", "chinese", "running", "program", "searle", "infers", "understanding", "created", "running", "program", "clearly", "whether", "inference", "valid", "turn", "metaphysical", "question", "identity", "person", "mind", "person", "understanding", "identical", "room", "operator", "inference", "unsound", "54", "simulation", "duplication", "evolution", "discussing", "cra", "searle", "argues", "important", "distinction", "simulation", "duplication", "one", "would", "mistake", "computer", "simulation", "weather", "weather", "computer", "simulation", "digestion", "real", "digestion", "searle", "concludes", "serious", "mistake", "confuse", "computer", "simulation", "understanding", "understanding", "face", "generally", "important", "distinction", "simulation", "real", "thing", "two", "problem", "emerge", "clear", "distinction", "always", "made", "heart", "biological", "anything", "artificial", "heart", "simulation", "heart", "functional", "duplicate", "heart", "heart", "made", "different", "material", "walking", "normally", "biological", "phenomenon", "performed", "using", "limb", "artificial", "limb", "walk", "simulate", "walking", "robot", "walk", "property", "needed", "certain", "kind", "thing", "highlevel", "property", "anything", "sharing", "property", "thing", "kind", "even", "differs", "lower", "level", "property", "chalmers", "1996", "offer", "principle", "governing", "simulation", "replication", "chalmers", "suggests", "contra", "searle", "harnad", "1989", "simulation", "x", "x", "namely", "property", "x", "organizational", "invariant", "property", "depends", "functional", "organization", "underlying", "system", "detail", "copeland", "2002", "argues", "churchturing", "thesis", "entail", "brain", "every", "machine", "simulated", "universal", "turing", "machine", "brain", "machine", "might", "primitive", "operation", "simple", "clerical", "routine", "carried", "hand", "example", "might", "human", "brain", "likely", "display", "genuine", "lowlevel", "randomness", "whereas", "computer", "carefully", "designed", "computer", "resort", "pseudorandom", "number", "apparent", "randomness", "needed", "sprevak", "2007", "raise", "related", "point", "turing", "1938", "princeton", "thesis", "described", "machine", "omachines", "omachines", "machine", "include", "function", "natural", "number", "turingmachine", "computable", "brain", "machine", "say", "sprevak", "possibility", "searle", "chinese", "room", "argument", "successfully", "deployed", "functionalist", "hypothesis", "brain", "instantiates", "omachine", "120", "copeland", "discus", "simulation", "duplication", "distinction", "connection", "brain", "simulator", "reply", "argues", "searle", "correctly", "note", "one", "infer", "x", "simulates", "property", "p", "conclusion", "therefore", "x", "property", "p", "arbitrary", "p", "copeland", "claim", "searle", "commits", "simulation", "fallacy", "extending", "cr", "argument", "traditional", "ai", "apply", "computationalism", "contrapositive", "inference", "logically", "equivalent", "x", "simulates", "x", "p", "therefore", "p", "understands", "chinese", "faulty", "step", "cr", "operator", "simulates", "neural", "net", "n", "case", "understands", "chinese", "therefore", "case", "n", "understands", "chinese", "copeland", "also", "note", "result", "siegelmann", "sontag", "1994", "showing", "connectionist", "network", "simulated", "universal", "turing", "machine", "particular", "connection", "weight", "real", "number", "another", "problem", "simulationduplication", "distinction", "arising", "process", "evolution", "searle", "wish", "see", "original", "intentionality", "genuine", "understanding", "property", "certain", "biological", "system", "presumably", "product", "evolution", "computer", "merely", "simulate", "property", "time", "chinese", "room", "scenario", "searle", "maintains", "system", "exhibit", "behavior", "complex", "human", "behavior", "simulating", "degree", "intelligence", "language", "comprehension", "one", "imagine", "simulating", "ability", "deal", "world", "yet", "understand", "thing", "also", "say", "behaviorally", "complex", "system", "might", "implemented", "ordinary", "material", "example", "tube", "water", "valve", "creates", "biological", "problem", "beyond", "mind", "problem", "noted", "early", "critic", "cr", "argument", "may", "presuppose", "others", "mind", "evolution", "make", "presupposition", "selection", "force", "drive", "biological", "evolution", "select", "basis", "behavior", "evolution", "select", "ability", "use", "information", "environment", "creatively", "intelligently", "long", "manifest", "behavior", "organism", "overt", "difference", "behavior", "set", "circumstance", "system", "understands", "one", "evolution", "select", "genuine", "understanding", "seems", "searle", "account", "mind", "genuinely", "understand", "meaning", "advantage", "creature", "merely", "process", "information", "using", "purely", "computational", "process", "thus", "position", "implies", "simulation", "understanding", "biologically", "adaptive", "real", "thing", "leaf", "u", "puzzle", "system", "genuine", "understanding", "could", "evolve", "original", "intentionality", "genuine", "understanding", "become", "epiphenomenal", "conclusion", "seen", "since", "appearance", "1980", "chinese", "room", "argument", "sparked", "discussion", "across", "discipline", "despite", "extensive", "discussion", "still", "consensus", "whether", "argument", "sound", "one", "end", "julian", "baggini", "2009", "assessment", "searle", "came", "perhaps", "famous", "counterexample", "history", "chinese", "room", "argument", "one", "intellectual", "punch", "inflicted", "much", "damage", "dominant", "theory", "functionalism", "many", "would", "argue", "never", "recovered", "whereas", "philosopher", "daniel", "dennett", "2013", "p", "320", "concludes", "chinese", "room", "argument", "clearly", "fallacious", "misleading", "argument", "hence", "consensus", "whether", "argument", "proof", "limit", "aspiration", "artificial", "intelligence", "computational", "account", "mind", "meanwhile", "work", "artificial", "intelligence", "natural", "language", "processing", "continued", "cra", "led", "stevan", "harnad", "others", "quest", "symbol", "grounding", "ai", "many", "philosophy", "dretske", "fodor", "millikan", "worked", "naturalistic", "theory", "mental", "content", "speculation", "nature", "consciousness", "continues", "many", "discipline", "computer", "moved", "lab", "pocket", "wrist", "time", "searle", "construction", "argument", "personal", "computer", "limited", "hobbyist", "device", "weizenbaum", "eliza", "text", "adventure", "game", "played", "dec", "computer", "included", "limited", "parser", "advanced", "parsing", "language", "limited", "computer", "researcher", "schank", "much", "changed", "next", "quarter", "century", "billion", "use", "natural", "language", "interrogate", "command", "virtual", "agent", "via", "computer", "carry", "pocket", "chinese", "room", "argument", "moderated", "claim", "produce", "ai", "natural", "language", "system", "manufacturer", "linking", "device", "internet", "thing", "make", "modest", "claim", "appliance", "manufacturer", "lg", "say", "second", "decade", "21st", "century", "brings", "experience", "conversing", "major", "appliance", "may", "may", "conversing", "apple", "le", "cautious", "lg", "describing", "capability", "virtual", "personal", "assistant", "application", "called", "siri", "apple", "say", "siri", "understands", "say", "know", "mean", "ibm", "quick", "claim", "much", "larger", "watson", "system", "superior", "language", "ability", "siri", "2011", "watson", "beat", "human", "champion", "television", "game", "show", "jeopardy", "feat", "relies", "heavily", "language", "ability", "inference", "ibm", "go", "claim", "distinguishes", "watson", "know", "know", "know", "know", "appears", "claiming", "form", "reflexive", "selfawareness", "consciousness", "watson", "computer", "system", "thus", "claim", "strong", "ai", "hardly", "chastened", "anything", "stronger", "exuberant", "time", "seen", "many", "others", "believe", "chinese", "room", "argument", "showed", "best", "computer", "simulate", "human", "cognition", "though", "separated", "three", "century", "leibniz", "searle", "similar", "intuition", "system", "consider", "respective", "thought", "experiment", "leibniz", "mill", "chinese", "room", "case", "consider", "complex", "system", "composed", "relatively", "simple", "operation", "note", "impossible", "see", "understanding", "consciousness", "could", "result", "simple", "argument", "u", "service", "highlighting", "serious", "problem", "face", "understanding", "meaning", "mind", "many", "issue", "raised", "chinese", "room", "argument", "may", "settled", "consensus", "nature", "meaning", "relation", "syntax", "biological", "basis", "consciousness", "continues", "significant", "disagreement", "process", "create", "meaning", "understanding", "consciousness", "well", "proven", "priori", "thought", "experiment"]}