{
    "url": "causal-models",
    "title": "Causal Models",
    "authorship": {
        "year": "Copyright \u00a9 2018",
        "author_text": "Christopher Hitchcock\n<cricky@caltech.edu>",
        "author_links": [
            {
                "http://hss.divisions.caltech.edu/people/christopher-r-hitchcock": "Christopher Hitchcock"
            },
            {
                "mailto:cricky%40caltech%2eedu": "cricky@caltech.edu"
            }
        ],
        "raw_html": "<div id=\"article-copyright\">\n<p>\n<a href=\"../../info.html#c\">Copyright \u00a9 2018</a> by\n\n<br/>\n<a href=\"http://hss.divisions.caltech.edu/people/christopher-r-hitchcock\" target=\"other\">Christopher Hitchcock</a>\n&lt;<a href=\"mailto:cricky%40caltech%2eedu\"><em>cricky<abbr title=\" at \">@</abbr>caltech<abbr title=\" dot \">.</abbr>edu</em></a>&gt;\n    </p>\n</div>"
    },
    "pubinfo": [
        "First published Tue Aug 7, 2018"
    ],
    "preamble": "\n\nCausal models are mathematical models representing causal\nrelationships within an individual system or population. They\nfacilitate inferences about causal relationships from statistical\ndata. They can teach us a good deal about the epistemology of\ncausation, and about the relationship between causation and\nprobability. They have also been applied to topics of interest to\nphilosophers, such as the logic of counterfactuals, decision theory,\nand the analysis of actual causation.\n",
    "toc": [
        {
            "#Intr": "1. Introduction"
        },
        {
            "#BasiTool": "2. Basic Tools"
        },
        {
            "#VariLogiLang": "2.1 Variables, Logic, and Language"
        },
        {
            "#Prob": "2.2 Probability"
        },
        {
            "#Grap": "2.3 Graphs"
        },
        {
            "#DeteStruEquaMode": "3. Deterministic Structural Equation Models"
        },
        {
            "#IntrSEMs": "3.1 Introduction to SEMs"
        },
        {
            "#StruCoun": "3.2 Structural Counterfactuals"
        },
        {
            "#ActuCaus": "3.3 Actual Causation"
        },
        {
            "#ProbCausMode": "4. Probabilistic Causal Models"
        },
        {
            "#StruEquaModeRandErro": "4.1 Structural Equation Models with Random Errors"
        },
        {
            "#MarkCond": "4.2 The Markov Condition"
        },
        {
            "#MiniFaitCond": "4.3 The Minimality and Faithfulness Conditions"
        },
        {
            "#IdenCausStru": "4.4 Identifiability of Causal Structure"
        },
        {
            "#IdenAssuAbouFuncForm": "4.5 Identifiability with Assumptions about Functional Form"
        },
        {
            "#LateCommCaus": "4.6 Latent Common Causes"
        },
        {
            "#Inte": "4.7 Interventions"
        },
        {
            "#InteDeciTheo": "4.8. Interventionist Decision Theory"
        },
        {
            "#CausDiscInte": "4.9 Causal Discovery with Interventions"
        },
        {
            "#Coun": "4.10 Counterfactuals"
        },
        {
            "#FurtRead": "5. Further Reading"
        },
        {
            "#Bib": "Bibliography"
        },
        {
            "#Aca": "Academic Tools"
        },
        {
            "#Oth": "Other Internet Resources"
        },
        {
            "#Rel": "Related Entries"
        }
    ],
    "main_text": "\n1. Introduction\n\nCausal modeling is an interdisciplinary field that has its origin in\nthe statistical revolution of the 1920s, especially in the work of the\nAmerican biologist and statistician Sewall Wright (1921). Important\ncontributions have come from computer science, econometrics,\nepidemiology, philosophy, statistics, and other disciplines. Given the\nimportance of causation to many areas of philosophy, there has been\ngrowing philosophical interest in the use of mathematical causal\nmodels. Two major works\u2014Spirtes, Glymour, and Scheines 2000\n(abbreviated SGS), and Pearl 2009\u2014have been particularly\ninfluential.\n\nA causal model makes predictions about the behavior of a system. In\nparticular, a causal model entails the truth value, or the\nprobability, of counterfactual claims about the system; it predicts\nthe effects of interventions; and it entails the probabilistic\ndependence or independence of variables included in the model. Causal\nmodels also facilitate the inverse of these inferences: if we have\nobserved probabilistic correlations among variables, or the outcomes\nof experimental interventions, we can determine which causal models\nare consistent with these observations. The discussion will focus\non what it is possible to do in \u201cin principle\u201d. For\nexample, we will consider the extent to which we can infer the correct\ncausal structure of a system, given perfect information about the\nprobability distribution over the variables in the system. This\nignores the very real problem of inferring the true probabilities from\nfinite sample data. In addition, the entry will discuss the application of\ncausal models to the logic of counterfactuals, the analysis of\ncausation, and decision theory.\n2. Basic Tools\n\nThis section introduces some of the basic formal tools used in causal\nmodeling, as well as terminology and notational conventions.\n2.1 Variables, Logic, and Language\n\nVariables are the basic building blocks of causal models.\nThey will be represented by italicized upper case letters. A variable\nis a function that can take a variety of values. The values of a\nvariable can represent the occurrence or non-occurrence of an event, a\nrange of incompatible events, a property of an individual or of a\npopulation of individuals, or a quantitative value. For instance, we\nmight want to model a situation in which Suzy throws a stone and a\nwindow breaks, and have variables S and W such that:\n\n\\(S = 1\\) represents Suzy throwing a rock; \\(S = 0\\) represents\nher not throwing\n\\(W = 1\\) represents the window breaking; \\(W = 0\\) represents the\nwindow remaining intact.\n\n\nIf we are modeling the influence of education on income in the United\nStates, we might use variables E and I such that:\n\n\\(E(i) = 0\\) if individual i has no high school education;\n1 if an individual has completed high school; 2 if an individual has\nhad some college education; 3 if an individual has a bachelor\u2019s\ndegree; 4 if an individual has a master\u2019s degree; and 5 if an\nindividual as a doctorate (including the highest degrees in law and\nmedicine).\n\\(I(i) = x\\) if individual i has a pre-tax income of\n$x per year.\n\n\nThe set of possible values of a variable is the range of that\nvariable. We will usually assume that variables have finitely many\npossible values, as this will keep the mathematics and the exposition\nsimpler. However, causal models can also feature continuous variables,\nand in some cases this makes an important difference.\n\nA world is a complete specification of a causal model; the\ndetails will depend upon the type of model. For now, we note that a\nworld will include, inter alia, an assignment of values to\nall of the variables in the model. If the variables represent the\nproperties of individuals in a population, a world will include an\nassignment of values to every variable, for every individual in the\npopulation. A variable can then be understood as a function whose\ndomain is a set of worlds, or a set of worlds and individuals.\n\nIf X is a variable in a causal model, and x is a\nparticular value in the range of X, then \\(X = x\\) is an\natomic proposition. The logical operations of negation\n(\u201cnot\u201d), conjunction (\u201cand\u201d), disjunction\n(\u201cor\u201d), the material conditional\n(\u201cif\u2026then\u2026\u201d), and the biconditional\n(\u201cif and only if\u201d) are represented by\n\u201c\\({\\sim}\\)\u201d, \u201c&\u201d, \u201c\\(\\lor\\)\u201d,\n\u201c\\(\\supset\\)\u201d, and \u201c\\(\\equiv\\)\u201d\nrespectively. Any proposition built out of atomic propositions and\nthese logical operators will be called a Boolean\nproposition. Note that when the variables are defined over individuals\nin a population, reference to an individual is not included\nin a proposition; rather, the proposition as a whole is true or false\nof the various individuals in the population.\n\nWe will use basic notation from set theory. Sets will appear in\nboldface.\n\n\\(\\mathbf{\\varnothing}\\) is the empty set (the set that has no\nmembers or elements).\n\\(x \\in \\bX\\) means that x is a member or element of the\nset \\(\\bX\\).\n\\(\\bX \\subseteq \\bY\\) means that \\(\\bX\\) is a subset of \\(\\bY\\);\ni.e., every member of \\(\\bX\\) is also a member of \\(\\bY\\). Note that\nboth \\(\\mathbf{\\varnothing}\\) and \\(\\bY\\) are subsets of \\(\\bY\\).\n\\(\\bX \\setminus \\bY\\) is the set that results from removing the\nmembers of \\(\\bY\\) from \\(\\bX\\).\n\\(\\forall\\) and \\(\\exists\\) are the universal and existential\nquantifiers, respectively.\n\n\nIf \\(\\bS = \\{x_1 , \\ldots ,x_n\\}\\) is a set of values in the range of\nX, then \\(X \\in \\bS\\) is used as shorthand for the disjunction\nof propositions of the form \\(X = x_i\\), for \\(i = 1,\\ldots\\), n.\nBoldface represents ordered sets or vectors. If\n\\(\\bX = \\{X_1 , \\ldots ,X_n\\}\\) is a vector of variables, and \\(\\bx =\n\\{x_1 , \\ldots ,x_n\\}\\) is a vector of values, with each value \\(x_i\\)\nin the range of the corresponding variable \\(X_i\\), then \\(\\bX = \\bx\\)\nis the conjunction of propositions of the form \\(X_i = x_i\\).\n2.2 Probability\n\nIn\n section 4,\n we will consider causal models that include probability. Probability\nis a function, P, that assigns values between zero and one, inclusive.\nThe domain of a probability function is a set of propositions that\nwill include all of the Boolean propositions described above, but\nperhaps others as well.\n\nSome standard properties of probability are the following:\n\nIf A is a contradiction, then \\(\\Pr(A) = 0\\).\nIf A is a tautology, then \\(\\Pr(A) = 1\\).\nIf \\(\\Pr(A \\amp B) = 0\\), then \\(\\Pr(A \\lor B) = \\Pr(A) +\n\\Pr(B)\\).\n\\(\\Pr({\\sim}A) = 1 - \\Pr(A)\\).\n\n\nSome further definitions:\n\n\n\nThe conditional probability of A given B, written \\(\\Pr(A\n\\mid B)\\) is standardly defined as follows: \n\n\\[\n\\Pr(A \\mid B) = \\frac{\\Pr(A \\amp B)}{\\Pr(B)}. \n\\]\n\n\nWe will ignore problems that might arise when \\(\\Pr(B) = 0\\).\n\nA and B are probabilistically independent\n(with respect to \\(\\Pr\\)) just in case \\(\\Pr(A \\amp B) = \\Pr(A) \\times\n\\Pr(B). A\\) and B are probabilistically dependent or\ncorrelated otherwise. If \\(\\Pr(B) \\gt 0\\), then A and\nB will be independent just in case \\(\\Pr(A \\mid B) =\n\\Pr(A)\\).\nVariables X and Y are probabilistically independent\njust in case all propositions of the form \\(X = x\\) and \\(Y = y\\) are\nprobabilistically independent.\nA and B are probabilistically independent\nconditional on C just in case \\(\\Pr(A \\amp B \\mid C) =\n\\Pr(A \\mid C) \\times \\Pr(B \\mid C)\\). If \\(\\Pr(B \\amp C) \\gt 0\\), this\nis equivalent to \\(\\Pr(A \\mid B \\amp C) = \\Pr(A \\mid C)\\). Following\nthe terminology of Reichenbach (1956), we will also say that\nC screens off B from A when these\nequalities hold. Conditional independence among variables is defined\nanalogously.\n\n\nAs a convenient shorthand, a probabilistic statement that contains\nonly a variable or set of variables, but no values, will be understood\nas having a universal quantification over all possible values of the\nvariable(s). Thus if \\(\\bX = \\{X_1 ,\\ldots ,X_m\\}\\) and \\(\\bY = \\{Y_1\n,\\ldots ,Y_n\\}\\), we may write \n\n\\[\n\\Pr(\\bX \\mid \\bY) = \\Pr(\\bX)\n\\]\n\n\nas shorthand for \n\n\\[\n\\begin{aligned}\n\\forall x_1 \\ldots \\forall x_m\\forall y_1 \\ldots \\forall y_n \n[\\Pr(X_1 =x_1 ,\\ldots ,X_m =x_m \\mid Y_1 =y_1 ,\\ldots ,Y_n =y_n) \\\\\n= \\Pr(X_1 =x_1 ,\\ldots ,X_m =x_m)]\n\\end{aligned}\n\\]\n\n\nwhere the domain of quantification for each variable will be the range\nof the relevant variable.\n\nWe will not presuppose any particular interpretation of probability\n(see the entry on\n interpretations of probability),\n but we will assume that frequencies in appropriately chosen samples\nprovide evidence about the underlying probabilities. For instance,\nsuppose there is a causal model including the variables E\nand I described above, with \\(\\Pr(E = 3) = .25\\). Then we expect\nthat if we survey a large, randomly chosen sample of American adults, we\nwill find that approximately a quarter of them have a Bachelor\u2019s\ndegree, but no higher degree. If the survey produces a sample frequency\nthat substantially differs from this, we have evidence that the model\nis inaccurate.\n2.3 Graphs\n\nIf \\(\\bV\\) is the set of variables included in a causal model, one way\nto represent the causal relationships among the variables in \\(\\bV\\)\nis by a graph. Although we will introduce and use graphs in\n section 3,\n they will play a more prominent role in\n section 4.\n We will discuss two types of graphs. The first is the directed\nacyclic graph (DAG). A directed graph \\(\\bG\\) on\nvariable set \\(\\bV\\) is a set of ordered pairs of variables in\n\\(\\bV\\). We represent this visually by drawing an arrow from X\nto Y just in case \\(\\langle X, Y\\rangle\\) is in \\(\\bG\\).\n Figure 1\n shows a directed graph on variable set \\(\\bV = \\{S, T, W, X, Y,\nZ\\}\\).\n\n\n\nFigure 1\n\n\nA path in a directed graph is a non-repeating sequence of\narrows that have endpoints in common. For example, in\n Figure 1\n there is a path from X to Z, which we can write as \\(X\n\\leftarrow T \\rightarrow Y \\rightarrow Z\\). A directed path\nis a path in which all the arrows point in the same direction; for\nexample, there is a directed path \\(S \\rightarrow T \\rightarrow Y\n\\rightarrow Z\\). A directed graph is acyclic, and hence a\nDAG, if there is no directed path from a variable to itself. Such a\ndirected path is called a cycle. The graph in Figure 1\ncontains no cycles, and hence is a DAG.\n\nThe relationships in the graph are often described using the language\nof genealogy. The variable X is a parent of Y\njust in case there is an arrow from X to Y. \\(\\bPA(Y)\\)\nwill denote the set of all parents of Y. In\n Figure 1,\n \\(\\bPA(Y) = \\{T, W\\}\\). X is an ancestor of Y\n(and Y is a descendant of \\(X)\\) just in case there is\na directed path from X to Y. However, it will be\nconvenient to deviate slightly from the genealogical analogy and\ndefine \u201cdescendant\u201d so that every variable is a descendant\nof itself. \\(\\bDE(X)\\) denotes the set of all descendants of X.\nIn Figure 1, \\(\\bDE(T) = \\{T,X, Y, Z\\}\\).\n\nAn arrow from Y to Z in a DAG represents that Y\nis a direct cause of \\(Z.\\) Roughly, this means that the\nvalue of Y makes some causal difference for the value of\nZ, and that Y influences Z through some process\nthat is not mediated by any other variable in \\(\\bV\\). Directness is\nrelative to a variable set: Y may be a direct cause of Z\nrelative to variable set \\(\\bV\\), but not relative to variable set\n\\(\\bV'\\) that includes some additional variable(s) that mediate the\ninfluence of Y on \\(Z.\\) As we develop our account of graphical\ncausal models in more detail, we will be able to say more precisely\nwhat it means for one variable to be a direct cause of another. While\nwe will not define \u201ccause\u201d, causal models presuppose a\nbroadly difference-making notion of causation, rather than a\ncausal process notion (Salmon 1984, Dowe 2000) or a mechanistic notion\n(Machamer, Darden, & Craver 2000; Glennan 2017). We will call the\nsystem of direct causal relations represented in a DAG such as\n Figure 1\n the causal structure on the variable set \\(\\bV\\).\n\nA second type of graph that we will consider is an acyclic\ndirected mixed graph (ADMG). An ADMG will contain double-headed\narrows, as well as single-headed arrows. A double-headed arrow\nrepresents a latent common cause. A latent common cause of\nvariables X and Y is a common cause that is not included\nin the variable set \\(\\bV\\). For example, suppose that X and\nY share a common cause L\n (Figure 2(a)).\n An ADMG on the variable set \\(\\bV = \\{X, Y\\}\\) will look like\n Figure 2(b).\n\n\n\n\n\n(a)\n\n\n\n\n(b)\n\n\n\nFigure 2\n\n\nWe can be a bit more precise. We only need to represent missing common\ncauses in this way when they are closest common causes. That\nis, a graph on \\(\\bV\\) should contain a double-headed arrow between\nX and Y when there is a variable L that is\nomitted from \\(\\bV\\), such that if L were added to \\(\\bV\\) it\nwould be a direct cause of X and Y.\n\nIn an ADMG, we expand the definition of a path to include\ndouble-headed arrows. Thus, \\(X \\leftrightarrow Y\\) is a path in the\nADMG shown in\n Figure 2(b).\n Directed path retains the same meaning, and a directed path\ncannot contain double-headed arrows.\n\nWe will adopt the convention that both DAGs and ADMGs represent the\npresence and absence of both direct causal relationships and\nlatent common causes. For example the DAG in\n Figure 1\n represents that W is a direct cause of Y, that X\nis not a direct cause of Y, and that there are no latent\ncommon causes. The absence of double-headed arrows from Figure 1\ndoes not show merely that we have chosen not to include latent common\ncauses in our representation; it shows that there are no latent common\ncauses.\n3. Deterministic Structural Equation Models\n\nIn this section, we introduce deterministic structural equation\nmodels (SEMs), postponing discussion of probability until\n Section 4.\n We will consider two applications of deterministic SEMs: the logic of\ncounterfactuals, and the analysis of actual causation.\n3.1 Introduction to SEMs\n\nA SEM characterizes a causal system with a set of variables, and a set\nof equations describing how each variable depends upon its immediate\ncausal predecessors. Consider a gas grill, used to cook meat. We can\ndescribe the operations of the grill using the following\nvariables:\n\nGas connected (1 if yes, 0 if no)\nGas knob (0 for off, 1 for low, 2 for medium, 3 for\nhigh)\nGas level (0 for off, 1 for low, 2 for medium, 3 for\nhigh)\nIgniter (1 if pressed, 0 if not)\nFlame (0 for off, 1 for low, 2 for medium, 3 for\nhigh)\nMeat on (0 for no, 1 for yes)\nMeat cooked (0 for raw, 1 for rare, 2 for medium, 3 for\nwell done)\n\n\nThus, for example, Gas knob = 1 means that the gas knob is\nset to low; Igniter = 1 means that the igniter is pressed,\nand so on. Then the equations might be:\n\nGas level = Gas connected \\(\\times\\) Gas\nknob\nFlame = Gas level \\(\\times\\)\nIgniter\nMeat cooked = Flame \\(\\times\\) Meat\non\n\n\nThe last equation, for example, tells us that if the meat is not put\non the grill, it will remain raw (Meat cooked = 0). If the\nmeat is put on the grill, then it will get cooked according to the\nlevel of the flame: if the flame is low (Flame = 1), the meat\nwill be rare (Meat cooked = 1), and so on.\n\nBy convention each equation has one effect variable on the left hand\nside, and one or more cause variables on the right hand side. We also\nexclude from each equation any variable that makes no difference to\nthe value of the effect variable. For example, the equation for\nGas level could be written as Gas level = (Gas\nconnected \\(\\times\\) Gas knob) \\(+\\) (0 \\(\\times\\) Meat\ncooked); but since the value of Meat cooked makes no\ndifference to the value of Gas level in this equation, we\nomit the variable Meat cooked. A SEM is acyclic if\nthe variables can be ordered so that variables never appear on the\nleft hand side of an equation after they have appeared on the right.\nOur example is acyclic, as shown by the ordering of variables given\nabove. In what follows, we will assume that SEMs are acyclic, unless\nstated otherwise.\n\nWe can represent this system of equations as a DAG\n (Figure 3):\n\n\n\nFigure 3\n\n\nAn arrow is drawn from variable X to variable Y\njust in case X figures as an argument in the equation for\nY. The graph contains strictly less information than the set of\nequations; in particular, the DAG gives us qualitative information\nabout which variables depend upon which others, but it does not tell\nus anything about the functional form of the dependence.\n\nThe variables in a model will typically depend upon further variables\nthat are not explicitly included in the model. For instance, the level\nof the flame will also depend upon the presence of oxygen. Variables that\nare not explicitly represented in the model are assumed to be fixed at\nvalues that make the equations appropriate. For example, in our model\nof the gas grill, oxygen is assumed to be present in sufficient\nquantity to sustain a flame ranging in intensity from low to high.\n\nIn our example, the variables Gas level, Flame, and\nMeat cooked are endogenous, meaning that their\nvalues are determined by other variables in the model. Gas\nconnected, Gas knob, Igniter, and Meat on are\nexogenous, meaning that their values are determined outside\nof the system. In all of the models that we will consider in\n section 3,\n the values of the exogenous variables are given or otherwise\nknown.\n\nFollowing Halpern (2016), we will call an assignment of values to the\nexogenous variables a context. In an acyclic SEM, a context\nuniquely determines the values of all the variables in the model. An\nacyclic SEM together with a context is a world (what Halpern\n2016 calls a \u201ccausal setting\u201d). For instance, if we add\nthe setting\n\nGas connected = 1\nGas knob = 3\nIgniter = 1\nMeat on = 1\n\n\nto our three equations above, we get a world in which Gas\nlevel = 3, Flame = 3, and Meat cooked = 3.\n\nThe distinctively causal or \u201cstructural\u201d content of a SEM\nderives from the way in which interventions are represented.\nTo intervene on a variable is to set the value of that variable by a\nprocess that overrides the usual causal structure, without interfering\nwith the causal processes governing the other variables. More\nprecisely, an intervention on a variable X overrides the normal\nequation for X, while leaving the other equations unchanged.\nFor example, to intervene on the variable Flame in our\nexample would be to set the flame to a specified level regardless of\nwhether the igniter is pressed or what the gas level is. (Perhaps, for\nexample, one could pour kerosene into the grill and light it with a\nmatch.) Woodward (2003) proposes that we think of an intervention as a\ncausal process that operates independently of the other variables in\nthe model. Randomized controlled trials aim to intervene in this\nsense. For example, a randomized controlled trial to test the efficacy\nof a drug for hypertension aims to determine whether each subject\ntakes the drug (rather than a placebo) by a random process such as a\ncoin flip. Factors such as education and health insurance that\nnormally influence whether someone takes the drug no longer play this\nrole for subjects in the trial population. Alternately, we could\nfollow the approach of Lewis (1979) and think of an intervention\nsetting the value of a variable by a minor \u201cmiracle\u201d.\n\nTo represent an intervention on a variable, we replace the\nequation for that variable with a new equation stating the value to\nwhich the variable is set. For example, if we intervene to set the\nlevel of flame at low, we would represent this by replacing\nthe equation Flame = Gas level \\(\\times\\)\nIgniter with Flame = 1. This creates a new causal\nstructure in which Flame is an exogenous variable;\ngraphically, we can think of the intervention as \u201cbreaking the\narrows\u201d pointing into Flame. The new system of\nequations can then be solved to discover what values the other\nvariables would take as a result of the intervention. In the world\ndescribed above, our intervention would produce the following set of\nequations:\n\nGas connected = 1\nGas knob = 3\nIgniter = 1\nMeat on = 1\nGas level = Gas connected \u00d7 Gas\nknob\nFlame = Gas level \u00d7 Igniter\nFlame = 1\nMeat cooked = Flame \u00d7 Meat\non\n\n\nWe have struck through the original equation for Flame to\nshow that it is no longer operative. The result is a new world with a\nmodified causal structure, with Gas level = 3, Flame\n= 1, and Meat cooked = 1. Since the equation connecting\nFlame to its causes is removed, any changes introduced by\nsetting Flame to 1 will only propagate forward through the\nmodel to the descendants of Flame. The intervention changes\nthe values of Flame and Meat cooked, but it does not\naffect the values of the other variables. We can represent\ninterventions on multiple variables in the same way, replacing the\nequations for all of the variables intervened on.\n\nInterventions help to give content to the arrows in the corresponding\nDAG. If variable \\(X_i\\) is a parent of \\(X_j\\), this means that there\nexists some setting for all of the other variables in the model, such\nthat when we set those variables to those values by means of an\nintervention, intervening on \\(X_i\\) can still make a difference for\nthe value of \\(X_j\\). For example, in our original model, Gas\nlevel is a parent of Flame. If we set the value of\nIgniter to 1 by means of an intervention, and set Gas\nknob, Gas connected, Meat on, and Meat cooked to any\nvalues at all, then intervening on the value of Gas level\nmakes a difference for the value of Flame. Setting the value\nof Gas level to 1 would yield a value of 1 for\nFlame; setting Gas level to 2 yields a\nFlame of 2; and so on.\n3.2 Structural Counterfactuals\n\nA counterfactual is a proposition in the form of a subjunctive\nconditional. The antecedent posits some circumstance, typically one\nthat is contrary to fact. For example, in our gas grill world, the\nflame was high, and the meat was well done. We might reason: \u201cif\nthe flame had been set to low, the meat would have been rare\u201d.\nThe antecedent posits a hypothetical state of affairs, and the\nconsequent describes what would have happened in that hypothetical\nsituation.\n\nDeterministic SEMs naturally give rise to a logic of counterfactuals.\nThese counterfactuals are called structural counterfactuals\nor interventionist counterfactuals. Structural\ncounterfactuals are similar in some ways to what Lewis (1979) calls\nnon-backtracking counterfactuals. In a non-backtracking\ncounterfactual, one does not reason backwards from a counterfactual\nsupposition to draw conclusions about the causes of the hypothetical\nsituation. For instance, one would not reason \u201cIf the meat had\nbeen cooked rare, then the flame would have been set to low\u201d.\nLewis (1979) proposes that we think of the antecedent of a\ncounterfactual as coming about through a minor \u201cmiracle\u201d.\nThe formalism for representing interventions described in the previous\nsection prevents backtracking from effects to causes.\n\nThe logic of structural counterfactuals has been developed by Galles\nand Pearl (1998), Halpern (2000), Briggs (2012), and Zhang\n(2013a). This section will focus on Briggs\u2019 formulation; it has\nthe richest language, but unlike the other approaches it can not be\napplied to causal models with cycles. Despite a shared concern with\nnon-backtracking counterfactuals, Briggs\u2019 logic differs in a\nnumber of ways from the more familiar logic of counterfactuals\ndeveloped by Stalnaker (1968) and Lewis (1973b).\n\nWe interpret the counterfactual conditional \\(A \\boxright B\\) as saying\nthat B would be true, if A were made true by an\nintervention. The language of structural counterfactuals does not\nallow the connective \u2018\\(A \\boxright B\\)\u2019 to appear in the antecedents of\ncounterfactuals. More precisely, we define well-formed formulas\n(wffs) for the language inductively:\n\nBoolean propositions are wffs\nIf A is a Boolean proposition, and B is a\nwff, then \\(A \\boxright B\\) is a wff\n\n\nThis means, for example, that \\(A \\boxright (B\\boxright (C\\boxright D))\\)\nis a wff, but \\(A\\boxright ((B\\boxright C)\\boxright D)\\) is not,\nsince the embedded counterfactual in the consequent does not have a\nBoolean proposition as an antecedent.\n\nConsider the world of the gas grill, described in the previous\nsection:\n\nGas connected = 1\nGas knob = 3\nIgniter = 1\nMeat on = 1\nGas level = Gas connected \\(\\times\\) Gas\nknob\nFlame = Gas level \\(\\times\\)\nIgniter\nMeat cooked = Flame \\(\\times\\) Meat\non\n\n\nTo evaluate the counterfactual \\({\\textit{Flame} = 1} \\boxright\n{\\textit{Meat cooked} = 1}\\) (if the flame had been set to\nlow, the meat would have been cooked rare), we replace the\nequation for Flame with the assignment Flame = 1. We\ncan then compute that Meat cooked = 1; the counterfactual is\ntrue. If the antecedent is a conjunction of atomic propositions, such\nas Flame = 1 and Igniter = 0, we replace all of the\nrelevant equations. A special case occurs when the antecedent conjoins\natomic propositions that assign different values to the same variable,\nsuch as Flame = 1 and Flame = 2. In this case, the\nantecedent is a contradiction, and the counterfactual is considered\ntrivially true.\n\nIf the antecedent is a disjunction of atomic propositions, or a\ndisjunction of conjunctions of atomic propositions, then the\nconsequent must be true when every possible intervention or\nset of interventions described by the antecedent is performed.\nConsider, for instance,  \n\n\\[\n\\begin{aligned}\n(({\\textit{Flame}= 1} \\amp {\\textit{Gas level}= 0}) \\lor ({\\textit{Flame}= 2} \\amp {\\textit{Meat on}= 0}))\\\\\n{} \\boxright\n ({\\textit{Meat cooked}= 1} \\lor {\\textit{Meat cooked}= 2}).\n\\end{aligned}\n\\]\n\n\nIf we perform the first intervention, we compute that Meat\ncooked = 1, so the consequent is true. However, if we perform the\nsecond intervention, we compute that Meat cooked = 0. Hence\nthe counterfactual comes out false. Some negations are treated as\ndisjunctions for this purpose. For example, \\({\\sim}(\\textit{Flame} =\n1)\\) would be treated in the same way as the disjunction\n\n \\[{\\textit{Flame} = 0} \\lor {\\textit{Flame} = 2} \\lor {\\textit{Flame} = 3}.\\]\n\n\nIf the consequent contains a counterfactual, we iterate the procedure.\nConsider the counterfactual:  \n\n\\[\n{\\textit{Flame} = 1} \\boxright  ({\\textit{Gas level} = 0} \\boxright  ({\\textit{Flame} = 2} \\boxright {\\textit{Meat cooked} = 1})).\n\\]\n\n\nTo evaluate this counterfactual, we first change the equation for\nFlame to Flame = 1. Then we change the equation for\nGas level to Gas level = 0. Then we change the\nequation for Flame again, from Flame = 1, to\nFlame = 2. Finally, we compute that Meat cooked = 2,\nso the counterfactual comes out false. Unlike the case where\nFlame = 1 and Flame = 2 are conjoined in the\nantecedent, the two different assignments for Flame do not\ngenerate an impossible antecedent. In this case, the interventions are\nperformed in a specified order: Flame is first set to 1, and\nthen set to 2.\n\nThe differences between structural counterfactuals and Stalnaker-Lewis\ncounterfactuals stem from the following two features of structural\ncounterfactuals:\n\nThe antecedent of a counterfactual is always thought of as being\nrealized by an intervention, even if the antecedent is already true in\na given world. For instance, in our gas grill world, Flame =\n3. Nonetheless, if we evaluate a counterfactual with antecedent\nFlame = 3 in this world, we replace the equation for\nFlame with Flame = 3.\n\n\nThe truth values of counterfactuals are determined solely by the\ncausal structures of worlds, together with the interventions specified\nin the their antecedents. No further considerations of\nsimilarity play a role. For example, the counterfactual \n\n\\[  \n{\\textit{Flame}= 1} \\lor {\\textit{Flame} = 2} \\boxright {\\textit{Flame}= 2} \n\\]\n\n\nwould be false in our gas grill world (and indeed in all possible\nworlds). We do not reason that a world in which Flame = 2 is\ncloser to our world (in which Flame = 3) than a\nworld in which Flame = 1.\n\n\nThese features of structural counterfactuals lead to some unusual\nproperties in the full language developed by Briggs (2012):\n\nThe analog of modus ponens fails for the structural\nconditional; i.e., from A and \\(A\\boxright B\\) we cannot infer\nB. For example, in our gas grill world, Flame = 3 and\n\n\\[\n  {\\textit{Flame} = 3} \\boxright {({\\textit{Gas level} = 2} \\boxright {\\textit{Meat cooked} = 3})} \n  \\]\n\n are both true, but \n\n\\[\n{\\textit{Gas level} = 2} \\boxright {\\textit{Meat cooked} = 3} \n\\]\n\n is false. To evaluate the\nlast counterfactual, we replace the equation for Gas level\nwith Gas level = 2. This results in Flame = 2 and\nMeat cooked = 2. To evaluate the prior counterfactual, we\nfirst substitute in the equation Flame = 3. Now, the value of\nFlame no longer depends upon the value of Gas level,\nso when we substitute in Gas level = 2, we get Meat\ncooked = 3. Even though the actual value of Flame is 3,\nwe evaluate the counterfactual by intervening on Flame to set\nit to 3. With this intervention in place, a further intervention on\nGas level makes no difference to Flame or Meat\ncooked.\nThe substitution of logically equivalent propositions into the\nantecedent of a counterfactual does not always preserve truth value.\nFor example, \n\n\\[\n{\\textit{Gas level} = 2} \\boxright {\\textit{Meat cooked} = 2} \n\\]\n\n is true, but \n\n\\[\\begin{align}\n& ({\\textit{Gas level} = 2} \\amp ({\\textit{Flame} = 2} \\lor {{\\sim}(\\textit{Flame} =2))}\\\\\n& \\qquad \\boxright\\, {\\textit{Meat cooked} = 2} \n\\end{align}\\]\n\n is false. The\nfirst counterfactual does not require us to intervene on the value of\nFlame, but the second counterfactual requires us to consider\ninterventions that set Flame to all of its possible\nvalues.\n\n\nTo handle the second kind of case, Briggs (2012) defines a relation of\nexact equivalence among Boolean propositions using\nthe state space semantics of Fine (2012). Within a world, the state\nthat makes a proposition true is the collection of values of variables\nthat contribute to the truth of the proposition. In our example world,\nthe state that makes Gas level = 3 true is the valuation\nGas level = 3. By contrast, the state that makes \n\n\\[\\textit{Gas level} = 3 \\amp (\\textit{Flame} = 2 \\lor {\\sim}(\\textit{Flame} = 2))\\] \n\n\ntrue includes both Gas level = 3 and Flame = 3.\nPropositions are exactly equivalent if they are made true by the same\nstates in all possible worlds. The truth value of a counterfactual is\npreserved when exactly equivalent propositions are substituted into\nthe antecedent.\n\nBriggs (2012) provides a sound and complete axiomatization for\nstructural counterfactuals in acyclic SEMs. The axioms and inference\nrules of this system are presented in\n Supplement on Briggs\u2019 Axiomatization.\n3.3 Actual Causation\n\nMany philosophers and legal theorists have been interested in the\nrelation of actual causation. This concerns the assignment of\ncausal responsibility for some event that occurs, based on how events\nactually play out. For example, suppose that Billy and Suzy are both\nholding rocks. Suzy throws her rock at a window, but Billy does not.\nSuzy\u2019s rock hits the window, which breaks. Then Suzy\u2019s\nthrow was the actual cause of the window breaking.\n\nWe can represent this story easily enough with a structural equation\nmodel. For variables, we choose:\n\n\\(B = 1\\) if Billy throws, 0 if he doesn\u2019t\n\\(S = 1\\) if Suzy throws, 0 if she doesn\u2019t\n\\(W = 1\\) if the window shatters, 0 if it doesn\u2019t\n\n\nOur context and equation will then be:\n\n\\(B = 0\\)\n\\(S = 1\\)\n\\(W = \\max(B, S)\\)\n\n\nThe equation for W tells us that the window would shatter if\neither Billy or Suzy throws their rock. The corresponding DAG is shown\nin\n Figure 4\n\n\n\n\nFigure 4\n\n\nBut we cannot simply read off the the relation of actual causation\nfrom the graph or from the equations. For example, the arrow from\nB to W in\n Figure 4\n cannot be interpreted as saying that Billy\u2019s (in)action is an\nactual cause of the window breaking. Note that while it is common to\ndistinguish between singular or token causation, and general or\ntype-level causation (see, e.g., Eells 1991, Introduction), that is\nnot what is at issue here. Our causal model does not represent any\nkind of causal generalization: it represents the actual and possible\nactions of Billy and Suzy at one particular place and time. Actual\ncausation is not just causal structure of the single case. A further\ncriterion for actual causation, defined in terms of the causal\nstructure together with the actual values of the variables, is\nneeded.\n\nFollowing Lewis (1973a), it is natural to try to analyze the relation\nof actual causation in terms of counterfactual dependence. In\nour model, the following propositions are all true:\n\n\\(S = 1\\)\n\\(W = 1\\)\n\\({S = 0}\\boxright {W = 0}\\)\n\n\nIn words: Suzy threw her rock, the window shattered, and if Suzy\nhadn\u2019t thrown her rock, the window wouldn\u2019t have\nshattered. In general, we might attempt to analyze actual causation as\nfollows:\n\n\n\\(X = x\\) is an actual cause of \\(Y = y\\) in world w just in\ncase:\n\nX and Y are different variables\n\\(X = x\\) and \\(Y = y\\) in w\nThere exist \\(x' \\ne x\\) and \\(y' \\ne y\\) such that \\({X = x'}\n\\boxright {Y = y'}\\) is true in w\n\n\n\nUnfortunately, this simple analysis will not work, for familiar\nreasons involving preemption and overdetermination.\nHere is an illustration of each:\n\n\nPreemption: Billy decides that he will give Suzy the\nopportunity to throw first. If Suzy throws her rock, he will not\nthrow, but if she doesn\u2019t throw, he will throw and his rock will\nshatter the window. In fact, Suzy throws her rock, which shatters the\nwindow. Billy does not throw.\n\nOverdetermination: Billy and Suzy throw their rocks\nsimultaneously. Their rocks hit the window at the same time,\nshattering it. Either rock by itself would have been sufficient to\nshatter the window.\n\n\nIn both of these cases, Suzy\u2019s throw is an actual cause\nof the window\u2019s shattering, but the shattering does not\ncounterfactually depend upon her throw: if Suzy hadn\u2019t thrown\nher rock, Billy\u2019s rock would have shattered the window. Much of\nthe work on counterfactual theories of causation since 1973 has been\ndevoted to addressing these problems.\n\nA number of authors have used SEMs to try to formulate adequate\nanalyses of actual causation in terms of counterfactuals, including\nBeckers & Vennekens (2018), Glymour & Wimberly (2007), Halpern\n(2016), Halpern & Pearl (2001, 2005), Hitchcock (2001), Pearl\n(2009: Chapter 10), Weslake (forthcoming), and Woodward (2003: Chapter\n2). As an illustration, consider one analysis based closely on\na proposal presented in Halpern (2016):\n\n\n(AC) \\(X = x\\) is an actual cause of \\(Y = y\\) in\nworld w just in case:\n\n\nX and Y are different variables\n\\(X = x\\) and \\(Y = y\\) in w\nThere exist disjoint sets of variables \\(\\bX\\) and \\(\\bZ\\) with\n\\(X \\in \\bX\\), with values \\(\\bX = \\bx\\) and \\(\\bZ = \\bz\\) in\nw, such that:\n\n\nThere exists \\(\\bx' \\ne \\bx\\) such that \n\n\\[({\\bX = \\bx'} \\amp {\\bZ = \\bz}) \\boxright  {Y \\ne y}\\] \n\n is true in\nw\nNo subset of \\(\\bX\\) satisfies (3a)\n\n\n\n\n\nThat is, X belongs to a minimal set of variables \\(\\bX\\), such\nthat when we intervene to hold the variables in \\(\\bZ\\) fixed at the\nvalues they actually take in w, Y counterfactually\ndepends upon the values of the variables in \\(\\bX.\\) We will\nillustrate this account with our examples of preemption and\noverdetermination.\n\nIn Preemption, let the variables B, S, and\nW be defined as above. Our context and equations are:\n\n\\(S = 1\\)\n\\(B = 1 - S\\)\n\\(W = \\max(B, S)\\)\n\n\nThat is: Suzy throws her rock; Billy will throw his rock if Suzy\ndoesn\u2019t; and the window will shatter if either throws their\nrock. The DAG is shown in\n Figure 5.\n\n\n\nFigure 5\n\n\nWe want to show that \\(S = 1\\) is an actual cause of \\(W = 1\\).\nConditions AC(1) and AC(2) are clearly satisfied. For condition AC(3),\nwe choose \\(\\bX = \\{S\\}\\) and \\(\\bZ = \\{B\\}\\). Since \\(B = 0\\) in\nPreemption, we want to fix \\(B = 0\\) while varying S.\nWe can see easily that \\({S = 0} \\amp {B = 0} \\boxright {W = 0}\\):\nreplacing the two equations for B and S with \\(B = 0\\)\nand \\(S = 0\\), the solution yields \\(W = 0\\). In words, this\ncounterfactual says that if neither Billy nor Suzy had thrown their\nrock, the window would not have shattered. Thus condition AC(3a) is\nsatisfied. AC(3b) is satisfied trivially, since \\(\\bX = \\{S\\}\\) is a\nsingleton set.\n\nHere is how AC works in this example. S influences W\nalong two different paths: the direct path \\(S \\rightarrow W\\) and the\nindirect path \\(S \\rightarrow B \\rightarrow W\\). These two paths\ninteract in such a way that they cancel each other out, and the value\nof S makes no net difference to the value of W. However,\nby holding B fixed at its actual value of 0, we eliminate the\ninfluence of S on W along that path. The result is that\nwe isolate the contribution that S made to W along the\ndirect path. AC defines actual causation as a particular kind of\npath-specific effect.\n\nTo treat Overdetermination, let B, S, and\nW keep the same meanings. Our setting and equation will be:\n\n\\(B = 1\\)\n\\(S = 1\\)\n\\(W = \\max(B, S)\\)\n\n\nThe graph is the same as that shown in\n Figure 4\n above. Again, we want to show that \\(S = 1\\) is an actual cause of\n\\(W = 1\\). Conditions AC(1) and AC(2) are obviously satisfied. For\nAC(3), we choose \\(\\bX = \\{B, S\\}\\) and \\(\\bZ = \\varnothing\\). For\ncondition AC(3a), we choose for our alternate setting \\(\\bX = \\bx'\\) \\(B =\n0\\) and \\(S = 0\\). Once again, the counterfactual \\({S = 0} \\amp {B =\n0} \\boxright {W = 0}\\) is true. Now, for AC(3b) we must show that \\(\\bX\n= \\{B, S\\}\\) is minimal. It is easy to check that \\(\\{B\\}\\) alone\nwon\u2019t satisfy AC(3a). Whether we take \\(\\bZ = \\varnothing\\) or\n\\(\\bZ = \\{S\\}\\), changing B to 0 (perhaps while also setting\nS to 1) will not change the value of W. A parallel\nargument shows that \\(\\{S\\}\\) alone won\u2019t satisfy AC(3a) either.\nThe key idea here is that S is a member of a minimal set of\nvariables that need to be changed in order to change the value of\nW.\n\nDespite these successes, none of the analyses of actual causation\ndeveloped so far perfectly captures our pre-theoretic intuitions in\nevery case. One strategy that has been pursued by a number of authors\nis to incorporate some distinction between default and\ndeviant values of variables, or between normal and\nabnormal conditions. See, e.g., Hall (2007), Halpern (2008;\n2016: Chapter 3), Halpern & Hitchcock (2015), Hitchcock (2007),\nand Menzies (2004). Blanchard & Schaffer (2017) present arguments\nagainst this approach. Glymour et al. (2010) raise a number of\nproblems for the project of trying to analyze actual causation.\n4. Probabilistic Causal Models\n\nIn this section, we will discuss causal models that incorporate\nprobability in some way. Probability may be used to represent our\nuncertainty about the value of unobserved variables in a particular\ncase, or the distribution of variable values in a population. Often we\nare interested in when some feature of the causal structure of a\nsystem can be identified from the probability distribution\nover values of variables, perhaps in conjunction with background\nassumptions and other observations. For example, we may know the\nprobability distribution over a set of variables \\(\\bV\\), and want to\nknow which causal structures over the variables in \\(\\bV\\) are\ncompatible with the distribution. In realistic scientific cases, we\nnever directly observe the true probability distribution P over a set\nof variables. Rather, we observe finite data that approximate the true\nprobability when sample sizes are large enough and observation\nprotocols are well-designed. We will not address these important\npractical concerns here. Rather, our focus will be on what it is\npossible to infer from probabilities, in principle if not in practice.\nWe will also consider the application of probabilistic causal models\nto decision theory and counterfactuals.\n4.1 Structural Equation Models with Random Errors\n\nWe can introduce probability into a SEM by means of a probability\ndistribution over the exogenous variables.\n\nLet \\(\\bV = \\{X_1, X_2 ,\\ldots ,X_n\\}\\) be a set of endogenous\nvariables, and \\(\\bU = \\{U_1, U_2 ,\\ldots ,U_n\\}\\) a corresponding set\nof exogenous variables. Suppose that each endogenous variable \\(X_i\\)\nis a function of its parents in \\(\\bV\\) together with \\(U_i\\), that\nis: \n\n\\[X_i = f_i (\\bPA(X_i), U_i).\\]\n\n\nAs a general rule, our graphical representation of this SEM will\ninclude only the endogenous variables \\(\\bV\\), and we use\n\\(\\bPA(X_i)\\) to denote the set of endogenous parents of\n\\(X_i . U_i\\) is sometimes called an error variable for\n\\(X_i\\): it is responsible for any difference between the actual value\nof \\(X_i\\) and the value predicted on the basis of \\(\\bPA(X_i)\\)\nalone. We may think of \\(U_i\\) as encapsulating all of the causes of\n\\(X_i\\) that are not included in \\(\\bV\\). The assumption that each\nendogenous variable has exactly one error variable is innocuous. If\nnecessary, \\(U_i\\) can be a vector of variables. For example, if\n\\(Y_1\\), \\(Y_2\\), and \\(Y_3\\) are all causes of \\(X_i\\) that are not\nincluded in \\(\\bV\\), we can let \\(U_i = \\langle Y_1, Y_2,\nY_3\\rangle\\). Moreover, the error variables need not be distinct or\nindependent from one another.\n\nAssuming that the system of equations is acyclic, an assignment of\nvalues to the exogenous variables \\(U_1\\), \\(U_2\\),\u2026 ,\\(U_n\\)\nuniquely determines the values of all the variables in the model.\nThen, if we have a probability distribution \\(\\Pr'\\) over the values\nof variables in \\(\\bU\\), this will induce a unique probability\ndistribution P on \\(\\bV\\).\n4.2 The Markov Condition\n\nSuppose we have a SEM with endogenous variables \\(\\bV\\), exogenous\nvariables \\(\\bU\\), probability distribution P on \\(\\bU\\) and \\(\\bV\\)\nas described in the previous section, and DAG \\(\\bG\\) representing the\ncausal structure on \\(\\bV\\). Pearl and Verma (1991) prove that if the\nerror variables \\(U_i\\) are probabilistically independent in P, then\nthe probability distribution on \\(\\bV\\) will satisfy the Markov\nCondition (MC) with respect to \\(\\bG\\). The Markov Condition has\nseveral formulations, which are equivalent when \\(\\bG\\) is a a DAG\n(Pearl 1988):\n\n\n(MCScreening_off)\n\nFor every variable X in \\(\\bV\\), and every set of variables\n\\(\\bY \\subseteq \\bV \\setminus \\bDE(X)\\),\n\\(\\Pr(X \\mid \\bPA(X) \\amp \\bY) = \\Pr(X \\mid \\bPA(X))\\).\n\n\n(MCFactorization)\n\nLet \\(\\bV = \\{X_1, X_2 , \\ldots ,X_n\\}\\). Then\n\\(\\Pr(X_1, X_2 , \\ldots ,X_n) = \\prod_i \\Pr(X_i \\mid \\bPA(X_i))\\).\n\n\n(MCd-separation)\n\nLet \\(X, Y \\in \\bV, \\bZ \\subseteq \\bV \\setminus \\{X, Y\\}\\). Then\n\\(\\Pr(X, Y \\mid \\bZ) = \\Pr(X \\mid \\bZ) \\times \\Pr(Y \\mid\n\\bZ)\\)\nif \\(\\bZ\\) d-separates X and Y in \\(\\bG\\)\n(explained below).\n\n\n\nLet us take some time to explain each of these formulations.\n\nMCScreening_off says that the parents of variable X\nscreen X off from all other variables, except for the\ndescendants of X. Given the values of the variables that are\nparents of X, the values of the variables in \\(\\bY\\) (which\nincludes no descendants of \\(X)\\), make no further difference to the\nprobability that X will take on any given value.\n\nMCFactorization tells us that once we know the conditional\nprobability distribution of each variable given its parents, \\(\\Pr(X_i\n\\mid \\bPA(X_i))\\), we can compute the complete joint distribution over\nall of the variables. It is relatively easy to see that MCFactorization follows from MCScreening_off. Since \\(\\bG\\)\nis acyclic, we may re-label the subscripts on the variables so that\nthey are ordered from \u2018earlier\u2019 to \u2018later\u2019,\nwith only earlier variables being ancestors of later ones. It follows\nfrom the probability calculus that \n\n\\[\\Pr(X_1, X_2 , \\ldots ,X_n)  = \\Pr(X_1) \\times \\Pr(X_2 \\mid X_1) \\times \\ldots \\times \\Pr(X_n \\mid X_1, X_2 , \\ldots ,X_{n-1})\\] \n\n (this is a version of\nthe theorem of total probability). For each term \\(\\Pr(X_i \\mid X_1,\nX_2 , \\ldots ,X_{i-1})\\), our ordering ensures that all of the parents\nof \\(X_i\\) will be included on the right hand side, and none of its\ndescendants will. MCScreening_off then tells us that we can\neliminate all of the terms from the right hand side except for the\nparents of \\(X_i\\).\n\nMCd-separation introduces the graphical notion of\nd-separation. As noted above, a path from X to Y\nis a sequence of variables \\(\\langle X = X_1 , \\ldots ,X_k =\nY\\rangle\\) such that for each \\(X_i\\), \\(X_{i+1}\\), there is either an\narrow from \\(X_i\\) to \\(X_{i+1}\\)or an arrow from \\(X_{i+1}\\) to\n\\(X_i\\) in \\(\\bG\\). A variable \\(X_i , 1 \\lt i \\lt k\\) is a\ncollider on the path just in case there is an arrow from\n\\(X_{i-1}\\) to \\(X_i\\) and from \\(X_{i+1}\\) to \\(X_i\\). In other\nwords, \\(X_i\\) is a collider just in case the arrows converge on\n\\(X_i\\) in the path. Let \\(\\bX, \\bY\\), and \\(\\bZ\\) be disjoint subsets\nof \\(\\bV\\). \\(\\bZ\\) d-separates \\(\\bX\\) and \\(\\bY\\) just in case every\npath \\(\\langle X_1 , \\ldots ,X_k\\rangle\\) from a variable in \\(\\bX\\)\nto a variable in \\(\\bY\\) contains at least one variable \\(X_i\\) such\nthat either: (i) \\(X_i\\) is a collider, and no descendant of \\(X_i\\)\n(including \\(X_i\\) itself) is in \\(\\bZ\\); or (ii) \\(X_i\\) is not a\ncollider, and \\(X_i\\) is in \\(\\bZ\\). Any path that meets this\ncondition is said to be blocked by \\(\\bZ\\). If \\(\\bZ\\) does\nnot d-separate \\(\\bX\\) and \\(\\bY\\), then \\(\\bX\\) and \\(\\bY\\)\nare d-connected by \\(\\bZ\\).\n\nNote that MC provides sufficient conditions for variables to be\nprobabilistically independent, conditional on others, but no necessary\ncondition.\n\nHere are some illustrations:\n\n\n\nFigure 6\n\n\nIn\n Figure 6,\n MC implies that X screens Y off from all of the other\nvariables, and W screens Z off from all of the other\nvariables. This is most easily seen from MCScreening_off.\nW also screens T off from all of the other variables,\nwhich is most easily seen from MCd-separation.\nT does not necessarily screen Y off from Z (or\nindeed anything from anything).\n\n\n\nFigure 7\n\n\nIn\n Figure 7,\n MC entails that X and Z will be unconditionally\nindependent, but not that they will be independent conditional on\nY. This is most easily seen from MCd-separation.\n\nLet \\(V_i\\) and \\(V_j\\) be two distinct variables in \\(\\bV\\), with\ncorresponding exogenous error variables \\(U_i\\) and \\(U_j\\),\nrepresenting causes of \\(V_i\\) and \\(V_j\\) that are excluded from the\n\\(\\bV\\). Suppose \\(V_i\\) and \\(V_j\\) share at least one common cause\nthat is excluded from \\(\\bV\\). In this case, we would not expect\n\\(U_i\\) and \\(U_j\\) to be probabilistically independent, and the\ntheorem of Pearl and Verma (1991) would not apply. In this case, the\ncausal relationship among the variables in \\(\\bV\\) would not be\nappropriately represented by a DAG, but would require an acyclic\ndirected mixed graph (ADMG) with a double-headed arrow connecting\n\\(V_i\\) and \\(V_j\\). We will discuss this kind of case in more detail\nin\n Section 4.6\n below.\n\nMC is not expected to hold for arbitrary sets of variables \\(\\bV\\),\neven when the DAG \\(\\bG\\) accurately represents the causal relations\namong those variables. For example, (MC) will typically fail in the\nfollowing kinds of case:\n\nIn an EPR (Einstein-Podolski-Rosen) set-up, we have two particles\nprepared in the singlet state. If X represents a spin\nmeasurement on one particle, Y a spin measurement (in the same\ndirection) on the other, then X and Y are perfectly\nanti-correlated. (One particle will be spin-up just in case the other\nis spin-down.) The measurements can be conducted sufficiently far away\nfrom each other that it is impossible for one outcome to causally\ninfluence the other. However, it can be shown that there is no (local)\ncommon cause Z that screens off the two measurement\noutcomes.\nThe variables in \\(\\bV\\) are not appropriately distinct. For\nexample, suppose that X, Y, and Z are variables\nthat are probabilistically independent and causally unrelated. Now\ndefine \\(U = X + Y\\) and \\(W = Y + Z\\), and let \\(\\bV = \\{U, W\\}\\).\nThen U and W will be probabilistically dependent, even\nthough there is no causal relation between them.\nMC may fail if the variables are too coarsely grained. Suppose\nX, Y, and Z are quantitative variables. Z\nis a common cause of X and Y, and neither X nor\nY causes the other. Suppose we replace Z with a coarser\nvariable, \\(Z'\\) indicating only whether Z is high or low. Then\nwe would not expect \\(Z'\\) to screen X off from Y. The\nvalue of X may well contain information about the value of\nZ beyond what is given by \\(Z'\\), and this may affect the\nprobability of Y.\n\n\nBoth SGS (2000) and Pearl (2009) contain statements of a principle\ncalled the Causal Markov Condition (CMC). The statements are\nin fact quite different from one another. In Pearl\u2019s\nformulation, (CMC) is just a statement of the mathematical theorem\ndescribed above: If each variable in \\(\\bV\\) is a deterministic\nproduct of its parents in \\(\\bV\\), together with an error term; and\nthe errors are probabilistically independent of each other; then the\nprobability distribution on \\(\\bV\\) will satisfy (MC) with respect to\nthe DAG \\(\\bG\\) representing the functional dependence relations among\nthe variables in \\(\\bV\\). Pearl interprets this result in the\nfollowing way: Macroscopic systems, he believes, are deterministic. In\npractice, however, we never have access to all of the causally\nrelevant variables affecting a macroscopic system. But if we include\nenough variables in our model so that the excluded variables are\nprobabilistically independent of one another, then our model will\nsatisfy the MC, and we will have a powerful set of analytic tools for\nstudying the system. Thus MC characterizes a point at which we have\nconstructed a useful approximation of the complete system.\n\nIn SGS (2000), the (CMC) has more the status of an empirical posit. If\n\\(\\bV\\) is set of macroscopic variables that are well-chosen, meaning\nthat they are free from the sorts of defects described above; \\(\\bG\\)\nis a DAG representing the causal structure on \\(\\bV\\); and P is the\nempirical probability distribution resulting from this causal\nstructure; then P can be expected to satisfy MC relative to \\(\\bG\\).\nThey defend this assumption in (at least) two ways:\n\nEmpirically, it seems that a great many systems do in fact satisfy\nMC.\nMany of the methods that are in fact used to detect causal\nrelationships tacitly presuppose the MC. In particular, the use of\nrandomized trials presupposes a special case of the MC. Suppose that\nan experimenter determines randomly which subjects will receive\ntreatment with a drug \\((D = 1)\\) and which will receive a placebo\n\\((D = 0)\\), and that under this regimen, treatment is\nprobabilistically correlated with recovery \\((R)\\). The effect of\nrandomization is to eliminate all of the parents of D, so MC\ntells us that if R is not a descendant of D, then\nR and D should be probabilistically independent. If we\ndo not make this assumption, how can we infer from the experiment that\nD is a cause of R?\n\n\nCartwright (1993, 2007: chapter 8) has argued that MC need not hold\nfor genuinely indeterministic systems. Hausman and Woodward (1999,\n2004) attempt to defend MC for indeterministic systems.\n\nA causal model that comprises a DAG and a probability distribution\nthat satisfies MC is called a causal Bayes net.\n4.3 The Minimality and Faithfulness Conditions\n\nThe MC states a sufficient condition but not a necessary condition for\nconditional probabilistic independence. As such, the MC by itself can\nnever entail that two variables are conditionally or unconditionally\ndependent. The Minimality and Faithfulness Conditions are two\nconditions that give necessary conditions for probabilistic\nindependence. (This is employing the terminology of Spirtes et\nal. (SGS 2000). Pearl (2009) contains a \u201cMinimality\nCondition\u201d that is slightly different from the one described\nhere.)\n\n(i) The Minimality Condition. Suppose that the DAG \\(\\bG\\) on\nvariable set \\(\\bV\\) satisfies MC with respect to the probability\ndistribution P. The Minimality Condition asserts that no sub-graph of\n\\(\\bG\\) over \\(\\bV\\) also satisfies the Markov Condition with respect\nto P. As an illustration, consider the variable set \\(\\{X, Y\\}\\), let\nthere be an arrow from X to Y, and suppose that X\nand Y are probabilistically independent of each other. This\ngraph would satisfy the MC with respect to P: none of the independence\nrelations mandated by the MC are absent (in fact, the MC mandates no\nindependence relations). But this graph would violate the Minimality\nCondition with respect to P, since the subgraph that omits the arrow\nfrom X to Y would also satisfy the MC. The Minimality\nCondition implies that if there is an arrow from X to Y,\nthen X makes a probabilistic difference for Y,\nconditional on the other parents of Y. In other words, if \\(\\bZ\n= \\bPA(Y) \\setminus \\{X\\}\\), there exist \\(\\bz\\), y, x,\n\\(x'\\) such that \\(\\Pr(Y = y \\mid X = x \\amp \\bZ = \\bz) \\ne \\Pr(Y = y\n\\mid X = x' \\amp \\bZ = \\bz)\\).\n\n(ii) The Faithfulness Condition. The Faithfulness Condition\n(FC) is the converse of the Markov Condition: it says that all of the\n(conditional and unconditional) probabilistic independencies that\nexist among the variables in \\(\\bV\\) are required by the MC.\nFor example, suppose that \\(\\bV = \\{X, Y, Z\\}\\). Suppose also that\nX and Z are unconditionally independent of one another,\nbut dependent, conditional upon Y. (The other two variable\npairs are dependent, both conditionally and unconditionally.) The\ngraph shown in\n Figure 8\n does not satisfy FC with respect to this distribution (colloquially,\nthe graph is not faithful to the distribution). MC, when applied to\nthe graph of Figure 8, does not imply the independence of X and\nZ. This can be seen by noting that X and Z are\nd-connected (by the empty set): neither the path \\(X\n\\rightarrow Z\\) nor \\(X \\rightarrow Y\\rightarrow Z\\) is blocked (by\nthe empty set). By contrast, the graph shown in\n Figure 7\n above is faithful to the described distribution. Note that Figure 8\ndoes satisfy the Minimality Condition with respect to the\ndistribution; no subgraph satisfies MC with respect to the described\ndistribution. In fact, FC is strictly stronger than the Minimality\nCondition.\n\n\n\nFigure 8\n\n\nHere are some other examples: In\n Figure 6\n above, there is a path \\(W\\rightarrow X\\rightarrow Y\\); FC implies\nthat W and Y should be probabilistically dependent. In\n Figure 7,\n FC implies that X and Z should be dependent,\nconditional on Y.\n\nFC can fail if the probabilistic parameters in a causal model are just\nso. In\n Figure 8,\n for example, X influences Z along two different\ndirected paths. If the effect of one path is to exactly undo the\ninfluence along the other path, then X and Z will be\nprobabilistically independent. If the underlying SEM is linear,\nSpirtes et al. (SGS 2000: Theorem 3.2) prove that the set of\nparameters for which Faithfulness is violated has Lebesgue measure 0.\nNonetheless, parameter values leading to violations of FC are\npossible, so FC does not seem plausible as a metaphysical or\nconceptual constraint upon the connection between causation and\nprobabilities. It is, rather, a methodological principle:\nGiven a distribution on \\(\\{X, Y, Z\\}\\) in which X and Z\nare independent, we should prefer the causal structure depicted in\n Figure 7\n to the one in Figure 8. This is not because Figure 8 is conclusively\nruled out by the distribution, but rather because it is preferable to\npostulate a causal structure that implies the independence of\nX and Z rather than one that is merely\nconsistent with independence. See Zhang and Spirtes 2016 for\ncomprehensive discussion of the role of FC.\n\nViolations of FC are often detectable in principle. For example,\nsuppose that the true causal structure is that shown in\n Figure 7,\n and that the probability distribution over X, Y, and\nZ exhibits all of the conditional independence relations\nrequired by MC. Suppose, moreover, that X and Z are\nindependent, conditional upon Y. This conditional independence\nrelation is not entailed by MC, so it constitutes a violation of FC.\nIt turns out that there is no DAG that is faithful to this probability\ndistribution. This tips us off that there is a violation of FC. While\nwe will not be able to infer the correct causal structure, we will at\nleast avoid inferring an incorrect one in this case. For details, see\nSteel 2006, Zhang & Spirtes 2008, and Zhang 2013b.\n\nResearchers have explored the consequences of adopting a variety of\nassumptions that are weaker than FC; see for example Ramsey et al.\n2006, Spirtes & Zhang 2014, and Zhalama et al. 2016.\n4.4 Identifiability of Causal Structure\n\nIf we have a set of variables \\(\\bV\\) and know the probability\ndistribution P on \\(\\bV\\), what can we infer about the causal\nstructure on \\(\\bV\\)? This epistemological question is closely related\nto the metaphysical question of whether it is possible to\nreduce causation to probability (as, e.g., Reichenbach 1956\nand Suppes 1970 proposed).\n\nPearl (1988: Chapter 3) proves the following theorem:\n\n\n(Identifiability with time-order)\nIf\n\nthe variables in \\(\\bV\\) are time-indexed, such that only earlier\nvariables can cause later ones;\nthe probability P assigns positive probability to every possible\nassignment of values of the variables in \\(\\bV\\);\nthere are no latent variables, so that the correct causal graph\n\\(\\bG\\) is a DAG;\nand the probability measure P satisfies the Markov and Minimality\nConditions with respect to \\(\\bG\\);\n\n\nthen it will be possible to uniquely identify \\(\\bG\\) on the basis of\nP.\n\n\nIt is relatively easy to see why this holds. For each variable\n\\(X_i\\), its parents must come from among the variables with lower\ntime indices, call them \\(X_1 ,\\ldots ,X_{i-1}\\). Any variables in\nthis group that are not parents of \\(X_i\\) will be nondescendants of\n\\(X_i\\); hence they will be screened off from \\(X_i\\) by its parents\n(from MCScreening_off). Thus we can start with the\ndistributions \\(\\Pr(X_i\\mid X_1 ,\\ldots ,X_{i-1})\\), and then weed out\nany variables from the right hand side that make no difference to the\nprobability distribution over \\(X_i\\). By the Minimality Condition, we\nknow that the variables so weeded are not parents of \\(X_i\\). Those\nvariables that remain are the parents of \\(X_i\\) in \\(\\bG\\).\n\nIf we don\u2019t have information about time ordering, or other\nsubstantive assumptions restricting the possible causal structures\namong the variables in \\(\\bV\\), then it will not always be possible to\nidentify the causal structure from probability alone. In general,\ngiven a probability distribution P on \\(\\bV\\), it is only possible to\nidentify a Markov equivalence class of causal structures.\nThis will be the set of all DAGs on \\(\\bV\\) that (together with MC)\nimply all and only the conditional independence relations contained in\nP. In other words, it will be the set of all DAGs \\(\\bG\\) such that P\nsatisfies MC and FC with respect to \\(\\bG\\). The PC algorithm\ndescribed by SGS (2000: 84\u201385) is one algorithm that generates\nthe Markov equivalence class for any probability distribution with a\nnon-empty Markov equivalence class.\n\nConsider two simple examples involving three variables \\(\\{X, Y,\nZ\\}\\). Suppose our probability distribution has the following\nproperties:\n\nX and Y are dependent unconditionally, and\nconditional on Z\nY and Z are dependent unconditionally, and\nconditional on X\nX and Z are dependent unconditionally, but\nindependent conditional on Y\n\n\nThen the Markov equivalence class is: \n\n\\[\nX \\rightarrow Y \\rightarrow Z\\\\\nX \\leftarrow Y \\leftarrow Z\\\\\nX \\leftarrow Y \\rightarrow Z\n\\]\n\n\nWe cannot determine from the probability distribution, together with\nMC and FC, which of these structures is correct.\n\nOn the other hand, suppose the probability distribution is as\nfollows:\n\nX and Y are dependent unconditionally, and\nconditional on Z\nY and Z are dependent unconditionally, and\nconditional on X\nX and Z are independent unconditionally, but\ndependent conditional on Y\n\n\nThen the Markov equivalence class is: \n\n\\[\nX \\rightarrow  Y \\leftarrow  Z\n\\]\n\n\nThis is the only DAG relative to which the given probability\ndistribution satisfies MC and FC.\n4.5 Identifiability with Assumptions about Functional Form\n\nSuppose we have a SEM with endogenous variables \\(\\bV\\) and exogenous\nvariables \\(\\bU\\), where each variable in \\(\\bV\\) is determined by an\nequation of the form: \n\n\\[X_i = f_i (\\bPA(X_i), U_i).\\]\n\n\nSuppose, moreover, that we have a probability distribution \\(\\Pr'\\) on\n\\(\\bU\\) in which all of the \\(U_i\\)s are independent. This will induce\na probability distribution P on \\(\\bV\\) that satisfies MC relative to\nthe correct causal DAG on \\(\\bV\\). In other words, our probabilistic\nSEM will generate a unique causal Bayes net. The methods described in\nthe previous section attempt to infer the underlying graph \\(\\bG\\)\nfrom relations of probabilistic dependence and independence. These\nmethods can do no better than identifying the Markov equivalence\nclass. Can we do better by making use of additional information about\nthe probability distribution P, beyond relations of dependence and\nindependence?\n\nThere is good news and there is bad news. First the bad news. If the\nvariables in \\(\\bV\\) are discrete, and we make no assumptions about\nthe form of the functions \\(f_i\\), then we can infer no more about the\nSEM than the Markov equivalence to which the graph belongs (Meek\n1995).\n\nMore bad news: If the variables in \\(\\bV\\) are continuous, the\nsimplest assumption, and the one that has been studied in most detail,\nis that the equations are linear with Gaussian\n(normal, or bell-shaped) errors. That is:\n\n\\(X_i = \\sum_j c_j X_j + U_i\\), where j ranges over the\nindices of \\(\\bPA(X_i)\\) and the \\(c_j\\)s are constants\n\\(Pr'\\) assigns a Gaussian distribution to each \\(U_i\\)\n\n\nIt turns out that with these assumptions, we can do no better than\ninferring the Markov equivalence class of the causal graph on \\(\\bV\\)\nfrom probabilistic dependence and independence (Geiger & Pearl\n1988).\n\nNow for the good news. There are fairly general assumptions that allow\nus to infer a good deal more. Here are some fairly simple cases:\n\n\n(LiNGaM) (Shimizu et al. 2006)\nIf:\n\nThe variables in \\(\\bV\\) are continuous;\nThe functions \\(f_i\\) are linear;\nThe probability distributions on the error variables \\(U_i\\) are\nnot Gaussian (or at most one is Gaussian);\nThe error variables \\(U_i\\) are probabilistically independent in\n\\(\\Pr'\\);\n\n\nthen the correct DAG on \\(\\bV\\) can be uniquely determined by the\ninduced probability distribution P on \\(\\bV\\).\n\n(Non-linear additive) (Hoyer et al. 2009)\nAlmost all functions of the following form allow the correct DAG on\n\\(\\bV\\) to be uniquely determined by the induced probability\ndistribution P on \\(\\bV\\).:\n\nThe functions \\(f_i\\) are nonlinear and the errors are additive\n(so \\(X_i = f_i (\\bPA(X_i)) + U_i\\), with \\(f_i\\) nonlinear);\nThe error variables \\(U_i\\) are probabilistically independent in\n\\(\\Pr'\\);\n\n\n\nIn fact, this case can be generalized considerably:\n\n(Post non-linear) (Zhang & Hyv\u00e4rinen\n2009)\nWith the exception of five specific cases that can be fully specified,\nall functions of the following form allow the correct DAG on \\(\\bV\\)\nto be uniquely determined by the induced probability distribution P on\n\\(\\bV\\).:\n\n\nThe functions have the form \\(X_i = g_i (f_i (\\bPA(X_i)) + U_i)\\)\nwith \\(f_i\\) and \\(g_i\\) nonlinear, and \\(g_i\\) invertible;\nThe error variables \\(U_i\\) are probabilistically independent in\n\\(\\Pr'\\);\n\n\n\nSee also Peters et al. (2017) for discussion.\n\nWhile there are specific assumptions behind these results, they are\nnonetheless remarkable. They entail, for example, that (given the\nassumptions of the theorems) knowing only the probability distribution\non two variables X and Y, we can infer whether X\ncauses Y or Y causes X.\n4.6 Latent Common Causes\n\nThe discussion so far has focused on the case where there are no\nlatent common causes of the variables in \\(\\bV\\), and the error\nvariables \\(U_i\\) can be expected to be probabilistically independent.\nAs we noted in\n Section 2.3\n above, we represent a latent common cause with a double-headed arrow.\nFor example, the acyclic directed mixed graph in\n Figure 9\n represents a latent common cause of X and Z. More\ngenerally, we can use an ADMG like Figure 9 to represent that the\nerror variables for X and Z are not probabilistically\nindependent.\n\n\n\nFigure 9\n\n\nIf there are latent common causes, we expect MCScreening_off and MCFactorization to fail if we apply them in a\nna\u00efve way. In\n Figure 9,\n Y is the only parent of Z shown in the graph, and if we\ntry to apply MCScreening_off, it tells us that Y should\nscreen X off from Z. However, we would expect X\nand Z to be correlated, even when we condition on Y, due\nto the latent common cause. The problem is that the graph is missing a\nrelevant parent of Z, namely the omitted common cause. However,\nsuppose that the probability distribution on \\(\\{L, X, Y, Z\\}\\)\nsatisfies MC with respect to the DAG that includes L as a\ncommon cause of X and Z. Then it turns out that the\nprobability distribution will still satisfy MCd-separation with respect to the ADMG of Figure 9. A causal\nmodel incorporating an ADMG and probability distribution satisfying\nMCd-separation is called a semi-Markov\ncausal model (SMCM).\n\nIf we allow that the correct causal graph may be an ADMG, we can still\napply MCd-separation, and ask which graphs imply the\nsame sets of conditional independence relations. The Markov\nequivalence class will be larger than it was when we did not allow for\nlatent variables. For instance, suppose that the probability\ndistribution on \\(\\{X, Y, Z\\}\\) has the following features:\n\nX and Y are dependent unconditionally, and\nconditional on Z\nY and Z are dependent unconditionally, and\nconditional on X\nX and Z are independent unconditionally, but\ndependent conditional on Y\n\n\nWe saw in\n Section 4.4\n that the only DAG that implies just these (in)dependencies is:\n\n\\[\nX \\rightarrow Y \\leftarrow Z\n\\]\n\n\nBut if we allow for the possibility of latent common causes, there\nwill be additional ADMGs that also imply just these (in)dependencies.\nFor example, the structure \n\n\\[\nX \\leftrightarrow Y \\leftrightarrow Z\n\\]\n\n\nis also in the Markov equivalence class, as are several others.\n\nLatent variables present a further complication. Unlike the case where\nthe error variables \\(U_i\\) are probabilistically independent, a SEM\nwith correlated error terms may imply probabilistic constraints in\naddition to conditional (in)dependence relations, even in the absence\nof further assumptions about functional form. This means that we may\nbe able to rule out some of the ADMGs in the Markov equivalence class\nusing different kinds of probabilistic constraints.\n4.7 Interventions\n\nA conditional probability such as \\(\\Pr(Y = y \\mid X = x)\\) gives us\nthe probability that Y will take the value y, given that\nX has been observed to take the value x. Often,\nhowever, we are interested in predicting the value of Y that\nwill result if we intervene to set the value of X\nequal to some particular value x. Pearl (2009) writes \\(\\Pr(Y =\ny \\mid \\ido(X = x))\\) to characterize this probability. The notation\nis misleading, since \\(\\ido(X = x)\\) is not an event in the original\nprobability space. It might be more accurate to write \\(\\Pr_{\\ido(X =\nx)} (Y = y)\\), but we will use Pearl\u2019s notation here. What is the\ndifference between observation and intervention? When we merely\nobserve the value that a variable takes, we are learning about the\nvalue of the variable when it is caused in the normal way, as\nrepresented in our causal model. Information about the value of the\nvariable will also provide us with information about its causes, and\nabout other effects of those causes. However, when we intervene, we\noverride the normal causal structure, forcing a variable to take a\nvalue it might not have taken if the system were left alone.\nGraphically, we can represent the effect of this intervention by\neliminating the arrows directed into the variable intervened upon.\nSuch an intervention is sometimes described as \u201cbreaking\u201d\nthose arrows. As we saw in Section\n 3.1, in the\ncontext of a SEM, we represent an intervention that sets X to\nx by replacing the equation for X with a new one\nspecifying that \\(X = x\\).\n\nAs we saw in\n Section 3.2,\n there is a close connection between interventions and\ncounterfactuals; in particular, the antecedents of structural\ncounterfactuals are thought of as being realized by interventions.\nNonetheless, Pearl (2009) distinguishes claims about interventions\nrepresented by the do operator from counterfactuals. The former\nare understood in the indicative mood; they concern interventions that\nare actually performed. Counterfactuals are in the subjunctive mood,\nand concern hypothetical interventions. This leads to an important\nepistemological difference between ordinary interventions and\ncounterfactuals: they behave differently in the way that they interact\nwith observations of the values of variables. In the case of\ninterventions, we are concerned with evaluating probabilities such\nas \n\n\\[\\Pr(\\bY = \\by \\mid \\bX =\\bx, \\ido(\\bZ = \\bz)).\\]\n\n\nWe assume that the intervention \\(\\ido(\\bZ = \\bz)\\) is being performed\nin the actual world, and hence that we are observing the values that\nother variables take \\((\\bX =\\bx)\\) in the same world where the\nintervention takes place. In the case of counterfactuals, we observe\nthe value of various variables in the actual world, in which there is\nno intervention. We then ask what would have happened if an\nintervention had been performed. The variables whose values\nwe observed may well take on different values in the\nhypothetical world where the intervention takes place. Here is a\nsimple illustration of the difference. Suppose that we have a causal\nmodel in which treatment with a drug causes recovery from a disease.\nThere may be other variables and causal relations among them as\nwell.\n\n\nIntervention:\n\n\nAn intervention was performed to treat a particular patient with\nthe drug, and it was observed that she did not recover.\nQuestion: What is the probability that she recovered,\ngiven the intervention and the observed evidence?\nAnswer: Zero, trivially.\n\n\n\nCounterfactual: \n\n\nIt was observed that a patient did not recover from the\ndisease.\nQuestion: What is the probability that she would have\nrecovered, had she been treated with the drug?\nAnswer: Nontrivial. The answer is not necessarily\nzero, nor is it necessarily P(recovery | treatment).\nIf we know that she was in fact treated, then we could infer that she\nwould not have recovered if treated. But we do not know whether she\nwas treated. The fact that she did not recover gives us partial\ninformation: it makes it less likely that she was in fact treated; it\nalso makes it more likely that she has a weak immune system, and so\non. We must make use of all of this information in trying to determine\nthe probability that she would have recovered if treated.\n\n\n\n\nWe will discuss interventions in the present section, and\ncounterfactuals in\n Section 4.10\n below.\n\nSuppose that we have an acyclic structural equation model with\nexogenous variables \\(\\bU\\) and endogenous variables \\(\\bV\\). We have\nequations of the form \n\n\\[X_i = f_i (\\bPA(X_i), U_i),\\]\n\n\nand a probability distribution \\(\\Pr'\\) on the exogenous variables\n\\(\\bU\\). \\(\\Pr'\\) then induces a probability distribution P on\n\\(\\bV\\). To represent an intervention that sets \\(X_k\\) to \\(x_k\\), we\nreplace the equation for \\(X_k\\) with \\(X_k = x_k\\). Now \\(\\Pr'\\)\ninduces a new probability distribution P* on \\(\\bV\\) (since settings\nof the exogenous variables \\(\\bU\\) give rise to different values of\nthe variables in \\(\\bV\\) after the intervention). P* is the new\nprobability distribution that Pearl writes as \\(\\Pr(\u2022 \\mid \\ido(X_k\n= x_k))\\).\n\nBut even if we do not have a complete SEM, we can often compute the\neffect of interventions. Suppose we have a causal model in which the\nprobability distribution P satisfies MC on the causal DAG \\(\\bG\\) over\nthe variable set \\(\\bV = \\{X_1, X_2 ,\\ldots ,X_n\\}\\). The most useful\nversion of MC for thinking about interventions is MCFactorization (see\n Section 4.2),\n which tells us: \n\n\\[\\Pr(X_1, X_2 , \\ldots ,X_n) = \\prod_i \\Pr(X_i \\mid \\bPA(X_i)).\\]\n\n\nNow suppose that we intervene by setting the value of \\(X_k\\) to\n\\(x_k\\). The post-intervention probability P* is the result of\naltering the factorization as follows: \n\n\\[\n\\Pr^*(X_1, X_2 , \\ldots ,X_n) = \\Pr'(X_k) \\times \\prod_{i \\ne k} \\Pr(X_i \\mid \\bPA(X_i)),\n\\]\n\n\nwhere \\(\\Pr'(X_k = x_k) = 1\\). The conditional probabilities of the\nform \\(\\Pr(X_i \\mid \\bPA(X_i))\\) for \\(i \\ne k\\) remain unchanged by\nthe intervention. This gives the same result as computing the result\nof an intervention using a SEM, when the latter is available. This\nresult can be generalized to the case where the intervention imposes a\nprobability distribution \\(\\Pr^{\\dagger}\\) on some subset of the\nvariables in \\(\\bV\\). For simplicity, let\u2019s re-label the\nvariables so that \\(\\{X_1, X_2 ,\\ldots ,X_k\\}\\) is the set of\nvariables that we intervene on. Then, the post-intervention\nprobability distribution is: \n\n\\[\n\\Pr^*(X_1, X_2 , \\ldots ,X_n) = \\Pr^{\\dagger}( X_1, X_2 ,\\ldots ,X_k) \\times \\prod_{k \\lt i \\le n} \\Pr(X_i \\mid \\bPA(X_i)).\n\\]\n\n\nThe Manipulation Theorem of SGS (2000: theorem 3.6)\ngeneralizes this formula to cover a much broader class of\ninterventions, including ones that don\u2019t break all the arrows\ninto the variables that are intervened on.\n\nPearl (2009: Chapter 3) develops an axiomatic system he calls the\ndo-calculus for computing post-intervention probabilities\nthat can be applied to systems with latent variables, where the causal\nstructure on \\(\\bV\\) is represented by an ADMG (including\ndouble-headed arrows) instead of a DAG. The axioms of this system are\npresented in\n Supplement on the do-calculus.\n One useful special case is given by the\n\n\nBack-Door Criterion. Let X and Y be variables\nin \\(\\bV\\), and \\(\\bZ \\subseteq \\bV \\setminus \\{X, Y\\}\\) such\nthat:\n\nno member of \\(\\bZ\\) is a descendant of X; and\nevery path between X and Y that terminates with an\narrow into X either (a) includes a non-collider in \\(\\bZ\\), or\n(b) includes a collider that has no descendants in \\(\\bZ\\);\n\n\nthen \\(\\Pr(Y \\mid \\ido(X), \\bZ) = \\Pr(Y \\mid X, \\bZ)\\).\n\n\nThat is, if we can find an appropriate conditioning set \\(\\bZ\\), the\nprobability resulting from an intervention on X will be the\nsame as the conditional probability corresponding to an observation of\nX.\n4.8. Interventionist Decision Theory\n\nEvidential Decision Theory of the sort developed by Jeffrey (1983),\nruns into well-known problems in variants of Newcomb\u2019s\nproblem (Nozick 1969). For example, suppose Cheryl believes the\nfollowing: She periodically suffers from a potassium deficiency. This\nstate produces two effects with high probability: It causes her to eat\nbananas, which she enjoys; and it causes her to suffer debilitating\nmigraines. On days when she suffers from the potassium deficiency, she\nhas no introspective access to this state. In particular, she is not\naware of any banana cravings. Perhaps she rushes to work every\nmorning, grabbing whatever is at hand to eat on her commute.\nCheryl\u2019s causal model is represented by the DAG in\n Figure 10.\n\n\n\n\n\nFigure 10\n\n\n\\(K = 1\\) represents potassium deficiency, \\(B = 1\\) eating a banana,\nand \\(M = 1\\) migraine. Her probabilities are as follows:\n\n\\[\n\\begin{aligned}\n\\Pr(K = 1) & = .2\\\\\n\\Pr(B = 1 \\mid K = 1) & = .9, &\\Pr(B  = 1 \\mid K = 0) & = .1\\\\\n\\Pr(M = 1 \\mid K = 1) & = .9, & \\Pr(M = 1 \\mid K = 0) & = .1\n\\end{aligned}\n\\]\n\n\nHer utility for the state of the world \\(w \\equiv \\{K = k, B = b, M =\nm\\}\\) is \\(\\Ur(w) = b - 20m\\). That is, she gains one unit of utility\nfor eating a banana, but loses 20 units for suffering a migraine. She\nassigns no intrinsic value to the potassium deficiency.\n\nCheryl is about to leave for work. Should she eat a banana? According\nto Evidential Decision Theory (EDT), Cheryl should maximize\nEvidential Expected Utility, where \n\n\\[\\EEU(B = b) = \\sum_w \\Pr(w \\mid B = b)\\Ur(w)\\]\n\n\nFrom the probabilities given, we can compute that: \n\n\\[\n\\begin{aligned}\n\\Pr(M = 1 \\mid B = 1) & \\approx .65\\\\\n\\Pr(M = 1 \\mid B = 0) & \\approx .12\n\\end{aligned}\n\\]\n\n\nEating a banana is strongly correlated with migraine, due to the\ncommon cause. Thus \n\n\\[\\begin{aligned}\n\\EEU(B = 1) &\\approx {-12}\\\\\n\\EEU(B = 0) & \\approx {-2.4}\n\\end{aligned}\n\\]\n\n\nSo EDT, at least in its simplest form, recommends abstaining from\nbananas. Although Cheryl enjoys them, they provide strong evidence\nthat she will suffer from a migraine.\n\nMany think that this is bad advice. Eating a banana does not\ncause Cheryl to get a migraine; it is a harmless pleasure. A\nnumber of authors have formulated versions of Causal Decision\nTheory (CDT) that aim to incorporate explicitly causal\nconsiderations (e.g., Gibbard & Harper 1978; Joyce 1999; Lewis\n1981; Skyrms 1980). Causal models provide a natural setting for CDT,\nan idea proposed by Meek and Glymour (1994) and developed by Hitchcock\n(2016), Pearl (2009: Chapter 4) and Stern (2017). The central idea is\nthat the agent should treat her action as an intervention.\nThis means that Cheryl should maximize her Causal Expected\nUtility: \n\n\\[\\CEU(B = b) = \\sum_w \\Pr(w \\mid \\ido(B = b))\\Ur(w)\\]\n\n\nNow we can compute \n\n\\[\n\\begin{aligned}\n\\Pr(M = 1 \\mid \\ido(B = 1)) & = .26\\\\\n\\Pr(M = 1 \\mid \\ido(B = 0)) & = .26\n\\end{aligned}\n\\]\n\n\nSo that now \n\n\\[\n\\begin{aligned}\n\\CEU(B = 1) &= {-4.2}\\\\\n\\CEU(B = 0) & = {-5.2}\n\\end{aligned}\n\\]\n\n\nThis yields the plausible result that eating a banana gives Cheryl a\nfree unit of utility. By intervening, Cheryl breaks the arrow from\nK to B and destroys the correlation between eating a\nbanana and suffering a migraine.\n\nMore generally, one can use the methods for calculating the effects of\ninterventions described in the previous section to compute the\nprobabilities needed to calculate Causal Expected Utility. Stern\n(2017) expands this approach to allow for agents who distribute their\ncredence over multiple causal models. Hitchcock (2016) shows how the\ndistinction between interventions and counterfactuals, discussed in\nmore detail in\n Section 4.10\n below, can be used to deflect a number of alleged counterexamples to\nCDT.\n\nThere is much more that can be said about the debate between EDT and\nCDT. For instance, if Cheryl knows that she is intervening, then she\nwill not believe herself to be accurately described by the causal\nstructure in\n Figure 10.\n Instead, she will believe herself to instantiate a causal structure\nin which the arrow from K to B is removed. In this\ncausal structure, if P satisfies MC, we will have \\(\\Pr(w \\mid B = b)\n= \\Pr(w \\mid \\ido(B = b))\\), and the difference between EDT and CDT\ncollapses. If there is a principled reason why a deliberating agent\nwill always believe herself to be intervening, then EDT will yield the\nsame normative recommendations as CDT, and will avoid counterexamples\nlike the one described above. Price\u2019s defense of EDT (Price\n1986) might be plausibly reconstructed along these lines. So the moral\nis not necessarily that CDT is normatively correct, but rather that\ncausal models may be fruitfully employed to clarify issues in decision\ntheory connected with causation.\n4.9 Causal Discovery with Interventions\n\nIn the previous section, we discussed how to use knowledge (or\nassumptions) about the structure of a causal graph \\(\\bG\\) to make\ninferences about the results of interventions. In this section, we\nexplore the converse problem. If we can intervene on variables and\nobserve the post-intervention probability distribution, what can we\ninfer about the underlying causal structure? This topic has been\nexplored extensively in the work of Eberhardt and his collaborators.\n(See, for example, Eberhardt & Scheines 2007 and Hyttinen et al.\n2013a.) Unsurprisingly, we can learn more about causal structure if we\ncan perform interventions than if we can only make passive\nobservations. However, just how much we can infer depends upon what\nkinds of interventions we can perform, and on what background\nassumptions we make.\n\nIf there are no latent common causes, so that the true causal\nstructure on \\(\\bV\\)\nis represented by a DAG \\(\\bG\\), then it will always be possible to discover\nthe complete causal structure using interventions. If we can only\nintervene on one variable at a time, we may need to separately\nintervene on all but one of the variables before the causal structure\nis uniquely identified. If we can intervene on multiple variables at\nthe same time, we can discover the true causal structure more\nquickly.\n\nIf there are latent common causes, so that the true causal structure\non \\(\\bV\\) is represented by an ADMG, then it may not be possible to\ndiscover the true causal structure using only single-variable\ninterventions.  (Although we can do this in the special case where the\nfunctions in the underlying structural equation model are all linear.)\nHowever, if we can intervene on multiple variables at the same time,\nthen it is possible to discover the true causal graph.\n\nEberhardt and collaborators have also explored causal discovery using\nsoft interventions. A soft intervention influences the value\nof a variable without breaking the arrows into that variable. For\ninstance, suppose we want to know whether increasing the income of\nparolees will lead to decreased recidivism. We randomly divide\nsubjects into treatment and control conditions, and give regular cash\npayments to those in the treatment condition. This is not an\nintervention on income per se, since income will still be\ninfluenced by usual factors: savings and investments, job training,\nhelp from family members, and so on. Soft interventions facilitate\ncausal inference because they create colliders, and as we have seen,\ncolliders have a distinct probabilistic signature. Counterintuitively,\nthis means that if we want to determine whether X causes\nY it is desirable to perform a soft intervention on Y\n(rather than X), to see if we can create a collider\n\\(I\\rightarrow Y\\leftarrow X\\) (where I is the intervention).\nSoft interventions are closely related to instrumental\nvariables. If there are no latent common causes, we can infer the\ntrue causal structure using soft interventions. Indeed, if we can\nintervene on every variable at once, we can determine the correct\ncausal structure from this one intervention. However, if there are\nlatent common causes, it is not in general possible to discover the\ncomplete causal structure using soft interventions. (Although this can\nbe done if we assume linearity.)\n4.10 Counterfactuals\n\nSection 3.3\n above discussed counterfactuals in the context of deterministic\ncausal models. The introduction of probability adds a number of\ncomplications. In particular, we can now talk meaningfully about the\nprobability of a counterfactual being true. Counterfactuals play a\ncentral role in the potential outcome framework for causal\nmodels pioneered by Neyman (1923), and developed by Rubin (1974) and\nRobins (1986), among others.\n\nCounterfactuals in the potential outcome framework interact with\nprobability differently than counterfactuals in Lewis\u2019s (1973b)\nframework. Suppose that Ted was exposed to asbestos and developed lung\ncancer. We are interested in the counterfactual: \u201cIf Ted had not\nbeen exposed to asbestos, he would not have developed lung\ncancer\u201d. Suppose that the processes by which cancer develops are\ngenuinely indeterministic. Then, it seems wrong to say that if Ted had\nnot been exposed to asbestos, he definitely would have developed lung\ncancer; and it seems equally wrong to say that he definitely would not\nhave developed lung cancer. In this case, Lewis would say that the\ncounterfactual \u201cIf Ted had not been exposed to asbestos, he\nwould not have developed lung cancer\u201d is determinately\nfalse. As a result, the objective probability of this\ncounterfactual being true is zero. On the other hand, a counterfactual\nwith objective probability in the consequent may be true:\n\u201cIf Ted had not been exposed to asbestos, his objective chance\nof developing lung cancer would have been .06\u201d. By contrast, in\nthe potential outcome framework, probability may be pulled out of the\nconsequent and applied to the counterfactual as a whole: The\nprobability of the counterfactual \u201cIf Ted had not been exposed\nto asbestos, he would have developed lung cancer\u201d can be\n.06.\n\nIf we have a complete structural equation model, we can assign\nprobabilities to counterfactuals, in light of observations. Let \\(\\bV\n= \\{X_1, X_2 ,\\ldots ,X_n\\}\\) be a set of endogenous variables, and\n\\(\\bU = \\{U_1, U_2 ,\\ldots ,U_n\\}\\) a set of exogenous variables. Our\nstructural equations have the form: \n\n\\[X_i = f_i (\\bPA(X_i), U_i)\\]\n\n\nWe have a probability distribution \\(\\Pr'\\) on \\(\\bU\\), which induces\na probability distribution P on \\(\\bU \\cup \\bV\\). Suppose that we\nobserve the value of some of the variables: \\(X_j = x_j\\) for all \\(j\n\\in \\bS \\subseteq \\{1,\\ldots ,n\\}\\). We now want to assess the\ncounterfactual \u201cif \\(X_k\\) had been \\(x_k\\), then \\(X_l\\) would\nhave been \\(x_l\\)\u201d, where k and l may be in\n\\(\\bS\\) but need not be. We can evaluate the probability of this\ncounterfactual using this three-step process:\n\nUpdate the probability P by conditioning on the observations, to\nget a new probability distribution \\(\\Pr(\u2022 \\mid \\cap_{j \\in \\bS}\nX_j = x_j)\\). Call the restriction of this probability function to\n\\(\\bU\\) \\(\\Pr''\\).\nReplace the equation for \\(X_k\\) with \\(X_k = x_k\\).\nUse the distribution \\(\\Pr''\\) on \\(\\bU\\) together with the\nmodified set of equations to induce a new probability distribution P*\non \\(\\bV\\). \\(\\Pr^*( X_l = x_l)\\) is then the probability of the\ncounterfactual.\n\n\nThis procedure differs from the procedure for interventions (discussed\nin\n Section 4.7)\n in that steps 1 and 2 have been reversed. We first update the\nprobability distribution, then perform the intervention. This reflects\nthe fact that the observations tell us about the actual world, in\nwhich the intervention did not (necessarily) occur.\n\nIf we do not have a complete SEM, it is not generally possible to\nidentify the probability of a counterfactual, but only to set upper\nand lower bounds. For example, suppose that we believe that asbestos\nexposure causes lung cancer, so that we posit a simple DAG:\n\n\\[A \\rightarrow L.\\]\n\n\nSuppose also that we have data for people similar to Ted which yields\nthe following probabilities: \n\n\\[\\begin{aligned}\n\\Pr(L = 1 \\mid A = 1) & = .11,\\\\\n\\Pr(L = 1 \\mid A = 0) & = .06.\n\\end{aligned}\\]\n\n\n(We are oversimplifying, and treating asbestos and lung cancer as\nbinary variables.) We observe that Ted was in fact exposed to asbestos\nand did in fact develop lung cancer. What is the probability of the\ncounterfactual: \u201cIf Ted had not been exposed to asbestos, he\nwould not have developed lung cancer\u201d? Pearl (2009) calls a\nprobability of this form a probability of necessity. It is\noften called the probability of causation, although this\nterminology is misleading for reasons discussed by Greenland and\nRobins (1988). This quantity is often of interest in tort law. Suppose\nthat Ted sues his employer for damages related to his lung cancer. He\nwould have to persuade a jury that his exposure to asbestos caused his\nlung cancer. American civil law requires a \u201cmore probable than\nnot\u201d standard of proof, and it employs a \u201cbut for\u201d\nor counterfactual definition of causation. Hence Ted must convince the\njury that it is more probable than not that he would not have\ndeveloped lung cancer if he had not been exposed.\n\nWe may divide the members of the population into four categories,\ndepending upon which counterfactuals are true of them:\n\ndoomed individuals will develop lung cancer no matter\nwhat\nimmune individuals will avoid lung cancer no matter\nwhat\nsensitive individuals will develop lung cancer just in\ncase they are exposed to asbestos\nreverse sensitive individuals will develop lung cancer\njust in case they are not exposed to asbestos\n\n\nIt is easiest to think of the population as being divided into four\ncategories, with each person being one of these four types. However,\nwe do not need to assume that the process is deterministic; it may be\nthe case that each person only has a certain probability of falling\ninto one of these categories.\n\nMathematically, this is equivalent to the following. Let \\(U_L\\) be\nthe error variable for \\(L. U_L\\) takes values of the form \\((u_1,\nu_2)\\) with each \\(u_i\\) being 0 or 1. \\((1, 1)\\) corresponds to\ndoomed, \\((0, 0)\\) to immune, \\((1, 0)\\) to\nsensitive, and \\((0, 1)\\) to reverse. That is, the\nfirst element tells us what value L will take if an individual\nis exposed to asbestos, and the second element what value L\nwill take if an individual is not exposed. The equation for L\nwill be \\(L = (A \\times u_1) + ((1 - A) \\times u_2)\\).\n\nLet us assume that the distribution of the error variable \\(U_L\\) is\nindependent of asbestos exposure A. The observed probability of\nlung cancer is compatible with both of the following probability\ndistributions over our four counterfactual categories:\n\n\\[\n\\begin{aligned}\n\\Pr_1(\\textit{doomed}) & = .06,  &\\Pr_2(\\textit{doomed}) &= 0,\\\\\n\\Pr_1(\\textit{immune}) & = .89,  &  \\Pr_2(\\textit{immune}) & = .83,\\\\\n\\Pr_1(\\textit{sensitive}) & = .05, & \\Pr_2(\\textit{sensitive}) & = .11, \\\\\n\\Pr_1(\\textit{reverse}) & = 0 & \\Pr_2(\\textit{reverse}) & = .06\n\\end{aligned}\n\\]\n\n \n\nMore generally, the observed probability is compatible with any\nprobability \\(\\Pr'\\) satisfying: \n\n\\[\n\\begin{aligned}\n\\Pr'(\\textit{doomed}) + \\Pr'(\\textit{senstive}) & = \\Pr(L \\mid A) & = .11;\\\\\n\\Pr'(\\textit{immune}) + \\Pr'(\\textit{reverse}) & = \\Pr({\\sim}L \\mid A) &  = .89;\\\\\n\\Pr'(\\textit{doomed}) + \\Pr'(\\textit{reverse}) & = \\Pr(L \\mid {\\sim}A) & = .06;\\\\\n\\Pr'(\\textit{immune}) + \\Pr'(\\textit{senstive}) & = \\Pr({\\sim}L \\mid {\\sim}A) & = .94.\\\\\n \\end{aligned}\n    \\]\n\n\n\\(\\Pr_1\\) and \\(\\Pr_2\\) are just the most extreme cases. From the fact\nthat Ted was exposed to asbestos and developed lung cancer, we know\nthat he is either sensitive or doomed. The\ncounterfactual of interest will be true just in case he is\nsensitive. Hence the probability of the counterfactual, given\nthe available evidence, is P(sensitive | sensitive\nor doomed). However, using \\(\\Pr_1\\) yields a conditional\nprobability of .45 (5/11), while \\(\\Pr_2\\) yields a conditional\nprobability of 1. Given the information available to us, all we can\nconclude is that the probability of necessity is between .45 and 1. To\ndetermine the probability more precisely, we would need to know the\nprobability distribution of the error variable.\n\nA closely related counterfactual quantity is what Pearl (2009) calls\nthe probability of sufficiency. Suppose that Teresa, unlike\nTed, was not exposed to asbestos, and did not develop lung cancer. The\nprobability of sufficiency is the probability that she would\nhave suffered lung cancer if she had been exposed. That is,\nthe probability of sufficiency is the probability that if the cause\nwere added to a situation in which it and the effect was absent, it\nwould have resulted in the effect occurring. The probability of\nsufficiency is closely related to the quantity that Sheps (1958)\ncalled the relative difference, and that Cheng (1997) calls\nthe causal power. Cheng\u2019s terminology reflects the idea\nthat the probability of sufficiency of C for E is the\npower of C to bring about E in cases where E is\nabsent. As in the case of the probability of necessity, if one does\nnot have a complete structural equation model, but only a Causal Bayes\nNet or Semi-Markov Causal Model, it is usually only possible to put\nupper and lower bounds on the probability of sufficiency. Using the\nprobabilities from the previous example, the probability of\nsufficiency of asbestos for lung cancer would be between .05 (5/94)\nand .12 (11/94).\n\nDetermining the probabilities of counterfactuals, even just upper and\nlower bounds, is computationally demanding. Balke and Pearl\u2019s\ntwin network method (Balke and Pearl (1994a), (1994b); Pearl (2009,\npp. 213 - 215)) and Richardson and Robins\u2019 split-node method\n(Richardson and Robins (2016)) are two methods that have been proposed\nfor solving this kind of problem.\n5. Further Reading\n\nThe most important works surveyed in this entry are Pearl 2009 and\nSpirtes, Glymour, & Scheines 2000. Pearl 2010, Pearl et al. 2016,\nand Pearl & Mackenzie 2018 are three overviews of Pearl\u2019s\nprogram. Pearl 2010 is the shortest, but the most technical. Pearl\n& Mackenzie 2018 is the least technical. Scheines 1997 and the\n\u201cIntroduction\u201d of Glymour & Cooper 1999 are accessible\nintroductions to the SGS program. Eberhardt 2009, Hausman 1999,\nGlymour 2009, and Hitchcock 2009 are short overviews that cover some\nof the topics raised in this entry.\n\nThe entry on\n causation and manipulability\n contains extensive discussion of interventions, and some discussion\nof causal models.\n\nHalpern (2016) engages with many of the topics in\n Section 3.\n See also the entry for\n counterfactual theories of causation.\n\nThe entry on\n probabilistic causation\n contains some overlap with the present entry. Some of the material from\n Section 4\n of this entry is also presented in Section 3 of that entry. That\nentry contains in addition some discussion of the connection between\nprobabilistic causal models and earlier probabilistic theories of\ncausation.\n\nEberhardt 2017 is a short survey that provides a clear introduction to\nmany of the topics covered in\n Sections 4.2\n through 4.6, as well as Section\n 4.9. Spirtes and\nZhang 2016 is a longer and more technical overview that covers much of\nthe same ground. It has particularly good coverage on the issues\nraised in\n Section 4.5.\n\nThe entries on\n decision theory\n and\n causal decision theory\n present more detailed background information about some of the issues\nraised in\n Section 4.8.\n\nThis entry has focused on topics that are likely to be of most\ninterest to philosophers. There are a number of important technical\nissues that have been largely ignored. Many of these address problems\nthat arise when various simplifying assumptions made here (such as\nacyclicity, and knowledge of the true probabilities) are\nrejected. Some of these issues are briefly surveyed along with\nreferences in\n Supplement on Further Topics in Causal Inference.\n",
    "bibliography": {
        "categories": [],
        "cat_ref_text": {
            "ref_list": [
                "Balke, Alexander and Judea Pearl, 1994a, \u201cProbabilistic\nEvaluation of Counterfactual Queries\u201d, in Barbara Hayes-Roth and\nRichard E Korf (eds.), <em>Proceedings of the Twelfth National\nConference on Artificial Intelligence</em>, Volume I, Menlo Park CA:\nAAAI Press, pp. 230\u2013237.\n [<a href=\"https://www.aaai.org/Papers/AAAI/1994/AAAI94-035.pdf\" target=\"other\">Balke &amp; Pearl 1994a available online</a>]",
                "\u2013\u2013\u2013, 1994b, \u201cCounterfactual Probabilities:\nComputational Methods, Bounds, and Applications\u201d, in Ramon Lopez\nde Mantaras and David Poole (eds.), <em>Proceedings of the Tenth\nConference on Uncertainty in Artificial Intelligence</em>, San\nFrancisco: Morgan Kaufmann, pp. 46\u201354.\n [<a href=\"https://arxiv.org/abs/1302.6784\" target=\"other\">Balke &amp; Pearl 1994b available online</a>]",
                "Bareinboim, Elias, and Judea Pearl, 2013, \u201cA General\nAlgorithm for Deciding Transportability of Experimental\nResults\u201d, <em>Journal of Causal Inference</em>, 1(1):\n107\u2013134. doi:10.1515/jci-2012-0004",
                "\u2013\u2013\u2013, 2014, \u201cTransportability from Multiple\nEnvironments with Limited Experiments: Completeness Results\u201d, in\nZoubin Ghahramani, Max Welling, Corinna Cortes, and Neil Lawrence and\nKilian Weinberger (eds.), <em>Advances of Neural Information\nProcessing 27 (NIPS Proceedings)</em>, 280\u2013288.\n [<a href=\"http://papers.nips.cc/paper/5536-transportability-from-multiple-environments-with-limited-experiments-completeness-results.pdf\" target=\"other\">Bareinboim &amp; Pearl 2014 available online</a>]",
                "\u2013\u2013\u2013, 2015, \u201cCausal Inference and the\nData-Fusion Problem\u201d, <em>Proceedings of the National Academy of\nSciences</em>, 113(27): 7345\u20137352.\ndoi:10.1073/pnas.1510507113",
                "Beckers, Sander and Joost Vennekens, 2018, \u201cA Principled\nApproach to Defining Actual Causation\u201d, <em>Synthese</em>,\n195(2): 835\u2013862. doi:10.1007/s11229-016-1247-1",
                "Beebee, Helen, Christopher Hitchcock, and Peter Menzies (eds.),\n2009, <em>The Oxford Handbook of Causation</em>, Oxford: Oxford\nUniversity Press.",
                "Blanchard, Thomas, and Jonathan Schaffer, 2017,\u201cCause\nwithout Default\u201d, in Helen Beebee, Christopher Hitchcock, and\nHuw Price (eds.). <em>Making a Difference</em>, Oxford: Oxford\nUniversity Press, pp. 175\u2013214.",
                "Briggs, Rachael, 2012, \u201cInterventionist\nCounterfactuals\u201d, <em>Philosophical Studies</em>160(1):\n139\u2013166. doi:10.1007/s11098-012-9908-5",
                "Cartwright, Nancy, 1993, \u201cMarks and Probabilities: Two Ways\nto Find Causal Structure\u201d, in Fritz Stadler (ed.),\n<em>Scientific Philosophy: Origins and Development</em>, Dordrecht:\nKluwer, 113\u2013119. doi:10.1007/978-94-017-2964-2_7",
                "\u2013\u2013\u2013, 2007, <em>Hunting Causes and Using\nThem</em>, Cambridge: Cambridge University Press.\ndoi:10.1017/CBO9780511618758",
                "Chalupka, Krzysztof, Frederick Eberhardt, and Pietro Perona, 2017,\n\u201cCausal Feature Learning: an Overview\u201d,\n<em>Behaviormetrika</em>, 44(1): 137\u2013167.\ndoi:10.1007/s41237-016-0008-2",
                "Cheng, Patricia, 1997, \u201cFrom Covariation to Causation: A\nCausal Power Theory\u201d, <em>Psychological Review</em>, 104(2):\n367\u2013 405. doi:10.1037/0033-295X.104.2.367",
                "Claassen, Tom and Tom Heskes, 2012, \u201cA Bayesian Approach to\nConstraint Based Causal Inference\u201d, in Nando de Freitas and\nKevin Murphy (eds.) <em>Proceedings of the Twenty-Eighth Conference on\nUncertainty in Artificial Intelligence</em>, Corvallis, OR: AUAI\nPress, pp. 207\u2013216.\n [<a href=\"http://arxiv.org/abs/1210.4866\" target=\"other\">Claassen &amp; Heskes 2012 available online</a>]",
                "Cooper, G. F. and Herskovits, E. 1992, \u201cA Bayesian Method\nfor the Induction of Probabilistic Networks from Data\u201d,\n<em>Machine Learning</em>, 9(4): 309\u2013347.\ndoi:10.1007/BF00994110",
                "Danks, David, and Sergey Plis, 2014, \u201cLearning Causal\nStructure from Undersampled Time Series\u201d, <em>JMLR Workshop and\nConference Proceedings (NIPS Workshop on Causality)</em>.\n [<a href=\"https://doi.org/10.1184/R1/6492101.v1\" target=\"other\">Danks &amp; Plis 2014 available online</a>]",
                "Dash, Denver and Marek Druzdzel, 2001, \u201cCaveats For Causal\nReasoning With Equilibrium Models\u201d, in Salem Benferhat and\nPhilippe Besnard (eds.) <em>Symbolic and Quantitative Approaches to\nReasoning with Uncertainty, 6th European Conference,\nProceedings. Lecture Notes in Computer Science 2143</em>, Berlin and\nHeidelberg: Springer,\npp. 92\u2013103. doi:10.1007/3-540-44652-4\\_18",
                "Dechter, Rina and Thomas Richardson (eds.), 2006, <em>Proceedings\nof the Twenty-Second Conference on Uncertainty in Artificial\nIntelligence</em>, Corvallis, OR: AUAI Press.",
                "Dowe, Phil, 2000, <em>Physical Causation</em>, Cambridge:\nUniversity of Cambridge Press. doi:10.1017/CBO9780511570650",
                "Eberhardt, Frederick, 2009, \u201cIntroduction to the\nEpistemology of Causation\u201d, <em>Philosophy Compass</em>, 4(6):\n913\u2013925. doi:10.1111/j.1747-9991.2009.00243.x",
                "\u2013\u2013\u2013, 2017, \u201cIntroduction to the\nFoundations of Causal Discovery\u201d, <em>International Journal of\nData Science and Analytics</em>, 3(2): 81\u201391.\ndoi:10.1007/s41060-016-0038-6",
                "Eberhardt, Frederick and Richard Scheines, 2007,\n\u201cInterventions and Causal Inference\u201d, <em>Philosophy of\nScience</em>, 74(5): 981\u2013995. doi:10.1086/525638 ",
                "Eells, Ellery, 1991, <em>Probabilistic Causality</em>, Cambridge:\nCambridge University Press. doi:10.1017/CBO9780511570667",
                "Eichler, Michael, 2012, \u201cCausal Inference in Time Series\nAnalysis\u201d, in Carlo Berzuini, Philip Dawid, and Luisa\nBernardinelli (eds.), <em>Causality: Statistical Perspectives and\nApplications</em>, Chichester, UK: Wiley, pp. 327\u2013354.\ndoi:10.1002/9781119945710.ch22",
                "Fine, Kit, 2012, \u201cCounterfactuals without Possible\nWorlds\u201d, <em>Journal of Philosophy</em>, 109(3): 221\u2013246.\ndoi:10.5840/jphil201210938",
                "Galles, David, and Judea Pearl, 1998, \u201cAn Axiomatic\nCharacterization of Causal Counterfactuals\u201d, <em>Foundations of\nScience</em>, 3(1): 151\u2013182. doi:10.1023/A:1009602825894",
                "Geiger, Dan and David Heckerman, 1994, \u201cLearning Gaussian\nNetworks\u201d, Technical Report MSR-TR-94-10, Microsoft\nResearch.",
                "Geiger, Dan and Judea Pearl, 1988, \u201cOn the Logic of Causal\nModels\u201d, in Ross Shachter, Tod Levitt, Laveen Kanal, and John\nLemmer (eds.), <em>Proceedings of the Fourth Conference on Uncertainty\nin Artificial Intelligence</em>, Corvallis, OR: AUAI Press, pp.\n136\u2013147.",
                "Gibbard, Alan, and William Harper, 1978, \u201cCounterfactuals\nand Two Kinds of Expected Utility\u201d, in Clifford Hooker, James\nLeach, and Edward McClennen (eds.), <em>Foundations and Applications\nof Decision Theory</em>, Dordrecht: Reidel, pp. 125\u201362.",
                "Glennan, Stuart, 2017, <em>The New Mechanical Philosophy</em>,\nOxford: Oxford University Press.",
                "Glymour, Clark, 2009, \u201cCausality and Statistics\u201d, in\nBeebee, Hitchcock, and Menzies 2009: 498\u2013522.",
                "Glymour, Clark and Gregory Cooper, 1999, <em>Computation,\nCausation, and Discovery</em>, Cambridge, MA: MIT Press.",
                "Glymour, Clark, David Danks, Bruce Glymour, Frederick Eberhardt,\nJoseph Ramsey, Richard Scheines, Peter Spirtes, Choh Man Teng, and\nJiji Zhang, 2010, \u201cActual Causation: a Stone Soup Essay\u201d,\n<em>Synthese</em>, 175(2): 169\u2013192.\ndoi:10.1007/s11229-009-9497-9",
                "Glymour, Clark and Frank Wimberly, 2007, \u201cActual Causes and\nThought Experiments\u201d, in Joseph Campbell, Michael\nO\u2019Rourke, and Harry Silverstein (eds.), <em>Causation and\nExplanation</em>, Cambridge, MA: MIT Press, pp. 43\u201368.",
                "Gong, Mingming, Kun Zhang, Bernhard Sch\u00f6lkopf, Dacheng Tao,\nand Philipp Geiger, 2015, \u201cDiscovering Temporal Causal Relations\nfrom Subsampled Data\u201d, in Francis Bach and David Blei (eds.),\n<em>Proceeding of the 32<sup>nd</sup> International Conference on\nMachine Learning</em>, 37: 1898\u20131906.\n [<a href=\"http://proceedings.mlr.press/v37/gongb15.html\" target=\"other\">Gong et al. 2015 available online</a>]",
                "Gong, Mingming, Kun Zhang, Bernhard Sch\u00f6lkopf, Clark Glymour,\nand Dacheng Tao, 2017, \u201cCausal Discovery from Temporally\nAggregated Time Series\u201d, in Gal Elidan and Kristian Kersting\n(eds.), <em>Proceedings of the Thirty-Third Conference on Uncertainty\nin Artificial Intelligence</em>, Corvallis, OR: AUAI Press.\n [<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5995575/\" target=\"other\">Gong et al. 2017 available online</a>]",
                "Greenland, Sander, and James Robins, 1988, \u201cConceptual\nProblems in the Definition and Interpretation of Attributable\nFractions\u201d, <em>American Journal of Epidemiology</em>, 128(6):\n1185\u20131197. doi:10.1093/oxfordjournals.aje.a115073",
                "Hall, Ned, 2007, \u201cStructural Equations and Causation\u201d,\n<em>Philosophical Studies</em>, 132(1): 109\u2013136.\ndoi:10.1007/s11098-006-9057-9",
                "Halpern, Joseph Y., 2000, \u201cAxiomatizing Causal\nReasoning\u201d, <em>Journal of Artificial Intelligence\nResearch</em>, 12: 317\u2013337.\n [<a href=\"https://www.jair.org/index.php/jair/article/view/10257\" target=\"other\">Halpern 2000 available online</a>]",
                "\u2013\u2013\u2013, 2008, \u201cDefaults and Normality in\nCausal Structures\u201d, in Gerhard Brewka and J\u00e9r\u00f4me\nLang (eds.), <em>Principles of Knowledge Representation and Reasoning:\nProceedings of the Eleventh International Conference</em>, Menlo Park,\nCA: AAAI Press, pp. 198\u2013208.",
                "\u2013\u2013\u2013, 2016, <em>Actual Causality</em>, Cambridge,\nMA: MIT Press.",
                "Halpern, Joseph Y. and Christopher Hitchcock, 2015, \u201cGraded\nCausation and Defaults\u201d, <em>British Journal for Philosophy of\nScience</em>, 66(2): 413\u201357. doi:10.1093/bjps/axt050",
                "Halpern, Joseph and Judea Pearl, 2001, \u201cCauses and\nExplanations: A Structural-Model Approach. Part I: Causes\u201d, in\nJohn Breese and Daphne Koller (eds.), <em>Proceedings of the\nSeventeenth Conference on Uncertainty in Artificial Intelligence</em>,\nSan Francisco: Morgan Kaufmann, pp. 194\u2013202",
                "\u2013\u2013\u2013, 2005, \u201cCauses and Explanations: A\nStructural-Model Approach. Part I: Causes\u201d, <em>British Journal\nfor the Philosophy of Science</em>, 56(4): 843\u2013887.\ndoi:10.1093/bjps/axi147",
                "Hausman, Daniel M., 1999, \u201cThe Mathematical Theory of\nCausation\u201d, <em>British Journal for the Philosophy of\nScience</em>, 50(1): 151\u2013162. doi:10.1093/bjps/50.1.151",
                "Hausman, Daniel M. and James Woodward, 1999, \u201cIndependence,\nInvariance, and the Causal Markov Condition\u201d, <em>British\nJournal for the Philosophy of Science</em>, 50(4): 521\u2013583.\ndoi:10.1093/bjps/50.4.521",
                "\u2013\u2013\u2013, 2004, \u201cModularity and the Causal\nMarkov Condition: a Restatement\u201d, <em>British Journal for the\nPhilosophy of Science</em>, 55(1): 147\u2013161.\ndoi:10.1093/bjps/55.1.147",
                "Hitchcock, Christopher, 2001, \u201cThe Intransitivity of\nCausation Revealed in Equations and Graphs\u201d, <em>Journal of\nPhilosophy</em>, 98(6): 273\u2013299. doi:10.2307/2678432",
                "\u2013\u2013\u2013, 2007, \u201cPrevention, Preemption, and\nthe Principle of Sufficient Reason\u201d, <em>Philosophical\nReview</em>, 116(4): 495\u2013532. doi:10.1215/00318108-2007-012",
                "\u2013\u2013\u2013, 2009, \u201cCausal Models\u201d, in\nBeebee, Hitchcock, and Menzies 2009: 299\u2013314.",
                "\u2013\u2013\u2013, 2016, \u201cConditioning, Intervening, and\nDecision\u201d, <em>Synthese</em>, 193(4): 1157\u20131176.\ndoi:10.1007/s11229-015-0710-8",
                "Hoyer, Patrik O., Dominik Janzing, Joris Mooij, Jonas Peters, and\nBernhard Sch\u00f6lkopf, 2009, \u201cNonlinear Causal Discovery with\nAdditive Noise Models\u201d, <em>Advances in Neural Information\nProcessing Systems</em>, 21: 689\u2013696.\n [<a href=\"https://papers.nips.cc/paper/3548-nonlinear-causal-discovery-with-additive-noise-models\" target=\"other\">Hoyer et al. 2009 available online</a>]",
                "Huang, Yimin and Marco Valtorta, 2006, \u201cPearl\u2019s\nCalculus of Intervention Is Complete\u201d, in Dechter and Richardson\n2006: 217\u2013224.\n [<a href=\"http://arxiv.org/abs/1206.6831\" target=\"other\">Huang &amp; Valtorta 2006 available online</a>]",
                "Hyttinen, Antti, Frederick Eberhardt, and Patrik O. Hoyer, 2013a,\n\u201cExperiment Selection for Causal Discovery\u201d, <em>Journal\nof Machine Learning Research</em>, 14: 3041\u20133071.\n [<a href=\"http://www.jmlr.org/papers/v14/hyttinen13a.html\" target=\"other\">Hyttinen, Eberhardt, &amp; Hoyer 2013a available online</a>]",
                "Hyttinen, Antti, Frederick Eberhardt, and Matti J\u00e4rvisalo,\n2014, \u201cConstraint-based Causal Discovery: Conflict Resolution\nwith Answer Set Programming\u201d, in Nevin Zhang and Jin Tian\n(eds.), <em>Proceedings of the Thirtieth Conference on Uncertainty in\nArtificial Intelligence</em>, Corvallis, OR: AUAI Press, pp.\n340\u2013349.",
                "\u2013\u2013\u2013, 2015, \u201cDo-calculus When the True\nGraph is Unknown\u201d, in Marina Meila and Tom Heskes (eds.),\n<em>Uncertainty in Artificial Intelligence: Proceedings of the\nThirty-First Conference</em>, Corvallis, OR: AUAI Press, pp.\n395\u2013404.",
                "Hyttinen, Antti, Patrik O. Hoyer, Frederick Eberhardt, and Matti\n\nJ\u00e4rvisalo, 2013b, \u201cDiscovering Cyclic Causal Models with\nLatent Variables: A General SAT-Based Procedure\u201d, in Nichols and\nSmyth 2013: 301\u2013310.",
                "Hyttinen, Antti, Sergey Plis, Matti J\u00e4rvisalo, Frederick\nEberhardt, and David Danks, 2016, \u201cCausal Discovery from\nSubsampled Time Series Data by Constraint Optimization\u201d, in\nAlessandro Antonucci, Giorgio Corani, Cassio Polpo Campos (eds.)\n<em>Proceedings of the Eighth International Conference on\nProbabilistic Graphical Models</em>, pp. 216\u2013227.",
                "Jeffrey, Richard, 1983, <em>The Logic of Decision</em>, Second\nEdition, Chicago: University of Chicago Press.",
                "Joyce, James M., 1999, <em>The Foundations of Causal Decision\nTheory</em>, Cambridge: Cambridge University Press.\ndoi:10.1017/CBO9780511498497",
                "Lewis, David, 1973a, \u201cCausation\u201d, <em>Journal of\nPhilosophy</em>, 70(17): 556\u2013567. doi:10.2307/2025310",
                "\u2013\u2013\u2013, 1973b, <em>Counterfactuals</em>, Oxford:\nBlackwell.",
                "\u2013\u2013\u2013, 1979, \u201cCounterfactual Dependence and\nTime\u2019s Arrow\u201d, <em>No\u00fbs</em>, 13(4): 455\u2013476.\ndoi:10.2307/2215339 ",
                "\u2013\u2013\u2013, 1981, \u201cCausal Decision Theory\u201d,\n<em>Australasian Journal of Philosophy</em>, 59(1): 5\u201330.\ndoi:10.1080/00048408112340011",
                "Machamer, Peter, Lindley Darden, and Carl Craver, 2000,\n\u201cThinking about Mechanisms\u201d, <em>Philosophy of\nScience</em>, 67(1): 1\u201325. doi:10.1086/392759",
                "Maier, Marc, Katerina Marazopoulou, David Arbour, and David\nJensen, 2013, \u201cA Sound and Complete Algorithm for Learning\nCausal Models from Relational Data\u201d, in Nichols and Smyth 2013:\n371\u2013380.\n [<a href=\"http://arxiv.org/abs/1309.6843\" target=\"other\">Maier et al. 2013 available online</a>]",
                "Maier, Marc, Brian Taylor, H\u00fcseyin Oktay, and David Jensen,\n2010, \u201cLearning Causal Models of Relational Domains\u201d, in\nMaria Fox and David Poole (eds.), <em>Proceedings of the Twenty-Fourth\nAAAI Conference on Artificial Intelligence</em>, (Menlo Park CA: AAAI\nPress), pp. 531\u2013538.\n [<a href=\"https://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/view/1919\" target=\"other\">Maier et al. 2010 available online</a>]",
                "Meek, Christopher, 1995, \u201cStrong Completeness and\nFaithfulness in Bayesian Networks\u201d, in Philippe Besnard and\nSteve Hanks (eds.) <em>Proceedings of the Eleventh Conference\nConference on Uncertainty in Artificial Intelligence</em>, San\nFrancisco: Morgan Kaufmann, pp. 411\u2013418.",
                "Meek, Christopher and Clark Glymour, 1994, \u201cConditioning and\nIntervening\u201d, <em>British Journal for Philosophy of\nScience,</em>, 45(4): 1001\u20131024. doi:10.1093/bjps/45.4.1001",
                "Menzies, Peter, 2004, \u201cCausal Models, Token Causation, and\nProcesses\u201d, <em>Philosophy of Science</em>, 71(5):\n820\u2013832. doi:10.1086/425057 ",
                "Mooij, Joris, Dominik Janzing, and Bernhard Sch\u00f6lkopf, 2013,\n\u201cFrom Ordinary Differential Equations to Structural Causal\nModels: the Deterministic Case\u201d, in Nichols and Smyth 2013:\n440\u2013448.",
                "Neal, Radford M., 2000, \u201cOn Deducing Conditional\nIndependence from d-separation in Causal Graphs with Feedback\u201d,\n<em>Journal of Artificial Intelligence Research</em>, 12: 87\u201391.\n [<a href=\"https://www.jair.org/index.php/jair/article/view/10250\" target=\"other\">Neal 2000 available online</a>]",
                "Neapolitan, Richard, 2004, <em>Learning Bayesian Networks</em>,\nUpper Saddle River, NJ: Prentice Hall.",
                "Neapolitan, Richard and Xia Jiang, 2016, \u201cThe Bayesian\nNetwork Story\u201d, in Alan H\u00e1jek and Christopher Hitchcock\n(eds.), <em>The Oxford Handbook of Probability and Philosophy</em>,\nOxford: Oxford University Press, pp. 183\u201399.",
                "Neyman, Jerzy, 1923 [1990], \u201cSur les Applications de la\nTh\u00e9orie des Probabilit\u00e9s aux Experiences Agricoles:\nEssai des Principes\u201d) <em>Roczniki Nauk Rolniczych, Tom</em>, X:\n1\u201351. Excerpts translated into English by D. M. Dabrowska and\nTerrence Speed, 1990, \u201cOn the Application of Probability Theory\nto Agricultural Experiments. Essay on Principles\u201d,\n<em>Statistical Science</em>, 5(4): 465\u201380.\ndoi:10.1214/ss/1177012031",
                "Ann Nichols and Padhraic Smyth (eds), 2013, <em>Proceedings of the\nTwenty-Ninth Conference on Uncertainty in Artificial\nIntelligence</em>, Corvallis, OR: AUAI Press.",
                "Nozick, Robert, 1969, \u201cNewcomb\u2019s Problem and Two\nPrinciples of Choice\u201d, in Nicholas Rescher (ed.), <em>Essays in\nHonor of Carl G. Hempel</em>, Dordrecht: Reidel, pp. 114\u2013146.\ndoi:10.1007/978-94-017-1466-2_7",
                "Pearl, Judea, 1988, <em>Probabilistic Reasoning in Intelligent\nSystems</em>, San Francisco: Morgan Kaufmann.",
                "\u2013\u2013\u2013, 1995, \u201cCausal Diagrams for Empirical\nResearch\u201d, <em>Biometrika</em>, 82(4): 669\u2013688.\ndoi:10.1093/biomet/82.4.669",
                "\u2013\u2013\u2013, 2009, <em>Causality: Models, Reasoning, and\nInference</em>, Second Edition, Cambridge: Cambridge University\nPress.",
                "\u2013\u2013\u2013, 2010, \u201cAn Introduction to Causal\nInference\u201d, <em>The International Journal of Biostatistics</em>,\n6(2): article 7, pp. 1\u201359. doi:10.2202/1557-4679.1203",
                "Pearl, Judea and Rina Dechter, 1996, \u201cIdentifying\nIndependencies in Causal Graphs with Feedback\u201d, in Eric Horvitz\nand Finn Jensen (eds.) <em>Proceedings of the Twelfth Conference on\nUncertainty in Artificial Intelligence</em>, San Francisco: Morgan\nKaufmann, pages 420\u2013426.",
                "Pearl, Judea, Madelyn Glymour, and Nicholas P. Jewell, 2016,\n<em>Causal Inference in Statistics: A Primer</em>, Chichester, UK:\nWiley.",
                "Pearl, Judea and Mackenzie, Dana, 2018, <em>The Book of Why: The\nNew Science of Cause and Effect.</em>, New York: Basic Books.",
                "Pearl, Judea and Verma, Thomas, 1991, \u201cA Theory of Inferred\nCausation\u201d, in James Allen, Richard Fiskes, and Erik Sandewall\n(eds.), <em>Principles of Knowledge Representation and Reasoning:\nProceedings of the Second International Conference</em>, San Mateo,\nCA: Morgan Kaufmann, pp. 441\u201352.",
                "Peters, Jonas, Dominik Janzing, and Bernhard Sch\u00f6lkopf, 2017,\n<em>Elements of Causal Inference: Foundations and Learning\nAlgorithms.</em>, Cambridge, MA: MIT Press. ",
                "Price, Huw, 1986, \u201cAgainst Causal Decision Theory\u201d,\n<em>Synthese</em>, 67(2): 195\u2013212. doi:10.1007/BF00540068",
                "Ramsey, Joseph, Peter Spirtes, and Jiji Zhang, 2006,\n\u201cAdjacency Faithfulness and Conservative Causal\nInference\u201d, in Dechter and Richardson 2006: 401\u2013408.\n [<a href=\"http://arxiv.org/abs/1206.6843\" target=\"other\">Ramsey, Spirtes, &amp; Zhang 2006 available online</a>]",
                "Reichenbach, Hans, 1956, <em>The Direction of Time</em>, Berkeley\nand Los Angeles: University of California Press.",
                "Richardson, Thomas, and James Robins, 2016, <em>Single World\nIntervention Graphs (SWIGs): A Unification of the Counterfactual and\nGraphical Approaches to Causality</em>, Hanover, MA: Now\nPublishers.",
                "Robins, James, 1986, \u201cA New Approach to Causal Inference in\nMortality Studies with a Sustained Exposure Period: Applications to\nControl of the Healthy Workers Survivor Effect\u201d,\n<em>Mathematical Modeling</em>, 7(9\u201312): 1393\u20131512.\ndoi:10.1016/0270-0255(86)90088-6",
                "Rubin, Donald, 1974, \u201cEstimating Causal Effects of\nTreatments in Randomized and Nonrandomized Studies\u201d, <em>Journal\nof Educational Psychology</em>, 66(5): 688\u2013701.\ndoi:10.1037/h0037350",
                "Salmon, Wesley, 1984, <em>Scientific Explanation and the Causal\nStructure of the World</em>, Princeton: Princeton University\nPress.",
                "Scheines, Richard, 1997, \u201cAn Introduction to Causal\nInference\u201d in V. McKim and S. Turner (eds.), <em>Causality in\nCrisis?</em>, Notre Dame: University of Notre Dame Press, pp.\n185\u2013199.",
                "Schulte, Oliver and Hassan Khosravi, 2012, \u201cLearning\nGraphical Models for Relational Data via Lattice Search\u201d,\n<em>Machine Learning</em>, 88(3): 331\u2013368.\ndoi:10.1007/s10994-012-5289-4",
                "Schulte, Oliver, Wei Luo, and Russell Greiner, 2010, \u201cMind\nChange Optimal Learning of Bayes Net Structure from Dependency and\nIndependency Data\u201d, <em>Information and Computation</em>,\n208(1): 63\u201382. doi:10.1016/j.ic.2009.03.009",
                "Shalizi, Cosma Rohilla, and Andrew C. Thomas, 2011,\n\u201cHomophily and Contagion are Generically Confounded in\nObservational Social Studies\u201d, <em>Sociological Methods and\nResearch</em>, 40(2): 211\u2013239. doi:10.1177/0049124111404820",
                "Sheps, Mindel C., 1958, \u201cShall We Count the Living or the\nDead?\u201d <em>New England Journal of Medicine</em>, 259(12):\n210\u20134. doi:10.1056/NEJM195812182592505",
                "Shimizu, Shohei, Patrik O. Hoyer, Aapo Hyv\u00e4rinen, and Antti\nKermine, 2006, \u201cA Linear Non-Gaussian Acyclic Model for Causal\nDiscovery\u201d, <em>Journal of Machine Learning Research</em>, 7:\n2003\u20132030.\n [<a href=\"http://www.jmlr.org/papers/v7/shimizu06a.html\" target=\"other\">Shimizu et al. 2006 available online</a>]",
                "Shpitser, Ilya and Judea Pearl, 2006, \u201cIdentification of\nConditional Interventional Distributions\u201d, in Dechter and\nRichardson 2006: 437\u2013444.\n [<a href=\"http://arxiv.org/abs/1206.6876\" target=\"other\">Shpister &amp; Pearl 2006 available online</a>]",
                "Skyrms, Brian, 1980, <em>Causal Necessity</em>, New Haven and\nLondon: Yale University Press.",
                "Spirtes, Peter, 1995, \u201cDirected Cyclic Graphical\nRepresentation of Feedback Models\u201d, in Philippe Besnard and\nSteve Hanks (eds.), <em>Proceedings of the Eleventh Conference on\nUncertainty in Artificial Intelligence</em>, San Francisco: Morgan\nKaufmann, pp. 491\u2013498.",
                "Spirtes, Peter, Clark Glymour, and Richard Scheines, [SGS] 2000,\n<em>Causation, Prediction and Search</em>, Second Edition, Cambridge,\nMA: MIT Press.",
                "Spirtes, Peter and Jiji Zhang, 2014, \u201cA Uniformly Consistent\nEstimator of Causal Effects under the <i>k</i>-Triangle-Faithfulness\nAssumption\u201d, <em>Statistical Science</em>, 29(4): 662\u2013678.\ndoi:10.1214/13-STS429",
                "Spirtes, Peter and Kun Zhang, 2016, \u201cCausal Discovery and\nInference: Concepts and Recent Methodological Advances\u201d,\n<em>Applied Informatics</em>, 3: 3. doi:10.1186/s40535-016-0018-x",
                "Stalnaker, Robert, 1968, \u201cA Theory of Conditionals\u201d,\nin Nicholas Rescher (ed.) <em>Studies in Logical Theory</em>,\nBlackwell: Oxford, pp. 98\u2013112.",
                "Steel, Daniel, 2006, \u201cHomogeneity, Selection, and the\nFaithfulness Condition\u201d. <em>Minds and Machines</em>, 16(3):\n303\u2013317. doi:10.1007/s11023-006-9032-4",
                "Stern, Reuben, 2017, \u201cInterventionist Decision\nTheory\u201d, <em>Synthese</em>, 194(10): 4133\u20134153.\ndoi:10.1007/s11229-016-1133-x",
                "Suppes, Patrick, 1970, <em>A Probabilistic Theory of\nCausality</em>, Amsterdam: North-Holland Publishing Company.",
                "Tillman, Robert E., and Frederick Eberhardt, 2014, \u201cLearning\nCausal Structure from Multiple Datasets with Similar Variable\nSets\u201d, <em>Behaviormetrika</em>, 41(1): 41\u201364.\ndoi:10.2333/bhmk.41.41",
                "Triantafillou, Sofia, and Ioannis Tsamardinos, 2015,\n\u201cConstraint-based Causal Discovery from Multiple Interventions\nover Overlapping Variable Sets\u201d, <em>Journal of Machine Learning\nResearch</em>, 16: 2147\u20132205.\n [<a href=\"http://www.jmlr.org/papers/v16/triantafillou15a.html\" target=\"other\">Triantafillou &amp; Tsamardinos 2015 available online</a>]",
                "Weslake, Brad, forthcoming, \u201cA Partial Theory of Actual\nCausation\u201d, <em>British Journal for the Philosophy of\nScience.</em>",
                "Woodward, James, 2003, <em>Making Things Happen: A Theory of\nCausal Explanation</em>, Oxford: Oxford University Press.\ndoi:10.1093/0195155270.001.0001",
                "Wright, Sewall, 1921, \u201cCorrelation and Causation\u201d,\n<em>Journal of Agricultural Research</em>, 20: 557\u201385.",
                "Zhalama, Jiji Zhang, and Wolfgang Mayer, 2016, \u201cWeakening\nFaithfulness: Some Heuristic Causal Discovery Algorithms\u201d,\n<em>International Journal of Data Science and Analytics</em>, 3(2):\n93\u2013104. doi:10.1007/s41060-016-0033-y",
                "Zhang, Jiji, 2008, \u201cCausal Reasoning with Ancestral\nGraphs\u201d, <em>Journal of Machine Learning Research</em>, 9:\n1437\u20131474.\n [<a href=\"http://www.jmlr.org/papers/v9/zhang08a.html\" target=\"other\">Zhang 2008 available online</a>]",
                "\u2013\u2013\u2013, 2013a, \u201cA Lewisian Logic of\nCounterfactuals\u201d, <em>Minds and Machines</em>, 23(1):\n77\u201393. doi:10.1007/s11023-011-9261-z",
                "\u2013\u2013\u2013, 2013b, \u201cA Comparison of Three\nOccam\u2019s Razors for Markovian Causal Models\u201d, <em>British\nJournal for Philosophy of Science</em>, 64(2): 423\u2013448.\ndoi:10.1093/bjps/axs005",
                "Zhang, Jiji and Peter Spirtes 2008, \u201cDetection of\nUnfaithfulness and Robust Causal Inference\u201d, <em>Minds and\nMachines</em>, 18(2): 239\u2013271.\ndoi:10.1007/s11023-008-9096-4",
                "\u2013\u2013\u2013, 2016, \u201cThe Three Faces of\nFaithfulness\u201d, <em>Synthese</em>, 193(4): 1011\u20131027.\ndoi:10.1007/s11229-015-0673-9",
                "Zhang, Kun, and Aapo Hyv\u00e4rinen, 2009, \u201cOn the\nIdentifiability of the Post-nonlinear Causal Model\u201d, in Jeff\nBilmes and Andrew Ng (eds.), <em>Proceeding of the Twenty-Fifth\nConference on Uncertainty in Artificial Intelligence</em>, (Corvallis,\nOR: AUAI Press), pp. 647\u2013655."
            ]
        },
        "raw_text": "<div id=\"bibliography\">\n<h2 id=\"Bib\">Bibliography</h2>\n<ul class=\"hanging\">\n<li>Balke, Alexander and Judea Pearl, 1994a, \u201cProbabilistic\nEvaluation of Counterfactual Queries\u201d, in Barbara Hayes-Roth and\nRichard E Korf (eds.), <em>Proceedings of the Twelfth National\nConference on Artificial Intelligence</em>, Volume I, Menlo Park CA:\nAAAI Press, pp. 230\u2013237.\n [<a href=\"https://www.aaai.org/Papers/AAAI/1994/AAAI94-035.pdf\" target=\"other\">Balke &amp; Pearl 1994a available online</a>]</li>\n<li>\u2013\u2013\u2013, 1994b, \u201cCounterfactual Probabilities:\nComputational Methods, Bounds, and Applications\u201d, in Ramon Lopez\nde Mantaras and David Poole (eds.), <em>Proceedings of the Tenth\nConference on Uncertainty in Artificial Intelligence</em>, San\nFrancisco: Morgan Kaufmann, pp. 46\u201354.\n [<a href=\"https://arxiv.org/abs/1302.6784\" target=\"other\">Balke &amp; Pearl 1994b available online</a>]</li>\n<li>Bareinboim, Elias, and Judea Pearl, 2013, \u201cA General\nAlgorithm for Deciding Transportability of Experimental\nResults\u201d, <em>Journal of Causal Inference</em>, 1(1):\n107\u2013134. doi:10.1515/jci-2012-0004</li>\n<li>\u2013\u2013\u2013, 2014, \u201cTransportability from Multiple\nEnvironments with Limited Experiments: Completeness Results\u201d, in\nZoubin Ghahramani, Max Welling, Corinna Cortes, and Neil Lawrence and\nKilian Weinberger (eds.), <em>Advances of Neural Information\nProcessing 27 (NIPS Proceedings)</em>, 280\u2013288.\n [<a href=\"http://papers.nips.cc/paper/5536-transportability-from-multiple-environments-with-limited-experiments-completeness-results.pdf\" target=\"other\">Bareinboim &amp; Pearl 2014 available online</a>]</li>\n<li>\u2013\u2013\u2013, 2015, \u201cCausal Inference and the\nData-Fusion Problem\u201d, <em>Proceedings of the National Academy of\nSciences</em>, 113(27): 7345\u20137352.\ndoi:10.1073/pnas.1510507113</li>\n<li>Beckers, Sander and Joost Vennekens, 2018, \u201cA Principled\nApproach to Defining Actual Causation\u201d, <em>Synthese</em>,\n195(2): 835\u2013862. doi:10.1007/s11229-016-1247-1</li>\n<li>Beebee, Helen, Christopher Hitchcock, and Peter Menzies (eds.),\n2009, <em>The Oxford Handbook of Causation</em>, Oxford: Oxford\nUniversity Press.</li>\n<li>Blanchard, Thomas, and Jonathan Schaffer, 2017,\u201cCause\nwithout Default\u201d, in Helen Beebee, Christopher Hitchcock, and\nHuw Price (eds.). <em>Making a Difference</em>, Oxford: Oxford\nUniversity Press, pp. 175\u2013214.</li>\n<li>Briggs, Rachael, 2012, \u201cInterventionist\nCounterfactuals\u201d, <em>Philosophical Studies</em>160(1):\n139\u2013166. doi:10.1007/s11098-012-9908-5</li>\n<li>Cartwright, Nancy, 1993, \u201cMarks and Probabilities: Two Ways\nto Find Causal Structure\u201d, in Fritz Stadler (ed.),\n<em>Scientific Philosophy: Origins and Development</em>, Dordrecht:\nKluwer, 113\u2013119. doi:10.1007/978-94-017-2964-2_7</li>\n<li>\u2013\u2013\u2013, 2007, <em>Hunting Causes and Using\nThem</em>, Cambridge: Cambridge University Press.\ndoi:10.1017/CBO9780511618758</li>\n<li>Chalupka, Krzysztof, Frederick Eberhardt, and Pietro Perona, 2017,\n\u201cCausal Feature Learning: an Overview\u201d,\n<em>Behaviormetrika</em>, 44(1): 137\u2013167.\ndoi:10.1007/s41237-016-0008-2</li>\n<li>Cheng, Patricia, 1997, \u201cFrom Covariation to Causation: A\nCausal Power Theory\u201d, <em>Psychological Review</em>, 104(2):\n367\u2013 405. doi:10.1037/0033-295X.104.2.367</li>\n<li>Claassen, Tom and Tom Heskes, 2012, \u201cA Bayesian Approach to\nConstraint Based Causal Inference\u201d, in Nando de Freitas and\nKevin Murphy (eds.) <em>Proceedings of the Twenty-Eighth Conference on\nUncertainty in Artificial Intelligence</em>, Corvallis, OR: AUAI\nPress, pp. 207\u2013216.\n [<a href=\"http://arxiv.org/abs/1210.4866\" target=\"other\">Claassen &amp; Heskes 2012 available online</a>]</li>\n<li>Cooper, G. F. and Herskovits, E. 1992, \u201cA Bayesian Method\nfor the Induction of Probabilistic Networks from Data\u201d,\n<em>Machine Learning</em>, 9(4): 309\u2013347.\ndoi:10.1007/BF00994110</li>\n<li>Danks, David, and Sergey Plis, 2014, \u201cLearning Causal\nStructure from Undersampled Time Series\u201d, <em>JMLR Workshop and\nConference Proceedings (NIPS Workshop on Causality)</em>.\n [<a href=\"https://doi.org/10.1184/R1/6492101.v1\" target=\"other\">Danks &amp; Plis 2014 available online</a>]</li>\n<li>Dash, Denver and Marek Druzdzel, 2001, \u201cCaveats For Causal\nReasoning With Equilibrium Models\u201d, in Salem Benferhat and\nPhilippe Besnard (eds.) <em>Symbolic and Quantitative Approaches to\nReasoning with Uncertainty, 6th European Conference,\nProceedings. Lecture Notes in Computer Science 2143</em>, Berlin and\nHeidelberg: Springer,\npp. 92\u2013103. doi:10.1007/3-540-44652-4\\_18</li>\n<li>Dechter, Rina and Thomas Richardson (eds.), 2006, <em>Proceedings\nof the Twenty-Second Conference on Uncertainty in Artificial\nIntelligence</em>, Corvallis, OR: AUAI Press.</li>\n<li>Dowe, Phil, 2000, <em>Physical Causation</em>, Cambridge:\nUniversity of Cambridge Press. doi:10.1017/CBO9780511570650</li>\n<li>Eberhardt, Frederick, 2009, \u201cIntroduction to the\nEpistemology of Causation\u201d, <em>Philosophy Compass</em>, 4(6):\n913\u2013925. doi:10.1111/j.1747-9991.2009.00243.x</li>\n<li>\u2013\u2013\u2013, 2017, \u201cIntroduction to the\nFoundations of Causal Discovery\u201d, <em>International Journal of\nData Science and Analytics</em>, 3(2): 81\u201391.\ndoi:10.1007/s41060-016-0038-6</li>\n<li>Eberhardt, Frederick and Richard Scheines, 2007,\n\u201cInterventions and Causal Inference\u201d, <em>Philosophy of\nScience</em>, 74(5): 981\u2013995. doi:10.1086/525638 </li>\n<li>Eells, Ellery, 1991, <em>Probabilistic Causality</em>, Cambridge:\nCambridge University Press. doi:10.1017/CBO9780511570667</li>\n<li>Eichler, Michael, 2012, \u201cCausal Inference in Time Series\nAnalysis\u201d, in Carlo Berzuini, Philip Dawid, and Luisa\nBernardinelli (eds.), <em>Causality: Statistical Perspectives and\nApplications</em>, Chichester, UK: Wiley, pp. 327\u2013354.\ndoi:10.1002/9781119945710.ch22</li>\n<li>Fine, Kit, 2012, \u201cCounterfactuals without Possible\nWorlds\u201d, <em>Journal of Philosophy</em>, 109(3): 221\u2013246.\ndoi:10.5840/jphil201210938</li>\n<li>Galles, David, and Judea Pearl, 1998, \u201cAn Axiomatic\nCharacterization of Causal Counterfactuals\u201d, <em>Foundations of\nScience</em>, 3(1): 151\u2013182. doi:10.1023/A:1009602825894</li>\n<li>Geiger, Dan and David Heckerman, 1994, \u201cLearning Gaussian\nNetworks\u201d, Technical Report MSR-TR-94-10, Microsoft\nResearch.</li>\n<li>Geiger, Dan and Judea Pearl, 1988, \u201cOn the Logic of Causal\nModels\u201d, in Ross Shachter, Tod Levitt, Laveen Kanal, and John\nLemmer (eds.), <em>Proceedings of the Fourth Conference on Uncertainty\nin Artificial Intelligence</em>, Corvallis, OR: AUAI Press, pp.\n136\u2013147.</li>\n<li>Gibbard, Alan, and William Harper, 1978, \u201cCounterfactuals\nand Two Kinds of Expected Utility\u201d, in Clifford Hooker, James\nLeach, and Edward McClennen (eds.), <em>Foundations and Applications\nof Decision Theory</em>, Dordrecht: Reidel, pp. 125\u201362.</li>\n<li>Glennan, Stuart, 2017, <em>The New Mechanical Philosophy</em>,\nOxford: Oxford University Press.</li>\n<li>Glymour, Clark, 2009, \u201cCausality and Statistics\u201d, in\nBeebee, Hitchcock, and Menzies 2009: 498\u2013522.</li>\n<li>Glymour, Clark and Gregory Cooper, 1999, <em>Computation,\nCausation, and Discovery</em>, Cambridge, MA: MIT Press.</li>\n<li>Glymour, Clark, David Danks, Bruce Glymour, Frederick Eberhardt,\nJoseph Ramsey, Richard Scheines, Peter Spirtes, Choh Man Teng, and\nJiji Zhang, 2010, \u201cActual Causation: a Stone Soup Essay\u201d,\n<em>Synthese</em>, 175(2): 169\u2013192.\ndoi:10.1007/s11229-009-9497-9</li>\n<li>Glymour, Clark and Frank Wimberly, 2007, \u201cActual Causes and\nThought Experiments\u201d, in Joseph Campbell, Michael\nO\u2019Rourke, and Harry Silverstein (eds.), <em>Causation and\nExplanation</em>, Cambridge, MA: MIT Press, pp. 43\u201368.</li>\n<li>Gong, Mingming, Kun Zhang, Bernhard Sch\u00f6lkopf, Dacheng Tao,\nand Philipp Geiger, 2015, \u201cDiscovering Temporal Causal Relations\nfrom Subsampled Data\u201d, in Francis Bach and David Blei (eds.),\n<em>Proceeding of the 32<sup>nd</sup> International Conference on\nMachine Learning</em>, 37: 1898\u20131906.\n [<a href=\"http://proceedings.mlr.press/v37/gongb15.html\" target=\"other\">Gong et al. 2015 available online</a>]</li>\n<li>Gong, Mingming, Kun Zhang, Bernhard Sch\u00f6lkopf, Clark Glymour,\nand Dacheng Tao, 2017, \u201cCausal Discovery from Temporally\nAggregated Time Series\u201d, in Gal Elidan and Kristian Kersting\n(eds.), <em>Proceedings of the Thirty-Third Conference on Uncertainty\nin Artificial Intelligence</em>, Corvallis, OR: AUAI Press.\n [<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5995575/\" target=\"other\">Gong et al. 2017 available online</a>]</li>\n<li>Greenland, Sander, and James Robins, 1988, \u201cConceptual\nProblems in the Definition and Interpretation of Attributable\nFractions\u201d, <em>American Journal of Epidemiology</em>, 128(6):\n1185\u20131197. doi:10.1093/oxfordjournals.aje.a115073</li>\n<li>Hall, Ned, 2007, \u201cStructural Equations and Causation\u201d,\n<em>Philosophical Studies</em>, 132(1): 109\u2013136.\ndoi:10.1007/s11098-006-9057-9</li>\n<li>Halpern, Joseph Y., 2000, \u201cAxiomatizing Causal\nReasoning\u201d, <em>Journal of Artificial Intelligence\nResearch</em>, 12: 317\u2013337.\n [<a href=\"https://www.jair.org/index.php/jair/article/view/10257\" target=\"other\">Halpern 2000 available online</a>]</li>\n<li>\u2013\u2013\u2013, 2008, \u201cDefaults and Normality in\nCausal Structures\u201d, in Gerhard Brewka and J\u00e9r\u00f4me\nLang (eds.), <em>Principles of Knowledge Representation and Reasoning:\nProceedings of the Eleventh International Conference</em>, Menlo Park,\nCA: AAAI Press, pp. 198\u2013208.</li>\n<li>\u2013\u2013\u2013, 2016, <em>Actual Causality</em>, Cambridge,\nMA: MIT Press.</li>\n<li>Halpern, Joseph Y. and Christopher Hitchcock, 2015, \u201cGraded\nCausation and Defaults\u201d, <em>British Journal for Philosophy of\nScience</em>, 66(2): 413\u201357. doi:10.1093/bjps/axt050</li>\n<li>Halpern, Joseph and Judea Pearl, 2001, \u201cCauses and\nExplanations: A Structural-Model Approach. Part I: Causes\u201d, in\nJohn Breese and Daphne Koller (eds.), <em>Proceedings of the\nSeventeenth Conference on Uncertainty in Artificial Intelligence</em>,\nSan Francisco: Morgan Kaufmann, pp. 194\u2013202</li>\n<li>\u2013\u2013\u2013, 2005, \u201cCauses and Explanations: A\nStructural-Model Approach. Part I: Causes\u201d, <em>British Journal\nfor the Philosophy of Science</em>, 56(4): 843\u2013887.\ndoi:10.1093/bjps/axi147</li>\n<li>Hausman, Daniel M., 1999, \u201cThe Mathematical Theory of\nCausation\u201d, <em>British Journal for the Philosophy of\nScience</em>, 50(1): 151\u2013162. doi:10.1093/bjps/50.1.151</li>\n<li>Hausman, Daniel M. and James Woodward, 1999, \u201cIndependence,\nInvariance, and the Causal Markov Condition\u201d, <em>British\nJournal for the Philosophy of Science</em>, 50(4): 521\u2013583.\ndoi:10.1093/bjps/50.4.521</li>\n<li>\u2013\u2013\u2013, 2004, \u201cModularity and the Causal\nMarkov Condition: a Restatement\u201d, <em>British Journal for the\nPhilosophy of Science</em>, 55(1): 147\u2013161.\ndoi:10.1093/bjps/55.1.147</li>\n<li>Hitchcock, Christopher, 2001, \u201cThe Intransitivity of\nCausation Revealed in Equations and Graphs\u201d, <em>Journal of\nPhilosophy</em>, 98(6): 273\u2013299. doi:10.2307/2678432</li>\n<li>\u2013\u2013\u2013, 2007, \u201cPrevention, Preemption, and\nthe Principle of Sufficient Reason\u201d, <em>Philosophical\nReview</em>, 116(4): 495\u2013532. doi:10.1215/00318108-2007-012</li>\n<li>\u2013\u2013\u2013, 2009, \u201cCausal Models\u201d, in\nBeebee, Hitchcock, and Menzies 2009: 299\u2013314.</li>\n<li>\u2013\u2013\u2013, 2016, \u201cConditioning, Intervening, and\nDecision\u201d, <em>Synthese</em>, 193(4): 1157\u20131176.\ndoi:10.1007/s11229-015-0710-8</li>\n<li>Hoyer, Patrik O., Dominik Janzing, Joris Mooij, Jonas Peters, and\nBernhard Sch\u00f6lkopf, 2009, \u201cNonlinear Causal Discovery with\nAdditive Noise Models\u201d, <em>Advances in Neural Information\nProcessing Systems</em>, 21: 689\u2013696.\n [<a href=\"https://papers.nips.cc/paper/3548-nonlinear-causal-discovery-with-additive-noise-models\" target=\"other\">Hoyer et al. 2009 available online</a>]</li>\n<li>Huang, Yimin and Marco Valtorta, 2006, \u201cPearl\u2019s\nCalculus of Intervention Is Complete\u201d, in Dechter and Richardson\n2006: 217\u2013224.\n [<a href=\"http://arxiv.org/abs/1206.6831\" target=\"other\">Huang &amp; Valtorta 2006 available online</a>]</li>\n<li>Hyttinen, Antti, Frederick Eberhardt, and Patrik O. Hoyer, 2013a,\n\u201cExperiment Selection for Causal Discovery\u201d, <em>Journal\nof Machine Learning Research</em>, 14: 3041\u20133071.\n [<a href=\"http://www.jmlr.org/papers/v14/hyttinen13a.html\" target=\"other\">Hyttinen, Eberhardt, &amp; Hoyer 2013a available online</a>]</li>\n<li>Hyttinen, Antti, Frederick Eberhardt, and Matti J\u00e4rvisalo,\n2014, \u201cConstraint-based Causal Discovery: Conflict Resolution\nwith Answer Set Programming\u201d, in Nevin Zhang and Jin Tian\n(eds.), <em>Proceedings of the Thirtieth Conference on Uncertainty in\nArtificial Intelligence</em>, Corvallis, OR: AUAI Press, pp.\n340\u2013349.</li>\n<li>\u2013\u2013\u2013, 2015, \u201cDo-calculus When the True\nGraph is Unknown\u201d, in Marina Meila and Tom Heskes (eds.),\n<em>Uncertainty in Artificial Intelligence: Proceedings of the\nThirty-First Conference</em>, Corvallis, OR: AUAI Press, pp.\n395\u2013404.</li>\n<li>Hyttinen, Antti, Patrik O. Hoyer, Frederick Eberhardt, and Matti\n\nJ\u00e4rvisalo, 2013b, \u201cDiscovering Cyclic Causal Models with\nLatent Variables: A General SAT-Based Procedure\u201d, in Nichols and\nSmyth 2013: 301\u2013310.</li>\n<li>Hyttinen, Antti, Sergey Plis, Matti J\u00e4rvisalo, Frederick\nEberhardt, and David Danks, 2016, \u201cCausal Discovery from\nSubsampled Time Series Data by Constraint Optimization\u201d, in\nAlessandro Antonucci, Giorgio Corani, Cassio Polpo Campos (eds.)\n<em>Proceedings of the Eighth International Conference on\nProbabilistic Graphical Models</em>, pp. 216\u2013227.</li>\n<li>Jeffrey, Richard, 1983, <em>The Logic of Decision</em>, Second\nEdition, Chicago: University of Chicago Press.</li>\n<li>Joyce, James M., 1999, <em>The Foundations of Causal Decision\nTheory</em>, Cambridge: Cambridge University Press.\ndoi:10.1017/CBO9780511498497</li>\n<li>Lewis, David, 1973a, \u201cCausation\u201d, <em>Journal of\nPhilosophy</em>, 70(17): 556\u2013567. doi:10.2307/2025310</li>\n<li>\u2013\u2013\u2013, 1973b, <em>Counterfactuals</em>, Oxford:\nBlackwell.</li>\n<li>\u2013\u2013\u2013, 1979, \u201cCounterfactual Dependence and\nTime\u2019s Arrow\u201d, <em>No\u00fbs</em>, 13(4): 455\u2013476.\ndoi:10.2307/2215339 </li>\n<li>\u2013\u2013\u2013, 1981, \u201cCausal Decision Theory\u201d,\n<em>Australasian Journal of Philosophy</em>, 59(1): 5\u201330.\ndoi:10.1080/00048408112340011</li>\n<li>Machamer, Peter, Lindley Darden, and Carl Craver, 2000,\n\u201cThinking about Mechanisms\u201d, <em>Philosophy of\nScience</em>, 67(1): 1\u201325. doi:10.1086/392759</li>\n<li>Maier, Marc, Katerina Marazopoulou, David Arbour, and David\nJensen, 2013, \u201cA Sound and Complete Algorithm for Learning\nCausal Models from Relational Data\u201d, in Nichols and Smyth 2013:\n371\u2013380.\n [<a href=\"http://arxiv.org/abs/1309.6843\" target=\"other\">Maier et al. 2013 available online</a>]</li>\n<li>Maier, Marc, Brian Taylor, H\u00fcseyin Oktay, and David Jensen,\n2010, \u201cLearning Causal Models of Relational Domains\u201d, in\nMaria Fox and David Poole (eds.), <em>Proceedings of the Twenty-Fourth\nAAAI Conference on Artificial Intelligence</em>, (Menlo Park CA: AAAI\nPress), pp. 531\u2013538.\n [<a href=\"https://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/view/1919\" target=\"other\">Maier et al. 2010 available online</a>]</li>\n<li>Meek, Christopher, 1995, \u201cStrong Completeness and\nFaithfulness in Bayesian Networks\u201d, in Philippe Besnard and\nSteve Hanks (eds.) <em>Proceedings of the Eleventh Conference\nConference on Uncertainty in Artificial Intelligence</em>, San\nFrancisco: Morgan Kaufmann, pp. 411\u2013418.</li>\n<li>Meek, Christopher and Clark Glymour, 1994, \u201cConditioning and\nIntervening\u201d, <em>British Journal for Philosophy of\nScience,</em>, 45(4): 1001\u20131024. doi:10.1093/bjps/45.4.1001</li>\n<li>Menzies, Peter, 2004, \u201cCausal Models, Token Causation, and\nProcesses\u201d, <em>Philosophy of Science</em>, 71(5):\n820\u2013832. doi:10.1086/425057 </li>\n<li>Mooij, Joris, Dominik Janzing, and Bernhard Sch\u00f6lkopf, 2013,\n\u201cFrom Ordinary Differential Equations to Structural Causal\nModels: the Deterministic Case\u201d, in Nichols and Smyth 2013:\n440\u2013448.</li>\n<li>Neal, Radford M., 2000, \u201cOn Deducing Conditional\nIndependence from d-separation in Causal Graphs with Feedback\u201d,\n<em>Journal of Artificial Intelligence Research</em>, 12: 87\u201391.\n [<a href=\"https://www.jair.org/index.php/jair/article/view/10250\" target=\"other\">Neal 2000 available online</a>]</li>\n<li>Neapolitan, Richard, 2004, <em>Learning Bayesian Networks</em>,\nUpper Saddle River, NJ: Prentice Hall.</li>\n<li>Neapolitan, Richard and Xia Jiang, 2016, \u201cThe Bayesian\nNetwork Story\u201d, in Alan H\u00e1jek and Christopher Hitchcock\n(eds.), <em>The Oxford Handbook of Probability and Philosophy</em>,\nOxford: Oxford University Press, pp. 183\u201399.</li>\n<li>Neyman, Jerzy, 1923 [1990], \u201cSur les Applications de la\nTh\u00e9orie des Probabilit\u00e9s aux Experiences Agricoles:\nEssai des Principes\u201d) <em>Roczniki Nauk Rolniczych, Tom</em>, X:\n1\u201351. Excerpts translated into English by D. M. Dabrowska and\nTerrence Speed, 1990, \u201cOn the Application of Probability Theory\nto Agricultural Experiments. Essay on Principles\u201d,\n<em>Statistical Science</em>, 5(4): 465\u201380.\ndoi:10.1214/ss/1177012031</li>\n<li>Ann Nichols and Padhraic Smyth (eds), 2013, <em>Proceedings of the\nTwenty-Ninth Conference on Uncertainty in Artificial\nIntelligence</em>, Corvallis, OR: AUAI Press.</li>\n<li>Nozick, Robert, 1969, \u201cNewcomb\u2019s Problem and Two\nPrinciples of Choice\u201d, in Nicholas Rescher (ed.), <em>Essays in\nHonor of Carl G. Hempel</em>, Dordrecht: Reidel, pp. 114\u2013146.\ndoi:10.1007/978-94-017-1466-2_7</li>\n<li>Pearl, Judea, 1988, <em>Probabilistic Reasoning in Intelligent\nSystems</em>, San Francisco: Morgan Kaufmann.</li>\n<li>\u2013\u2013\u2013, 1995, \u201cCausal Diagrams for Empirical\nResearch\u201d, <em>Biometrika</em>, 82(4): 669\u2013688.\ndoi:10.1093/biomet/82.4.669</li>\n<li>\u2013\u2013\u2013, 2009, <em>Causality: Models, Reasoning, and\nInference</em>, Second Edition, Cambridge: Cambridge University\nPress.</li>\n<li>\u2013\u2013\u2013, 2010, \u201cAn Introduction to Causal\nInference\u201d, <em>The International Journal of Biostatistics</em>,\n6(2): article 7, pp. 1\u201359. doi:10.2202/1557-4679.1203</li>\n<li>Pearl, Judea and Rina Dechter, 1996, \u201cIdentifying\nIndependencies in Causal Graphs with Feedback\u201d, in Eric Horvitz\nand Finn Jensen (eds.) <em>Proceedings of the Twelfth Conference on\nUncertainty in Artificial Intelligence</em>, San Francisco: Morgan\nKaufmann, pages 420\u2013426.</li>\n<li>Pearl, Judea, Madelyn Glymour, and Nicholas P. Jewell, 2016,\n<em>Causal Inference in Statistics: A Primer</em>, Chichester, UK:\nWiley.</li>\n<li>Pearl, Judea and Mackenzie, Dana, 2018, <em>The Book of Why: The\nNew Science of Cause and Effect.</em>, New York: Basic Books.</li>\n<li>Pearl, Judea and Verma, Thomas, 1991, \u201cA Theory of Inferred\nCausation\u201d, in James Allen, Richard Fiskes, and Erik Sandewall\n(eds.), <em>Principles of Knowledge Representation and Reasoning:\nProceedings of the Second International Conference</em>, San Mateo,\nCA: Morgan Kaufmann, pp. 441\u201352.</li>\n<li>Peters, Jonas, Dominik Janzing, and Bernhard Sch\u00f6lkopf, 2017,\n<em>Elements of Causal Inference: Foundations and Learning\nAlgorithms.</em>, Cambridge, MA: MIT Press. </li>\n<li>Price, Huw, 1986, \u201cAgainst Causal Decision Theory\u201d,\n<em>Synthese</em>, 67(2): 195\u2013212. doi:10.1007/BF00540068</li>\n<li>Ramsey, Joseph, Peter Spirtes, and Jiji Zhang, 2006,\n\u201cAdjacency Faithfulness and Conservative Causal\nInference\u201d, in Dechter and Richardson 2006: 401\u2013408.\n [<a href=\"http://arxiv.org/abs/1206.6843\" target=\"other\">Ramsey, Spirtes, &amp; Zhang 2006 available online</a>]</li>\n<li>Reichenbach, Hans, 1956, <em>The Direction of Time</em>, Berkeley\nand Los Angeles: University of California Press.</li>\n<li>Richardson, Thomas, and James Robins, 2016, <em>Single World\nIntervention Graphs (SWIGs): A Unification of the Counterfactual and\nGraphical Approaches to Causality</em>, Hanover, MA: Now\nPublishers.</li>\n<li>Robins, James, 1986, \u201cA New Approach to Causal Inference in\nMortality Studies with a Sustained Exposure Period: Applications to\nControl of the Healthy Workers Survivor Effect\u201d,\n<em>Mathematical Modeling</em>, 7(9\u201312): 1393\u20131512.\ndoi:10.1016/0270-0255(86)90088-6</li>\n<li>Rubin, Donald, 1974, \u201cEstimating Causal Effects of\nTreatments in Randomized and Nonrandomized Studies\u201d, <em>Journal\nof Educational Psychology</em>, 66(5): 688\u2013701.\ndoi:10.1037/h0037350</li>\n<li>Salmon, Wesley, 1984, <em>Scientific Explanation and the Causal\nStructure of the World</em>, Princeton: Princeton University\nPress.</li>\n<li>Scheines, Richard, 1997, \u201cAn Introduction to Causal\nInference\u201d in V. McKim and S. Turner (eds.), <em>Causality in\nCrisis?</em>, Notre Dame: University of Notre Dame Press, pp.\n185\u2013199.</li>\n<li>Schulte, Oliver and Hassan Khosravi, 2012, \u201cLearning\nGraphical Models for Relational Data via Lattice Search\u201d,\n<em>Machine Learning</em>, 88(3): 331\u2013368.\ndoi:10.1007/s10994-012-5289-4</li>\n<li>Schulte, Oliver, Wei Luo, and Russell Greiner, 2010, \u201cMind\nChange Optimal Learning of Bayes Net Structure from Dependency and\nIndependency Data\u201d, <em>Information and Computation</em>,\n208(1): 63\u201382. doi:10.1016/j.ic.2009.03.009</li>\n<li>Shalizi, Cosma Rohilla, and Andrew C. Thomas, 2011,\n\u201cHomophily and Contagion are Generically Confounded in\nObservational Social Studies\u201d, <em>Sociological Methods and\nResearch</em>, 40(2): 211\u2013239. doi:10.1177/0049124111404820</li>\n<li>Sheps, Mindel C., 1958, \u201cShall We Count the Living or the\nDead?\u201d <em>New England Journal of Medicine</em>, 259(12):\n210\u20134. doi:10.1056/NEJM195812182592505</li>\n<li>Shimizu, Shohei, Patrik O. Hoyer, Aapo Hyv\u00e4rinen, and Antti\nKermine, 2006, \u201cA Linear Non-Gaussian Acyclic Model for Causal\nDiscovery\u201d, <em>Journal of Machine Learning Research</em>, 7:\n2003\u20132030.\n [<a href=\"http://www.jmlr.org/papers/v7/shimizu06a.html\" target=\"other\">Shimizu et al. 2006 available online</a>]</li>\n<li>Shpitser, Ilya and Judea Pearl, 2006, \u201cIdentification of\nConditional Interventional Distributions\u201d, in Dechter and\nRichardson 2006: 437\u2013444.\n [<a href=\"http://arxiv.org/abs/1206.6876\" target=\"other\">Shpister &amp; Pearl 2006 available online</a>]</li>\n<li>Skyrms, Brian, 1980, <em>Causal Necessity</em>, New Haven and\nLondon: Yale University Press.</li>\n<li>Spirtes, Peter, 1995, \u201cDirected Cyclic Graphical\nRepresentation of Feedback Models\u201d, in Philippe Besnard and\nSteve Hanks (eds.), <em>Proceedings of the Eleventh Conference on\nUncertainty in Artificial Intelligence</em>, San Francisco: Morgan\nKaufmann, pp. 491\u2013498.</li>\n<li>Spirtes, Peter, Clark Glymour, and Richard Scheines, [SGS] 2000,\n<em>Causation, Prediction and Search</em>, Second Edition, Cambridge,\nMA: MIT Press.</li>\n<li>Spirtes, Peter and Jiji Zhang, 2014, \u201cA Uniformly Consistent\nEstimator of Causal Effects under the <i>k</i>-Triangle-Faithfulness\nAssumption\u201d, <em>Statistical Science</em>, 29(4): 662\u2013678.\ndoi:10.1214/13-STS429</li>\n<li>Spirtes, Peter and Kun Zhang, 2016, \u201cCausal Discovery and\nInference: Concepts and Recent Methodological Advances\u201d,\n<em>Applied Informatics</em>, 3: 3. doi:10.1186/s40535-016-0018-x</li>\n<li>Stalnaker, Robert, 1968, \u201cA Theory of Conditionals\u201d,\nin Nicholas Rescher (ed.) <em>Studies in Logical Theory</em>,\nBlackwell: Oxford, pp. 98\u2013112.</li>\n<li>Steel, Daniel, 2006, \u201cHomogeneity, Selection, and the\nFaithfulness Condition\u201d. <em>Minds and Machines</em>, 16(3):\n303\u2013317. doi:10.1007/s11023-006-9032-4</li>\n<li>Stern, Reuben, 2017, \u201cInterventionist Decision\nTheory\u201d, <em>Synthese</em>, 194(10): 4133\u20134153.\ndoi:10.1007/s11229-016-1133-x</li>\n<li>Suppes, Patrick, 1970, <em>A Probabilistic Theory of\nCausality</em>, Amsterdam: North-Holland Publishing Company.</li>\n<li>Tillman, Robert E., and Frederick Eberhardt, 2014, \u201cLearning\nCausal Structure from Multiple Datasets with Similar Variable\nSets\u201d, <em>Behaviormetrika</em>, 41(1): 41\u201364.\ndoi:10.2333/bhmk.41.41</li>\n<li>Triantafillou, Sofia, and Ioannis Tsamardinos, 2015,\n\u201cConstraint-based Causal Discovery from Multiple Interventions\nover Overlapping Variable Sets\u201d, <em>Journal of Machine Learning\nResearch</em>, 16: 2147\u20132205.\n [<a href=\"http://www.jmlr.org/papers/v16/triantafillou15a.html\" target=\"other\">Triantafillou &amp; Tsamardinos 2015 available online</a>]</li>\n<li>Weslake, Brad, forthcoming, \u201cA Partial Theory of Actual\nCausation\u201d, <em>British Journal for the Philosophy of\nScience.</em></li>\n<li>Woodward, James, 2003, <em>Making Things Happen: A Theory of\nCausal Explanation</em>, Oxford: Oxford University Press.\ndoi:10.1093/0195155270.001.0001</li>\n<li>Wright, Sewall, 1921, \u201cCorrelation and Causation\u201d,\n<em>Journal of Agricultural Research</em>, 20: 557\u201385.</li>\n<li>Zhalama, Jiji Zhang, and Wolfgang Mayer, 2016, \u201cWeakening\nFaithfulness: Some Heuristic Causal Discovery Algorithms\u201d,\n<em>International Journal of Data Science and Analytics</em>, 3(2):\n93\u2013104. doi:10.1007/s41060-016-0033-y</li>\n<li>Zhang, Jiji, 2008, \u201cCausal Reasoning with Ancestral\nGraphs\u201d, <em>Journal of Machine Learning Research</em>, 9:\n1437\u20131474.\n [<a href=\"http://www.jmlr.org/papers/v9/zhang08a.html\" target=\"other\">Zhang 2008 available online</a>]</li>\n<li>\u2013\u2013\u2013, 2013a, \u201cA Lewisian Logic of\nCounterfactuals\u201d, <em>Minds and Machines</em>, 23(1):\n77\u201393. doi:10.1007/s11023-011-9261-z</li>\n<li>\u2013\u2013\u2013, 2013b, \u201cA Comparison of Three\nOccam\u2019s Razors for Markovian Causal Models\u201d, <em>British\nJournal for Philosophy of Science</em>, 64(2): 423\u2013448.\ndoi:10.1093/bjps/axs005</li>\n<li>Zhang, Jiji and Peter Spirtes 2008, \u201cDetection of\nUnfaithfulness and Robust Causal Inference\u201d, <em>Minds and\nMachines</em>, 18(2): 239\u2013271.\ndoi:10.1007/s11023-008-9096-4</li>\n<li>\u2013\u2013\u2013, 2016, \u201cThe Three Faces of\nFaithfulness\u201d, <em>Synthese</em>, 193(4): 1011\u20131027.\ndoi:10.1007/s11229-015-0673-9</li>\n<li>Zhang, Kun, and Aapo Hyv\u00e4rinen, 2009, \u201cOn the\nIdentifiability of the Post-nonlinear Causal Model\u201d, in Jeff\nBilmes and Andrew Ng (eds.), <em>Proceeding of the Twenty-Fifth\nConference on Uncertainty in Artificial Intelligence</em>, (Corvallis,\nOR: AUAI Press), pp. 647\u2013655.</li>\n</ul>\n</div>"
    },
    "related_entries": {
        "entry_list": [
            "causation: and manipulability",
            "causation: counterfactual theories of",
            "causation: probabilistic",
            "causation: the metaphysics of",
            "conditionals: counterfactual",
            "decision theory",
            "decision theory: causal",
            "logic: conditionals",
            "probability, interpretations of",
            "quantum theory: the Einstein-Podolsky-Rosen argument in",
            "rational choice, normative: expected utility",
            "Reichenbach, Hans: common cause principle"
        ],
        "entry_link": [
            {
                "../causation-mani/": "causation: and manipulability"
            },
            {
                "../causation-counterfactual/": "causation: counterfactual theories of"
            },
            {
                "../causation-probabilistic/": "causation: probabilistic"
            },
            {
                "../causation-metaphysics/": "causation: the metaphysics of"
            },
            {
                "../counterfactuals/": "conditionals: counterfactual"
            },
            {
                "../decision-theory/": "decision theory"
            },
            {
                "../decision-causal/": "decision theory: causal"
            },
            {
                "../logic-conditionals/": "logic: conditionals"
            },
            {
                "../probability-interpret/": "probability, interpretations of"
            },
            {
                "../qt-epr/": "quantum theory: the Einstein-Podolsky-Rosen argument in"
            },
            {
                "../rationality-normative-utility/": "rational choice, normative: expected utility"
            },
            {
                "../physics-Rpcc/": "Reichenbach, Hans: common cause principle"
            }
        ]
    },
    "academic_tools": {
        "listed_text": [
            "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=causal-models\" target=\"other\">How to cite this entry</a>.",
            "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://leibniz.stanford.edu/friends/preview/causal-models/\" target=\"other\">Preview the PDF version of this entry</a> at the\n <a href=\"https://leibniz.stanford.edu/friends/\" target=\"other\">Friends of the SEP Society</a>.",
            "<img alt=\"inpho icon\" src=\"../../symbols/inpho.png\"/>",
            "<a href=\"https://www.inphoproject.org/entity?sep=causal-models&amp;redirect=True\" target=\"other\">Look up topics and thinkers related to this entry</a>\n at the Internet Philosophy Ontology Project (InPhO).",
            "<img alt=\"phil papers icon\" src=\"../../symbols/pp.gif\"/>",
            "<a href=\"http://philpapers.org/sep/causal-models/\" target=\"other\">Enhanced bibliography for this entry</a>\nat <a href=\"http://philpapers.org/\" target=\"other\">PhilPapers</a>, with links to its database."
        ],
        "listed_links": [
            {
                "https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=causal-models": "How to cite this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/preview/causal-models/": "Preview the PDF version of this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/": "Friends of the SEP Society"
            },
            {
                "https://www.inphoproject.org/entity?sep=causal-models&redirect=True": "Look up topics and thinkers related to this entry"
            },
            {
                "http://philpapers.org/sep/causal-models/": "Enhanced bibliography for this entry"
            },
            {
                "http://philpapers.org/": "PhilPapers"
            }
        ]
    },
    "other_internet_resources": {
        "listed_text": [
            "<a href=\"http://causality.cs.ucla.edu/blog/\" target=\"other\">Causal Analysis and Theory in Practice</a>",
            "<a href=\"http://bayes.cs.ucla.edu/BOOK-2K/\" target=\"other\"><em>Causality</em>, 2nd Edition, 2009</a>,\n  Judea Pearl's web page on his book.",
            "<a href=\"http://www.phil.cmu.edu/tetrad/\" target=\"other\">The Tetrad Project</a>.",
            "<a href=\"http://www.phil.cmu.edu/projects/csr/\" target=\"other\">Causal and Statistical Reasoning</a>,\n  The Carnegie Mellon Curriculum, Core Site Materials."
        ],
        "listed_links": [
            {
                "http://causality.cs.ucla.edu/blog/": "Causal Analysis and Theory in Practice"
            },
            {
                "http://bayes.cs.ucla.edu/BOOK-2K/": "Causality, 2nd Edition, 2009"
            },
            {
                "http://www.phil.cmu.edu/tetrad/": "The Tetrad Project"
            },
            {
                "http://www.phil.cmu.edu/projects/csr/": "Causal and Statistical Reasoning"
            }
        ]
    }
}