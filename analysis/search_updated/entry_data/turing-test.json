{
    "url": "turing-test",
    "title": "The Turing Test",
    "authorship": {
        "year": "Copyright \u00a9 2021",
        "author_text": "Graham Oppy\n<Graham.Oppy@monash.edu>\nDavid Dowe\n<David.Dowe@monash.edu>",
        "author_links": [
            {
                "http://profiles.arts.monash.edu.au/graham-oppy/": "Graham Oppy"
            },
            {
                "mailto:Graham%2eOppy%40monash%2eedu": "Graham.Oppy@monash.edu"
            },
            {
                "http://www.csse.monash.edu.au/~dld/David.Dowe.publications.html": "David Dowe"
            },
            {
                "mailto:David%2eDowe%40monash%2eedu": "David.Dowe@monash.edu"
            }
        ],
        "raw_html": "<div id=\"article-copyright\">\n<p>\n<a href=\"../../info.html#c\">Copyright \u00a9 2021</a> by\n\n<br/>\n<a href=\"http://profiles.arts.monash.edu.au/graham-oppy/\" target=\"other\">Graham Oppy</a>\n&lt;<a href=\"mailto:Graham%2eOppy%40monash%2eedu\"><em>Graham<abbr title=\" dot \">.</abbr>Oppy<abbr title=\" at \">@</abbr>monash<abbr title=\" dot \">.</abbr>edu</em></a>&gt;<br/>\n<a href=\"http://www.csse.monash.edu.au/~dld/David.Dowe.publications.html\" target=\"other\">David Dowe</a>\n&lt;<a href=\"mailto:David%2eDowe%40monash%2eedu\"><em>David<abbr title=\" dot \">.</abbr>Dowe<abbr title=\" at \">@</abbr>monash<abbr title=\" dot \">.</abbr>edu</em></a>&gt;\n    </p>\n</div>"
    },
    "pubinfo": [
        "First published Wed Apr 9, 2003",
        "substantive revision Mon Oct 4, 2021"
    ],
    "preamble": "\n\nThe phrase \u201cThe Turing Test\u201d is most properly used to\nrefer to a proposal made by Turing (1950) as a way of dealing with the\nquestion whether machines can think. According to Turing, the question\nwhether machines can think is itself \u201ctoo meaningless\u201d to\ndeserve discussion (442). However, if we consider the more\nprecise\u2014and somehow related\u2014question whether a digital\ncomputer can do well in a certain kind of game that Turing describes\n(\u201cThe Imitation Game\u201d), then\u2014at least in\nTuring\u2019s eyes\u2014we do have a question that admits of precise\ndiscussion. Moreover, as we shall see, Turing himself thought that it\nwould not be too long before we did have digital computers that could\n\u201cdo well\u201d in the Imitation Game.\n\nThe phrase \u201cThe Turing Test\u201d is sometimes used more\ngenerally to refer to some kinds of behavioural tests for the presence\nof mind, or thought, or intelligence in putatively minded entities.\nSo, for example, it is sometimes suggested that The Turing Test is\nprefigured in Descartes\u2019 Discourse on the Method.\n(Copeland (2000:527) finds an anticipation of the test in the 1668\nwritings of the Cartesian de Cordemoy. Abramson (2011a) presents\narchival evidence that Turing was aware of Descartes\u2019 language\ntest at the time that he wrote his 1950 paper. Gunderson (1964)\nprovides an early instance of those who find that Turing\u2019s work\nis foreshadowed in the work of Descartes.)\n\nThe phrase \u201cThe Turing Test\u201d is also sometimes used to\nrefer to certain kinds of purely behavioural allegedly logically\nsufficient conditions for the presence of mind, or thought, or\nintelligence, in putatively minded entities. So, for example, Ned\nBlock\u2019s \u201cBlockhead\u201d thought experiment is often said\nto be a (putative) knockdown objection to The Turing Test. (Block\n(1981) contains a direct discussion of The Turing Test in this\ncontext.) Here, what a proponent of this view has in mind is the idea\nthat it is logically possible for an entity to pass the kinds\nof tests that Descartes and (at least allegedly) Turing have in\nmind\u2014to use words (and, perhaps, to act) in just the kind of way\nthat human beings do\u2014and yet to be entirely lacking in\nintelligence, not possessed of a mind, etc.\n\nThe subsequent discussion takes up the preceding ideas in the order in\nwhich they have been introduced. First, there is a discussion of\nTuring\u2019s paper (1950), and of the arguments contained therein.\nSecond, there is a discussion of current assessments of various\nproposals that have been called \u201cThe Turing Test\u201d (whether\nor not there is much merit in the application of this label to the\nproposals in question). Third, there is a brief discussion of some\nrecent writings on The Turing Test, including some discussion of the\nquestion whether The Turing Test sets an appropriate goal for research\ninto artificial intelligence. Finally, there is a very short\ndiscussion of Searle\u2019s Chinese Room argument, and, in\nparticular, of the bearing of this argument on The Turing Test.\n\nFor other introductory discussions of the Turing Test, from a range of\nperspectives, see, for example: Copeland (2000), Damassino and Novelli\n(2020), French (2000), Korukonda (2003), Moor (2008), Neufeld and\nFinnestad (2020a) (2020b), Proudfoot and Copeland (2008), Saygin et\nal. (2000), and Shieber (2004). For further information about Turing\nhimself, see, for example: Cooper and van Leeuwen (2013), Copeland et\nal. (2017), Hodges (1983), Millican and Clark (1999) and Turing\n(1992).\n",
    "toc": [
        {
            "#Tur195ImiGam": "1. Turing (1950) and the Imitation Game"
        },
        {
            "#Tur195ResObj": "2. Turing (1950) and Responses to Objections"
        },
        {
            "#TheObj": "2.1 The Theological Objection"
        },
        {
            "#HeaSanObj": "2.2 The \u2018Heads in the Sand\u2019 Objection"
        },
        {
            "#MatObj": "2.3 The Mathematical Objection"
        },
        {
            "#ArgCon": "2.4 The Argument from Consciousness"
        },
        {
            "#ArgVarDis": "2.5 Arguments from Various Disabilities"
        },
        {
            "#LadLovObj": "2.6 Lady Lovelace\u2019s Objection"
        },
        {
            "#ArgConNerSys": "2.7 Argument from Continuity of the Nervous System"
        },
        {
            "#ArgInfBeh": "2.8 Argument from Informality of Behavior"
        },
        {
            "#ArgExtSenPer": "2.9 Argument from Extra-Sensory Perception"
        },
        {
            "#SomMinIssAri": "3. Some Minor Issues Arising"
        },
        {
            "#IntImiGam": "3.1 Interpreting the Imitation Game"
        },
        {
            "#TurPre": "3.2 Turing\u2019s Predictions"
        },
        {
            "#UseDis": "3.3 A Useful Distinction"
        },
        {
            "#FurNot": "3.4 A Further Note"
        },
        {
            "#AssCurStaTurTes": "4. Assessment of the Current Standing of The Turing Test"
        },
        {
            "#LogNecSufCon": "4.1 (Logically) Necessary and Sufficient Conditions"
        },
        {
            "#LogSufCon": "4.2 Logically Sufficient Conditions"
        },
        {
            "#Cri": "4.3 Criteria"
        },
        {
            "#ProSup": "4.4 Probabilistic Support"
        },
        {
            "#AltTes": "5. Alternative Tests"
        },
        {
            "#TurTesTooHar": "5.1 The Turing Test is Too Hard"
        },
        {
            "#TurTesTooNar": "5.2 The Turing Test is Too Narrow"
        },
        {
            "#TurTesTooEas": "5.3 The Turing Test is Too Easy"
        },
        {
            "#ShoTurTesConHar": "5.4 Should the Turing Test be Considered Harmful?"
        },
        {
            "#ChiRoo": "6. The Chinese Room"
        },
        {
            "#MeasInt": "7. Brief Notes on Intelligence"
        },
        {
            "#Bib": "Bibliography"
        },
        {
            "#Aca": "Academic Tools"
        },
        {
            "#Oth": "Other Internet Resources"
        },
        {
            "#Rel": "Related Entries"
        }
    ],
    "main_text": "\n1. Turing (1950) and the Imitation Game\n\nTuring (1950) describes the following kind of game. Suppose that we\nhave a person, a machine, and an interrogator. The interrogator is in\na room separated from the other person and the machine. The object of\nthe game is for the interrogator to determine which of the other two\nis the person, and which is the machine. The interrogator knows the\nother person and the machine by the labels \u2018X\u2019\nand \u2018Y\u2019\u2014but, at least at the beginning of\nthe game, does not know which of the other person and the machine is\n\u2018X\u2019\u2014and at the end of the game says either\n\u2018X is the person and Y is the machine\u2019\nor \u2018X is the machine and Y is the\nperson\u2019. The interrogator is allowed to put questions to the\nperson and the machine of the following kind: \u201cWill X\nplease tell me whether X plays chess?\u201d Whichever of the\nmachine and the other person is X must answer questions that\nare addressed to X. The object of the machine is to try to\ncause the interrogator to mistakenly conclude that the machine is the\nother person; the object of the other person is to try to help the\ninterrogator to correctly identify the machine. About this game,\nTuring (1950) says:\n\nI believe that in about fifty years\u2019 time it will be possible to\nprogramme computers, with a storage capacity of about 109,\nto make them play the imitation game so well that an average\ninterrogator will not have more than 70 percent chance of making the\nright identification after five minutes of questioning. \u2026 I\nbelieve that at the end of the century the use of words and general\neducated opinion will have altered so much that one will be able to\nspeak of machines thinking without expecting to be contradicted.\n\n\nThere are at least two kinds of questions that can be raised about\nTuring\u2019s predictions concerning his Imitation Game. First, there\nare empirical questions, e.g., Is it true that we now\u2014or will\nsoon\u2014have made computers that can play the imitation game so\nwell that an average interrogator has no more than a 70 percent chance\nof making the right identification after five minutes of questioning?\nSecond, there are conceptual questions, e.g., Is it true that, if an\naverage interrogator had no more than a 70 percent chance of making\nthe right identification after five minutes of questioning, we should\nconclude that the machine exhibits some level of thought, or\nintelligence, or mentality?\n\nThere is little doubt that Turing would have been disappointed by the\nstate of play at the end of the twentieth century. Participants in the\nLoebner Prize Competition\u2014an annual event in which computer\nprogrammes are submitted to the Turing Test\u2014 had come nowhere\nnear the standard that Turing envisaged. A quick look at the\ntranscripts of the participants for the preceding decade reveals that\nthe entered programs were all easily detected by a range of\nnot-very-subtle lines of questioning. Moreover, major players in the\nfield regularly claimed that the Loebner Prize Competition was an\nembarrassment precisely because we were still so far from having a\ncomputer programme that could carry out a decent conversation for a\nperiod of five minutes\u2014see, for example, Shieber (1994). It was\nwidely conceded on all sides that the programs entered in the Loebner\nPrize Competition were designed solely with the aim of winning the\nminor prize of best competitor for the year, with no thought that the\nembodied strategies would actually yield something capable of passing\nthe Turing Test.\n\nAt the end of the second decade of the twenty-first century, it is\nunclear how much has changed. On the one hand, there have been\ninteresting developments in language generators. In particular, the\nrelease of Open AI\u2019s GPT-3 (Brown, et al. 2020, Other Internet\nResources) has prompted a flurry of excitement. GPT-3 is quite good at\ngenerating fiction, poetry, press releases, code, music, jokes,\ntechnical manuals, and news articles. Perhaps, as Chalmers speculates\n(2020, Other Internet Resources), GPT-3 \u201csuggests a potential\nmindless path to artificial general intelligence\u201d. But, of\ncourse, GPT-3 is not close to passing the Turing Test: GPT-3 neither\nperceives nor acts, and it is, at best, highly contentious whether it\nis a site of understanding. What remains to be seen is whether, within\nthe next couple of generations of language generators \u2013 GPT-4 or\nGPT-5 \u2013 we have something that can be linked to perceptual\ninputs and behavioural outputs in a way that does produce something\ncapable of passing the Turing Test. (For further discussion, see\nFloridi and Chiriatti (2020).)\n\nOn the other hand, as, for example, Floridi (2008) complains, there\nare other ways in which progress has been frustratingly slow. In 2014,\nclaims emerged that, because the computer program Eugene\nGoostman had fooled 33% of judges in the Turing Test 2014\ncompetition, it had \u201cpassed the Turing Test\u201d. But there\nhave been other one-off competitions in which similar results have\nbeen achieved. Back in 1991, PC Therapist had 50% of judges\nfooled. And, in a 2011 demonstration, Cleverbot had an even\nhigher success rate. In all three of these cases, the size of the\ntrial was very small, and the result was not reliably projectible: in\nno case were there strong grounds for holding that an average\ninterrogator had no more than a 70% chance of making the right\ndetermination about the relevant program after five minutes of\nquestioning. Moreover\u2014and much more importantly\u2014we must\ndistinguish between the test the Turing proposed, and the particular\nprediction that he made about how things would be by the end of the\ntwentieth century. The percentage chance of making the correct\nidentification, the time interval over which the test takes place, and\nthe number of conversational exchanges required are all adjustable\nparameters in the Test, despite the fact that they are fixed in the\nparticular prediction that Turing made. Even if Turing was very far\nout in the prediction that he made about how things would be by the\nend of the twentieth century, it remains possible that the test that\nhe proposes is a good one. However, before one can endorse the\nsuggestion that the Turing Test is good, there are various objections\nthat ought to be addressed.\n\nSome people have suggested that the Turing Test is chauvinistic: it\nonly recognizes intelligence in things that are able to sustain a\nconversation with us. Why couldn\u2019t it be the case that there are\nintelligent things that are unable to carry on a conversation, or, at\nany rate, unable to carry on a conversation with creatures like us?\n(See, for example, French (1990).) Perhaps the intuition behind this\nquestion can be granted; perhaps it is unduly chauvinistic to insist\nthat anything that is intelligent has to be capable of sustaining a\nconversation with us. (On the other hand, one might think that, given\nthe availability of suitably qualified translators, it ought to be\npossible for any two intelligent agents that speak different languages\nto carry on some kind of conversation.) But, in any case, the charge\nof chauvinism is completely beside the point. What Turing claims is\nonly that, if something can carry out a conversation with us, then we\nhave good grounds to suppose that that thing has intelligence of the\nkind that we possess; he does not claim that only something that can\ncarry out a conversation with us can possess the kind of intelligence\nthat we have.\n\nOther people have thought that the Turing Test is not sufficiently\ndemanding: we already have anecdotal evidence that quite unintelligent\nprograms (e.g., ELIZA\u2014for details of which, see Weizenbaum\n(1966)) can seem to ordinary observers to be loci of intelligence for\nquite extended periods of time. Moreover, over a short period of\ntime\u2014such as the five minutes that Turing mentions in his\nprediction about how things will be in the year 2000\u2014it might\nwell be the case that almost all human observers could be taken in by\ncunningly designed but quite unintelligent programs. However, it is\nimportant to recall that, in order to pass Turing\u2019s Test, it is\nnot enough for the computer program to fool \u201cordinary\nobservers\u201d in circumstances other than those in which the test\nis supposed to take place. What the computer program has to be able to\ndo is to survive interrogation by someone who knows that one of the\nother two participants in the conversation is a machine. Moreover, the\ncomputer program has to be able to survive such interrogation with a\nhigh degree of success over a repeated number of trials. (Turing says\nnothing about how many trials he would require. However, we can safely\nassume that, in order to get decent evidence that there is no more\nthan a 70% chance that a machine will be correctly identified as a\nmachine after five minutes of conversation, there will have to be a\nreasonably large number of trials.) If a computer program could do\nthis quite demanding thing, then it does seem plausible to claim that\nwe would have at least prima facie reason for thinking that\nwe are in the presence of intelligence. (Perhaps it is worth\nemphasizing again that there might be all kinds of intelligent\nthings\u2014including intelligent machines\u2014that would not pass\nthis test. It is conceivable, for example, that there might be\nmachines that, as a result of moral considerations, refused to lie or\nto engage in pretence. Since the human participant is supposed to do\neverything that he or she can to help the interrogator, the question\n\u201cAre you a machine?\u201d would quickly allow the interrogator\nto sort such (pathological?) truth-telling machines from humans.)\n\nAnother contentious aspect of Turing\u2019s paper (1950) concerns his\nrestriction of the discussion to the case of \u201cdigital\ncomputers.\u201d On the one hand, it seems clear that this\nrestriction is really only significant for the prediction that Turing\nmakes about how things will be in the year 2000, and not for the\ndetails of the test itself. (Indeed, it seems that if the test that\nTuring proposes is a good one, then it will be a good test for any\nkinds of entities, including, for example, animals, aliens, and analog\ncomputers. That is: if animals, aliens, analog computers, or any other\nkinds of things, pass the test that Turing proposes, then there will\nbe as much reason to think that these things exhibit intelligence as\nthere is reason to think that digital computers that pass the test\nexhibit intelligence.) On the other hand, it is actually a highly\ncontroversial question whether \u201cthinking machines\u201d would\nhave to be digital computers; and it is also a controversial question\nwhether Turing himself assumed that this would be the case. In\nparticular, it is worth noting that the seventh of the objections that\nTuring (1950) considers addresses the possibility of continuous state\nmachines, which Turing explicitly acknowledges to be different from\ndiscrete state machines. Turing appears to claim that, even if we are\ncontinuous state machines, a discrete state machine would be able to\nimitate us sufficiently well for the purposes of the Imitation Game.\nHowever, it seems doubtful that the considerations that he gives are\nsufficient to establish that, if there are continuous state machines\nthat pass the Turing Test, then it is possible to make discrete state\nmachines that pass the test as well. (Turing himself was keen to point\nout that some limits had to be set on the notion of\n\u201cmachine\u201d in order to make the question about\n\u201cthinking machines\u201d interesting:\n\nIt is natural that we should wish to permit every kind of engineering\ntechnique to be used in our machine. We also wish to allow the\npossibility that an engineer or team of engineers may construct a\nmachine which works, but whose manner of operation cannot be\nsatisfactorily described by its constructors because they have applied\na method which is largely experimental. Finally, we wish to exclude\nfrom the machines men born in the usual manner. It is difficult to\nframe the definitions so as to satisfy these three conditions. One\nmight for instance insist that the team of engineers should all be of\none sex, but this would not really be satisfactory, for it is probably\npossible to rear a complete individual from a single cell of the skin\n(say) of a man. To do so would be a feat of biological technique\ndeserving of the very highest praise, but we would not be inclined to\nregard it as a case of \u2018constructing a thinking machine\u2019.\n(435/6)\n\n\nBut, of course, as Turing himself recognized, there is a large class\nof possible \u201cmachines\u201d that are neither digital nor\nbiotechnological.) More generally, the crucial point seems to be that,\nwhile Turing recognized that the class of machines is potentially much\nlarger than the class of discrete state machines, he was himself\nvery confident that properly engineered discrete state\nmachines could succeed in the Imitation Game (and, moreover, at the\ntime that he was writing, there were certain discrete state\nmachines\u2014\u201celectronic computers\u201d\u2014that loomed\nvery large in the public imagination).\n2. Turing (1950) and Responses to Objections\n\nAlthough Turing (1950) is pretty informal, and, in some ways rather\nidiosyncratic, there is much to be gained by considering the\ndiscussion that Turing gives of potential objections to his claim that\nmachines\u2014and, in particular, digital computers\u2014can\n\u201cthink\u201d. Turing gives the following labels to the\nobjections that he considers: (1) The Theological Objection; (2) The\n\u201cHeads in the Sand\u201d Objection; (3) The Mathematical\nObjection; (4) The Argument from Consciousness; (5) Arguments from\nVarious Disabilities; (6) Lady Lovelace\u2019s Objection; (7)\nArgument from Continuity of the Nervous System; (8) The Argument from\nInformality of Behavior; and (9) The Argument from Extra-Sensory\nPerception. We shall consider these objections in the corresponding\nsubsections below. (In some\u2014but not all\u2014cases, the\ncounter-arguments to these objections that we discuss are also\nprovided by Turing.)\n2.1 The Theological Objection\n\nSubstance dualists believe that thinking is a function of a\nnon-material, separately existing, substance that somehow\n\u201ccombines\u201d with the body to make a person. So\u2014the\nargument might go\u2014making a body can never be sufficient to\nguarantee the presence of thought: in themselves, digital computers\nare no different from any other merely material bodies in being\nutterly unable to think. Moreover\u2014to introduce the\n\u201ctheological\u201d element\u2014it might be further added\nthat, where a \u201csoul\u201d is suitably combined with a body,\nthis is always the work of the divine creator of the universe: it is\nentirely up to God whether or not a particular kind of body is imbued\nwith a thinking soul. (There is well known scriptural support for the\nproposition that human beings are \u201cmade in God\u2019s\nimage\u201d. Perhaps there is also theological support for the claim\nthat only God can make things in God\u2019s image.)\n\nThere are several different kinds of remarks to make here. First,\nthere are many serious objections to substance dualism. Second, there\nare many serious objections to theism. Third, even if theism and\nsubstance dualism are both allowed to pass, it remains quite unclear\nwhy thinking machines are supposed to be ruled out by this combination\nof views. Given that God can unite souls with human bodies, it is hard\nto see what reason there is for thinking that God could not unite\nsouls with digital computers (or rocks, for that matter!). Perhaps, on\nthis combination of views, there is no especially good reason why,\namongst the things that we can make, certain kinds of digital\ncomputers turn out to be the only ones to which God gives\nsouls\u2014but it seems pretty clear that there is also no\nparticularly good reason for ruling out the possibility that God would\nchoose to give souls to certain kinds of digital computers. Evidence\nthat God is dead set against the idea of giving souls to certain kinds\nof digital computers is not particularly thick on the ground.\n2.2 The \u2018Heads in the Sand\u2019 Objection\n\nIf there were thinking machines, then various consequences would\nfollow. First, we would lose the best reasons that we have for\nthinking that we are superior to everything else in the universe\n(since our cherished \u201creason\u201d would no longer be something\nthat we alone possess). Second, the possibility that we might be\n\u201csupplanted\u201d by machines would become a genuine worry: if\nthere were thinking machines, then very likely there would be machines\nthat could think much better than we can. Third, the possibility that\nwe might be \u201cdominated\u201d by machines would also become a\ngenuine worry: if there were thinking machines, who\u2019s to say\nthat they would not take over the universe, and either enslave or\nexterminate us?\n\nAs it stands, what we have here is not an argument against the claim\nthat machines can think; rather, we have the expression of various\nfears about what might follow if there were thinking machines. Someone\nwho took these worries seriously\u2014and who was persuaded that it\nis indeed possible for us to construct thinking machines\u2014might\nwell think that we have here reasons for giving up on the project of\nattempting to construct thinking machines. However, it would be a\nmajor task\u2014which we do not intend to pursue here\u2014to\ndetermine whether there really are any good reasons for taking these\nworries seriously.\n2.3 The Mathematical Objection\n\nSome people have supposed that certain fundamental results in\nmathematical logic that were discovered during the 1930s\u2014by\nG\u00f6del (first incompleteness theorem) and Turing (the halting\nproblem)\u2014have important consequences for questions about digital\ncomputation and intelligent thought. (See, for example, Lucas (1961)\nand Penrose (1989); see, too, Hodges (1983:414) who mentions\nPolanyi\u2019s discussions with Turing on this matter.) Essentially,\nthese results show that within a formal system that is strong enough,\nthere are a class of true statements that can be expressed but not\nproven within the system (see the entry on\n G\u00f6del\u2019s incompleteness theorems).\n Let us say that such a system is \u201csubject to the Lucas-Penrose\nconstraint\u201d because it is constrained from being able to prove a\nclass of true statements expressible within the system.\n\nTuring (1950:444) himself observes that these results from\nmathematical logic might have implications for the Turing test:\n\nThere are certain things that [any digital computer] cannot do. If it\nis rigged up to give answers to questions as in the imitation game,\nthere will be some questions to which it will either give a wrong\nanswer, or fail to give an answer at all however much time is allowed\nfor a reply. (444)\n\n\nSo, in the context of the Turing test, \u201cbeing subject to the\nLucas-Penrose constraint\u201d implies the existence of a class of\n\u201cunanswerable\u201d questions. However Turing noted that in the\ncontext of the Turing test, these \u201cunanswerable\u201d questions\nare only a concern if humans can answer them. His \u201cshort\u201d\nreply was that it is not clear that humans are free from such a\nconstraint themselves. Turing then goes on to add that he does not\nthink that the argument can be dismissed \u201cquite so\nlightly.\u201d\n\nTo make the argument more precise, we can write it as follows:\n\nLet C be a digital computer.\nSince C is subject to the Lucas-Penrose constraint, there is an\n\u201cunanswerable\u201d question q for C.\nIf an entity, E, is not subject to the Lucas-Penrose constraint,\nthen there are no \u201cunanswerable\u201d questions for E.\nThe human intellect is not subject to the Lucas-Penrose\nconstraint.\nThus, there are no \u201cunanswerable\u201d questions for the\nhuman intellect.\nThe question q is therefore \u201canswerable\u201d to the human\nintellect.\nBy asking question q, a human could determine if the responder is\na computer or a human.\nThus C may fail the Turing test.\n\n\nOnce the argument is laid out as above, it becomes clear that premise\n(3) should be challenged. Putting that aside, we note that one\ninterpretation of Turing\u2019s \u201cshort\u201d reply is that\nclaim (4) is merely asserted\u2014without any kind of proof. The\n\u201cshort\u201d reply then leads us to examine whether humans are\nfree from the Lucas-Penrose constraint.\n\nIf humans are subject to the Lucas-Penrose constraint then the\nconstraint does not provide any basis for distinguishing humans from\ndigital computers. If humans are free from the Lucas-Penrose\nconstraint, then (granting premise 3) it follows that digital\ncomputers may fail the Turing test and thus, it seems, cannot\nthink.\n\nHowever, there remains a question as to whether being free from the\nconstraint is necessary for the capacity to think. It may be that the\nTuring test is too strict. Since, by hypothesis, we are free from the\nLucas-Penrose constraint, we are, in some sense, too good at asking\nand answering questions. Suppose there is a thinking entity that is\nsubject to the Lucas-Penrose constraint. By an argument analogous to\nthe one above, it can fail the Turing test. Thus, an entity which can\nthink would fail the Turing test.\n\nWe can respond to this concern by noting that the construction of\nquestions suggested by the results from mathematical\nlogic\u2014G\u00f6del, Turing, etc.\u2014are extremely complicated,\nand require extremely detailed information about the language and\ninternal programming of the digital computer (which, of course, is not\navailable to the interrogators in the Imitation Game). At the very\nleast, much more argument is required to overthrow the view that the\nTuring Test could remain a very high quality statistical test for the\npresence of mind and intelligence even if digital computers differ\nfrom human beings in being subject to the Lucas-Penrose constraint.\n(See Bowie 1982, Dietrich 1994, Feferman 1996, Abramson 2008, and\nSection 6.3 of the entry on\n G\u00f6del\u2019s incompleteness theorems,\n for further discussion.)\n2.4 The Argument from Consciousness\n\nTuring cites Professor Jefferson\u2019s Lister Oration for\n1949 as a source for the kind of objection that he takes to fall under\nthis label:\n\nNot until a machine can write a sonnet or compose a concerto because\nof thoughts and emotions felt, and not by the chance fall of symbols,\ncould we agree that machine equals brain\u2014that is, not only write\nit but know that it had written it. No mechanism could feel (and not\nmerely artificially signal, an easy contrivance) pleasure at its\nsuccesses, grief when its valves fuse, be warmed by flattery, be made\nmiserable by its mistakes, be charmed by sex, be angry or depressed\nwhen it cannot get what it wants. (445/6)\n\n\nThere are several different ideas that are being run together here,\nand that it is profitable to disentangle. One idea\u2014the one upon\nwhich Turing first focuses\u2014is the idea that the only way in\nwhich one could be certain that a machine thinks is to be the machine,\nand to feel oneself thinking. A second idea, perhaps, is that the\npresence of mind requires the presence of a certain kind of\nself-consciousness (\u201cnot only write it but know that it had\nwritten it\u201d). A third idea is that it is a mistake to take a\nnarrow view of the mind, i.e. to suppose that there could be a\nbelieving intellect divorced from the kinds of desires and emotions\nthat play such a central role in the generation of human behavior\n(\u201cno mechanism could feel \u2026\u201d).\n\nAgainst the solipsistic line of thought, Turing makes the effective\nreply that he would be satisfied if he could secure agreement on the\nclaim that we might each have just as much reason to suppose that\nmachines think as we have reason to suppose that other people\nthink. (The point isn\u2019t that Turing thinks that solipsism is a\nserious option; rather, the point is that following this line of\nargument isn\u2019t going to lead to the conclusion that there are\nrespects in which digital computers could not be our intellectual\nequals or superiors.)\n\nAgainst the other lines of thought, Turing provides a little\n\u201cviva voce\u201d that is intended to illustrate the\nkind of evidence that he supposes one might have that a machine is\nintelligent. Given the right kinds of responses from the machine, we\nwould naturally interpret its utterances as evidence of\npleasure, grief, warmth, misery, anger, depression, etc.\nPerhaps\u2014though Turing doesn\u2019t say this\u2014the only way\nto make a machine of this kind would be to equip it with sensors,\naffective states, etc., i.e., in effect, to make an artificial\nperson. However, the important point is that if the claims\nabout self-consciousness, desires, emotions, etc. are right, then\nTuring can accept these claims with equanimity: his claim is\nthen that a machine with a digital computing \u201cbrain\u201d can\nhave the full range of mental states that can be enjoyed by adult\nhuman beings.\n2.5 Arguments from Various Disabilities\n\nTuring considers a list of things that some people have claimed\nmachines will never be able to do: (1) be kind; (2) be resourceful;\n(3) be beautiful; (4) be friendly; (5) have initiative; (6) have a\nsense of humor; (7) tell right from wrong; (8) make mistakes; (9) fall\nin love; (10) enjoy strawberries and cream; (11) make someone fall in\nlove with one; (12) learn from experience; (13) use words properly;\n(14) be the subject of one\u2019s own thoughts; (15) have as much\ndiversity of behavior as a man; (16) do something really new.\n\nAn interesting question to ask, before we address these claims\ndirectly, is whether we should suppose that intelligent creatures from\nsome other part of the universe would necessarily be able to do these\nthings. Why, for example, should we suppose that there must be\nsomething deficient about a creature that does not enjoy\u2014or that\nis not able to enjoy\u2014strawberries and cream? True enough, we\nmight suppose that an intelligent creature ought to have the capacity\nto enjoy some kinds of things\u2014but it seems unduly chauvinistic\nto insist that intelligent creatures must be able to enjoy just the\nkinds of things that we do. (No doubt, similar considerations apply to\nthe claim that an intelligent creature must be the kind of thing that\ncan make a human being fall in love with it. Yes, perhaps, an\nintelligent creature should be the kind of thing that can love and be\nloved; but what is so special about us?)\n\nSetting aside those tasks that we deem to be unduly chauvinistic, we\nshould then ask what grounds there are for supposing that no digital\ncomputing machine could do the other things on the list.\nTuring suggests that the most likely ground lies in our prior\nacquaintance with machines of all kinds: none of the machines that any\nof us has hitherto encountered has been able to do these things. In\nparticular, the digital computers with which we are now familiar\ncannot do these things. (Except perhaps for make mistakes: after all,\neven digital computers are subject to \u201cerrors of\nfunctioning.\u201d But this might be set aside as an irrelevant\ncase.) However, given the limitations of storage capacity and\nprocessing speed of even the most recent digital computers, there are\nobvious reasons for being cautious in assessing the merits of this\ninductive argument.\n\n(A different question worth asking concerns the progress that has been\nmade until now in constructing machines that can do the kinds of\nthings that appear on Turing\u2019s list. There is at least room for\ndebate about the extent to which current computers can: make mistakes,\nuse words properly, learn from experience, be beautiful, etc.\nMoreover, there is also room for debate about the extent to which\nrecent advances in other areas may be expected to lead to further\nadvancements in overcoming these alleged disabilities. Perhaps, for\nexample, recent advances in work on artificial sensors may one day\ncontribute to the production of machines that can enjoy strawberries\nand cream. Of course, if the intended objection is to the notion that\nmachines can experience any kind of feeling of enjoyment, then it is\nnot clear that work on particular kinds of artificial sensors is to\nthe point.)\n2.6 Lady Lovelace\u2019s Objection\n\nOne of the most popular objections to the claim that there can be\nthinking machines is suggested by a remark made by Lady Lovelace in\nher memoir on Babbage\u2019s Analytical Engine:\n\nThe Analytical Engine has no pretensions to originate anything. It can\ndo whatever we know how to order it to perform (cited by Hartree,\np. 70)\n\n\nThe key idea is that machines can only do what we know how to\norder them to do (or that machines can never do anything really new,\nor anything that would take us by surprise). As Turing says, one way\nto respond to these challenges is to ask whether we can ever do\nanything \u201creally new.\u201d Suppose, for instance, that the\nworld is deterministic, so that everything that we do is fully\ndetermined by the laws of nature and the boundary conditions of the\nuniverse. There is a sense in which nothing \u201creally new\u201d\nhappens in a deterministic universe\u2014though, of course, the\nuniverse\u2019s being deterministic would be entirely compatible with\nour being surprised by events that occur within it. Moreover\u2014as\nTuring goes on to point out\u2014there are many ways in which even\ndigital computers do things that take us by surprise; more needs to be\nsaid to make clear exactly what the nature of this suggestion is.\n(Yes, we might suppose, digital computers are\n\u201cconstrained\u201d by their programs: they can\u2019t do\nanything that is not permitted by the programs that they have. But\nhuman beings are \u201cconstrained\u201d by their biology and their\ngenetic inheritance in what might be argued to be just the same kind\nof way: they can\u2019t do anything that is not permitted by the\nbiology and genetic inheritance that they have. If a program were\nsufficiently complex\u2014and if the processor(s) on which it ran\nwere sufficiently fast\u2014then it is not easy to say whether the\nkinds of \u201cconstraints\u201d that would remain would necessarily\ndiffer in kind from the kinds of constraints that are imposed by\nbiology and genetic inheritance.)\n\nBringsjord et al. (2001) claim that Turing\u2019s response to the\nLovelace Objection is \u201cmysterious\u201d at best, and\n\u201cincompetent\u201d at worst (p.4). In their view,\nTuring\u2019s claim that \u201ccomputers do take us by\nsurprise\u201d is only true when \u201csurprise\u201d is given a\nvery superficial interpretation. For, while it is true that computers\ndo things that we don\u2019t intend them to do\u2014because\nwe\u2019re not smart enough, or because we\u2019re not careful\nenough, or because there are rare hardware errors, or\nwhatever\u2014it isn\u2019t true that there are any cases in which\nwe should want to say that a computer has originated\nsomething. Whatever merit might be found in this objection, it seems\nworth pointing out that, in the relevant sense of\norigination, human beings \u201coriginate something\u201d\non more or less every occasion in which they engage in conversation:\nthey produce new sentences of natural language that it is appropriate\nfor them to produce in the circumstances in which they find\nthemselves. Thus, on the one hand\u2014for all that Bringsjord et al.\nhave argued\u2014The Turing Test is a perfectly good test for the\npresence of \u201corigination\u201d (or \u201ccreativity,\u201d or\nwhatever). Moreover, on the other hand, for all that Bringsjord et al.\nhave argued, it remains an open question whether a digital computing\ndevice is capable of \u201corigination\u201d in this sense (i.e.\ncapable of producing new sentences that are appropriate to the\ncircumstances in which the computer finds itself). So we are not\noverly inclined to think that Turing\u2019s response to the Lovelace\nObjection is poor; and we are even less inclined to think that Turing\nlacked the resources to provide a satisfactory response on this\npoint.\n2.7 Argument from Continuity of the Nervous System\n\nThe human brain and nervous system is not much like a digital\ncomputer. In particular, there are reasons for being skeptical of the\nclaim that the brain is a discrete-state machine. Turing observes that\na small error in the information about the size of a nervous impulse\nimpinging on a neuron may make a large difference to the size of the\noutgoing impulse. From this, Turing infers that the brain is likely to\nbe a continuous-state machine; and he then notes that, since\ndiscrete-state machines are not continuous-state machines, there might\nbe reason here for thinking that no discrete-state machine can be\nintelligent.\n\nTuring\u2019s response to this kind of argument seems to be that a\ncontinuous-state machine can be imitated by discrete-state machines\nwith very small levels of error. Just as differential analyzers can be\nimitated by digital computers to within quite small margins of error,\nso too, the conversation of human beings can be imitated by digital\ncomputers to margins of error that would not be detected by ordinary\ninterrogators playing the imitation game. It is not clear that this is\nthe right kind of response for Turing to make. If someone thinks that\nreal thought (or intelligence, or mind, or whatever) can only be\nlocated in a continuous-state machine, then the fact\u2014if, indeed,\nit is a fact\u2014that it is possible for discrete-state machines to\npass the Turing Test shows only that the Turing Test is no good. A\nbetter reply is to ask why one should be so confident that real\nthought, etc. can only be located in continuous-state machines (if,\nindeed, it is right to suppose that we are not discrete-state\nmachines). And, before we ask this question, we would do well to\nconsider whether we really do have such good reason to suppose that,\nfrom the standpoint of our ability to think, we are not essentially\ndiscrete-state machines. (As Block (1981) points out, it seems that\nthere is nothing in our concept of intelligence that rules out\nintelligent beings with quantised sensory devices; and nor is there\nanything in our concept of intelligence that rules out intelligent\nbeings with digital working parts.)\n2.8 Argument from Informality of Behavior\n\nThis argument relies on the assumption that there is no set of rules\nthat describes what a person ought to do in every possible set of\ncircumstances, and on the further assumption that there is a set of\nrules that describes what a machine will do in every possible set of\ncircumstances. From these two assumptions, it is supposed to\nfollow\u2014somehow!\u2014that people are not machines. As Turing\nnotes, there is some slippage between \u201cought\u201d and\n\u201cwill\u201d in this formulation of the argument. However, once\nwe make the appropriate adjustments, it is not clear that an obvious\ndifference between people and digital computers emerges.\n\nSuppose, first, that we focus on the question of whether there are\nsets of rules that describe what a person and a machine\n\u201cwill\u201d do in every possible set of circumstances. If the\nworld is deterministic, then there are such rules for both persons and\nmachines (though perhaps it is not possible to write down the rules).\nIf the world is not deterministic, then there are no such rules for\neither persons or machines (since both persons and machines can be\nsubject to non-deterministic processes in the production of their\nbehavior). Either way, it is hard to see any reason for supposing that\nthere is a relevant difference between people and machines that bears\non the description of what they will do in all possible sets of\ncircumstances. (Perhaps it might be said that what the objection\ninvites us to suppose is that, even though the world is not\ndeterministic, humans differ from digital machines precisely because\nthe operations of the latter are indeed deterministic. But, if the\nworld is non-deterministic, then there is no reason why digital\nmachines cannot be programmed to behave non-deterministically, by\nallowing them to access input from non-deterministic features of the\nworld.)\n\nSuppose, instead, that we focus on the question of whether there are\nsets of rules that describe what a person and a machine\n\u201cought\u201d to do in every possible set of circumstances.\nWhether or not we suppose that norms can be codified\u2014and quite\napart from the question of which kinds of norms are in\nquestion\u2014it is hard to see what grounds there could be for this\njudgment, other than the question-begging claim that machines are not\nthe kinds of things whose behavior could be subject to norms. (And, in\nthat case, the initial argument is badly mis-stated: the claim ought\nto be that, whereas there are sets of rules that describe what a\nperson ought to do in every possible set of circumstances, there are\nno sets of rules that describe what machines ought to do in\nall possible sets of circumstances!)\n2.9 Argument from Extra-Sensory Perception\n\nThe strangest part of Turing\u2019s paper is the few paragraphs on\nESP. Perhaps it is intended to be tongue-in-cheek, though, if it is,\nthis fact is poorly signposted by Turing. Perhaps, instead, Turing was\ninfluenced by the apparently scientifically respectable results of J.\nB. Rhine. At any rate, taking the text at face value, Turing seems to\nhave thought that there was overwhelming empirical evidence for\ntelepathy (and he was also prepared to take clairvoyance, precognition\nand psychokinesis seriously). Moreover, he also seems to have thought\nthat if the human participant in the game was telepathic, then the\ninterrogator could exploit this fact in order to determine the\nidentity of the machine\u2014and, in order to circumvent this\ndifficulty, Turing proposes that the competitors should be housed in a\n\u201ctelepathy-proof room.\u201d Leaving aside the point that, as a\nmatter of fact, there is no current statistical support for\ntelepathy\u2014or clairvoyance, or precognition, or\ntelekinesis\u2014it is worth asking what kind of theory of the nature\nof telepathy would have appealed to Turing. After all, if humans can\nbe telepathic, why shouldn\u2019t digital computers be so as well? If\nthe capacity for telepathy were a standard feature of any sufficiently\nadvanced system that is able to carry out human conversation, then\nthere is no in-principle reason why digital computers could not be the\nequals of human beings in this respect as well. (Perhaps this response\nassumes that a successful machine participant in the imitation game\nwill need to be equipped with sensors, etc. However, as we noted\nabove, this assumption is not terribly controversial. A plausible\nconversationalist has to keep up to date with goings-on in the\nworld.)\n\nAfter discussing the nine objections mentioned above, Turing goes on\nto say that he has \u201cno very convincing arguments of a positive\nnature to support my views. If I had I should not have taken such\npains to point out the fallacies in contrary views.\u201d (454)\nPerhaps Turing sells himself a little short in this self-assessment.\nFirst of all\u2014as his brief discussion of solipsism makes\nclear\u2014it is worth asking what grounds we have for attributing\nintelligence (thought, mind) to other people. If it is plausible to\nsuppose that we base our attributions on behavioral tests or\nbehavioral criteria, then his claim about the appropriate test to\napply in the case of machines seems apt, and his conjecture that\ndigital computing machines might pass the test seems like a\nreasonable\u2014though controversial\u2014empirical conjecture.\nSecond, subsequent developments in the philosophy of mind\u2014and,\nin particular, the fashioning of functionalist theories of the\nmind\u2014have provided a more secure theoretical environment in\nwhich to place speculations about the possibility of thinking\nmachines. If mental states are functional states\u2014and if mental\nstates are capable of realisation in vastly different kinds of\nmaterials\u2014then there is some reason to think that it is an\nempirical question whether minds can be realised in digital computing\nmachines. Of course, this kind of suggestion is open to challenge; we\nshall consider some important philosophical objections in the later\nparts of this review.\n3. Some Minor Issues Arising\n\nThere are a number of much-debated issues that arise in connection\nwith the interpretation of various parts of Turing (1950), and that we\nhave hitherto neglected to discuss. What has been said in the first\ntwo sections of this document amounts to our interpretation of what\nTuring has to say (perhaps bolstered with what we take to be further\nrelevant considerations in those cases where Turing\u2019s remarks\ncan be fairly readily improved upon). But since some of this\ninterpretation has been contested, it is probably worth noting where\nthe major points of controversy have been.\n3.1 Interpreting the Imitation Game\n\nTuring (1950) introduces the imitation game by describing a game in\nwhich the participants are a man, a woman, and a human interrogator.\nThe interrogator is in a room apart from the other two, and is set the\ntask of determining which of the other two is a man and which is a\nwoman. Both the man and the woman are set the task of trying to\nconvince the interrogator that they are the woman. Turing recommends\nthat the best strategy for the woman is to answer all questions\ntruthfully; of course, the best strategy for the man will require some\nlying. The participants in this game also use teletypewriter to\ncommunicate with one another\u2014to avoid clues that might be\noffered by tone of voice, etc. Turing then says: \u201cWe now ask the\nquestion, \u2018What will happen when a machine takes the part of A\nin this game?\u2019 Will the interrogator decide wrongly as often\nwhen the game is played like this as he does when the game is played\nbetween a man and a woman?\u201d (434).\n\nNow, of course, it is possible to interpret Turing as here\nintending to say what he seems literally to say, namely, that the new\ngame is one in which the computer must pretend to be a woman, and the\nother participant in the game is a woman. (For discussion, see, for\nexample, Genova (1994) and Traiger (2000).) And it is also\npossible to interpret Turing as intending to say that the new\ngame is one in which the computer must pretend to be a woman, and the\nother participant in the game is a man who must also pretend to be a\nwoman. However, as Copeland (2000), Piccinini (2000), and Moor (2001)\nconvincingly argue, the rest of Turing\u2019s article, and material\nin other articles that Turing wrote at around the same time, very\nstrongly support the claim that Turing actually intended the standard\ninterpretation that we gave above, viz. that the computer is to\npretend to be a human being, and the other participant in the game is\na human being of unspecified gender. Moreover, as Moor (2001) argues,\nthere is no reason to think that one would get a better test if the\ncomputer must pretend to be a woman and the other participant in the\ngame is a man pretending to be a woman; and, indeed, there is some\nreason to think that one would get a worse test. Perhaps it would make\nno difference to the effectiveness of the test if the computer must\npretend to be a woman, and the other participant is a woman (any more\nthan it would make a difference if the computer must pretend to be an\naccountant and the other participant is an accountant); however, this\nconsideration is simply insufficient to outweigh the strong textual\nevidence that supports the standard interpretation of the imitation\ngame that we gave at the beginning of our discussion of Turing (1950).\n(For a dissenting view about many of the matters discussed in this\nparagraph, see Sterrett (2000; 2020).)\n3.2 Turing\u2019s Predictions\n\nAs we noted earlier, Turing (1950) makes the claim that:\n\nI believe that in about fifty years\u2019 time it will be possible to\nprogramme computers, with a storage capacity of about 109,\nto make them play the imitation game so well that an average\ninterrogator will not have more than 70 percent chance of making the\nright identification after five minutes of questioning. \u2026 I\nbelieve that at the end of the century the use of words and general\neducated opinion will have altered so much that one will be able to\nspeak of machines thinking without expecting to be contradicted.\n\n\nMost commentators contend that this claim has been shown to be\nmistaken: in the year 2000, no-one was able to program\ncomputers to make them play the imitation game so well that an average\ninterrogator had no more than a 70% chance of making the correct\nidentification after five minutes of questioning. Copeland (2000)\nargues that this contention is seriously mistaken: \u201cabout fifty\nyears\u201d is by no means \u201cexactly fifty years,\u201d and it\nremains open that we may soon be able to do the required programming.\nAgainst this, it should be noted that Turing (1950) goes on\nimmediately to refer to how things will be \u201cat the end of the\ncentury,\u201d which suggests that not too much can be read into the\nqualifying \u201cabout.\u201d However, as Copeland (2000) points\nout, there are other more cautious predictions that Turing makes\nelsewhere (e.g., that it would be \u201cat least 100 years\u201d\nbefore a machine was able to pass an unrestricted version of his\ntest); and there are other predictions that are made in Turing (1950)\nthat seem to have been vindicated. In particular, it is plausible to\nclaim that, in the year 2000, educated opinion had altered to the\nextent that, in many quarters, one could speak of the possibility of\nmachines\u2019 thinking\u2014and of machines\u2019\nlearning\u2014without expecting to be contradicted. As Moor (2001)\npoints out, \u201cmachine intelligence\u201d is not the oxymoron\nthat it might have been taken to be when Turing first started thinking\nabout these matters.\n3.3 A Useful Distinction\n\nThere are two different theoretical claims that are run together in\nmany discussions of The Turing Test that can profitably be separated.\nOne claim holds that the general scheme that is described in\nTuring\u2019s Imitation Game provides a good test for the presence of\nintelligence. (If something can pass itself off as a person under\nsufficiently demanding test conditions, then we have very good reason\nto suppose that that thing is intelligent.) Another claim holds that\nan appropriately programmed computer could pass the kind of test that\nis described in the first claim. We might call the first claim\n\u201cThe Turing Test Claim\u201d and the second claim \u201cThe\nThinking Machine Claim\u201d. Some objections to the claims made in\nTuring (1950) are objections to the Thinking Machine Claim, but not\nobjections to the Turing Test Claim. (Consider, for example, the\nargument of Searle (1982), which we discuss further in Section 6.)\nHowever, other objections are objections to the Turing Test Claim.\nUntil we get to Section 6, we shall be confining our attention to\ndiscussions of the Turing Test Claim.\n3.4 A Further Note\n\nIn this article, we follow the standard philosophical convention\naccording to which \u201ca mind\u201d means \u201cat least one\nmind\u201d. If \u201cpassing the Turing Test\u201d implies\nintelligence, then \u201cpassing the Turing Test\u201d implies the\npresence of at least one mind. We cannot here explore recent\ndiscussions of \u201cswarm intelligence\u201d, \u201ccollective\nintelligence\u201d, and the like. However, it is surely clear that\ntwo people taking turns could \u201cpass the Turing Test\u201d in\ncircumstances in which we should be very reluctant to say that there\nis a \u201ccollective mind\u201d that has the minds of the two as\ncomponents.\n4. Assessment of the Current Standing of The Turing Test\n\nGiven the initial distinction that we made between different ways in\nwhich the expression The Turing Test gets interpreted in the\nliterature, it is probably best to approach the question of the\nassessment of the current standing of The Turing Test by dividing\ncases. True enough, we think that there is a correct interpretation of\nexactly what test it is that is proposed by Turing (1950); but a\ncomplete discussion of the current standing of The Turing Test should\npay at least some attention to the current standing of other tests\nthat have been mistakenly supposed to be proposed by Turing\n(1950).\n\nThere are a number of main ideas to be investigated. First, there is\nthe suggestion that The Turing Test provides logically necessary and\nsufficient conditions for the attribution of intelligence. Second,\nthere is the suggestion that The Turing Test provides logically\nsufficient\u2014but not logically necessary\u2014conditions for the\nattribution of intelligence. Third, there is the suggestion that The\nTuring Test provides \u201ccriteria\u201d\u2014defeasible\nsufficient conditions\u2014for the attribution of intelligence.\nFourth\u2014and perhaps not importantly distinct from the previous\nclaim\u2014there is the suggestion that The Turing Test provides\n(more or less strong) probabilistic support for the attribution of\nintelligence. We shall consider each of these suggestions in turn.\n4.1 (Logically) Necessary and Sufficient Conditions\n\nIt is doubtful whether there are very many examples of people who have\nexplicitly claimed that The Turing Test is meant to provide conditions\nthat are both logically necessary and logically sufficient for the\nattribution of intelligence. (Perhaps Block (1981) is one such case.)\nHowever, some of the objections that have been proposed against The\nTuring Test only make sense under the assumption that The Turing Test\ndoes indeed provide logically necessary and logically sufficient\nconditions for the attribution of intelligence; and many more of the\nobjections that have been proposed against The Turing Test only make\nsense under the assumption that The Turing Test provides necessary and\nsufficient conditions for the attribution of intelligence, where the\nmodality in question is weaker than the strictly logical, e.g., nomic\nor causal.\n\nConsider, for example, those people who have claimed that The Turing\nTest is chauvinistic; and, in particular, those people who have\nclaimed that it is surely logically possible for there to be something\nthat possesses considerable intelligence, and yet that is not able to\npass The Turing Test. (Examples: Intelligent creatures might fail to\npass The Turing Test because they do not share our way of life;\nintelligent creatures might fail to pass The Turing Test because they\nrefuse to engage in games of pretence; intelligent creatures might\nfail to pass The Turing Test because the pragmatic conventions that\ngovern the languages that they speak are so very different from the\npragmatic conventions that govern human languages. Etc.) None of this\ncan constitute objections to The Turing Test unless The Turing Test\ndelivers necessary conditions for the attribution of\nintelligence.\n\nFrench (1990) offers ingenious arguments that are intended to show\nthat \u201cthe Turing Test provides a guarantee not of intelligence,\nbut of culturally-oriented intelligence.\u201d But, of course,\nanything that has culturally-oriented intelligence has\nintelligence; so French\u2019s objections cannot be taken to be\ndirected towards the idea that The Turing Test provides sufficient\nconditions for the attribution of intelligence. Rather\u2014as we\nshall see later\u2014French supposes that The Turing Test establishes\nsufficient conditions that no machine will ever satisfy. That is, in\nFrench\u2019s view, what is wrong with The Turing Test is that it\nestablishes utterly uninteresting sufficient conditions for the\nattribution of intelligence.\n\nFloridi and Chiriatti (2020: 683) say that The Turing Test provides\nnecessary but insufficient conditions for intelligence: not passing\nThe Turing Test disqualifies an AI from being intelligent, but passing\nThe Turing Test is not sufficient to qualify an AI as intelligent.\nHowever, they also say that \u201cany reader ... will be well\nacquainted with the nature of the test, so we shall not describe\nit.\u201d The account that they would give of The Turing Test\nmust be quite different from the account of The Turing Test that we\nhave been presenting.\n4.2 Logically Sufficient Conditions\n\nThere are many philosophers who have supposed that The Turing Test is\nintended to provide logically sufficient conditions for the\nattribution of intelligence. That is, there are many philosophers who\nhave supposed that The Turing Test claims that it is logically\nimpossible for something that lacks intelligence to pass The Turing\nTest. (Often, this supposition goes with an interpretation according\nto which passing The Turing Test requires rather a lot, e.g.,\nproducing behavior that is indistinguishable from human behavior over\nan entire lifetime.)\n\nThere are well-known arguments against the claim that passing The\nTuring Test\u2014or any other purely behavioral test\u2014provides\nlogically sufficient conditions for the attribution of intelligence.\nThe standard objection to this kind of analysis of\nintelligence (mind, thought) is that a being whose behavior was\nproduced by \u201cbrute force\u201d methods ought not to count as\nintelligent (as possessing a mind, as having thoughts).\n\nConsider, for example, Ned Block\u2019s Blockhead. Blockhead\nis a creature that looks just like a human being, but that is\ncontrolled by a \u201cgame-of-life look-up tree,\u201d i.e. by a\ntree that contains a programmed response for every discriminable input\nat each stage in the creature\u2019s life. If we agree that Blockhead\nis logically possible, and if we agree that Blockhead is not\nintelligent (does not have a mind, does not think), then Blockhead is\na counterexample to the claim that the Turing Test provides a\nlogically sufficient condition for the ascription of intelligence.\nAfter all, Blockhead could be programmed with a look-up tree that\nproduces responses identical with the ones that you would\ngive over the entire course of your life (given the same\ninputs).\n\nThere are perhaps only two ways in which someone who claims that The\nTuring Test offers logically sufficient conditions for the attribution\nof intelligence can respond to Block\u2019s argument. First, it could\nbe denied that Blockhead is a logical possibility; second, it could be\nclaimed that Blockhead would be intelligent (have a mind, think).\n\nIn order to deny that Blockhead is a logical possibility, it seems\nthat what needs to be denied is the commonly accepted link between\nconceivability and logical possibility: it certainly seems that\nBlockhead is conceivable, and so, if (properly circumscribed)\nconceivability is sufficient for logical possibility, then it seems\nthat we have good reason to accept that Blockhead is a logical\npossibility. Since it would take us too far away from our present\nconcerns to explore this issue properly, we merely note that it\nremains a controversial question whether (properly circumscribed)\nconceivability is sufficient for logical possibility. (For further\ndiscussion of this issue, see Crooke (2002).)\n\nThe question of whether Blockhead is intelligent (has a mind, thinks)\nmay seem straightforward, but\u2014despite Block\u2019s confident\nassertion that Blockhead \u201chas all of the intelligence of a\ntoaster\u201d\u2014it is not obvious that we should deny that\nBlockhead is intelligent. Blockhead may not be a particularly\nefficient processor of information; but it is at least a processor of\ninformation, and that\u2014in combination with the behavior that is\nproduced as a result of the processing of information\u2014might well\nbe taken to be sufficient grounds for the attribution of some\nlevel of intelligence to Blockhead. For further critical discussion of\nthe argument of Block (1981), see McDermott (2014), and Pautz and\nStoljar (2019).\n4.3 Criteria\n\nIn his Philosophical Investigations, Wittgenstein famously\nwrites: \u201cAn \u2018inner process\u2019 stands in need of\noutward criteria\u201d (580). Exactly what Wittgenstein meant by this\nremark is unclear, but one way in which it might be interpreted is as\nfollows: in order to be justified in ascribing a \u201cmental\nstate\u201d to some entity, there must be some true claims about the\nobservable behavior of that entity that, (perhaps) together with other\ntrue claims about that entity (not themselves couched in\n\u201cmentalistic\u201d vocabulary), entail that the entity has the\nmental state in question. If no true claims about the observable\nbehavior of the entity can play any role in the justification of the\nascription of the mental state in question to the entity, then there\nare no grounds for attributing that kind of mental state to the\nentity.\n\nThe claim that, in order to be justified in ascribing a mental state\nto an entity, there must be some true claims about the observable\nbehavior of that entity that alone\u2014i.e. without the addition of\nany other true claims about that entity\u2014entail that the entity\nhas the mental state in question, is a piece of philosophical\nbehaviorism. It may be\u2014for all that we are able to\nargue\u2014that Wittgenstein was a philosophical behaviorist; it may\nbe\u2014for all that we are able to argue\u2014that Turing was one,\ntoo. However, if we go by the letter of the account given in the\nprevious paragraph, then all that need follow from the claim that the\nTuring Test is criterial for the ascription of intelligence (thought,\nmind) is that, when other true claims (not themselves couched in terms\nof mentalistic vocabulary) are conjoined with the claim that an entity\nhas passed the Turing Test, it then follows that the entity in\nquestion has intelligence (thought, mind).\n\n(Note that the parenthetical qualification that the additional true\nclaims not be couched in terms of mentalistic vocabulary is only one\nway in which one might try to avoid the threat of trivialization. The\ndifficulty is that the addition of the true claim that an entity has a\nmind will always produce a set of claims that entails that that entity\nhas a mind, no matter what other claims belong to the set!)\n\nTo see how the claim that the Turing Test is merely criterial for the\nascription of intelligence differs from the logical behaviorist claim\nthat the Turing Test provides logically sufficient conditions for the\nascription of intelligence, it suffices to consider the question of\nwhether it is nomically possible for there to be a\n\u201chand simulation\u201d of a Turing Test program. Many people\nhave supposed that there is good reason to deny that Blockhead is a\nnomic (or physical) possibility. For example, in The Physics of\nImmortality, Frank Tipler provides the following argument in\ndefence of the claim that it is physically impossible to \u201chand\nsimulate\u201d a Turing-Test-passing program:\n\nIf my earlier estimate that the human brain can code as much as\n1015 bits is correct, then since an average book codes\nabout 106 bits \u2026 it would require more than 100\nmillion books to code the human brain. It would take at least thirty\nfive-story main university libraries to hold this many books. We know\nfrom experience that we can access any memory in our brain in about\n100 seconds, so a hand simulation of a Turing Test-passing program\nwould require a human being to be able to take off the shelf, glance\nthrough, and return to the shelf all of these 100 million books in 100\nseconds. If each book weighs about a pound (0.5 kilograms), and on the\naverage the book moves one yard (one meter) in the process of taking\nit off the shelf and returning it, then in 100 seconds the energy\nconsumed in just moving the books is 3 x 1019 joules; the\nrate of energy consumption is 3 x 1011 megawatts. Since a\nhuman uses energy at a normal rate of 100 watts, the power required is\nthe bodily power of 3 x 1015 human beings, about a million\ntimes the current population of the entire earth. A typical large\nnuclear power plant has a power output of 1,000 megawatts, so a hand\nsimulation of the human program requires a power output equal to that\nof 300 million large nuclear power plants. As I said, a man can no\nmore hand-simulate a Turing Test-passing program than he can jump to\nthe Moon. In fact, it is far more difficult. (40)\n\n\nWhile there might be ways in which the details of Tipler\u2019s\nargument could be improved, the general point seems clearly right: the\nkind of combinatorial explosion that is required for a look-up tree\nfor a human being is ruled out by the laws and boundary conditions\nthat govern the operations of the physical world. But, if this is\nright, then, while it may be true that Blockhead is a logical\npossibility, it follows that Blockhead is not a nomic or\nphysical possibility. And then it seems natural to hold that\nThe Turing Test does indeed provide nomically sufficient\nconditions for the attribution of intelligence: given everything else\nthat we already know\u2014or, at any rate, take ourselves to\nknow\u2014about the universe in which we live, we would be fully\njustified in concluding that anything that succeeds in passing The\nTuring Test is, indeed, intelligent (possessed of a mind, and so\nforth).\n\nThere are ways in which the argument in the previous paragraph might\nbe resisted. At the very least, it is worth noting that there is a\nserious gap in the argument that we have just rehearsed. Even if we\ncan rule out \u201chand simulation\u201d of intelligence, it does\nnot follow that we have ruled out all other kinds of mere simulation\nof intelligence. Perhaps\u2014for all that has been argued so\nfar\u2014there are nomically possible ways of producing mere\nsimulations of intelligence. But, if that\u2019s right, then passing\nThe Turing Test need not be so much as criterial for the possession of\nintelligence: it need not be that given everything else that we\nalready know\u2014or, at any rate, take ourselves to know\u2014about\nthe universe in which we live, we would be fully justified in\nconcluding that anything that succeeds in passing The Turing Test is,\nindeed, intelligent (possessed of a mind, and so forth).\n\n(McDermott (2014) calculates that a look-up table for a participant\nwho makes 50 conversational exchanges would have about\n1022278 nodes. It is tempting to take this calculation to\nestablish that it is neither nomically nor physically possible for\nthere to be a \u201chand simulation\u201d of a Turing Test program,\non the grounds that the required number of nodes could not be fitted\ninto a space much much larger than the entire observable\nuniverse.)\n4.4 Probabilistic Support\n\nWhen we look at the initial formulation that Turing provides of his\ntest, it is clear that he thought that the passing of the test would\nprovide probabilistic support for the hypothesis of intelligence.\nThere are at least two different points to make here. First, the\nprediction that Turing makes is itself probabilistic: Turing\npredicts that, in about fifty years from the time of his writing, it\nwill be possible to programme digital computers to make them play the\nimitation game so well that an average interrogator will have no more\nthan a seventy per cent chance of making the right identification\nafter five minutes of questioning. Second, the probabilistic nature of\nTuring\u2019s prediction provides good reason to think that the\ntest that Turing proposes is itself of a probabilistic\nnature: a given level of success in the imitation game\nproduces\u2014or, at any rate, should produce\u2014a specifiable\nlevel of increase in confidence that the participant in question is\nintelligent (has thoughts, is possessed of a mind). Since Turing\ndoesn\u2019t tell us how he supposes that levels of success in the\nimitation game correlate with increases in confidence that the\nparticipant in question is intelligent, there is a sense in which The\nTuring Test is greatly underspecified. Relevant variables clearly\ninclude: the length of the period of time over which the questioning\nin the game takes place (or, at any rate, the \u201camount\u201d of\nquestioning that takes place); the skills and expertise of the\ninterrogator (this bears, for example, on the \u201cdepth\u201d and\n\u201cdifficulty\u201d of the questioning that takes place); the\nskills and expertise of the third player in the game; and the number\nof independent sessions of the game that are run (particularly when\nthe other participants in the game differ from one run to the next).\nClearly, a machine that is very successful in many different runs of\nthe game that last for quite extended periods of time and that involve\nhighly skilled participants in the other roles has a much stronger\nclaim to intelligence than a machine that has been successful in a\nsingle, short run of the game with highly inexpert participants. That\na machine has succeeded in one short run of the game against inexpert\nopponents might provide some reason for increase in confidence that\nthe machine in question is intelligent: but it is clear that results\non subsequent runs of the game could quickly overturn this initial\nincrease in confidence. That a machine has done much better than\nchance over many long runs of the imitation game against a variety of\nskilled participants surely provides much stronger evidence that the\nmachine is intelligent. (Given enough evidence of this kind, it seems\nthat one could be quite confident indeed that the machine is\nintelligent, while still\u2014of course\u2014recognizing that\none\u2019s judgment could be overturned by further evidence, such as\na series of short runs in which it does much worse than chance against\nparticipants who use the same strategy over and over to expose the\nmachine as a machine.)\n\nThe probabilistic nature of The Turing Test is often overlooked. True\nenough, Moor (1976, 2001)\u2014along with various other\ncommentators\u2014has noted that The Turing Test is\n\u201cinductive,\u201d i.e. that \u201cThe Turing Test\u201d\nprovides no more than defeasible evidence of intelligence. However, it\nis one thing to say that success in \u201ca rigorous Turing\ntest\u201d provides no more than defeasible evidence of intelligence;\nit is quite another to note the probabilistic features to which we\nhave drawn attention in the preceding paragraph. Consider, for\nexample, Moor\u2019s observation (Moor 2001:83) that \u201c\u2026\ninductive evidence gathered in a Turing test can be outweighed by new\nevidence. \u2026 If new evidence shows that a machine passed the\nTuring Test by remote control run by a human behind the scenes, then\nreassessment is called for.\u201d This\u2014and other similar\npassages\u2014seems to us to suggest that Moor supposes that a\n\u201crigorous Turing test\u201d is a one-off event in which the\nmachine either succeeds or fails. But this interpretation of The\nTuring Test is vulnerable to the kind of objection lodged by\nBringsjord (1994): even on a moderately long single run with\nrelatively expert participants, it may not be all that unlikely that\nan unintelligent machine serendipitously succeeds in the imitation\ngame. In our view, given enough sufficiently long runs with different\nsufficiently expert participants, the likelihood of serendipitous\nsuccess can be made as small as one wishes. Thus, while\nBringsjord\u2019s \u201cargument from serendipity\u201d has force\nagainst some versions of The Turing Test, it has no force against the\nmost plausible interpretation of the test that Turing actually\nproposed.\n\nIt is worth noting that it is quite easy to construct more\nsophisticated versions of \u201cThe Imitation Game\u201d that yield\nmore fine-grained statistical data. For example, rather than getting\nthe judges to issue Yes/No verdicts about both of the participants in\nthe game, one could get the judges to provide probabilistic answers.\n(\u201cI give a 75% probability to the claim that A is the machine,\nand only 25% probability to the claim that B is the machine.\u201d)\nThis point is important when one comes to consider criticisms of the\n\u201cmethodology\u201d implicit in \u201cThe Turing Test\u201d.\n(For further discussion of the probabilistic nature of \u201cThe\nTuring Test\u201d, see Shieber (2007).)\n5. Alternative Tests\n\nSome of the literature about The Turing Test is concerned with\nquestions about the framing of a test that can provide a suitable\nguide to future research in the area of Artificial Intelligence. The\nidea here is very simple. Suppose that we have the ambition to produce\nan artificially intelligent entity. What tests should we take as\nsetting the goals that putatively intelligent artificial systems\nshould achieve? Should we suppose that The Turing Test provides an\nappropriate goal for research in this field? In assessing these\nproposals, there are two different questions that need to be borne in\nmind. First, there is the question whether it is a useful goal for AI\nresearch to aim to make a machine that can pass the given test\n(administered over the specified length of time, at the specified\ndegree of success). Second, there is the question of the appropriate\nconclusion to draw about the mental capacities of a machine that does\nmanage to pass the test (administered over the specified length of\ntime, at the specified degree of success).\n\nOpinion on these questions is deeply divided. Some people suppose that\nThe Turing Test does not provide a useful goal for research in AI\nbecause it is far too difficult to produce a system that can pass the\ntest. Other people suppose that The Turing Test does not provide a\nuseful goal for research in AI because it sets a very narrow target\n(and thus sets unnecessary restrictions on the kind of research that\ngets done). Some people think that The Turing Test provides an\nentirely appropriate goal for research in AI; while other people think\nthat there is a sense in which The Turing Test is not really demanding\nenough, and who suppose that The Turing Test needs to be extended in\nvarious ways in order to provide an appropriate goal for AI. We shall\nconsider some representatives of each of these positions in turn.\n\nThere are some people who continue to endorse The Turing Test. For\nexample, Neufeld and Finnestad (2020a) (2020b) argue that The Turing\nTest is no barrier to progress in AI, requires no significant\nredefinition, and does not shut down other avenues of investigation.\nMaybe we do better just to take The Turing Test to define a watershed\nrather than a threshold towards which we might hope to make\nincremental progression.\n5.1 The Turing Test is Too Hard\n\nSome people have claimed that The Turing Test doesn\u2019t set an\nappropriate goal for current research in AI because we are plainly so\nfar away from attaining this goal. Amongst these people there are some\nwho have gone on to offer reasons for thinking that it is doubtful\nthat we shall ever be able to create a machine that can pass The\nTuring Test\u2014or, at any rate, that it is doubtful that we shall\nbe able to do this at any time in the foreseeable future. Perhaps the\nmost interesting arguments of this kind are due to French (1990); at\nany rate, these are the arguments that we shall go on to consider.\n(Cullen (2009) sets out similar considerations.)\n\nAccording to French, The Turing Test is \u201cvirtually\nuseless\u201d as a real test of intelligence, because nothing without\na \u201chuman subcognitive substrate\u201d could pass the test, and\nyet the development of an artificial \u201chuman cognitive\nsubstrate\u201d is almost impossibly difficult. At the very least,\nthere are straightforward sets of questions that reveal\n\u201clow-level cognitive structure\u201d and that\u2014in\nFrench\u2019s view\u2014are almost certain to be successful in\nseparating human beings from machines.\n\nFirst, if interrogators are allowed to draw on the results of research\ninto, say, associative priming, then there is data that will\nvery plausibly separate human beings from machines. For example, there\nis research that shows that, if humans are presented with series of\nstrings of letters, they require less time to recognize that a string\nis a word (in a language that they speak) if it is preceded by a\nrelated word (in the language that they speak), rather than by an\nunrelated word (in the language that they speak) or a string of\nletters that is not a word (in the language that they speak). Provided\nthat the interrogator has accurate data about average recognition\ntimes for subjects who speak the language in question, the\ninterrogator can distinguish between the machine and the human simply\nby looking at recognition times for appropriate series of strings of\nletters. Or so says French. It isn\u2019t clear to us that this is\nright. After all, the design of The Turing Test makes it hard to see\nhow the interrogator will get reliable information about response\ntimes to series of strings of symbols. The point of putting the\ncomputer in a separate room and requiring communication by teletype\nwas precisely to rule out certain irrelevant ways of identifying the\ncomputer. If these requirements don\u2019t already rule out\nidentification of the computer by the application of tests of\nassociative priming, then the requirements can surely be altered to\nbring it about that this is the case. (Perhaps it is also worth noting\nthat administration of the kind of test that French imagines is not\nordinary conversation; nor is it something that one would expect that\nany but a few expert interrogators would happen upon. So, even if the\ncircumstances of The Turing Test do not rule out the kind of procedure\nthat French here envisages, it is not clear that The Turing Test will\nbe impossibly hard for machines to pass.)\n\nSecond, at a slightly higher cognitive level, there are certain kinds\nof \u201cratings games\u201d that French supposes will be very\nreliable discriminators between humans and machines. For instance, the\n\u201cNeologism Ratings Game\u201d\u2014which asks participants to\nrank made-up words on their appropriateness as names for given kinds\nof entities\u2014and the \u201cCategory Rating\nGame\u201d\u2014which asks participants to rate things of one\ncategory as things of another category\u2014are both, according to\nFrench, likely to prove highly reliable in discriminating between\nhumans and machines. For, in the first case, the ratings that humans\nmake depend upon large numbers of culturally acquired associations\n(which it would be well-nigh impossible to identify and describe, and\nhence which it would (arguably) be well-nigh impossible to program\ninto a computer). And, in the second case, the ratings that people\nactually make are highly dependent upon particular social and cultural\nsettings (and upon the particular ways in which human life is\nexperienced). To take French\u2019s examples, there would be\nwidespread agreement amongst competent English speakers in the\ntechnologically developed Western world that \u201cFlugblogs\u201d\nis not an appropriate name for a breakfast cereal, while\n\u201cFlugly\u201d is an appropriate name for a child\u2019s teddy\nbear. And there would also be widespread agreement amongst competent\nspeakers of English in the developed world that pens rate higher as\nweapons than grand pianos rate as wheelbarrows. Again, there are\nquestions that can be raised about French\u2019s argument here. It is\nnot clear to us that the data upon which the ratings games rely is as\nreliable as French would have us suppose. (At least one of us thinks\nthat \u201cFlugly\u201d would be an entirely inappropriate name for\na child\u2019s teddy bear, a response that is due to the similarity\nbetween the made-up word \u201cFlugly\u201d and the word\n\u201cFugly,\u201d that had some currency in the primarily\nundergraduate University college that we both attended. At least one\nof us also thinks that young children would very likely be delighted\nto eat a cereal called \u201cFlugblogs,\u201d and that a good answer\nto the question about ratings pens and grand pianos is that it all\ndepends upon the pens and grand pianos in question. What if the grand\npiano has wheels? What if the opponent has a sword or a sub-machine\ngun? It isn\u2019t obvious that a refusal to play this kind of\nratings game would necessarily be a give-away that one is a machine.)\nMoreover, even if the data is reliable, it is not obvious that any but\na select group of interrogators will hit upon this kind of strategy\nfor trying to unmask the machine; nor is it obvious that it is\nimpossibly hard to build a machine that is able to perform in the way\nin which typical humans do on these kinds of tests. In particular,\nif\u2014as Turing assumes\u2014it is possible to make learning\nmachines that can be \u201ctrained up\u201d to learn how to do\nvarious kinds of tasks, then it is quite unclear why these machines\ncouldn\u2019t acquire just the same kinds of \u201csubcognitive\ncompetencies\u201d that human children acquire when they are\n\u201ctrained up\u201d in the use of language.\n\nThere are other reasons that have been given for thinking that The\nTuring Test is too hard (and, for this reason, inappropriate in\nsetting goals for current research into artificial intelligence). In\ngeneral, the idea is that there may well be features of human\ncognition that are particularly hard to simulate, but that are not in\nany sense essential for intelligence (or thought, or possession of a\nmind). The problem here is not merely that The Turing Test really does\ntest for human intelligence; rather, the problem here is the\nfact\u2014if indeed it is a fact\u2014that there are quite\ninessential features of human intelligence that are extraordinarily\ndifficult to replicate in a machine. If this complaint is\njustified\u2014if, indeed, there are features of human intelligence\nthat are extraordinarily difficult to replicate in machines,\nand that could and would be reliably used to unmask machines\nin runs of The Turing Test\u2014then there is reason to worry about\nthe idea that The Turing Test sets an appropriate direction for\nresearch in artificial intelligence. However, as our discussion of\nFrench shows, there may be reason for caution in supposing that the\nkinds of considerations discussed in the present section show that we\nare already in a position to say that The Turing Test does indeed set\ninappropriate goals for research in artificial intelligence.\n5.2 The Turing Test is Too Narrow\n\nThere are authors who have suggested that The Turing Test does not set\na sufficiently broad goal for research in the area of artificial\nintelligence. Amongst these authors, there are many who suppose that\nThe Turing Test is too easy. (We go on to consider some of these\nauthors in the next sub-section.) But there are also some authors who\nhave supposed that, even if the goal that is set by The Turing Test is\nvery demanding indeed, it is nonetheless too restrictive.\n\nObjection to the notion that the Turing Test provides a logically\nsufficient condition for intelligence can be adapted to the goal of\nshowing that the Turing Test is too restrictive. Consider, for\nexample, Gunderson (1964). Gunderson has two major complaints to make\nagainst The Turing Test. First, he thinks that success in\nTuring\u2019s Imitation Game might come for reasons other than the\npossession of intelligence. But, second, he thinks that success in the\nImitation Game would be but one example of the kinds of things that\nintelligent beings can do and\u2014hence\u2014in itself could not be\ntaken as a reliable indicator of intelligence. By way of analogy,\nGunderson offers the case of a vacuum cleaner salesman who claims that\nhis product is \u201call-purpose\u201d when, in fact, all it does is\nto suck up dust. According to Gunderson, Turing is in the same\nposition as the vacuum cleaner salesman if he is prepared to\nsay that a machine is intelligent merely on the basis of its success\nin the Imitation Game. Just as \u201call purpose\u201d entails the\nability to do a range of things, so, too, \u201cthinking\u201d\nentails the possession of a range of abilities (beyond the mere\nability to succeed in the Imitation Game).\n\nThere is an obvious reply to the argument that we have here attributed\nto Gunderson, viz. that a machine that is capable of success in the\nImitation Game is capable of doing a large range of different kinds of\nthings. In order to carry out a conversation, one needs to have many\ndifferent kinds of cognitive skills, each of which is capable of\napplication in other areas. Apart from the obvious general cognitive\ncompetencies\u2014memory, perception, etc.\u2014there are many\nparticular competencies\u2014rudimentary arithmetic abilities,\nunderstanding of the rules of games, rudimentary understanding of\nnational politics, etc.\u2014which are tested in the course of\nrepeated runs of the Imitation Game. It is inconceivable that that\nthere be a machine that is startlingly good at playing the Imitation\nGame, and yet unable to do well at any other tasks that might\nbe assigned to it; and it is equally inconceivable that there is a\nmachine that is startlingly good at the Imitation Game and yet that\ndoes not have a wide range of competencies that can be displayed in a\nrange of quite disparate areas. To the extent that Gunderson considers\nthis line of reply, all that he says is that there is no reason to\nthink that a machine that can succeed in the Imitation Game\nmust have more than a narrow range of abilities; we think\nthat there is no reason to believe that this reply should be taken\nseriously.\n\nMore recently, Erion (2001) has defended a position that has some\naffinity to that of Gunderson. According to Erion, machines might be\n\u201ccapable of outperforming human beings in limited tasks in\nspecific environments, [and yet] still be unable to act skillfully in\nthe diverse range of situations that a person with common sense\ncan\u201d (36). On one way of understanding the claim that Erion\nmakes, he too believes that The Turing Test only identifies one\namongst a range of independent competencies that are possessed by\nintelligent human beings, and it is for this reason that he proposes a\nmore comprehensive \u201cCartesian Test\u201d that \u201cinvolves a\nmore careful examination of a creature\u2019s language, [and] also\ntests the creature\u2019s ability to solve problems in a wide variety\nof everyday circumstances\u201d (37). In our view, at least when The\nTuring Test is properly understood, it is clear that anything that\npasses The Turing Test must have the ability to solve problems in a\nwide variety of everyday circumstances (because the interrogators will\nuse their questions to probe these\u2014and other\u2014kinds of\nabilities in those who play the Imitation Game).\n5.3 The Turing Test is Too Easy\n\nThere are authors who have suggested that The Turing Test should be\nreplaced with a more demanding test of one kind or another. It is not\nat all clear that any of these tests actually proposes a better goal\nfor research in AI than is set by The Turing Test. However, in this\nsection, we shall not attempt to defend that claim; rather, we shall\nsimply describe some of the further tests that have been proposed, and\nmake occasional comments upon them. (One preliminary point upon which\nwe wish to insist is that Turing\u2019s Imitation Game was devised\nagainst the background of the limitations imposed by then current\ntechnology. It is, of course, not essential to the game that tele-text\ndevices be used to prevent direct access to information about the sex\nor genus of participants in the game. We shall not advert to these\nrelatively mundane kinds of considerations in what follows.)\n5.3.1 The Total Turing Test\n\nHarnad (1989, 1991) claims that a better test than The Turing Test\nwill be one that requires responses to all of our inputs, and not\nmerely to text-formatted linguistic inputs. That is, according to\nHarnad, the appropriate goal for research in AI has to be to construct\na robot with something like human sensorimotor capabilities. Harnad\nalso considers the suggestion that it might be an appropriate goal for\nAI to aim for \u201cneuromolecular indistinguishability,\u201d but\nrejects this suggestion on the grounds that once we know how to make a\nrobot that can pass his Total Turing Test, there will be no problems\nabout mind-modeling that remain unsolved. It is an interesting\nquestion whether the test that Harnad proposes sets a more appropriate\ngoal for AI research. In particular, it seems worth noting that it is\nnot clear that there could be a system that was able to pass The\nTuring Test and yet that was not able to pass The Total Turing Test.\nSince Harnad himself seems to think that it is quite likely that\n\u201cfull robotic capacities [are] \u2026 necessary to generate\n\u2026 successful linguistic performance,\u201d it is unclear why\nthere is reason to replace The Turing Test with his extended test.\n(This point against Harnad can be found in Hauser (1993:227), and\nelsewhere.)\n5.3.2 The Lovelace Test\n\nBringsjord et al. (2001) propose that a more satisfactory aim for AI\nis provided by a certain kind of meta-test that they call the Lovelace\nTest. They say that an artificial agent A, designed by human\nH, passes the Lovelace Test just in case three conditions are jointly\nsatisfied: (1) the artificial agent A produces output\nO; (2) A\u2019s outputting O is not the\nresult of a fluke hardware error, but rather the result of processes\nthat A can repeat; and (3) H\u2014or someone who\nknows what H knows and who has H\u2019s\nresources\u2014cannot explain how A produced O by\nappeal to A\u2019s architecture, knowledge-base and core\nfunctions. Against this proposal, it seems worth noting that there are\nquestions to be raised about the interpretation of the third\ncondition. If a computer program is long and complex, then no human\nagent can explain in complete detail how the output was\nproduced. (Why did the computer output 3.16 rather than 3.17?) But if\nwe are allowed to give a highly schematic explanation\u2014the\ncomputer took the input, did some internal processing and then\nproduced an answer\u2014then it seems that it will turn out to be\nvery hard to support the claim that human agents ever do anything\ngenuinely creative. (After all, we too take external input, perform\ninternal processing, and produce outputs.) What is missing from the\naccount that we are considering is any suggestion about the\nappropriate level of explanation that is to be provided. It\nis quite unclear why we should suppose that there is a relevant\ndifference between people and machines at any level of explanation;\nbut, if that\u2019s right, then the test in question is trivial. (One\nmight also worry that the proposed test rules out by fiat the\npossibility that creativity can be best achieved by using genuine\nrandomising devices.)\n5.3.3 The Truly Total Turing Test\n\nSchweizer (1998) claims that a better test than The Turing Test will\nadvert to the evolutionary history of the subjects of the test. When\nwe attribute intelligence to human beings, we rely on an extensive\nhistorical record of the intellectual achievements of human beings. On\nthe basis of this historical record, we are able to claim that human\nbeings are intelligent; and we can rely upon this claim when we\nattribute intelligence to individual human beings on the basis of\ntheir behavior. According to Schweizer, if we are to attribute\nintelligence to machines, we need to be able to advert to a comparable\nhistorical record of cognitive achievements. So, it will only be when\nmachines have developed languages, written scientific treatises,\ncomposed symphonies, invented games, and the like, that we shall be in\na position to attribute intelligence to individual machines on the\nbasis of their behavior. Of course, we can still use The Turing Test\nto determine whether an individual machine is intelligent: but our\nanswer to the question won\u2019t depend merely upon whether or not\nthe machine is successful in The Turing Test; there is the further\n\u201cevolutionary\u201d condition that also must be satisfied.\nAgainst Schweizer, it seems worth noting that it is not at all clear\nthat our reason for granting intelligence to other humans on the basis\nof their behavior is that we have prior knowledge of the collective\ncognitive achievements of human beings.\n\n5.3.4 Further Proposals\n\nDamassino (2020) suggests that it would be better to require test\nsubjects to produce an enquiry in which performance is assessed along\nthree dimensions: (a) comparison with human performance; (b) success\nin completing the enquiry; and (c) efficiency in completing the\nenquiry (minimisation of the number of questions asked in completing\nthe enquiry). The motivation given for this proposal is that, because\nThe Turing Test attracts projects whose primary ambition is to fool\njudges, it is concerned with whether or how well test subjects perform\non their allocated tasks. It seems to us that there is nothing here\nthat impugns The Turing Test. It does not count against The\nTuring Test that public competitions based on it with prizes\nattached lead to gaming, given that everyone knows that those prizes\nare being awarded to entries that clearly do not pass The Turing Test.\nIf anything is impugned here, it is the public competitions, rather\nthan The Turing Test.\n\nKulikov (2020) suggests that there is value in considering\nPreferential Engagement Tests or Meaningful Engagement Tests. Even\nthough computers can now beat the best humans at chess, many people\nprefer to play chess with humans rather than with expert chess-playing\ncomputers. Perhaps, even if computers could pass The Turing Test,\npeople would prefer to carry on conversations with humans rather than\nwith expert conversational computers. We think that this kind of\nspeculation relies upon assumptions about what could make for expert\nconversational partners. If our conversational partners need to be\nable to update information about their surroundings in real\ntime\u2014for example, while watching a game of football\u2014then\nwe will not think that there is a direct path from GPT-3 to expert\nconversational partners. If only androids can be expert conversational\npartners, then it is less clear that Preferential Engagement Tests or\nMeaningful Engagement Tests will track anything other than\nanthropocentric bias.\n5.4 Should the Turing Test be Considered Harmful?\n\nPerhaps the best known attack on the suggestion that The Turing Test\nprovides an appropriate research goal for AI is due to Hayes and Ford\n(1995). Among the controversial claims that Hayes and Ford make, there\nare at least the following:\n\nTuring suggested the imitation game as a definite goal for program\nof research.\nTuring intended The Turing Test to be a gender test rather than a\nspecies test.\nThe task of trying to make a machine that is successful in The\nTuring Test is so extremely difficult that no one could seriously\nadopt the creation of such a machine as a research goal.\nThe Turing Test suffers from the basic design flaw that it sets\nout to confirm a \u201cnull hypothesis\u201d, viz. that there is no\ndifference in behavior between certain machines and humans.\nNo null effect experiment can provide an adequate criterion for\nintelligence, since the question can always arise that the judges did\nnot look hard enough (and did not raise the right kinds of questions).\nBut, if this question is left open, then there is no stable endpoint\nof enquiry.\nNull effect experiments cannot measure anything: The Turing Test\ncan only test for complete success. (\u201cA man who failed to seem\nfeminine in 10% of what he said would almost always fail the Imitation\ngame.\u201d)\nThe Turing Test is really a test of the ability of the human\nspecies to discriminate its members from human imposters. (\u201cThe\ngender test \u2026 is a test of making a mechanical\ntransvestite.\u201d)\nThe Turing Test is circular: what it fails to detect cannot be\n\u201cintelligence\u201d or\u201chumanity\u201d, since many humans\nwould fail The Turing Test. Indeed, \u201csince one of the players\nmust be judged to be a machine, half the human population would fail\nthe species test\u201d.\nThe perspective of The Turing Test is arrogant and parochial: it\nmistakenly assumes that we can understand human cognition without\nfirst obtaining a firm grasp of the basic principles of\ncognition.\nThe Turing Test does not admit of weaker, different, or even\nstronger forms of intelligence than those deemed human.\n\n\nSome of these claims seem straightforwardly incorrect. Consider (h),\nfor example. In what sense can it be claimed that 50% of the human\npopulation would fail \u201cthe species test\u201d? If \u201cthe\nspecies test\u201d requires the interrogator to decide which of two\npeople is a machine, why should it be thought that the verdict of the\ninterrogator has any consequences for the assessment of the\nintelligence of the person who is judged to be a machine? (Remember,\ntoo, that one of the conditions for \u201cthe species\ntest\u201d\u2014as it is originally described by Hayes and\nFord\u2014is that one of the contestants is a machine. While\nthe machine can \u201cdemonstrate\u201d its intelligence by winning\nthe imitation game, a person cannot \u201cdemonstrate\u201d their\nlack of intelligence by failing to win.)\n\nIt seems wrong to say that The Turing Test is defective because it is\na \u201cnull effect experiment\u201d. True enough, there is a sense\nin which The Turing Test does look for a \u201cnull result\u201d: if\nordinary judges in the specified circumstances fail to identify the\nmachine (at a given level of success), then there is a given\nlikelihood that the machine is intelligent. But the point of insisting\non \u201cordinary judges\u201d in the specified circumstances is\nprecisely to rule out irrelevant ways of identifying the machine (i.e.\nways of identifying the machine that are not relevant to the question\nwhether it is intelligent). There might be all kinds of irrelevant\ndifferences between a given kind of machine and a human\nbeing\u2014not all of them rendered undetectable by the experimental\nset-up that Turing describes\u2014but The Turing Test will remain a\ngood test provided that it is able to ignore these irrelevant\ndifferences.\n\nIt also seems doubtful that it is a serious failing of The Turing Test\nthat it can only test for \u201ccomplete success\u201d. On the one\nhand, if a man has a one in ten chance of producing a claim that is\nplainly not feminine, then we can compute the chance that he will be\ndiscovered in a game in which he answers N\nquestions\u2014and, if N is sufficiently small, then it\nwon\u2019t turn out that \u201che would almost always fail to\nwin\u201d. On the other hand, as we noted at the end of Section 4.4\nabove, if one were worried about the \u201cYES/NO\u201d nature of\n\u201cThe Turing Test\u201d, then one could always get the judges to\nproduce probabilistic verdicts instead. This change preserves the\ncharacter of The Turing Test, but gives it scope for greater\nstatistical sophistication.\n\nWhile there are (many) other criticisms that can be made of the claims\ndefended by Hayes and Ford (1995), it should be acknowledged that they\nare right to worry about the suggestion that The Turing Test provides\nthe defining goal for research in AI. There are various reasons why\none should be loathe to accept the proposition that the one central\nambition of AI research is to produce artificial people. However it is\nworth pointing out that there is no reason to think that Turing\nsupposed that The Turing Test defined the field of AI research (and\nthere is not much evidence that any other serious thinkers have\nthought so either). Turing himself was well aware that there might be\nnon-human forms of intelligence\u2014cf. (j) above. However, all of\nthis remains consistent with the suggestion that it is quite\nappropriate to suppose that The Turing Test sets one long\nterm goal for AI research: one thing that we might well aim to do\neventually is to produce artificial people. If\u2014as Hayes\nand Ford claim\u2014that task is almost impossibly difficult, then\nthere is no harm in supposing that the goal is merely an\nambit goal to which few resources should be committed; but we\nmight still have good reason to allow that it is a goal.\n\nOthers who have argued that we need to \u201cmove beyond\u201d The\nTuring Test include Hern\u00e1ndez-Orallo (2000) (2020) and\nMarcus (2020).\n6. The Chinese Room\n\nThere are many different objections to The Turing Test which have\nsurfaced in the literature during the past fifty years, but which we\nhave not yet discussed. We cannot hope to canvass all of these\nobjections here. However, there is one argument\u2014Searle\u2019s\n\u201cChinese Room\u201d argument\u2014that is mentioned so often\nin connection with the Turing Test that we feel obliged to end with\nsome discussion of it.\n\nIn Minds, Brains and Programs and elsewhere, John Searle\nargues against the claim that \u201cappropriately programmed\ncomputers literally have cognitive states\u201d (64). Clearly enough,\nSearle is here disagreeing with Turing\u2019s claim that an\nappropriately programmed computer could think. There is much that is\ncontroversial about Searle\u2019s argument; we shall just consider\none way of understanding what it is that he is arguing\nfor.\n\nThe basic structure of Searle\u2019s argument is very well known. We\ncan imagine a \u201chand simulation\u201d of an intelligent\nagent\u2014in the case described, a speaker of a Chinese\nlanguage\u2014in circumstances in which we might well be very\nreluctant to allow that there is any appropriate intelligence lying\nbehind the simulated behavior. (Thus, what we are invited to suppose\nis a logical possibility is not so very different from what Block\ninvites us to suppose is a logical possibility. However, the argument\nthat Searle goes on to develop is rather different from the argument\nthat Block defends.) Moreover\u2014and this is really the\nkey point for Searle\u2019s argument\u2014the \u201chand\nsimulation\u201d in question is, in all relevant respects, simply a\nspecial kind of digital computation. So, there is a possible\nworld\u2014doubtless one quite remote from the actual world\u2014in\nwhich a digital computer simulates intelligence but in which the\ndigital computer does not itself possess intelligence. But, if we\nconsider any digital computer in the actual world, it will not differ\nfrom the computer in that remote possible world in any way which could\nmake it the case that the computer in the actual world is more\nintelligent than the computer in that remote possible world. Given\nthat we agree that the \u201chand simulating\u201d computer in the\nChinese Room is not intelligent, we have no option but to conclude\nthat digital computers are simply not the kinds of things that\ncan be intelligent.\n\nSo far, the argument that we have described arrives at the conclusion\nthat no appropriately programmed computer can think. While this\nconclusion is not one that Turing accepted, it is important to note\nthat it is compatible with the claim that The Turing Test is a good\ntest for intelligence. This is because, for all that has been argued,\nit may be that it is not nomically possible to provide any\n\u201chand simulation\u201d of intelligence (and, in particular,\nthat it is not possible to simulate intelligence using any kind of\ncomputer). In order to turn Searle\u2019s argument\u2014at least in\nthe way in which we have developed it\u2014into an objection to The\nTuring Test, we need to have some reason for thinking that it is at\nleast nomically possible to simulate intelligence using\ncomputers. (If it is nomically impossible to simulate intelligence\nusing computers, then the alleged fact that digital computers cannot\ngenuinely possess intelligence casts no doubt at all on the usefulness\nof the Turing Test, since digital computers are nomically disqualified\nfrom the range of cases in which there is mere simulation of\nintelligence.) In the absence of reason to believe this, the most that\nSearle\u2019s argument yields is an objection to Turing\u2019s\nconfidently held belief that digital computing machines will one day\npass The Turing Test. (Here, as elsewhere, we are supposing that, for\nany kind of creature C, there is a version of The Turing Test in which\nC takes the role of the machine in the specific test that Turing\ndescribes. This general format for testing for the presence of\nintelligence would not necessarily be undermined by the success of\nSearle\u2019s Chinese Room argument.)\n\nThere are various responses that might be made to the argument that we\nhave attributed to Searle. One kind of response is to dispute the\nclaim that there is no intelligence present in the case of the Chinese\nRoom. (Suppose that the \u201chand simulation\u201d is embedded in a\nrobot that is equipped with appropriate sensors, etc. Suppose,\nfurther, that the \u201chand simulation\u201d involves updating the\nprocess of \u201chand simulation,\u201d etc. If enough details of\nthis kind are added, then it becomes quite unclear whether we do want\nto say that we still haven\u2019t described an intelligent system.)\nAnother kind of response is to dispute the claim that digital\ncomputers in the actual world could not be relevantly different from\nthe system that operates in the Chinese Room in that remote possible\nworld. (If we suppose that the core of the Chinese Room is a kind of\ngiant look-up table, then it may well be important to note that\ndigital computers in the actual world do not work with look-up tables\nin that kind of way.) Doubtless there are other possible lines of\nresponse as well. However, it would take us out of our way to try to\ntake this discussion further. (One good place to look for further\ndiscussion of these matters is Braddon-Mitchell and Jackson\n(1996).)\n7. Brief Notes on Intelligence\n\nThere are radically different views about the measurement of\nintelligence that have not been canvassed in this article. Our concern\nhas been to discuss Turing (1950) and its legacy. But, of course, a\nmore wide-ranging discussion would also consider, for example,\nresearch on the measurement of intelligence using the mathematical and\ncomputational resources of Algorithmic Information Theory, Kolmogorov\nComplexity Theory, Minimum Message Length (MML) Theory, and so forth.\n(For an introduction to this literature, see Hernandez-Orallo and Dowe\n(2010), and the list of references contained therein. For a more\ngeneral introduction to research into AI, see Marquis et al.\n(2020).)\n\nMore broadly, there are radically different views about our\nconcept--or concepts--of intelligence that have not been canvassed in\nthis article. There is a dispute, for example, about whether Turing is\nbest interpreted as working with a response-dependent concept of\nintelligence. (Pro: Proudfoot (2013) (2020); contra: Wheeler (2020).)\nRelatedly, there is a dispute about whether intelligence bears some\nkind of necessary relationship to symmetrical relations of recognition\nbetween agents, as suggested in Mallory (2020) There is also a\nbroader dispute about whether we should think that useful notions of\nintelligence are always domain specific, or whether we should rather\nsuppose that there is something important in the idea of general,\ndomain independent intelligence.\n\nAnd there are radically different views about the most likely paths to\nbuilding general intelligence (assuming that there is such a thing as\ngeneral intelligence). For example, Crosby (2020) suggests that the\nbest way forwards may be to try to make machines that can pass animal\ncognition tests, i.e. that can create predictive models of their\nenvironment from sensory input. (There are clear precusors to this\nline of thought in, for example, Brooks (1990).)\n",
    "bibliography": {
        "categories": [],
        "cat_ref_text": {
            "ref_list": [
                "Abramson, D., 2008, \u201cTuring\u2019s Responses to Two\nObjections,\u201d <em>Minds and Machines</em>, 18: 147\u201367.",
                "\u2013\u2013\u2013, 2011a, \u201cDescartes\u2019 Influence on\nTuring,\u201d <em>Studies in History and Philosophy of Science</em>,\n42: 544\u2013551.",
                "\u2013\u2013\u2013, 2011b, \u201cPhilosophy of Mind is (in\nPart) Philosophy of Computer Science,\u201d <em>Minds and\nMachines</em>, 21: 203\u2013219.",
                "Arnold, T. and Scheutz, M., 2016, \u201cAgainst the Moral Turing\nTest: Accountable Design and the Moral Reasoning of Autonomous\nSystems,\u201d <em>Ethics and Information Technology</em>, 18:\n103\u201315.",
                "Barone, P., et al., 2020 \u201cA Minimal Turing Test: Reciprocal\nSensorimotor Contingencies for Interaction Detection,\u201d\n<em>Frontiers in Human Neuroscience</em> 14.",
                "Block, N., 1981, \u201cPsychologism and Behaviorism,\u201d\n<em>Philosophical Review</em>, 90: 5\u201343.",
                "Boolos, G. and Jeffrey, R., 1980, <em>Computability and\nLogic</em>, Second Edition, Cambridge: Cambridge University\nPress.",
                "Bowie, L., 1982, \u201cLucas\u2019s Number is Finally Up,\u201d\n<em>Journal of Philosophical Logic</em>, 11: 279\u201385.",
                "Braddon-Mitchell, D. and Jackson, F., 1996, <em>The Philosophy of\nMind and Cognition</em>, Oxford: Blackwell.",
                "Bringsjord, S., Bello, P. and Ferrucci, D., 2001,\n\u201cCreativity, the Turing Test, and the (Better) Lovelace\nTest,\u201d <em>Minds and Machines</em>, 11: 3\u201327.",
                "Brooks, R., 1990, \u201cElephants Don\u2019t Play\nChess,\u201d <em>Robotics and Autonomous Signals</em>, 6:\n3\u201315.",
                "Chalmers, D., 1995, \u201cOn Implementing a Computation,\u201d\n<em>Minds and Machines</em>, 4: 391\u2013402.",
                "Churchland, P. M. and Churchland, P. S., 1990, \u201cCould a\nMachine Think?\u201d <em>Scientific American</em>, 262 (1):\n32\u201337.",
                "Clark, A., 1997, <em>Being There: Putting Brain, Body and World\nTogether Again</em>, Cambridge: MIT Press.",
                "Cooper, S. and van Leeuwen, J. (eds.) 2013, <em>Alan Turing: His\nWork and Impact</em>, London: Elsevier.",
                "Copeland, J. (ed.), 1999, \u201cA Lecture and Two Radio\nBroadcasts on Machine Intelligence by Alan Turing,\u201d in K.\nFurukawa, D. Michie, and S. Muggleton (eds.), <em>Machine\nIntelligence</em> 15, Oxford: Oxford University Press.",
                "\u2013\u2013\u2013, 2000, \u201cThe Turing Test,\u201d\n<em>Minds and Machines</em>, 10: 519\u201339.",
                "Copeland, J. and Sylvan, R., 1999, \u201cBeyond the Universal\nTuring Machine,\u201d <em>Australasian Journal of Philosophy</em>,\n77: (1): 46\u201366.",
                "Copeland, J., et al. (eds.), 2017, <em>The Turing Guide</em>,\nOxford: Oxford University Press.",
                "Crooke, A., 2002, <em>Confabulating Consciousness</em>, Ph.D.\nDissertation, Philosophy Department, Monash University.",
                "Crosby, M., 2020, \u201cBuilding Thinking Machines by Solving\nAnimal Cognition Tasks,\u201d <em>Minds and Machines</em>, 30:\n589\u2013615.",
                "Cullen, J., 2009, \u201cImitation Versus Communication: Testing\nfor Human-Like Intelligence,\u201d <em>Minds and Machines</em>, 19:\n237\u201354.",
                "Damassino, N., 2020, \u201cThe Questioning Turing\nTest,\u201d <em>Minds and Machines</em>, 30: 563\u201387.",
                "\u2013\u2013\u2013, and Novelli, N., 2020, \u201cRethinking,\nReworking and Revolutionising the Turing Test,\u201d <em>Minds and\nMachines</em>, 30: 463\u20138.",
                "Davidson, D., 1990, \u201cTuring\u2019s Test,\u201d in K. Said,\n(ed.), <em>Modelling the Mind</em>, Oxford: Oxford University Press,\n1\u201311.",
                "Dennett, D., 1985, \u201cCan Machines Think?\u201d in M. Shafto\n(ed.), <em>How We Know</em>, Cambridge, MA: Harper and Row.",
                "Dietrich, E. (ed.), 1994, <em>Thinking Computers and Virtual\nPersons: Essays on the Intentionality of Machines</em>, San Diego:\nAcademic Press.",
                "Dreyfus, H &amp; Dreyfus, S., 1986, <em>Mind Over Machine</em>,\nNew York: Free Press.",
                "Epstein, R. et al. 2009, <em>Parsing the Turing Test</em>\nDordrecht: Springer.",
                "Erion, G., 2001, \u201cThe Cartesian Test for Automatism,\u201d\n<em>Minds and Machines</em>, 11: 29\u201339.",
                "Feferman, S., 1996, \u201cPenrose\u2019s G\u00f6delian\nArgument,\u201d, <em>Psyche</em>, 2: 21\u201332.",
                "Floridi, L., Taddeo, M., and Turilli, M., 2008,\n\u201cTuring\u2019s Imitation Game: Still an Impossible Challenge\nfor all Machines and some Judges,\u201d, <em>Minds and Machines</em>,\n19: 145\u201350.",
                "Floridi, L. and Chiriatti, N. (2020) \u201cGPT-3: It\u2019s Nature,\nScope, Limits and Consequences,\u201d <em>Minds and\nMachines</em>, 30: 681\u201394.",
                "French, R., 1990, \u201cSubcognition and the Limits of the Turing\nTest,\u201d <em>Mind</em>, 99: 53\u201365.",
                "French, R., 2000, \u201cThe Turing Test: The First Fifty Years\n<em>Trends in Cognitive Sciences</em>,\u201d 4: 115\u201321.",
                "Genova, J., 1994, \u201cTuring\u2019s Sexual Guessing\nGame,\u201d <em>Social Epistemology</em>, 8: 313\u201326.",
                "Gerdes, A. and \u00d8hstr\u00f8m, P., 2015, \u201cIssues in\nRobot Ethics Seen Through the Lens of a Moral Turing\nTest,\u201d <em>Journal of Information, Communication and Ethics\nin Society</em>, 13: 98\u2013109.",
                "Gunderson, K., 1964, \u201cDescartes, La Mettrie, Language and\nMachines,\u201d <em>Philosophy</em>, 39: 193\u2013222.",
                "\u2013\u2013\u2013, 1985, <em>Mentality and Machines</em>, 2nd\nedition, Minneapolis: University of Minnesota Press.",
                "Harnad, S., 1989, \u201cMinds, Machines and Searle,\u201d\n<em>Journal of Theoretical and Experimental Artificial\nIntelligence</em>, 1: 5\u201325.",
                "\u2013\u2013\u2013, 1991, \u201cOther Bodies, Other Minds: A\nMachine Incarnation of an Old Philosophical Problem,\u201d <em>Minds\nand Machines</em>, 1: 43\u201354.",
                "Harnad, S. and  Dror, I., 2006, \u201cDistributed\nCognition, Cognising, Autonomy and the Turing Test,\u201d\n<em>Pragmatics and Cognition</em>, 14: 209\u201313.",
                "Haugeland, J., 1981, \u201cSemantic Engines: An Introduction to\nMind Design,\u201d in J. Haugeland (ed.), <em>Mind Design:\nPhilosophy, Psychology, Artificial Intelligence</em>, Cambridge: MIT\nPress, 1\u201334.",
                "Hauser, L., 1993, \u201cReaping the Whirlwind: Reply to\nHarnad\u2019s Other Bodies, Other Minds,\u201d <em>Minds and\nMachines</em>, 3: 219\u201338.",
                "Hauser, L., 2001, \u201cLook Who\u2019s Moving the Goalposts\nNow,\u201d <em>Minds and Machines</em>, 11: 41\u201351.",
                "Hayes, P., and Ford, K., 1995, \u201cTuring Test Considered\nHarmful,\u201d <em>Proceedings of the Fourteenth International Joint\nConference on Artificial Intelligence</em>, Montreal: Morgan Kaufmann,\n972\u2013977.",
                "Hern\u00e1ndez-Orallo, J., 2000, \u201cBeyond the Turing\nTest,\u201d <em>Journal of Logic, Language and Information</em>, 9:\n447\u201366.",
                "\u2013\u2013\u2013, 2020, \u201cTwenty Years Beyond the Turing\nTest: Moving Beyond the Human Judges,\u201d <em>Minds and\nMachines</em>, 30: 533\u201362.",
                "Hern\u00e1ndez-Orallo, J. and Dowe, D. L., 2010,\n\u201cMeasuring Universal Intelligence: Towards an Anytime\nIntelligence Test,\u201d <em>Artificial Intelligence</em>, 174:\n1508\u201339.",
                "Hodges, A., 1983, <em>Alan Turing: The Enigma</em>, London:\nBurnett with Hutchinson.",
                "Hofstadter, D., 1982, \u201cThe Turing Test: A Coffee-House\nConversation,\u201d in D. Hofstadter and D. Dennett (eds.), <em>The\nMind\u2019s I: Fantasies and Reflections on Self and Soul</em>,\nLondon: Penguin, 69\u201395.",
                "Kobosko, S., et al., 2013 \u201cPassing an Enhanced Turing Test:\nInteracting with Lifelike Computer Representations of Specific\nIndividuals,\u201d <em>Journal of Intelligent Systems</em>, 22:\n365\u2013415.",
                "Korukonda, A., 2003, \u201cTaking Stock of the Turing Test: A\nReview, Analysis and Appraisal of Issues Surrounding Thinking\nMachines,\u201d <em>International Journal of Human-Computer\nStudies</em> 58: 240\u201357.",
                "Kulikov, V., 2020, \u201cPreferential Engagement: What can we\nLearn from Online Chess?,\u201d <em>Minds and Machines</em>, 30:\n617\u201336.",
                "Leavitt, D., 2007, <em>The Man Who Knew Too Much: Alan Turing and\nthe Invention of the Computer</em> London: Phoenix.",
                "Levesque, H., 2017, <em>Commonsense, the Turing Test, and the\nQuest for Real AI</em>, Cambridge, MA: MIT Press.",
                "Lewis, D., 1969, \u201cLucas against mechanism,\u201d\n<em>Philosophy</em>, 44: 231\u2013233.",
                "\u2013\u2013\u2013, 1979, \u201cLucas against mechanism\nII,\u201d <em>Canadian Journal of Philosophy</em>, 9:\n373\u2013376.",
                "Lucas, J., 1961, \u201cMinds, Machines and G\u00f6del,\u201d\n<em>Philosophy</em>, 36: 120\u20134.",
                "Lupowski, P., 2011, \u201cA Formal Approach to Exploring the\nInterrogator\u2019s Perspective in the Turing Test,\u201d <em>Logical and\nLogical Philosophy</em> 20: 139\u201358.",
                "Lupowski, P. and Jurowska, P., 2019, \u201cMinimum Intelligent\nSignal Test as an Alternative to the Turing Test,\u201d\n<em>Diametros</em>, 59: 35\u201347.",
                "Lyre, H. 2020, \u201cThe State Space of Artificial\nIntelligence\u201d, <em>Minds and Machines</em>, 30: 325\u201347.",
                "Mallory, F. 2020, \u201cIn Defence of a Reciprocal Turing\nTest,\u201d <em>Minds and Machines</em>, 30: 659\u201380.",
                "Marcus, G., 2020 \u201cThe Next Decade in AI: Four Steps Towards\nRobust Artificial Intelligence,\u201d arXiv:2002.06177.",
                "Marquis, P., et al. (eds.), 2020, <em>A Guided Tour of Artificial\nIntelligence Research</em>, Cham: Springer.",
                "Masum, H., Christensen, S., and Oppacher, F., 2003, \u201cThe\nTuring Ratio: A Framework for Open-Ended Task Metrics,\u201d\n<em>Journal of Evolution and Technology</em>, 13(2),\n <a href=\"https://www.jetpress.org/volume13/TuringRatio.pdf\" target=\"other\">available online</a>.",
                "McDermott, D., 2014, \u201cOn the Claim that a Look-Up Table\nProgram could Pass the Turing Test,\u201d <em>Minds and\nMachines</em>, 24: 143\u201388.",
                "Millican, P. and Clark, A., (eds.), 1999, <em>Machines and\nThought: The Legacy of Alan Turing</em>, two volumes, Oxford:\nClarendon.",
                "Moor, J., 1976, \u201cAn Analysis of Turing\u2019s Test,\u201d\n<em>Philosophical Studies</em>, 30: 249\u201357.",
                "\u2013\u2013\u2013, 2001, \u201cThe Status and Future of the\nTuring Test,\u201d <em>Minds and Machines</em>, 11: 77\u201393.",
                "___, ed., 2003 <em>The Turing Test: The Elusive Standard of\nArtificial Intelligence</em> Dordrecht: Springer.",
                "Neufeld, E. and Finnestad, S., 2020a, \u201cIn Defense of the\nTuring Test,\u201d <em>AI and Society</em> 35: 819\u201327.",
                "Neufeld, E. and Finnestad, S., 2020b, \u201cImitation Game:\nThreshold or Watershed?,\u201d <em>Minds and Machines</em>, 30:\n637\u201357.",
                "Pautz, A. and Stoljar, D. (eds.), 2019, <em>Blockheads! Essays on\nNed Block\u2019s Philosophy of Mind and Consciousness</em>,\nCambridge, MA: MIT Press.",
                "Penrose, R., 1989, <em>The Emperor\u2019s New Mind</em>, Oxford:\nOxford University Press.",
                "Piccinini, G., 2000, \u201cTuring\u2019s Rules for the Imitation\nGame,\u201d <em>Minds and Machines</em>, 10: 573\u201385.",
                "Proudfoot, D., 2013, \u201cRethinking Turing\u2019s Test,\u201d\n<em>Journal of Philosophy</em>, 110: 391\u2013411.",
                "\u2013\u2013\u2013, 2020, \u201cRethinking Turing\u2019s Test and\nthe Philosophical Implications,\u201d <em>Minds and Machines</em>,\n30: 487\u2013512.",
                "Proudfoot, D. and Copeland, J. 2008 \u201cTuring\u2019s Test: A\nPhilosophical and Historical Guide,\u201d in R. Epstein et al.,\n(eds.), <em>Parsing the Turing Test: Philosophical and Methodological\nIssues</em>, Dordrecht: Springer, 119\u201338.",
                "Rapaport, W., 2000, \u201cHow to Pass a Turing Test: Syntactic\nSemantics, Natural-Language Understanding, and First-Person\nCognition,\u201d <em>Journal of Logic, Language and Information</em>,\n9: 467\u201390.",
                "Saygin, A., Cicekli, I., and Akman, V., 2000, \u201cTuring Test:\n50 Years Later,\u201d <em>Minds and Machines</em>, 10:\n463\u2013518.",
                "Schweizer, P., 1998, \u201cThe Truly Total Turing Test,\u201d\n<em>Minds and Machines</em>, 8: 263\u201372.",
                "\u2013\u2013\u2013, 2012, \u201cThe Externalist Foundation of\na Truly Total Turing Test,\u201d <em>Minds and Machines</em>, 22:\n191\u2013212.",
                "Searle, J., 1981, \u201cMinds, Brains, and Programs,\u201d\n<em>Behavioral and Brain Sciences</em>, 3: 417\u201357.",
                "Shah, H. and Warwick, K., 2010, \u201cHidden Interlocutor\nMisidentification in Practical Turing Tests,\u201d <em>Minds and\nMachines</em>, 203: 441\u201354.",
                "Shieber, S., 1994, \u201cLessons from a restricted Turing\nTest,\u201d <em>Communications of the Association for Computing\nMachinery</em>, 37: 70\u20138.\n [<a href=\"http://www.eecs.harvard.edu/~shieber/Biblio/Papers/loebner-rev-html/loebner-rev-html.html\" target=\"other\">Preprint available online</a>].",
                "\u2013\u2013\u2013, (ed.), 2004, <em>The Turing Test: Verbal\nBehaviour as the Mark of Intelligence</em>, Cambridge: MIT Press.",
                "\u2013\u2013\u2013, 2007, \u201cThe Turing Test as Interactive\nProof,\u201d <em>No\u00fbs</em>, 41: 686\u2013713.",
                "\u2013\u2013\u2013, 2014, \u201cThere can be no\nTuring-Test-Passing Memory Machines,\u201d <em>Philosophers\u2019\nImprint</em>, 14: 1\u201313.",
                "Sparrow, R., 2004, \u201cThe Turing Triage Test,\u201d\n<em>Ethics and Information Technology</em>, 6: 203\u201313.",
                "Srinivasan, B. and Shah, K., 2019, \u201cTowards a Unified\nFramework for Developing Ethical and Practical Turing\nTests,\u201d <em>AI and Society</em>, 34: 145\u201352.",
                "Sterrett, S., 2000, \u201cTuring\u2019s Two Tests for\nIntelligence,\u201d <em>Minds and Machines</em>, 10:\n541\u201359.",
                "Sterrett, S., 2020, \u201cThe Genius of the \u2018Original\nImitation Game\u2019 Test,\u201d <em>Minds and Machines</em>, 30:\n469\u201386.",
                "Traiger, S., 2000, \u201cMaking the Right Identification in the\nTuring Test,\u201d <em>Minds and Machines</em>, 10:\n561\u2013572.",
                "Turing, A., 1950, \u201cComputing Machinery and\nIntelligence,\u201d <em>Mind</em>, 59 (236): 433\u201360.",
                "Turing, A. 1992, <em>The Collected Works of A. M. Turing</em>,\nedited by P. Furbank, London: North-Holland.",
                "Warwick, K. et al., 2013, \u201cSome Implications of a Sample of\nPractical Turing Tests,\u201d <em>Minds and Machines</em>, 23:\n163\u201377.",
                "Weizenbaum, J., 1966, \u201cELIZA-A Computer Program for the\nStudy of Natural Language Communication Between Men and\nMachines,\u201d <em>Communications of the ACM</em>, 9:\n36\u201345.",
                "Wheeler, M., 2020 \u201cDeceptive Appearances: The Turing Test,\nResponse Dependence, and Intelligence as an Emotional Concept,\u201d\n<em>Minds and Machines</em> 30: 513\u201332.",
                "Whitby, B., 1996, \u201cThe Turing Test: AI\u2019s Biggest Blind\nAlley?\u201d in P. Millican and A. Clark (eds.), <em>Machines and\nThought: The Legacy of Alan Turing</em>, Volume 1, Oxford:\nClarendon.",
                "Zdenek, S., 2001, \u201cPassing Loebner\u2019s Turing Test: A\nCase of Conflicting Discourse Functions,\u201d <em>Minds and\nMachines</em>, 11: 53\u201376."
            ]
        },
        "raw_text": "<div id=\"bibliography\">\n<h2><a name=\"Bib\">Bibliography</a></h2>\n<ul class=\"hanging\">\n<li>Abramson, D., 2008, \u201cTuring\u2019s Responses to Two\nObjections,\u201d <em>Minds and Machines</em>, 18: 147\u201367.</li>\n<li>\u2013\u2013\u2013, 2011a, \u201cDescartes\u2019 Influence on\nTuring,\u201d <em>Studies in History and Philosophy of Science</em>,\n42: 544\u2013551.</li>\n<li>\u2013\u2013\u2013, 2011b, \u201cPhilosophy of Mind is (in\nPart) Philosophy of Computer Science,\u201d <em>Minds and\nMachines</em>, 21: 203\u2013219.</li>\n<li>Arnold, T. and Scheutz, M., 2016, \u201cAgainst the Moral Turing\nTest: Accountable Design and the Moral Reasoning of Autonomous\nSystems,\u201d <em>Ethics and Information Technology</em>, 18:\n103\u201315.</li>\n<li>Barone, P., et al., 2020 \u201cA Minimal Turing Test: Reciprocal\nSensorimotor Contingencies for Interaction Detection,\u201d\n<em>Frontiers in Human Neuroscience</em> 14.</li>\n<li>Block, N., 1981, \u201cPsychologism and Behaviorism,\u201d\n<em>Philosophical Review</em>, 90: 5\u201343.</li>\n<li>Boolos, G. and Jeffrey, R., 1980, <em>Computability and\nLogic</em>, Second Edition, Cambridge: Cambridge University\nPress.</li>\n<li>Bowie, L., 1982, \u201cLucas\u2019s Number is Finally Up,\u201d\n<em>Journal of Philosophical Logic</em>, 11: 279\u201385.</li>\n<li>Braddon-Mitchell, D. and Jackson, F., 1996, <em>The Philosophy of\nMind and Cognition</em>, Oxford: Blackwell.</li>\n<li>Bringsjord, S., Bello, P. and Ferrucci, D., 2001,\n\u201cCreativity, the Turing Test, and the (Better) Lovelace\nTest,\u201d <em>Minds and Machines</em>, 11: 3\u201327.</li>\n<li>Brooks, R., 1990, \u201cElephants Don\u2019t Play\nChess,\u201d <em>Robotics and Autonomous Signals</em>, 6:\n3\u201315.</li>\n<li>Chalmers, D., 1995, \u201cOn Implementing a Computation,\u201d\n<em>Minds and Machines</em>, 4: 391\u2013402.</li>\n<li>Churchland, P. M. and Churchland, P. S., 1990, \u201cCould a\nMachine Think?\u201d <em>Scientific American</em>, 262 (1):\n32\u201337.</li>\n<li>Clark, A., 1997, <em>Being There: Putting Brain, Body and World\nTogether Again</em>, Cambridge: MIT Press.</li>\n<li>Cooper, S. and van Leeuwen, J. (eds.) 2013, <em>Alan Turing: His\nWork and Impact</em>, London: Elsevier.</li>\n<li>Copeland, J. (ed.), 1999, \u201cA Lecture and Two Radio\nBroadcasts on Machine Intelligence by Alan Turing,\u201d in K.\nFurukawa, D. Michie, and S. Muggleton (eds.), <em>Machine\nIntelligence</em> 15, Oxford: Oxford University Press.</li>\n<li>\u2013\u2013\u2013, 2000, \u201cThe Turing Test,\u201d\n<em>Minds and Machines</em>, 10: 519\u201339.</li>\n<li>Copeland, J. and Sylvan, R., 1999, \u201cBeyond the Universal\nTuring Machine,\u201d <em>Australasian Journal of Philosophy</em>,\n77: (1): 46\u201366.</li>\n<li>Copeland, J., et al. (eds.), 2017, <em>The Turing Guide</em>,\nOxford: Oxford University Press.</li>\n<li>Crooke, A., 2002, <em>Confabulating Consciousness</em>, Ph.D.\nDissertation, Philosophy Department, Monash University.</li>\n<li>Crosby, M., 2020, \u201cBuilding Thinking Machines by Solving\nAnimal Cognition Tasks,\u201d <em>Minds and Machines</em>, 30:\n589\u2013615.</li>\n<li>Cullen, J., 2009, \u201cImitation Versus Communication: Testing\nfor Human-Like Intelligence,\u201d <em>Minds and Machines</em>, 19:\n237\u201354.</li>\n<li>Damassino, N., 2020, \u201cThe Questioning Turing\nTest,\u201d <em>Minds and Machines</em>, 30: 563\u201387.</li>\n<li>\u2013\u2013\u2013, and Novelli, N., 2020, \u201cRethinking,\nReworking and Revolutionising the Turing Test,\u201d <em>Minds and\nMachines</em>, 30: 463\u20138.</li>\n<li>Davidson, D., 1990, \u201cTuring\u2019s Test,\u201d in K. Said,\n(ed.), <em>Modelling the Mind</em>, Oxford: Oxford University Press,\n1\u201311.</li>\n<li>Dennett, D., 1985, \u201cCan Machines Think?\u201d in M. Shafto\n(ed.), <em>How We Know</em>, Cambridge, MA: Harper and Row.</li>\n<li>Dietrich, E. (ed.), 1994, <em>Thinking Computers and Virtual\nPersons: Essays on the Intentionality of Machines</em>, San Diego:\nAcademic Press.</li>\n<li>Dreyfus, H &amp; Dreyfus, S., 1986, <em>Mind Over Machine</em>,\nNew York: Free Press.</li>\n<li>Epstein, R. et al. 2009, <em>Parsing the Turing Test</em>\nDordrecht: Springer.</li>\n<li>Erion, G., 2001, \u201cThe Cartesian Test for Automatism,\u201d\n<em>Minds and Machines</em>, 11: 29\u201339.</li>\n<li>Feferman, S., 1996, \u201cPenrose\u2019s G\u00f6delian\nArgument,\u201d, <em>Psyche</em>, 2: 21\u201332.</li>\n<li>Floridi, L., Taddeo, M., and Turilli, M., 2008,\n\u201cTuring\u2019s Imitation Game: Still an Impossible Challenge\nfor all Machines and some Judges,\u201d, <em>Minds and Machines</em>,\n19: 145\u201350.</li>\n<li>Floridi, L. and Chiriatti, N. (2020) \u201cGPT-3: It\u2019s Nature,\nScope, Limits and Consequences,\u201d <em>Minds and\nMachines</em>, 30: 681\u201394.</li>\n<li>French, R., 1990, \u201cSubcognition and the Limits of the Turing\nTest,\u201d <em>Mind</em>, 99: 53\u201365.</li>\n<li>French, R., 2000, \u201cThe Turing Test: The First Fifty Years\n<em>Trends in Cognitive Sciences</em>,\u201d 4: 115\u201321.</li>\n<li>Genova, J., 1994, \u201cTuring\u2019s Sexual Guessing\nGame,\u201d <em>Social Epistemology</em>, 8: 313\u201326.</li>\n<li>Gerdes, A. and \u00d8hstr\u00f8m, P., 2015, \u201cIssues in\nRobot Ethics Seen Through the Lens of a Moral Turing\nTest,\u201d <em>Journal of Information, Communication and Ethics\nin Society</em>, 13: 98\u2013109.</li>\n<li>Gunderson, K., 1964, \u201cDescartes, La Mettrie, Language and\nMachines,\u201d <em>Philosophy</em>, 39: 193\u2013222.</li>\n<li>\u2013\u2013\u2013, 1985, <em>Mentality and Machines</em>, 2nd\nedition, Minneapolis: University of Minnesota Press.</li>\n<li>Harnad, S., 1989, \u201cMinds, Machines and Searle,\u201d\n<em>Journal of Theoretical and Experimental Artificial\nIntelligence</em>, 1: 5\u201325.</li>\n<li>\u2013\u2013\u2013, 1991, \u201cOther Bodies, Other Minds: A\nMachine Incarnation of an Old Philosophical Problem,\u201d <em>Minds\nand Machines</em>, 1: 43\u201354.</li>\n<li>Harnad, S. and  Dror, I., 2006, \u201cDistributed\nCognition, Cognising, Autonomy and the Turing Test,\u201d\n<em>Pragmatics and Cognition</em>, 14: 209\u201313.</li>\n<li>Haugeland, J., 1981, \u201cSemantic Engines: An Introduction to\nMind Design,\u201d in J. Haugeland (ed.), <em>Mind Design:\nPhilosophy, Psychology, Artificial Intelligence</em>, Cambridge: MIT\nPress, 1\u201334.</li>\n<li>Hauser, L., 1993, \u201cReaping the Whirlwind: Reply to\nHarnad\u2019s Other Bodies, Other Minds,\u201d <em>Minds and\nMachines</em>, 3: 219\u201338.</li>\n<li>Hauser, L., 2001, \u201cLook Who\u2019s Moving the Goalposts\nNow,\u201d <em>Minds and Machines</em>, 11: 41\u201351.</li>\n<li>Hayes, P., and Ford, K., 1995, \u201cTuring Test Considered\nHarmful,\u201d <em>Proceedings of the Fourteenth International Joint\nConference on Artificial Intelligence</em>, Montreal: Morgan Kaufmann,\n972\u2013977.</li>\n<li>Hern\u00e1ndez-Orallo, J., 2000, \u201cBeyond the Turing\nTest,\u201d <em>Journal of Logic, Language and Information</em>, 9:\n447\u201366.</li>\n<li>\u2013\u2013\u2013, 2020, \u201cTwenty Years Beyond the Turing\nTest: Moving Beyond the Human Judges,\u201d <em>Minds and\nMachines</em>, 30: 533\u201362.</li>\n<li>Hern\u00e1ndez-Orallo, J. and Dowe, D. L., 2010,\n\u201cMeasuring Universal Intelligence: Towards an Anytime\nIntelligence Test,\u201d <em>Artificial Intelligence</em>, 174:\n1508\u201339.</li>\n<li>Hodges, A., 1983, <em>Alan Turing: The Enigma</em>, London:\nBurnett with Hutchinson.</li>\n<li>Hofstadter, D., 1982, \u201cThe Turing Test: A Coffee-House\nConversation,\u201d in D. Hofstadter and D. Dennett (eds.), <em>The\nMind\u2019s I: Fantasies and Reflections on Self and Soul</em>,\nLondon: Penguin, 69\u201395.</li>\n<li>Kobosko, S., et al., 2013 \u201cPassing an Enhanced Turing Test:\nInteracting with Lifelike Computer Representations of Specific\nIndividuals,\u201d <em>Journal of Intelligent Systems</em>, 22:\n365\u2013415.</li>\n<li>Korukonda, A., 2003, \u201cTaking Stock of the Turing Test: A\nReview, Analysis and Appraisal of Issues Surrounding Thinking\nMachines,\u201d <em>International Journal of Human-Computer\nStudies</em> 58: 240\u201357.</li>\n<li>Kulikov, V., 2020, \u201cPreferential Engagement: What can we\nLearn from Online Chess?,\u201d <em>Minds and Machines</em>, 30:\n617\u201336.</li>\n<li>Leavitt, D., 2007, <em>The Man Who Knew Too Much: Alan Turing and\nthe Invention of the Computer</em> London: Phoenix.</li>\n<li>Levesque, H., 2017, <em>Commonsense, the Turing Test, and the\nQuest for Real AI</em>, Cambridge, MA: MIT Press.</li>\n<li>Lewis, D., 1969, \u201cLucas against mechanism,\u201d\n<em>Philosophy</em>, 44: 231\u2013233.</li>\n<li>\u2013\u2013\u2013, 1979, \u201cLucas against mechanism\nII,\u201d <em>Canadian Journal of Philosophy</em>, 9:\n373\u2013376.</li>\n<li>Lucas, J., 1961, \u201cMinds, Machines and G\u00f6del,\u201d\n<em>Philosophy</em>, 36: 120\u20134.</li>\n<li>Lupowski, P., 2011, \u201cA Formal Approach to Exploring the\nInterrogator\u2019s Perspective in the Turing Test,\u201d <em>Logical and\nLogical Philosophy</em> 20: 139\u201358.</li>\n<li>Lupowski, P. and Jurowska, P., 2019, \u201cMinimum Intelligent\nSignal Test as an Alternative to the Turing Test,\u201d\n<em>Diametros</em>, 59: 35\u201347.</li>\n<li>Lyre, H. 2020, \u201cThe State Space of Artificial\nIntelligence\u201d, <em>Minds and Machines</em>, 30: 325\u201347.</li>\n<li>Mallory, F. 2020, \u201cIn Defence of a Reciprocal Turing\nTest,\u201d <em>Minds and Machines</em>, 30: 659\u201380.</li>\n<li>Marcus, G., 2020 \u201cThe Next Decade in AI: Four Steps Towards\nRobust Artificial Intelligence,\u201d arXiv:2002.06177.</li>\n<li>Marquis, P., et al. (eds.), 2020, <em>A Guided Tour of Artificial\nIntelligence Research</em>, Cham: Springer.</li>\n<li>Masum, H., Christensen, S., and Oppacher, F., 2003, \u201cThe\nTuring Ratio: A Framework for Open-Ended Task Metrics,\u201d\n<em>Journal of Evolution and Technology</em>, 13(2),\n <a href=\"https://www.jetpress.org/volume13/TuringRatio.pdf\" target=\"other\">available online</a>.</li>\n<li>McDermott, D., 2014, \u201cOn the Claim that a Look-Up Table\nProgram could Pass the Turing Test,\u201d <em>Minds and\nMachines</em>, 24: 143\u201388.</li>\n<li>Millican, P. and Clark, A., (eds.), 1999, <em>Machines and\nThought: The Legacy of Alan Turing</em>, two volumes, Oxford:\nClarendon.</li>\n<li>Moor, J., 1976, \u201cAn Analysis of Turing\u2019s Test,\u201d\n<em>Philosophical Studies</em>, 30: 249\u201357.</li>\n<li>\u2013\u2013\u2013, 2001, \u201cThe Status and Future of the\nTuring Test,\u201d <em>Minds and Machines</em>, 11: 77\u201393.</li>\n<li>___, ed., 2003 <em>The Turing Test: The Elusive Standard of\nArtificial Intelligence</em> Dordrecht: Springer.</li>\n<li>Neufeld, E. and Finnestad, S., 2020a, \u201cIn Defense of the\nTuring Test,\u201d <em>AI and Society</em> 35: 819\u201327.</li>\n<li>Neufeld, E. and Finnestad, S., 2020b, \u201cImitation Game:\nThreshold or Watershed?,\u201d <em>Minds and Machines</em>, 30:\n637\u201357.</li>\n<li>Pautz, A. and Stoljar, D. (eds.), 2019, <em>Blockheads! Essays on\nNed Block\u2019s Philosophy of Mind and Consciousness</em>,\nCambridge, MA: MIT Press.</li>\n<li>Penrose, R., 1989, <em>The Emperor\u2019s New Mind</em>, Oxford:\nOxford University Press.</li>\n<li>Piccinini, G., 2000, \u201cTuring\u2019s Rules for the Imitation\nGame,\u201d <em>Minds and Machines</em>, 10: 573\u201385.</li>\n<li>Proudfoot, D., 2013, \u201cRethinking Turing\u2019s Test,\u201d\n<em>Journal of Philosophy</em>, 110: 391\u2013411.</li>\n<li>\u2013\u2013\u2013, 2020, \u201cRethinking Turing\u2019s Test and\nthe Philosophical Implications,\u201d <em>Minds and Machines</em>,\n30: 487\u2013512.</li>\n<li>Proudfoot, D. and Copeland, J. 2008 \u201cTuring\u2019s Test: A\nPhilosophical and Historical Guide,\u201d in R. Epstein et al.,\n(eds.), <em>Parsing the Turing Test: Philosophical and Methodological\nIssues</em>, Dordrecht: Springer, 119\u201338.</li>\n<li>Rapaport, W., 2000, \u201cHow to Pass a Turing Test: Syntactic\nSemantics, Natural-Language Understanding, and First-Person\nCognition,\u201d <em>Journal of Logic, Language and Information</em>,\n9: 467\u201390.</li>\n<li>Saygin, A., Cicekli, I., and Akman, V., 2000, \u201cTuring Test:\n50 Years Later,\u201d <em>Minds and Machines</em>, 10:\n463\u2013518.</li>\n<li>Schweizer, P., 1998, \u201cThe Truly Total Turing Test,\u201d\n<em>Minds and Machines</em>, 8: 263\u201372.</li>\n<li>\u2013\u2013\u2013, 2012, \u201cThe Externalist Foundation of\na Truly Total Turing Test,\u201d <em>Minds and Machines</em>, 22:\n191\u2013212.</li>\n<li>Searle, J., 1981, \u201cMinds, Brains, and Programs,\u201d\n<em>Behavioral and Brain Sciences</em>, 3: 417\u201357.</li>\n<li>Shah, H. and Warwick, K., 2010, \u201cHidden Interlocutor\nMisidentification in Practical Turing Tests,\u201d <em>Minds and\nMachines</em>, 203: 441\u201354.</li>\n<li>Shieber, S., 1994, \u201cLessons from a restricted Turing\nTest,\u201d <em>Communications of the Association for Computing\nMachinery</em>, 37: 70\u20138.\n [<a href=\"http://www.eecs.harvard.edu/~shieber/Biblio/Papers/loebner-rev-html/loebner-rev-html.html\" target=\"other\">Preprint available online</a>].</li>\n<li>\u2013\u2013\u2013, (ed.), 2004, <em>The Turing Test: Verbal\nBehaviour as the Mark of Intelligence</em>, Cambridge: MIT Press.</li>\n<li>\u2013\u2013\u2013, 2007, \u201cThe Turing Test as Interactive\nProof,\u201d <em>No\u00fbs</em>, 41: 686\u2013713.</li>\n<li>\u2013\u2013\u2013, 2014, \u201cThere can be no\nTuring-Test-Passing Memory Machines,\u201d <em>Philosophers\u2019\nImprint</em>, 14: 1\u201313.</li>\n<li>Sparrow, R., 2004, \u201cThe Turing Triage Test,\u201d\n<em>Ethics and Information Technology</em>, 6: 203\u201313.</li>\n<li>Srinivasan, B. and Shah, K., 2019, \u201cTowards a Unified\nFramework for Developing Ethical and Practical Turing\nTests,\u201d <em>AI and Society</em>, 34: 145\u201352.</li>\n<li>Sterrett, S., 2000, \u201cTuring\u2019s Two Tests for\nIntelligence,\u201d <em>Minds and Machines</em>, 10:\n541\u201359.</li>\n<li>Sterrett, S., 2020, \u201cThe Genius of the \u2018Original\nImitation Game\u2019 Test,\u201d <em>Minds and Machines</em>, 30:\n469\u201386.</li>\n<li>Traiger, S., 2000, \u201cMaking the Right Identification in the\nTuring Test,\u201d <em>Minds and Machines</em>, 10:\n561\u2013572.</li>\n<li>Turing, A., 1950, \u201cComputing Machinery and\nIntelligence,\u201d <em>Mind</em>, 59 (236): 433\u201360.</li>\n<li>Turing, A. 1992, <em>The Collected Works of A. M. Turing</em>,\nedited by P. Furbank, London: North-Holland.</li>\n<li>Warwick, K. et al., 2013, \u201cSome Implications of a Sample of\nPractical Turing Tests,\u201d <em>Minds and Machines</em>, 23:\n163\u201377.</li>\n<li>Weizenbaum, J., 1966, \u201cELIZA-A Computer Program for the\nStudy of Natural Language Communication Between Men and\nMachines,\u201d <em>Communications of the ACM</em>, 9:\n36\u201345.</li>\n<li>Wheeler, M., 2020 \u201cDeceptive Appearances: The Turing Test,\nResponse Dependence, and Intelligence as an Emotional Concept,\u201d\n<em>Minds and Machines</em> 30: 513\u201332.</li>\n<li>Whitby, B., 1996, \u201cThe Turing Test: AI\u2019s Biggest Blind\nAlley?\u201d in P. Millican and A. Clark (eds.), <em>Machines and\nThought: The Legacy of Alan Turing</em>, Volume 1, Oxford:\nClarendon.</li>\n<li>Zdenek, S., 2001, \u201cPassing Loebner\u2019s Turing Test: A\nCase of Conflicting Discourse Functions,\u201d <em>Minds and\nMachines</em>, 11: 53\u201376.</li>\n</ul>\n</div>"
    },
    "related_entries": {
        "entry_list": [
            "artificial intelligence",
            "Chinese room argument",
            "functionalism",
            "G\u00f6del, Kurt: incompleteness theorems",
            "logic: provability",
            "Turing, Alan"
        ],
        "entry_link": [
            {
                "../artificial-intelligence/": "artificial intelligence"
            },
            {
                "../chinese-room/": "Chinese room argument"
            },
            {
                "../functionalism/": "functionalism"
            },
            {
                "../goedel-incompleteness/": "G\u00f6del, Kurt: incompleteness theorems"
            },
            {
                "../logic-provability/": "logic: provability"
            },
            {
                "../turing/": "Turing, Alan"
            }
        ]
    },
    "academic_tools": {
        "listed_text": [
            "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=turing-test\" target=\"other\">How to cite this entry</a>.",
            "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://leibniz.stanford.edu/friends/preview/turing-test/\" target=\"other\">Preview the PDF version of this entry</a> at the\n <a href=\"https://leibniz.stanford.edu/friends/\" target=\"other\">Friends of the SEP Society</a>.",
            "<img alt=\"inpho icon\" src=\"../../symbols/inpho.png\"/>",
            "<a href=\"https://www.inphoproject.org/entity?sep=turing-test&amp;redirect=True\" target=\"other\">Look up topics and thinkers related to this entry</a>\n at the Internet Philosophy Ontology Project (InPhO).",
            "<img alt=\"phil papers icon\" src=\"../../symbols/pp.gif\"/>",
            "<a href=\"https://philpapers.org/sep/turing-test/\" target=\"other\">Enhanced bibliography for this entry</a>\nat <a href=\"https://philpapers.org/\" target=\"other\">PhilPapers</a>, with links to its database."
        ],
        "listed_links": [
            {
                "https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=turing-test": "How to cite this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/preview/turing-test/": "Preview the PDF version of this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/": "Friends of the SEP Society"
            },
            {
                "https://www.inphoproject.org/entity?sep=turing-test&redirect=True": "Look up topics and thinkers related to this entry"
            },
            {
                "https://philpapers.org/sep/turing-test/": "Enhanced bibliography for this entry"
            },
            {
                "https://philpapers.org/": "PhilPapers"
            }
        ]
    },
    "other_internet_resources": {
        "listed_text": [
            "Brown, T., et al., 2020,\n <a href=\"https://arxiv.org/abs/2005.14165\" target=\"other\">Language Models are Few-Shot Learners</a>,\n description of GPT-3 at archive.org.",
            "Chalmers, D., 2020,\n \u201c<a href=\"https://dailynous.com/2020/07/30/philosophers-gpt-3/#chalmers\" target=\"other\">GPT-3 and General Intelligence</a>,\u201d\n blog post a dailynous.com.",
            "<a href=\"http://dailynous.com/2020/07/30/philosophers-gpt-3/\" target=\"other\">Philosophers on GPT-3</a>,\n at dailynous.com.",
            "<a href=\"http://www.turing.org.uk/turing/Turing.html\" target=\"other\">Alan Turing Home Page</a>\n (Andrew Hodges, Wadham College, Oxford).",
            "\u201c<a href=\"https://www.cs.ox.ac.uk/activities/ieg/e-library/sources/t_article.pdf\" target=\"other\">Computing machinery and intelligence</a>\u201d \nby Alan Turing (1950).",
            "<a href=\"https://www.ocf.berkeley.edu/~arihuang/academic/research/loebner.html\" target=\"other\">The Loebner Prize</a>.",
            "<a href=\"http://www.rci.rutgers.edu/~cfs/472_html/Intro/NYT_Intro/History/MachineIntelligence1.html\" target=\"other\">Machine Intelligence Part 1: The Turing Test and Loebner Prize</a>\n (Ashley Dunn).",
            "<a href=\"https://www.theverge.com/2019/2/1/18205610/google-captcha-ai-robot-human-difficult-artificial-intelligence\" target=\"other\">Why CAPTCHAS Have Gotten So Difficult</a>(Josh\n Dzieza).",
            "<a href=\"https://arxiv.org/pdf/1906.05833.pdf\" target=\"other\">There is no General AI: Why Turing Machines Cannot Pass the Turing Test</a>\n (Jobst Landgrebe and Barry Smith)."
        ],
        "listed_links": [
            {
                "https://arxiv.org/abs/2005.14165": "Language Models are Few-Shot Learners"
            },
            {
                "https://dailynous.com/2020/07/30/philosophers-gpt-3/#chalmers": "GPT-3 and General Intelligence"
            },
            {
                "http://dailynous.com/2020/07/30/philosophers-gpt-3/": "Philosophers on GPT-3"
            },
            {
                "http://www.turing.org.uk/turing/Turing.html": "Alan Turing Home Page"
            },
            {
                "https://www.cs.ox.ac.uk/activities/ieg/e-library/sources/t_article.pdf": "Computing machinery and intelligence"
            },
            {
                "https://www.ocf.berkeley.edu/~arihuang/academic/research/loebner.html": "The Loebner Prize"
            },
            {
                "http://www.rci.rutgers.edu/~cfs/472_html/Intro/NYT_Intro/History/MachineIntelligence1.html": "Machine Intelligence Part 1: The Turing Test and Loebner Prize"
            },
            {
                "https://www.theverge.com/2019/2/1/18205610/google-captcha-ai-robot-human-difficult-artificial-intelligence": "Why CAPTCHAS Have Gotten So Difficult"
            },
            {
                "https://arxiv.org/pdf/1906.05833.pdf": "There is no General AI: Why Turing Machines Cannot Pass the Turing Test"
            }
        ]
    }
}