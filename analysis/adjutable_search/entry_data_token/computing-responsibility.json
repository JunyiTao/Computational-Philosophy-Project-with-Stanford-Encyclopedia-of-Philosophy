{"url": "computing-responsibility", "title": "Computing and Moral Responsibility", "authorship": {"year": "Copyright \u00a9 2023", "author_text": "Merel Noorman\n<merelnoorman@gmail.com>", "author_links": [{"mailto:merelnoorman%40gmail%2ecom": "merelnoorman@gmail.com"}], "raw_html": "<div id=\"article-copyright\">\n<p>\n<a href=\"../../info.html#c\">Copyright \u00a9 2023</a> by\n\n<br/>\nMerel Noorman\n&lt;<a href=\"mailto:merelnoorman%40gmail%2ecom\"><em>merelnoorman<abbr title=\" at \">@</abbr>gmail<abbr title=\" dot \">.</abbr>com</em></a>&gt;\n    </p>\n</div>"}, "pubinfo": ["First published Wed Jul 18, 2012", "substantive revision Thu Feb 2, 2023"], "preamble": "\n\nTraditionally philosophical discussions on moral responsibility have\nfocused on the human components of moral action. Accounts of how to\nascribe moral responsibility usually describe human agents performing\nactions that have well-defined, direct consequences. In today\u2019s\nincreasingly technological society, however, human activity cannot be\nproperly understood without making reference to technological\nartifacts, which complicates the ascription of moral responsibility\n(Jonas 1984; Doorn & van de Poel\n 2012).[1]\n As we interact with and through these artifacts, they affect the\ndecisions that we make and how we make them (Latour 1992, Verbeek\n2021). They persuade, facilitate and enable particular human cognitive\nprocesses, actions or attitudes, while constraining, discouraging and\ninhibiting others. For instance, internet search engines prioritize\nand present information in a particular order, thereby influencing\nwhat internet users get to see. As Verbeek points out, such\ntechnological artifacts are \u201cactive mediators\u201d that\n\u201cactively co-shape people\u2019s being in the world: their\nperception and actions, experience and existence\u201d (2006, p.\n364). As active mediators, they are a key part of human action and as\na result they challenge conventional notions of moral responsibility\nthat do not account for the active role of technology (Jonas 1984;\nJohnson 2001; Swierstra and Waelbers 2012).\n\nComputing presents a particular case for understanding the role of\ntechnology in moral responsibility. As computer technologies have\nbecome a more integral part of daily activities, automate more\ndecision-making processes and continue to transform the way people\ncommunicate and relate to each other, they have further complicated\nthe already problematic tasks of attributing moral responsibility. The\ngrowing pervasiveness of computer technologies in everyday life, the\ngrowing complexities of these technologies and the new possibilities\nthat they provide raise new kinds of questions: who is responsible for\nthe information published on the Internet? To what extent and for what\nperiod of time are developers of computer technologies accountable for\nuntoward consequences of their products? And as computer technologies\nbecome more complex and behave increasingly autonomous can or should\nhumans still be held responsible for the behavior of these\ntechnologies?\n\nThis entry will first look at the challenges that computing poses to\nconventional notions of moral responsibility. The discussion will then\nreview two different ways in which various authors have addressed\nthese challenges: 1) by reconsidering the idea of moral agency and 2)\nby rethinking the concept of moral responsibility itself.\n", "toc": [{"#ChaMorRes": "1. Challenges to moral responsibility"}, {"#CauCon": "1.1 Causal contribution"}, {"#ConCon": "1.2 Considering the consequences"}, {"#FreAct": "1.3 Free to act"}, {"#ComMorAge": "2. Can computers be moral agents?"}, {"#ComMorResAge": "2.1 Computers as morally responsible agents"}, {"#CreAutMorAge": "2.2 Creating autonomous moral agents"}, {"#ExpConMorAge": "2.3 Expanding the concept of moral agency"}, {"#RetConMorRes": "3. Rethinking the concept of moral responsibility"}, {"#AssResp": "3.1 Assigning responsibility"}, {"#RespAsPrac": "3.2 Responsibility as practice"}, {"#Concl": "4. Conclusion"}, {"#Bib": "Bibliography"}, {"#Aca": "Academic Tools"}, {"#Oth": "Other Internet Resources"}, {"#JouLin": "Journals On-line"}, {"#Cen": "Centers"}, {"#Org": "Organizations"}, {"#Blo": "Blogs"}, {"#Rel": "Related Entries"}], "main_text": "\n1. Challenges to moral responsibility\n\nMoral responsibility is about human action and its intentions and\nconsequences (Fisher 1999, Eshleman 2016, Talbert 2022). Generally\nspeaking a person or a group of people is morally responsible when\ntheir voluntary actions have morally significant outcomes that would\nmake it appropriate to blame or praise them. Thus, we may consider it\na person\u2019s moral responsibility to jump in the water and try to\nrescue another person, when she sees that person drowning. If she\nmanages to pull the person from the water we are likely to praise her,\nwhereas if she refuses to help we may blame her. Ascribing moral\nresponsibility establishes a link between a person or a group of\npeople and someone or something affected by the actions of this person\nor group. The person or group that performs the action and causes\nsomething to happen is often referred to as the agent. The\nperson, group or thing that is affected by the action is referred to\nas the patient. Establishing a link in terms of moral\nresponsibility between the agent and the patient can be done both\nretrospectively as well as prospectively. That is, sometimes\nascriptions of responsibility involve giving an account of who was at\nfault for an accident and who should be punished. It can also be about\nprospectively determining the obligations and duties a person has to\nfulfill in the future and what she ought to do.\n\nHowever, the circumstances under which it is appropriate to ascribe\nmoral responsibility are not always clear. On the one hand the concept\nhas varying meanings and debates continue on what sets moral\nresponsibility apart from other kinds of responsibility (Hart 1968,\nTalbert 2022, Tigard 2021a). The concept is intertwined and sometimes\noverlaps with notions of accountability, liability, blameworthiness,\nrole-responsibility and causality. Opinions also differ on which\nconditions warrant the attribution of moral responsibility; whether it\nrequires an agent with free will or not and whether humans are the\nonly entities to which moral responsibility can be attributed (see the\nentry on\n moral responsibility).\n \n\nOn the other hand, it can be difficult to establish a direct link\nbetween the agent and the patient because of the complexity involved\nin human activity, in particular in today\u2019s technological\nsociety. Individuals and institutions generally act with and in\nsociotechnical systems in which tasks are distributed among\nhuman and technological components, which mutually affect each other\nin different ways depending on the context (Bijker, Hughes and Pinch\n1987, Felt et al. 2016). Increasingly complex technologies can\nexacerbate the difficulty of identifying who or what is\n\u2018responsible\u2019. When something goes wrong, a retrospective\naccount of what happened is expected and the more complex the system,\nthe more challenging is the task of ascribing responsibility (Johnson\nand Powers 2005). Indeed, Matthias argues that there is a growing\n\u2018responsibility gap\u2019: the more complex computer\ntechnologies become and the less human beings can directly control or\nintervene in the behavior of these technologies, the less we can\nreasonably hold human beings responsible for these technologies\n(Matthias, 2004).\n\nThe increasing pervasiveness of computer technologies poses various\nchallenges to figuring out what moral responsibility entails and how\nit should be properly ascribed. To explain how computing complicates\nthe ascription of responsibility we have to consider the conditions\nunder which it makes sense to hold someone responsible. Despite the\nongoing philosophical debates on the issue, most analysis of moral\nresponsibility share at least the following three conditions (Eshleman\n2016; Jonas 1984):\n\nThere should be a causal connection between the person and the\noutcome of actions. A person is usually only held responsible if they\nhad some control over the outcome of events.\nThe subject has to have knowledge of and be able to consider the\npossible consequences of her actions. We tend to excuse someone from\nblame if they could not have known that their actions would lead to a\nharmful event.\nThe subject has to be able to freely choose to act in certain way.\nThat is, it does not make sense to hold someone responsible for a\nharmful event if her actions were completely determined by outside\nforces.\n\n\nA closer look at these three conditions shows that computing can\ncomplicate the applicability of each of these conditions.\n1.1 Causal contribution\n\nIn order for a person to be held morally responsible for a particular\nevent, she has to be able to exert some kind of influence on that\nevent. It does not make sense to blame someone for an accident if she\ncould not have avoided it by acting differently or if she had no\ncontrol over the events leading up to the accident.\n\nHowever, computer technologies can obscure the causal connections\nbetween a person\u2019s actions and the eventual consequences.\nTracing the sequence of events that led to a computer-related\ncatastrophic incident, such as a plane crash, usually leads in many\ndirections, as such incidents are seldom the result of a single error\nor mishap. Technological accidents are commonly the product of an\naccumulation of mistakes, misunderstanding or negligent behavior of\nvarious individuals involved in the development, use and maintenance\nof computer systems, including designers, engineers, technicians,\nregulators, managers, users, manufacturers, sellers, resellers and\neven policy makers.\n\nThe involvement of multiple actors in the development and deployment\nof technologies gives rise to what is known as the problem of\n\u2018many hands\u2019: it is difficult to determine who was\nresponsible for what when multiple individuals contributed to the\noutcome of events (Jonas 1984; Friedman 1990; Nissenbaum 1994; van de\nPoel et al. 2015). One classic example of the problem of many hands in\ncomputing is the case of the malfunctioning radiation treatment\nmachine Therac-25 (Leveson and Turner 1993; Leveson 1995). This\ncomputer-controlled machine was designed for the radiation treatment\nof cancer patients as well as for X-rays. During a two-year period in\nthe 1980s the machine massively overdosed six patients, contributing\nto the eventual death of three of them. These incidents were the\nresult of the combination of a number of factors, including software\nerrors, inadequate testing and quality assurance, exaggerated claims\nabout the reliability, bad interface design, overconfidence in\nsoftware design, and inadequate investigation or follow-up on accident\nreports. Nevertheless, in their analysis of the events Leveson and\nTurner conclude that it is hard to place the blame on a single person.\nThe actions or negligence of all those involved might not have proven\nfatal were it not for the other contributing events. A more recent\nexample of the problem of many hands is the crash of two 737 MAX\npassenger aircraft in late 2018 and early 2019. These crashes led to\nmulitple investigations that highlighted various factors that\ncontributed to the tragic outcome, include design and human errors as\nwell as organizational culture and lack of training (Heckert et al\n2020). This is not to say that there is no moral responsibility in\nthese case (Nissenbaum 1994; Gotterbarn 2001; Coeckelbergh 2012;\nFloridi 2013, Santonio De Sio et al, 2021), as many actors could have\nacted differently, but it makes it more difficult to retrospectively\nidentify the appropriate person that can be called upon to answer and\nmake amends for the outcome.\n\nAdding to the problem of many hands is the temporal and physical\ndistance that computing creates between a person and the consequences\nof her actions, as this distance can blur the causal connection\nbetween actions and events (Friedman 1990). Computational technologies\nextend the reach of human activity through time and space. With the\nhelp of social media and communication technologies people can\ninteract with others on the other side of the world. Satellites and\nadvanced communication technologies allow pilots to fly a remotely\ncontrolled drone from their ground-control station half way across the\nworld. These technologies enable people to act over greater distances,\nbut this remoteness can dissociate the original actions from its\neventual consequences (Waelbers 2009; Polder-Verkiel 2012;\nCoeckelbergh 2013). When a person uses a technological artifact to\nperform an action thousands of miles a way, that person might not know\nthe people that will be affected and she might not directly, or only\npartially, experience the consequences. This can reduce the sense of\nresponsibility the person feels and it may interfere with her ability\nto fully comprehend the significance of her actions. Similarly, the\ndesigners of an automated decision-making system determine ahead of\ntime how decisions should be made, but they will rarely see how these\ndecisions will impact the individuals they affect. Their original\nactions in programming the system may have effects on people years\nlater.\n\nThe problem of many hands and the distancing effects of the use of\ntechnology illustrate the mediating role of technological artifacts in\nthe confusion about moral responsibility. Technological artifacts\nbring together the various different intentions of their creators and\nusers. People create and deploy technologies with the objective of\nproducing some effect in the world. Software developers develop an\nautomated content moderation tool, often at the request of their\nmanagers or clients, with the aim of shielding particular content from\nusers and influencing what these users can or cannot read. The\nsoftware has inscribed in its design the various intentions of the\ndevelopers, managers and clients; it is poised to behave, given a\nparticular input, according to their ideas about which information is\nappropriate (Friedman 1997, Gorwa, Binns, & Katzenbach 2020).\nMoral responsibility can therefore not be attributed without looking\nat the causal efficacy of these artifacts and how they constrain and\nenable particular human activities.\n\nHowever, although technological artefacts may influence and shape\nhuman action, they do not determine it. They are not isolated\ninstruments that mean and work the same regardless of why, by whom,\nand in what context they are used; they have interpretive flexibility\n(Bijker et al. 1987) or multistability (Ihde\n 1990).[2]\n Although the design of the technology provides a set of conditions\nfor action, the form and meaning of these actions is the result of how\nhuman agents choose to use these technologies in particular contexts.\nPeople often use technologies in ways unforeseen by their designers.\nThis interpretive flexibility makes it difficult for designers to\nanticipate all the possible outcomes of the use of their technologies.\nThe mediating role of computer technologies complicates the effort of\nretrospectively tracing back the causal connection between actions and\noutcomes, but it also complicates forward-looking responsibility.\n1.2 Considering the consequences\n\nAs computer technologies shape how people perceive and experience the\nworld, they affect the second condition for attributing moral\nresponsibility. In order to make appropriate decisions a person has to\nbe able to consider and deliberate about the consequences of their\nactions. They have to be aware of the possible risks or harms that\ntheir actions might cause. It is unfair to hold someone responsible\nfor something if they could not have reasonably known that their\nactions might lead to harm.\n\nOn the one hand, computer technologies can help users to think through\nwhat their actions or choices may lead to. They help the user to\ncapture, store, organize and analyze data and information (Zuboff\n1982). For example, one often-named advantage of remote-controlled\nrobots used by the armed forces or rescue workers is that they enable\ntheir operators to acquire information that would not be able\navailable without them. They allow their operators to look\n\u201cbeyond the next hill\u201d or \u201caround the next\ncorner\u201d and they can thus help operators to reflect on what the\nconsequences of particular tactical decisions might be (US Department\nof Defense 2009). Similarly, data analysis tools can find patterns in\nlarge volumes of data that human data analysts cannot manually process\n(Boyd and Crawford 2012).\n\nOn the other hand the use of computers can constrain the ability of\nusers to understand or consider the outcomes of their actions. These\ncomplex technologies, which are never fully free from errors,\nincreasingly hide the automated processes behind the interface (Van\nden Hoven 2002). An example that illustrates how computer technologies\ncan limit understanding of the outcomes are the controversial risk\nassessment tools used by judges in several states in the U.S. for\nparole decisions and sentencing. In 2016 a civil society organization\nfound, based on an analysis of the risk scores of 7000 defendants\nproduced by one particular algorithm, that the scores poorly reflected\nthe actual recidivism rate and seemed to have a racial bias (Angwin et\nal. 2016). Regardless of whether its findings were correct or not,\nwhat is particularly relevant here is that the investigation also\nshowed that judges did not have a full understanding of how the\nprobabilities were calculated, in part because the algorithm was\nproprietary. The judges were basing their sentencing on the suggestion\nof an algorithm that they did not fully understand. This is the case\nfor most computer technologies today. Users only see part of the many\ncomputations that a computer performs and are for the most part\nunaware of how it performs them; they usually only have a partial\nunderstanding of the assumptions, models and theories on which the\ninformation on their computer screen is based. The increasing\ncomplexity of computer systems and their reliance on opaque machine\nlearning algorithms makes it even more difficult to understand what is\nhappening behind the interface (Pasquale 2015, Diakopoulos 2020). \n\nThe opacity of many computer systems can get in the way of assessing\nthe validity and relevance of the information and can prevent a user\nfrom making appropriate decisions. People have a tendency to either\nrely too much or not enough on the accuracy automated systems\n(Cummings 2004; Parasuraman & Riley 1997). This tendency is called\nautomation bias. A person\u2019s ability to act responsibly,\nfor example, can suffer when she distrust the automation as result of\na high rate of false alarms. In the Therac 25 case, one of the\nmachine\u2019s operators testified that she had become used to the\nmany cryptic error messages the machine gave and most did not involve\npatient safety (Leveson and Turner 1993, p.24). She tended to ignore\nthem and therefore failed to notice when the machine was set to\noverdose a patient. Too much reliance on automated systems can have\nequally disastrous consequences. In 1988 the missile cruiser U.S.S.\nVincennes shot down an Iranian civilian jet airliner, killing all 290\npassengers onboard, after it mistakenly identified the airliner as an\nattacking military aircraft (Gray 1997). The cruiser was equipped with\nan Aegis defensive system that could automatically track and target\nincoming missiles and enemy aircrafts. Analyses of the events leading\nup to the incident showed that overconfidence in the abilities of the\nAegis system prevented others from intervening when they could have.\nTwo other warships nearby had correctly identified the aircraft as\ncivilian. Yet, they did not dispute the Vincennes\u2019\nidentification of the aircraft as a military aircraft. In a later\nexplanation Lt. Richard Thomas of one of the nearby ships stated,\n\u201cWe called her Robocruiser\u2026 she always seemed to have a\npicture\u2026 She always seemed to be telling everybody to get on or\noff the link as though her picture was better\u201d (as quoted in\nGray 1997, p. 34). The captains of both ships thought that the\nsophisticated Aegis system provided the crew of Vincennes with\ninformation they did not have.\n\nConsidering the possible consequences of one\u2019s actions is\nfurther complicated as computer technologies make it possible for\nhumans to do things that they could not do before. Several decades\nago, the philosopher Ladd pointed out, \u201c[C]omputer technology\nhas created new modes of conduct and new social institutions, new\nvices and new virtues, new ways of helping and new ways of abusing\nother people\u201d (Ladd 1989, p. 210\u201311). Computer\ntechnologies of today have had a similar effect. The social or legal\nconventions that govern what we can do with these technologies take\nsome time to emerge and the initial absence of these conventions\ncontributes to confusion about responsibilities (Taddeo and Floridi\n2015). For example, the ability for users to upload and share text,\nvideos and images publicly on the Internet raised a whole set of\nquestions about who is responsible for the content of the uploaded\nmaterial. Such questions were at the heart of the debate about the\nconviction of three Google executives in Italy for a violation of the\ndata protection act (Sartor and Viola de Azevedo Cunha 2010). The case\nconcerned a video on YouTube of four students assaulting a disabled\nperson. In response to a request by the Italian Postal Police, Google,\nas owner of YouTube, took the video down two months after the students\nuploaded it. The judge, nonetheless, ruled that Google was criminally\nliable for processing the video without taking adequate precautionary\nmeasures to avoid privacy violations. The judge also held Google\nliable for failing to adequately inform the students, who uploaded the\nvideos, of their data protection obligations (p. 367). In the ensuing\ndebate about the verdict, those critical of the ruling insisted that\nit threatened the freedom of expression on the Internet and it sets a\ndangerous precedent that can be used by authoritarian regimes to\njustify web censorship (see also Singel 2010). Moreover, they claimed\nthat platform providers could not be held responsible for the actions\nof their users, as they could not realistically approve every upload\nand it was not their job to censure. Yet, others instead argued that\nit would be immoral for Google to be exempt from liability for the\ndamage that others suffered due to Google\u2019s profitable\ncommercial activity. Cases like this one show that in the confusion\nabout the possibilities and limitations of new technologies it can be\ndifficult to determine one\u2019s moral obligations to others.\n\nThe lack of experience with new technological innovations can also\naffect what counts as negligent use of the technology. In order to\noperate a new computer system, users typically have to go through a\nprocess of training and familiarization with the system. It requires\nskill and experience to understand and imagine how the system will\nbehave (Coeckelbergh and Wackers 2007). Friedman describes the case of\na programmer who invented and was experimenting with a \u2018computer\nworm\u2019, a piece of code that can replicate itself. At the time\nthis was a relatively new computational entity (1990). The programmer\nreleased the worm on the Internet, but the experiment quickly got out\nof the control when the code replicated much faster than he had\nexpected (see also Denning 1989). Today we would not find this a\nsatisfactory excuse, familiar as we have become with computer worms,\nviruses and other forms of malware. However, Friedman poses the\nquestion of whether the programmer really acted in a negligent way if\nthe consequences were truly unanticipated. Does the computer\ncommunity\u2019s lack of experience with a particular type of\ncomputational entity influence what we judge to be negligent\nbehavior?\n1.3 Free to act\n\nThe freedom to act is probably the most important condition for\nattributing moral responsibility and also one of the most contested\n(Talbert 2022). We tend to excuse people from moral blame if they had\nno other choice but to act in the way that they did. We typically do\nnot hold people responsible if they were coerced or forced to take\nparticular actions. In moral philosophy, the freedom to act can also\nmean that a person has free will or autonomy (Fisher 1999). Someone\ncan be held morally responsible because she acts on the basis of her\nown authentic thoughts and motivations and has the capacity to control\nher behavior (Johnson 2001). Note that this conception of autonomy is\ndifferent from the way the term \u2018autonomy\u2019 is often used\nin computer science, where it tends to refer to the ability of a robot\nor computer system to independently perform (i.e. without the\n\u2018human in the loop\u2019) complex tasks in unpredictable\nenvironments for extended periods of time (Noorman 2009, Zerilli et al\n2021).\n\nNevertheless, there is little consensus on what capacities human\nbeings have, that other entities do not have, which enables them to\nact freely (see the entries on\n free will,\n autonomy in moral and political philosophy,\n personal autonomy\n and\n compatibilism).\n Does it require rationality, emotion, intentionality or cognition?\nIndeed, one important debate in moral philosophy centers on the\nquestion of whether human beings really have autonomy or free will?\nAnd, if not, can moral responsibility still be attributed (Talbert\n2022)?\n\nIn practice, attributing autonomy or free will to humans on the basis\nof the fulfillment of a set of conditions turns out to be a less than\nstraightforward endeavor. We attribute autonomy to persons in degrees.\nAn adult is generally considered to be more autonomous than a child.\nAs individuals in a society our autonomy is thought to vary because we\nare manipulated, controlled or influenced by forces outside of\nourselves, such as by our parents or through peer pressure. Moreover,\ninternal physical or psychological influences, such as addictions or\nmental problems, are perceived as further constraining the autonomy of\na person.\n\nComputing, like other technologies, adds an additional layer of\ncomplexity to determining whether someone is free to act, as it\naffects the choices that humans have and how they make them. One of\nthe biggest application areas of computing is the automation of\ndecision-making processes and control. Automation can help to\ncentralize and increase control over multiple processes for those in\ncharge, while it limits the discretionary power of human operators on\nthe lower-end of the decision-making chain. An example is provided by\nthe automation of decision-making in public administration (Bovens and\nZouridis 2002). Large public sector organizations have over the last\ndecades progressively standardized and formalized their production\nprocesses. In a number of countries, the process of issuing decisions\nabout (student) loans, social benefits, speeding tickets or tax\nreturns is carried out to a significant extent by computer systems.\nThis has reduced the scope of the administrative discretion that many\nofficials, such as tax inspectors, welfare workers, and policy\nofficers, have in deciding how to apply formal policy rules in\nindividual cases (Eubanks 2018). In some cases, citizens no longer\ninteract with officials that have significant responsibility in\napplying their knowledge of the rules and regulations to decide what\nis appropriate (e.g., would it be better to let someone off with a\nwarning or is a speeding ticket required?). Rather, decisions are\npre-programmed in the algorithms that apply the same measures and\nrules regardless of the person or the context (e.g., a speeding camera\ndoes not care about the context or personal circumstances), and the\nhuman beings that citizens do interact with have little opportunity to\ninterrogate or change decisions (Dignum 2020). Responsibility for\ndecisions made, in these cases, has moved from \u2018street-level\nbureaucrats\u2019 to the \u2018system-level bureaucrats\u2019, such\nas managers and computer experts, that decide on how to convert policy\nand legal frameworks into algorithms and decision-trees (Bovens and\nZouridis 2002).\n\nThe automation of bureaucratic processes illustrates that some\ncomputer technologies are intentionally designed to limit the\ndiscretion of some human beings. An example is the anti-alcohol lock\nthat is already in use in a number of countries, including the USA,\nCanada, Sweden and the UK. It requires the driver to pass a breathing\ntest before she can start the car. This technology forces a particular\nkind of action and leaves the driver with hardly any choice. Other\ntechnologies might have a more subtle way of steering behavior, by\neither persuading or nudging users (Verbeek 2016). For example, the\nonboard computer devices in some cars that show, in real-time,\ninformation about fuel consumption can encourage the driver to\noptimize fuel efficiency. Such technologies are designed with the\nexplicit aim of making humans behave responsibly by limiting their\noptions or persuading them to choose in a certain way.\n\nNot all these technologies are designed to stimulate morally good\nbehavior. Yeung notes that these kinds of decision-guidance techniques\nhave become a key element of current day Big-Data analytic techniques,\nas used on social media and in advertising. She argues that these\n\u2018hyper nudges\u2019 are extremely powerful techniques to\nmanipulate the behaviour of internet users and users of Internet of\nThings (IoT) devices due to their networked, continuously updated,\ndynamic and pervasive nature. As they gather data from a wide range of\nsources about users to continuously make predictions in real-time\nabout the habits and preferences of users, they can target\nadvertisement, information and price incentives to gently and\nunobtrusively nudge these users in directions preferred by the those\nthat control the algorithms (Yeung, 2017). When these nudges are\nhardly noticeable and have a powerful effect, one can wonder how\nautonomous the decision making of these users is. This is the case,\nfor example, with dark patterns, which use interfaces on websites or\napps that are designed to trick users to do things that did not intend\nto do, such as purchasing additional expensive insurance (Ravenscraft\n2020). \n\nVerbeek notes that critics of the idea of intentionally developing\ntechnology to enforce morally desirable behavior have argued that it\njettisons the democratic principles of our society and threatens human\ndignity. They argue that it deprives humans of their ability and\nrights to make deliberate decisions and to act voluntarily. In\naddition, critics have claimed that if humans are not acting freely,\ntheir actions cannot be considered moral. These objections can be\ncountered, as Verbeek argues, by pointing to the rules, norms,\nregulations and a host of technological artifacts that already set\nconditions for actions that humans are able or allowed to perform.\nMoreover, he notes, technological artifacts, as active mediators,\naffect the actions and experiences of humans, but they do not\ndetermine them. Some people have creatively circumvented the strict\nmorality of earlier versions of the alcohol lock by having an air pump\nin the car (Vidal 2004). Nevertheless, these critiques underline the\nissues at stake in automating decision-making processes: computing can\nset constraints on the freedom a person has to act and thus affects\nthe extent to which she can be held morally responsible.\n\nThe challenges that computer technologies present with regard to the\nconditions for ascribing responsibility indicate the limitations of\nconventional ethical frameworks in dealing with the question of moral\nresponsibility. Traditional models of moral responsibility seem to be\ndeveloped for the kinds of actions performed by an individual that\nhave directly visible consequences (Waelbers 2009, Coeckelbergh\n2009). However, in today\u2019s society attributions of\nresponsibility to an individual or a group of individuals are\nintertwined with the artifacts with which they interact as well as\nwith intentions and actions of other human agents that these artifacts\nmediate. Acting with computer technologies may require a different\nkind of analysis of who can be held responsible and what it means to\nbe morally responsible. Below I discuss two ways in which scholars\nhave taken up this challenge: 1) reconsidering what it means to be a\nmoral agent and 2) reconsidering the concept of moral\nresponsibility.\n2. Can computers be moral agents?\n\nMoral responsibility is generally attributed to moral agents and, at\nleast in Western philosophical traditions, moral agency has been a\nconcept exclusively reserved for human beings (Johnson 2001; Doorn and\nvan de Poel 2012). Unlike animals or natural disasters, human beings\nin these traditions can be the originators of morally significant\nactions, as they can freely choose to act in one way rather than\nanother way and deliberate about the consequences of this choice. And,\nalthough some people are inclined to anthropomorphize computers and\ntreat them as if they were moral agents (Reeves and Nass 1996; Nass\nand Moon 2000; Rosenthal-von der P\u00fctten 2013), most\nphilosophers agree that current computer technologies should not be\ncalled moral agents, if that would mean that they could be held\nmorally responsible. However, the limitations of traditional ethical\nvocabularies in thinking about the moral dimensions of computing have\nled some authors to rethink the concept of moral agency. It should be\nnoted that some authors have also argued for a reconsideration of\nWestern philosophical anthropocentric conceptions of moral patiency,\nin particular in regard to the question concerning the moral standing\nof artificial agents or robots (Floridi 2016, Gunkel 2020,\nCoeckelbergh 2020). The following will nevertheless focus on\nmoral agency as these reflections on moral patiency tend to not\naddress the challenges to moral responsibility.\n2.1 Computers as morally responsible agents\n\nThe increasing complexity of computer technology and the advances in\nArtificial Intelligence (AI), challenge the idea that human beings are\nthe only entities to which moral responsibility can or should be\nascribed (Bechtel 1985; Kroes and Verbeek 2014). Dennett, for example,\nsuggested that holding a computer morally responsible is possible if\nit concerned a higher-order intentional computer system (1997). An\nintentional system, according to him, is one that can be predicted and\nexplained by attributing beliefs and desires to it, as well as\nrationality. In other words, its behavior can be described by assuming\nthe systems has mental states and that it acts according what it\nthinks it ought to do, given its beliefs and desires. At the time,\nDennett noted that many computers were already intentional systems,\nbut they lacked the higher-order ability to reflect on and reason\nabout their mental states. They did not have beliefs about their\nbeliefs or thoughts about desires. Dennett suggested that the\nfictional HAL 9000 that featured in the movie 2001: A Space\nOdyssey would qualify as a higher-order intentional system that\ncan be held morally responsible. Although advances in AI might not\nlead to HAL, he did see the development of computers systems with\nhigher-order intentionality as a real possibility.\n\nSullins argues in line with Dennett that moral agency is not\nrestricted to human beings (2006). He proposes that computers systems\nor, more specifically, robots are moral agents when they have a\nsignificant level of autonomy and they can be regarded at an\nappropriate level of abstraction as exhibiting intentional behavior. A\nrobot, according to Sullins, would be significantly autonomous if it\nwas not under the direct control of other agents in performing its\ntasks. Note that Sullins interprets autonomy in a narrow sense in\ncomparison to the conception of autonomy in moral philosophy as\nproperty of human beings. He adds as a third condition that a robot\nalso has to be in a position of responsibility to be a moral agent.\nThat is, the robot performs some social role that carries with it some\nresponsibilities and in performing this role the robot appears to have\n\u2018beliefs\u2019 about and an understanding of its duties towards\nother moral agents (p. 28). To illustrate what kind of capabilities\nare required for \u201cfull moral agency\u201d, he draws an analogy\nwith a human nurse. He argues that if a robot was autonomous enough to\ncarry out the same duties as a human nurse and had an understanding of\nits role and responsibilities in the health care systems, then it\nwould be a \u201cfull moral agent\u201d. Sullins maintains that it\nwill be some time before machines with these kinds of capabilities\nwill be available, but \u201ceven the modest robots of today can be\nseen to be moral agents of a sort under certain, but not all, levels\nof abstraction and are deserving of moral consideration\u201d (p.\n29).\n\nEchoing objections to the early project of (strong) AI (Sack\n 1997),[3]\n critics of analyses such as presented by Dennett and Sullins, have\nobjected to the idea that computer technologies can have capacities\nthat make human beings moral agents, such as mental states,\nintentionality, common sense, emotion or empathy (Johnson 2006; Kuflik\n1999; Nyholm 2018). They, for instance, point out that it makes no\nsense to treat computer system as moral agents that can be held\nresponsible, for they cannot suffer and thus cannot be punished\n(Sparrow 2007; Asaro 2011). Veliz argues that computers may act like\nmoral agents, but they lack sentience and are therefore \u2018moral\nzombies\u2019 (2021). Hakli and Makela argue that computers\ncannot have the kind of autonomy required for moral agency, because\ntheir capacities are a result of engineering and programming which\nundermines the autonomy of robots and disqualifies them as moral\nagents (2019). Or they argue, as Stahl does, that computers are not\ncapable of moral reasoning, because they do not have the capacity to\nunderstand the meaning of the information that they process (2006). In\norder to comprehend the meaning of moral statements an agent has to be\npart of the form of life in which the statement is meaningful; it has\nto be able to take part in moral discourses. Similar to the debates\nabout AI, critics continue to draw a distinction between humans and\ncomputers by noting various capacities that computers do not, and\ncannot, have that would justify the attribution of moral agency.\n\nSome other critics do not contest that human beings might be able to\nbuild computer systems with the required capacities for moral agency,\nbut question whether it is ethically appropriate to do so. Bryson, for\ninstance, argues that even if it was possible to create artifacts with\nsuch capacities \u2013 and she assumes this might very well be\npossible \u2013 human beings have a choice in the matter\n(2018). She defines a moral agent as \u201csomething deemed\nresponsible by a society for its actions\u201d (p. 16). Society can\nthus at one point deem it appropriate to view certain computer systems\nas moral agents, for instance as it would provide a short cut to\nfiguring out how responsibility should be distributed. However, she\nargues that there is no necessary or predetermined position for these\ntechnologies in our society. This is because, she notes, computer\ntechnologies ethical frameworks are \u201cartefacts of our societies,\nand therefore subject to human control\u201d (p. 15). We can choose\nwhat capacities we equip these artefacts with, and she sees no\ncoherent reason for creating artificial agents that human beings have\nto compete with in terms of moral agency or patiency.\n2.2 Creating autonomous moral agents\n\nIn the absence of any definitive arguments for or against the\npossibility of future computer systems being morally responsible,\nresearchers within the field of machine ethics aim to further develop\nthe discussion by focusing instead on creating computer system that\ncan behave as if they are moral agents (Moor 2006,\nCervantes et al 2019 , Zoshak and Dew 2021). Research\nwithin this field has been concerned with the design and development\nof computer systems that can independently determine what the right\nthing to do would be in a given situation. According to Allen and\nWallach, such autonomous moral agents (AMAs) would have to be\ncapable of reasoning about the moral and social significance of their\nbehavior and use their assessment of the effects their behavior has on\nsentient beings to make appropriate choices (2012; see also Wallach\nand Allen 2009 and Allen et al. 2000). Such abilities are needed, they\nargue, because computers are becoming more and more complex and\ncapable of operating without direct human control in different\ncontexts and environments. Progressively autonomous technologies\nalready in development, such as military robots, driverless cars or\ntrains and service robots in the home and for healthcare, will be\ninvolved in moral situations that directly affect the safety and\nwell-being of humans. An autonomous bomb disposal robot might in the\nfuture be faced with the decision which bomb it should defuse first,\nin order to minimize casualties. Similarly, a moral decision that a\ndriverless car might have to make is whether to break for a crossing\ndog or avoid the risk of causing injury to the driver behind him. Such\ndecisions require judgment. Currently operators make such moral\ndecisions, or the decision is already inscribed in the design of the\ncomputer system. Machine ethics, Wallach and Allen argue, goes one\nstep beyond making engineers aware of the values they build into the\ndesign of their products, as it seeks to build ethical decision-making\ninto the machines.\n\nTo further specify what it means for computers to make ethical\ndecisions or to put \u2018ethics in the machine\u2019, Moor\ndistinguished between three different kinds of ethical agents:\nimplicit ethical agents, explicit ethical agents, and full ethical\nagents (2006). The first kind of agent is a computer that has the\nethics of its developers inscribed in their design. These agents are\nconstructed to adhere to the norms and values of the contexts in which\nthey are developed or will be used. Thus, ATM tellers are designed to\nhave a high level of security to prevent unauthorized people from\ndrawing money from accounts. An explicit ethical agent is a computer\nthat can \u2018do ethics\u2019. In other words, it can on the basis\nof an ethical model determine what would be the right thing to do,\ngiven certain inputs. The ethical model can be based on ethical\ntraditions, such as Kantian, Confucianism, Ubuntu, or utilitarian\nethics\u2014depending on the preferences of its creators. These\nagents would \u2018make ethical decisions\u2019 on behalf of its\nhuman users (and developers). Such agents are akin to the autonomous\nmoral agents described by Allen and Wallach. Finally, Moor defined\nfull ethical agents as entities that can make ethical judgments and\ncan justify them, much like human beings can. He claimed that although\nat the time of his writing there were no computer technologies that\ncould be called fully ethical, it is an empirical question whether or\nnot it would be possible in the future. Few, if any, philosophers\ntoday would argue that this question has been answered in the postive.\n\n\nThe effort to build AMAs raises the question of how this effort\naffects the ascription of moral responsibility. As human beings would\ndesign these artificial agents to behave within pre-specified\nformalized ethical frameworks, it is likely that responsibility will\nstill be ascribed to these human actors and those that deploy these\ntechnologies. However, as Allen and Wallach acknowledge, the danger of\nexclusively focusing on equipping robots with moral decision-making\nabilities, rather than also looking at the sociotechnical systems in\nwhich these robots are embedded, is that it may cause further\nconfusion about the distribution of responsibility (2012). Robots with\nmoral decision-making capabilities may present similar challenges to\nascribing responsibility as other technologies, when they introduce\nnew complexities that further obfuscate causal connections that lead\nback to their creators and users.\n2.3 Expanding the concept of moral agency\n\nThe prospect of increasingly autonomous and intelligent computer\ntechnologies and the growing difficulty of finding responsible human\nagents lead Floridi and Sanders to take a different approach (2004).\nThey propose to extend the class of moral agents to include artificial\nagents, while disconnecting moral agency and moral accountability from\nthe notion of moral responsibility. They contend that \u201cthe\ninsurmountable difficulties for the traditional and now rather\noutdated view that a human can be found accountable for certain kinds\nof software and even hardware\u201d demands a different approach (p.\n372). Instead, they suggest that artificial agents should be\nacknowledged as moral agents that can be held accountable, but not\nresponsible. To illustrate they draw a comparison between artificial\nagents and dogs as sources of moral actions. Dogs can be the cause of\na morally charged action, like damaging property or helping to save a\nperson\u2019s life, as in the case of search-and-rescue dogs. We can\nidentify them as moral agents even though we generally do not hold\nthem morally responsible, according to Floridi and Sanders: they are\nthe source of a moral action and can be held morally accountable by\ncorrecting or punishing them.\n\nJust like animals, Floridi and Sanders argue, artificial agents can be\nseen as sources of moral actions and thus can be held morally\naccountable when they can be conceived of as behaving like a moral\nagent from an appropriate level of abstraction. The notion of\nlevels of abstraction refers to the stance one adopts towards and\nentity to predict and explain its behavior. At a low level of\nabstraction we would explain the behavior of a system in terms of its\nmechanical or biological processes. At a higher level of abstraction\nit can help to describe the behavior of a system in terms of beliefs,\ndesires and thoughts. If at a high enough level a computational system\ncan effectively be described as being interactive, autonomous and\nadaptive, then it can be held accountable according to Floridi and\nSanders (p. 352). It, thus, does not require personhood or free will\nfor an agent to be morally accountable; rather the agent has to act as\nif it had intentions and was able to make choices.\n\nThe advantage of disconnecting accountability from responsibility,\naccording to Floridi and Sanders, is that it places the focus on moral\nagenthood, accountability and censure, instead of on figuring out\nwhich human agents are responsible. \u201cWe are less likely to\nassign responsibility at any cost, forced by the necessity to identify\na human moral agent. We can liberate technological development of AAs\n[Artificial Agents] from being bound by the standard limiting\nview\u201d (p. 376). When artificial agents \u2018behave\nbadly\u2019 they can be dealt with directly, when their autonomous\nbehavior and complexity makes it too difficult to distribute\nresponsibility among human agents. Immoral agents can be modified or\ndeleted. It is then possible to attribute moral accountability even\nwhen moral responsibility cannot be determined.\n\nCritics of Floridi\u2019s and Sanders\u2019 view on accountability\nand moral agency argue that placing the focus of analysis on\ncomputational artifacts by treating them as moral agents will draw\nattention away from the humans that deploy and develop them. Johnson,\nfor instance, makes the case that computer technologies remain\nconnected to the intentionality of their creators and users (2006).\nShe argues that although computational artifacts are a part of the\nmoral world and should be recognized as entities that have moral\nrelevance, they are not moral agents, for they are not intentional.\nThey are not intentional, because they do not have mental states or a\npurpose that comes from the freedom to act. She emphasizes that\nalthough these artifacts are not intentional, they do have\nintentionality, but their intentionality is related to their\nfunctionality. They are human-made artifacts and their design and use\nreflect the intentions of designers and users. Human users, in turn,\nuse their intentionality to interact with and through the software. In\ninteracting with the artifacts they activate the inscribed intentions\nof the designers and developers. It is through human activity that\ncomputer technology is designed, developed, tested, installed,\ninitiated and provided with input and instructions to perform\nspecified tasks. Without this human activity, computers would do\nnothing. Attributing independent moral agency to computers, Johnson\nclaims, disconnects them from the human behavior that creates, deploys\nand uses them. It turns the attention away from the forces that shape\ntechnological development and limits the possibility for intervention.\nFor instance, it leaves the issue of sorting out who is responsible\nfor dealing with malfunctioning or immoral artificial agents or who\nshould make amends for the harmful events they may cause. It postpones\nthe question of who has to account for the conditions under which\nartificial agents are allowed to operate (Noorman 2009).\n\nYet, technologies can still be part of moral action, without being a\nmoral agent. Several philosophers have stressed that moral\nresponsibility cannot be properly understood without recognizing the\nactive role of technology in shaping human action (Jonas 1984; Verbeek\n2006; Johnson and Powers 2005; Nyholm 2018). Johnson, for\ninstance, claims that although computers are not moral agents, the\nartifact designer, the artifact, and the artifact user should all be\nthe focus of moral evaluation as they are all at work in an action\n(Johnson 2006). Humans create these artifacts and inscribe in them\ntheir particular values and intentions to achieve particular effects\nin the world and in turn these technological artifacts influence what\nhuman beings can and cannot do and affect how they perceive and\ninterpret the world.\n\nSimilarly, Verbeek maintains that technological artifacts alone do not\nhave moral agency, but moral agency is hardly ever\n\u2018purely\u2019 human. Moral agency generally involves a\nmediating artifact that shapes human behavior, often in way not\nanticipated by the designer (2008). Moral decisions and actions are\nco-shaped by technological artifacts. He suggests that in all forms of\nhuman action there are three forms of agency at work: 1) the agency of\nthe human performing the action; 2) the agency of the designer who\nhelped shaped the mediating role of the artifacts and 3) the artifact\nmediating human action. The agency of artifacts is inextricably linked\nto the agency of its designers and users, but it cannot be reduced to\neither of them. For him, then, a subject that acts or makes moral\ndecisions is a composite of human and technological components. Moral\nagency is not merely located in a human being, but in a complex blend\nof humans and technologies.\n\nIn later papers, Floridi explores the concept of distributed moral\nactions (2013, 2016). He argues that some moral significant outcomes\ncannot be reduced to the moral significant actions of some\nindividuals. Morally neutral actions of several individuals can still\nresult in morally significant events. Individuals might not have\nintended to cause harm, but nevertheless their combined actions may\nstill result in moral harm to someone or something. In order to deal\nwith the problem of subsequently assigning moral responsibility for\nsuch distributed moral actions, he argues that the focus of analysis\nshould shift from the agents to the patients of moral actions. A moral\naction can then be evaluated in terms of the harm to the patient,\nregardless of the intentions of the agents involved. Assigning\nresponsibility then focuses on whether or not an agent is causally\naccountable for the outcome and on adjusting their behavior to prevent\nharm. If the agents causally accountable - be they artificial or\nbiological - are autonomous, can interact with each other and their\nenvironments and can learn from their interactions they can be held\nresponsible for distributed moral actions, according to Floridi\n(2016).\n3. Rethinking the concept of moral responsibility\n\nIn light of the noted difficulties in ascribing moral responsibility,\nseveral authors have critiqued the way in which the concept is used\nand interpreted in relation to computing. They claim that the\ntraditional models or frameworks for dealing with moral responsibility\nfall short and propose different perspectives or interpretations to\naddress some of the difficulties. Some of these will be discussed in\nthis section. \n3.1 Assigning responsibility\n\nOne approach is to rethink how moral responsibility is assigned\n(Gotterbarn 2001; Waelbers 2009). When it comes to computing\npractitioners, Gotterbarn identifies a potential to side-step or avoid\nresponsibility by looking for someone else to blame. He attributes\nthis potential to two pervasive misconceptions about responsibility.\nThe first misconception is that computing is an ethically neutral\npractice. That is, according to Gotterbarn, the misplaced belief that\ntechnological artifacts and the practices of building them are\nethically neutral is often used to justify a narrow\ntechnology-centered focus on the development of computer system\nwithout taking the broader context in which these technologies operate\ninto account. This narrow focus can have detrimental consequences.\nGotterbarn gives the tragic case of a patient\u2019s death as a\nresult of a faulty X-ray device as an example. A programmer was given\nthe assignment to write a program that could lower or raise the X-ray\ndevice on a pole, after an X-ray technician set the required height.\nThe programmer focused on solving the given puzzle, but failed to take\naccount of the circumstances in which the device would be used and the\ncontingencies that might occur. He, thus, did not consider the\npossibility that a patient could accidentally be in the way of the\ndevice moving up and down the pole. This oversight eventually resulted\nin a tragic accident. A patient was crushed by the device, when a\ntechnician set the device to tabletop height, not realizing that the\npatient was still underneath it. According to Gotterbarn, computer\npractitioners have a moral responsibility to consider such\ncontingencies, even though they may not be legally required to do so.\nThe design and use of technological artifacts is a moral activity and\nthe choice for one particular design solution over another has real\nand material consequences.\n\nThe second misconception is that responsibility is only about\ndetermining blame when something goes wrong. Computer practitioners,\naccording to Gotterbarn, have conventionally adopted a malpractice\nmodel of responsibility that focuses on determining the appropriate\nperson to blame for harmful incidents (2001). This malpractice model\nleads to all sorts of excuses to shirk responsibility. In particular,\nthe complexities that computer technologies introduce allow computer\npractitioners to side-step responsibility. The distance between\ndevelopers and the effects of the use of the technologies they create\ncan, for instance, be used to claim that there is no direct and\nimmediate causal link that would tie developers to a malfunction.\nDevelopers can argue that their contribution to the chain of events\nwas negligible, as they are part of a team or larger organization and\nthey had limited opportunity to do otherwise. The malpractice model,\naccording to Gotterbarn, entices computer practitioners to distance\nthemselves from accountability and blame.\n\nThe two misconceptions are based on a particular retrospective view of\nresponsibility that places the focus on that which exempts one from\nblame and liability. In reference to Ladd, Gotterbarn calls this\nnegative responsibility and distinguishes it from positive\nresponsibility (see also Ladd 1989). Positive responsibility\nemphasizes \u201cthe virtue of having or being obliged to have regard\nfor the consequences that his or her actions have on others\u201d\n(Gotterbarn 2001, p. 227). Positive responsibility entails that part\nof the professionalism of computer experts is that they strive to\nminimize foreseeable undesirable events. It focuses on what ought to\nbe done rather than on blaming or punishing others for irresponsible\nbehavior. Gotterbarn argues that the computing professions should\nadopt a positive concept of responsibility, as it emphasizes the\nobligations and duties of computer practitioners to have regard for\nthe consequences of one\u2019s actions and to minimize the\npossibility of causing harm. Computer practitioners have a moral\nresponsibility to avoid harm and to deliver a properly working\nproduct, according to him, regardless of whether they will be held\naccountable if things turn out differently.\n\nThe emphasis on the positive moral responsibility of computer\npractitioners raises the question of how far this responsibility\nreaches, in particular in light of systems that many hands help create\nand the difficulties involved in anticipating contingencies that might\ncause a system to malfunction (Stieb 2008; Miller 2008). To what\nextent can developers and manufacturers be expected to exert\nthemselves to anticipate or prevent the consequences of the use of\ntheir technologies or possible \u2018bugs\u2019 in their code?\nComputer systems today are generally incomprehensible to any single\nprogrammer and it seems unlikely that complex computer systems can be\ncompletely error free. Martin argues in this respect that developers\nand the companies that decide to sell a computer technology\ninto a particular context are responsible for the ethical\nimplication of the use of these technologies in that context (2019).\nThey are responsible for these implications because they are\nknowledgeable about the design decisions and are in a unique position\nto inscribe in the technology particular ideas (and biases) about what\nthe technology should do and how it should do it. Thus, a company that\ncreates and sells a risk-assessment system into the context of\njudicial decision-making is responsible for the ethical implications\nof biases resulting from its use and its opaqueness. The company\nwillingly \u201ctakes on the obligation to understand the values of\nthe decision to ensure the algorithms\u2019 ethical implications is\ncongruent with the context\u201d (p. 10). This, however, leaves open\nthe question of what their responsibility is outside of that context.\nShould manufacturers of mobile phones have anticipated that their\nproducts would be used in roadside bombs? Manufacturers and their\ndesigners and engineers cannot foresee all the possible conditions\nunder which their products will eventually operate. Moreover, how much\ncontrol should a person have to be or feel responsible for the outcome\nof events? Such questions speak to what Santonio de Sio and Meccaci\n(2020) call the \u201cactive responsibility gap\u201d. They\ndescribed active responsibility in much the same way as positive\nresponsibility, in that it relates to the moral obligations of persons\nto ensure that the behavior of the systems they design, control, or\nuse minimizes harm. A gap in this responsibility, according to them,\nresults from these persons not being sufficiently aware, capable and\nmotivated to see and act according to these obligations (p.\n1059).\n\nTo address gaps in active responsibility (as well as backward-looking\ngaps in responsibility), Santonio de Sio and Meccaci suggest an\napproach that underlines the need to look at the broader\nsociotechnical system of human agents and technologies. Assigning\nresponsibility requires looking at the whole chain of design,\ndevelopment and use from a social, technical as well as organizational\nperspective. Each element in this change can be adjusted in an effort\nto address responsibility gaps, including the design of the computer\nsystems as well as the organization that uses it. They base their\napproach on the idea of designing sociotechnial systems for\nmeaningful human control as developed by Santonio de Sio and\nvan den Hoven (2018). Meaningful human control is a concept that\noriginally gained currency in the context of autonomous lethal weapons\nas an approach to addressing responsibility gaps. The question of what\nit means to have control over a technological system becomes\nparticularly pertinent in situations where weapon systems are\ndelegated tasks involved in target selection and engagement (Ekelhof\n2019). Santoni di Sio and van den Hoven (2018) developed their\nconception of meaningful human control to get a more\n\u2018actionable analysis of control\u2019 that can help engineers,\ncomputing professionals, policy makers and designers think about how\nto design sociotechnical systems with responsibility in mind.\n\nSantonio de Sio and van den Hoven assume that technology is part of\nthe decisional mechanisms through which human agents carry out actions\nin the world and these mechanisms should be responsive to moral\nreasons for an agent to have control. Meaningful human control is thus\nconditional on the extent to which an outcome can be connected to the\ndecisional mechanisms of human agents. To elucidate these connections,\nthey formulate two necessary conditions for meaningful control called\ntracking and tracing.\n\nTracking requires that the whole sociotechnical system of\ntechnical, human and organizational elements should be responsive to\nmoral reasons of the relevant agents and to the relevant facts of the\ncircumstances. That is, the behaviour of the system should reflect the\nreasons, values, and intentions of these actors given particular\ncircumstances. For example, if a machine learning system is trained to\ndistinguish huskies from wolves, the system should act according to\nthe relevant reasons for doing this (e.g. alarming a farmer to the\npresence of a wolf). A machine learning system that is trained to make\nthis distinction, but has only been shown pictures of wolves in the\nsnow and huskies in more urban environments, it may deduce that the\nrelevant distinguishing feature is snow. When shown a picture of a\nwolf in urban environment, it might subsequently misclassify the wolf\nas a husky. In this case, the system did not properly track the\nreasons of relevant human agents and the facts of the environment.\nNote that this tracking relation can involve multiple human agents\nalong the chain. That is, the moral reasons do not necessarily have to\ncome from the operator or user, they can also come from policy makers,\ndesigners or programmers.\n\nTracing requires that outcomes can be traced back to earlier\ndecisions made by human agents that place them in the position\nresulting in the outcome. An example that the authors give is a drunk\ndriver causing a serious accident. Even if the driver does not fulfill\nthe conditions of responsibility at the time of the accident - because\nof mental incapacitation \u2013 the driver did make the earlier\ndecision of drinking too much. The tracing condition, as\nSantoni di Sio and van den Hoven formulate it, assumes that it is\npossible that more than one human agent is involved in the actions\nthat led to the outcome and that their actions are mediated by\nnon-human systems. According to them, the tracing condition requires\nthat the whole sociotechnical system is designed such that at least\none human agent can have sufficient knowledge and moral awareness to\nbe a potential target of legitimate response for the behavior of the\nsystem.\n\nSantonio de Sio and van den Hoven\u2019s understanding of meaningful\nhuman control does not only have implications for the design of\ncomputer systems, but also for the design of the environment and the\nsocial and institutional practices. Tracking and tracing of relevant\nhuman moral reasons occurs on all these levels of design.\n3.2 Responsibility as practice\n\nSantonio de Sio and van den Hoven\u2019s analysis of meaningful human\ncontrol draws attention to the social function of moral\nresponsibility, which provides yet another perspective on the issue\n(Stahl 2006; Tigard 2021b). Both prospectively and retrospectively,\nresponsibility works to organize social relations between people and\nbetween people and institutions. It sets expectations between people\nfor the fulfillment of certain obligations and duties and provides the\nmeans to correct or encourage certain behavior. For instance, a\nrobotics company is expected to build in safeguards that prevent\nrobots from harming humans. If the company fails to live up to this\nexpectation, it will be held accountable and in some cases it will\nhave to pay for damages or undergo some other kind of punishment. The\npunishment or prospect of punishment can encourage the company to have\nmore regard for system safety, reliability, sound design and the risks\ninvolved in their production of robots. It might trigger the company\nto take actions to prevent future accidents. Yet, it might also\nencourage it to find ways to shift the blame. The idea that\nresponsibility is about interpersonal relationships and expectations\nabout duties and obligations places the focus on the practices of\nholding someone responsible (Strawson 1962, Talbert 2022).\n\nThe particular practices and social structures that are in place to\nascribe responsibility and hold people accountable, have an influence\non how we relate to technologies. Just before the turn of the century,\nNissenbaum already noted that the difficulties in attributing moral\nresponsibility can, to a large extent, be traced back to the\nparticular characteristics of the organizational and cultural context\nin which computer technologies are embedded. She argued that how we\nconceive of the nature, capacities and limitations of computing is of\ninfluence on the answerability of those who develop and use computer\ntechnologies (1997). At the time, she observed a systematic erosion of\naccountability in our increasingly computerized society, where she\nconceived of accountability as a value and a practice that places an\nemphasis on preventing harm and risk. Accountability means there will\nbe someone, or several people, to answer not only for the malfunctions\nin life-critical systems that cause or risk grave injuries and cause\ninfrastructure and large monetary losses, but even for the malfunction\nthat cause individual losses of time, convenience, and contentment\n(1994, p. 74). It can be used as \u201ca powerful tool for motivating\nbetter practices, and consequently more reliable and trustworthy\nsystems\u201d (1997, p. 43). Holding people accountable for the harms\nor risks caused by computer systems provides a strong incentive to\nminimize them and can provide a starting point for assigning just\npunishment.\n\nCultural and organizational practices, at the time, however seemed to\ndo the opposite, due to \u201cthe conditions under which computer\ntechnologies are commonly developed and deployed, coupled with popular\nconceptions about the nature, capacities and limitations of\ncomputing\u201d (p. 43). Nissenbaum identified four barriers to\naccountability in society: (1) the problem of many hands, (2) the\nacceptance of computer bugs as an inherent element of large software\nsystems, (3) using the computer as scapegoat and (4) ownership without\nliability. According to Nissenbaum people have a tendency to shirk\nresponsibility and to shift the blame to others when accidents occur.\nThe problem of many hands and the idea that software bugs are an\ninevitable by-product of complex computer systems are too easily\naccepted as excuses for not answering for harmful outcomes. People are\nalso inclined to point the finger at the complexity of the computer\nand argue that \u201cit was the computer\u2019s fault\u201d when\nthings go wrong. Finally, she perceived a tendency of companies to\nclaim ownership of the software they developed, but to dismiss the\nresponsibilities that come with ownership. To illustrate, she pointed\nto extended license agreements that assert a manufacturer\u2019s\nownership of software, but disclaim any accountability for the quality\nor performance of the product. \n\nThese four barriers, Nissenbaum argued, stand in the way of a\n\u201cculture of accountability\u201d that is aimed at maintaining\nclear lines of accountability. Such a culture fosters a strong sense\nof responsibility as a virtue to be encouraged and everyone connected\nto an outcome of particular actions is answerable for it.\nAccountability, according to Nissenbaum, is different from liability.\nLiability is about looking for a person to blame and to compensate for\ndamages suffered after the event. Once that person has been found,\nothers can be let \u2018off the hook\u2019, which may encourage\npeople to look for excuses, such as blaming the computer.\nAccountability, however, applies to all those involved. It requires a\nparticular kind of organizational context, one in which answerability\nworks to entice people to pay greater attention to system safety,\nreliability and sound design, in order to establish a culture of\naccountability (see also Martin 2019, Herkert et AL 2020) . An\norganization that places less value on accountability and that has\nlittle regards for responsibilities in organizing their production\nprocesses is more likely to allow their technological products to\nbecome incomprehensible. Nissenbaum\u2019s analysis illustrates that\nour practices of holding someone responsible - the established ways of\nholding people to account and of conveying expectations about duties\nand obligations - are continuously changing and negotiated, partly as\na response to the introduction of new technologies (see also Noorman\n2012).\n\nA lack of such a culture of accountability can lead to responsibility\nbeing attributed to the wrong people. These people become, what\nMadeleine Clare Elish calls the moral crumple zone (2019).\nThese human actors absorb responsibility, even though they only have\nvery limited or no control over the systems they work with. Elish\nargues that, given the complexity of technological systems, the media\nand the public tend to blame accidents on human error and misattribute\nresponsibility to the nearest human operator, such as a pilot or\nmaintenance personnel, rather than the technological systems or the\ndecision-makers higher up the chain.\n\nNissenbaum argued that the context in which technologies are developed\nand used has a significant influence on the ascription of moral\nresponsibility, but several authors have stressed that moral\nresponsibility cannot be properly understood without recognizing the\nactive role of technology in shaping human action (Jonas 1984; Verbeek\n2006; Johnson and Powers 2005; Waelbers 2009). According to Johnson\nand Powers it is not enough to just look at what humans intend and do.\n\u201cAscribing more responsibility to persons who act with\ntechnology requires coming to grips with the behavior of the\ntechnology\u201d (2005, p. 107). One has to consider the\nvarious ways in which technological artifacts mediate human actions.\nMoral responsibility is, thus, not only about how the actions of a\nperson or a group of people affect others in a morally significant\nway; it is also about how their actions are shaped by technology.\nMoral responsibility from this perspective is not located in an\nindividual or an interpersonal relationship, but is distributed among\nhumans and technologies.\n4. Conclusion\n\nComputer technologies have challenged conventional conceptions of\nmoral responsibility and have raised questions about how to distribute\nresponsibility appropriately. Can human beings still be held\nresponsible for the behavior of complex computer technologies that\nthey have limited control over or understanding of? Are human beings\nthe only agents that can be held morally responsible or can the\nconcept of moral agent be extended to include artificial computational\nentities? In response to such questions philosophers have reexamined\nthe concepts of moral agency and moral responsibility. Although there\nis no clear consensus on what these concepts should entail in an\nincreasingly digital society, what is clear from the discussions is\nthat any reflection on these concepts will need to address how these\ntechnologies affect human action and where responsibility for action\nbegins and ends.\n", "bibliography": {"categories": [], "cat_ref_text": {"ref_list": ["Allen, C. &amp; W. Wallach, 2012. \u201cMoral Machines.\nContradiction in Terms or Abdication of Human Responsibility?\u201d\nin P. Lin, K. Abney, and G. Bekey (eds.), <em>Robot ethics. The ethics\nand social implications of robotics.</em> Cambridge, Massachusetts:\nMIT Press.", "Allen, C., G. Varner &amp; J. Zinser, 2000. \u201cProlegomena to\nany Future Artificial Moral Agent,\u201d <em>Journal of Experimental\nand Theoretical Artificial Intelligence</em>, 12: 251\u2013261.", "Allen, C. W. Wallach &amp; I. Smit, 2006. \u201cWhy Machine\nEthics?\u201d <em>Intelligent Systems, IEEE ,</em>\n21(4):12\u201317.", "Angwin, J., J. Larson, S. Mattu &amp; L. Kirchner, 2016.\n\u201cMachine Bias. There is software that is used across the county\nto predict future criminals. And it is biased against blacks\u201d,\n<em>ProPublica</em>, May 23,\n 2016,<a href=\"https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\" target=\"other\">Angwin et al. 2016 available online</a>", "Asaro, P., 2011. \u201cA Body to Kick, But Still No Soul to Damn:\nLegal Perspectives on Robotics,\u201d in P. Lin, K. Abney, and G.\nBekey (eds.) <em>Robot Ethics: The Ethical and Social Implications of\nRobotics</em>, Cambridge, MA: MIT Press.", "Bechtel, W., 1985. \u201cAttributing Responsibility to Computer\nSystems,\u201d <em>Metaphilosophy</em>, 16(4): 296\u2013306", "Bijker, W. E., T. P. Hughes, &amp; T. Pinch, 1987. <em>The Social\nConstruction of Technological Systems: New Directions in the Sociology\nand History of Technology</em>, London, UK: The MIT Press.", "Bovens, M. &amp; S. Zouridis, 2002. \u201cFrom street-level to\nsystem-level bureaucracies: how information and communication\ntechnology is transforming administrative discretion and\nconstitutional control,\u201d <em>Public Administration Review</em>,\n62(2):174\u2013184.", "Boyd, D. &amp; K. Crawford, 2012. \u201cCritical Questions for\nBig Data: Provocations for a Cultural, Technological, and Scholarly\nPhenomenon,\u201d <em>Information, Communication, &amp; Society</em>\n15(5): 662\u2013679.", "Bryson, J.J., 2018. \u201cPatiency is not a virtue: the design of\nintelligent systems and systems of ethics,\u201d <em>Ethics and\nInformation Technology</em>, 20(1): 15-26.", "Cervantes, J.A., L\u00f3pez, S., Rodr\u00edguez, L.F.,\nCervantes, S., Cervantes, F. and Ramos, F., 2020.\u201cArtificial\nmoral agents: A survey of the current status,\u201d <em>Science and\nEngineering Ethics</em>, 26(2): 501-532.", "Coeckelbergh, M., 2009. \u201cVirtual moral agency, virtual moral\nresponsibility: on the moral significance of the appearance,\nperception, and performance of artificial agents,\u201d <em>AI &amp;\nSociety</em>, 24: 181\u2013189.", "\u2013\u2013\u2013, 2012. \u201cMoral responsibility,\ntechnology, and experiences of the tragic: From Kierkegaard to\noffshore engineering,\u201d <em>Science and Engineering Ethics</em>,\n18(1): 35-48.", "\u2013\u2013\u2013, 2013. \u201cDrones, information\ntechnology, and distance: mapping the moral epistemology of remote\nfighting,\u201d <em>Ethics and Information Technology</em>, 15(2):\n87-98.", "Coeckelbergh, M., 2020. \u201cArtificial intelligence,\nresponsibility attribution, and a relational justification of\nexplainability,\u201d <em>Science and Engineering Ethics</em>, 26(4):\n2051-2068.", "Coeckelbergh, M. &amp; R. Wackers, 2007. \u201cImagination,\nDistributed Responsibility and Vulnerable Technological Systems: the\nCase of Snorre A,\u201d <em>Science and Engineering Ethics</em>,\n13(2): 235\u2013248.", "Cummings, M. L., 2004. \u201cAutomation Bias in Intelligent Time\nCritical Decision Support Systems,\u201d published online: 19 Jun 2012,\nAmerican Institute of Aeronautics and Astronautics.\ndoi:10.2514/6.2004-6313", "Diakopoulos, N., 2020. \u201cTransparency\u201d. In M. Dubber,\nF. Pasquale, &amp; S. Das (Eds.), <em>Oxford handbook of ethics and\nAI</em> (pp. 197\u2013214). Oxford University Press.", "Dennett, D. C., 1997. \u201cWhen HAL Kills, Who\u2019s to Blame?\nComputer Ethics,\u201d in <em>HAL\u2019s Legacy: 2001\u2019s\nComputer as Dream and Reality</em>, D. G. Stork (ed.), Cambridge, MA:\nMIT Press.", "Denning, P. J., 1989. \u201cThe Science of Computing: The\nInternet Worm,\u201d <em>American Scientist</em>, 77(2):\n126\u2013128.", "Dignum, V., 2020. \u201cResponsibility and artificial\nintelligence,\u201d <em>The Oxford Handbook of Ethics of AI</em>,\nMarkus D. Drubber et al. (eds.), Oxford: Oxford University Press, pp.\n214-231.", "Doorn, N. &amp; van de Poel, I., 2012. \u201cEditors Overview:\nMoral Responsibility in Technology and Engineering,\u201d <em>Science\nand Engineering Ethics</em>, 18: 1\u201311.", "Ekelhof, M., 2019. \u201cMoving beyond semantics on autonomous\nweapons: Meaningful human control in operation,\u201d <em>Global\nPolicy</em>, 10(3): 343-348.", "Elish, M.C., 2019. \u201cMoral crumple zones: Cautionary tales in\nhuman-robot interaction,\u201d <em>Engaging Science, Technology, and\nSociety</em>, 5: 40-60.", "Eshleman, A., 2016. \u201cMoral Responsibility,\u201d in <em>The\nStanford Encyclopedia of Philosophy</em> (Winter 2016 Edition), E. N.\nZalta (ed.), URL =\n &lt;<a href=\"https://plato.stanford.edu/archives/win2016/entries/moral-responsibility/\">https://plato.stanford.edu/archives/win2016/entries/moral-responsibility/</a>&gt;", "Eubanks, V., 2018. \u201cAutomating inequality: How high-tech\ntools profile, police, and punish the poor,\u201d New York: St.\nMartin\u2019s Press.", "Felt, U., Fouch\u00e9, R., Miller, C. A., &amp; Smith-Doerr, L.,\n2016. <em>The Handbook of Science and Technology Studies</em>,\nCambridge, MA: MIT Press.", "Fisher, J. M., 1999. \u201cRecent work on moral\nresponsibility,\u201d <em>Ethics</em>, 110(1): 93\u2013139.&gt;", "Floridi, L., &amp; J. Sanders, 2004. \u201cOn the Morality of\nArtificial Agents,\u201d <em>Minds and Machines</em>, 14(3):\n349\u2013379.", "Floridi, L., 2013. \u201cDistributed morality in an information\nsociety,\u201d <em>Science and Engineering Ethics</em>, 19(3):\n727\u2013743.", "\u2013\u2013\u2013, 2016. \u201cFaultless responsibility: on\nthe nature and allocation of moral responsibility for distributed\nmoral actions,\u201d <em>Philosophical Transactions of the Royal\nSociety A</em> (Mathematical Physical and Engineering Sciences),\n374(2083); doi: 10.1098/rsta.2016.0112", "Friedman, B., 1990. \u201cMoral Responsibility and Computer\nTechnology,\u201d Institute of Education Sciences ERIC Number ED321737, \n [<a href=\"https://files.eric.ed.gov/fulltext/ED321737.pdf\" target=\"other\">Friedman 1990 available online</a>].", "\u2013\u2013\u2013 (ed.), 1997. <em>Human Values and the Design\nof Computer Technology</em>, Stanford: CSLI Publications; New York:\nCambridge University Press.", "Gorwa, R., Binns, R., &amp; Katzenbach, C., 2020.\n\u201cAlgorithmic content moderation: Technical and political\nchallenges in the automation of platform governance,\u201d <em>Big\nData &amp; Society</em>, 7(1); first online 28 February 2020.\ndoi:10.1177/2053951719897945", "Gotterbarn D., 2001. \u201cInformatics and professional\nresponsibility,\u201d <em>Science and Engineering Ethics</em>, 7(2):\n221\u2013230.", "Graubard, S. R., 1988. <em>The Artificial Intelligence Debate:\nFalse Starts, Real Foundations</em>, Cambridge, MA: MIT Press.", "Gray, C. H., 1997. \u201cAI at War: The Aegis System in\nCombat,\u201d <em>Directions and Implications of Advanced\nComputing</em>, D. Schuler, (ed.), New York: Ablex, pp.\n62\u201379.", "Gunkel, D. J., 2020. \u201cA vindication of the rights of\nmachines,\u201d in <em>Machine Ethics and Robot Ethics</em>, W.\nWallach and P. Asaro (eds.), London: Routledge, pp. 511-530.", "Hakli, R. and M\u00e4kel\u00e4, P., 2019. \u201cMoral\nresponsibility of robots and hybrid agents,\u201d <em>The\nMonist</em>, 102(2): 259-275.", "Hart, H. L. A., 1968. <em>Punishment and Responsibility</em>,\nOxford: Oxford University Press.", "Herkert, J., Borenstein, J., &amp; Miller, K., 2020. \u201cThe\nBoeing 737 MAX: Lessons for engineering ethics\u201d. <em>Science and\nengineering ethics</em>, <em>26</em>, 2957-2974.", "Hughes, T.P., 1987. \u201cThe evolution of Large Technological\nSystem,\u201d in W. E. Bijker, T. P. Hughes, &amp; T. Pinch (eds.),\n<em>The Social Construction of Technological Systems</em>, Cambridge,\nMA: The MIT Press, pp. 51\u201382.", "IJsselsteijn, W., Y. de Korte, C. Midden, B. Eggen, &amp; E. Hoven\n(eds.), 2006. <em>Persuasive Technology</em>, Berlin:\nSpringer-Verlag.", "Johnson, D. G., 2001. <em>Computer Ethics</em>, 3rd edition, Upper\nSaddle River, New Jersey: Prentice Hall.", "\u2013\u2013\u2013, 2006. \u201cComputer Systems: Moral\nEntities but not Moral Agents,\u201d <em>Ethics and Information\nTechnology</em>, 8: 195\u2013204.", "Johnson, D. G. &amp; T. M. Power, 2005. \u201cComputer systems\nand responsibility: A normative look at technological\ncomplexity,\u201d <em>Ethics and Information Technology</em>, 7:\n99\u2013107.", "Jonas, H., 1984. <em>The Imperative of Responsibility. In search\nof an Ethics for the Technological Age</em>, Chicago: The Chicago\nUniversity Press.", "Kroes, P.&amp; P.P. Verbeek (eds.), 2014. <em>The Moral Status of\nTechnical Artefacts</em>, Dordrecht: Springer", "Kuflik, A., 1999. \u201cComputers in Control: Rational Transfer\nof Authority or Irresponsible Abdication of Authority?\u201d\n<em>Ethics and Information Technology</em>, 1: 173\u2013184.", "Ladd. J., 1989. \u201cComputers and Moral Responsibility. A\nFramework for an Ethical Analysis,\u201d in C.C. Gould (ed.), <em>The\nInformation Web. Ethical and Social Implications of Computer\nNetworking</em>, Boulder, Colorado: Westview Press, pp.\n207\u2013228.", "Latour, B., 1992. \u201cWhere are the Missing Masses? The\nSociology of a Few Mundane Artefacts,\u201d in W. Bijker &amp; J. Law\n(eds.), <em>Shaping Technology/Building Society: Studies in\nSocio-Technical Change</em>, Cambridge, Massachusetts: The MIT press,\npp. 225\u2013258.", "Leveson, N. G. &amp; C. S. Turner, 1993. \u201cAn Investigation\nof the Therac-25 Accidents,\u201d <em>Computer</em>, 26(7):\n18\u201341.", "Leveson, N., 1995. \u201cMedical Devices: The Therac-25,\u201d\nin N. Leveson, <em>Safeware. System, Safety and Computers</em>,\nBoston: Addison-Wesley.", "Martin, K., 2019. \u201cEthical implications and accountability\nof algorithms,\u201d <em>Journal of Business Ethics</em>, 160(4):\n835-850.", "Matthias, A., 2004. \u201cThe responsibility gap: Ascribing\nresponsibility for the actions of learning automata,\u201d <em>Ethics\nand Information Technology</em>, 6: 175\u2013183.", "McCorduck, P., 1979. <em>Machines Who Think</em>, San Francisco:\nW.H. Freeman and Company.", "Miller, K. W., 2008. \u201cCritiquing a critique,\u201d\n<em>Science and Engineering Ethics</em>, 14(2): 245\u2013249.", "Moor, J.H., 2006. \u201cThe Nature, Importance, and Difficulty of\nMachine Ethics,\u201d <em>Intelligent Systems</em> (IEEE), 21(4):\n18\u201321.", "Nissenbaum, H., 1994. \u201cComputing and Accountability,\u201d\n<em>Communications of the Association for Computing Machinery</em>,\n37(1): 72\u201380.", "\u2013\u2013\u2013, 1997. \u201cAccountability in a\nComputerized Society,\u201d in B. Friedman (ed.), <em>Human Values\nand the Design of Computer Technology</em>, Cambridge: Cambridge\nUniversity Press, pp. 41\u201364.", "Nass, C. &amp; Y. Moon, 2000. \u201cMachines and mindlessness:\nSocial responses to computers,\u201d <em>Journal of Social\nIssues</em>, 56(1): 81\u2013103.", "Noorman, M., 2009. <em>Mind the Gap: A Critique of\nHuman/Technology Analogies in Artificial Agents Discourse</em>,\nMaastricht: Universitaire Pers Maastricht.", "\u2013\u2013\u2013, 2012. \u201cResponsibility Practices and\nUnmanned Military Technologies,\u201d <em>Science and Engineering\nEthics</em>, 20(3): 809\u2013826.", "Nihl\u00e9n Fahlquist, J., Doorn, N., &amp;Van de Poel, I.,\n2015. \u201cDesign for the value of responsibility,\u201d in\n<em>Handbook of ethics, values and technological design\u202f\u202f:\nSources, Theory, Values and Application Domains</em>, Jeroen van den\nHoven, Ibo van de Poel and Pieter Vermaas (eds.), Dordrecht:\nSpringer.", "Nyholm, S., 2018. \u201cAttributing agency to automated systems:\nReflections on human-robot collaborations and\nresponsibility-loci,\u201d <em>Science and engineering ethics</em>,\n24(4): 1201-1219.", "Parasuraman, R. &amp; V. Riley, 1997. \u201cHumans and\nAutomation: Use, Misuse, Disuse, Abuse,\u201d <em>Human Factors: the\nJournal of the Human Factors Society</em>, 39(2): 230\u2013253.", "Pasquale, F., 2015. <em>The black box society: The secret\nalgorithms that control money and information</em>. Cambridge, MA:\nHarvard University Press.", "Polder-Verkiel, S. E., 2012. \u201cOnline responsibility: Bad\nsamaritanism and the influence of internet mediation,\u201d\n<em>Science and engineering ethics</em>, 18(1): 117-141.", "Ravenscraft, E., (2020). \u201cHow to Spot\u2014and\nAvoid\u2014Dark Patterns on the Web\u201d, <em>Wired</em>, July 29,\n <a href=\"https://www.wired.com/story/how-to-spot-avoid-dark-patterns/\" target=\"other\">Ravenscraft 2020 available online</a>.", "Reeves, B. &amp; C. Nass, 1996. <em>The Media Equation: How People\nTreat Computers, Television, and New Media Like Real People and\nPlaces</em>, Cambridge: Cambridge University Press.", "Rosenthal-von der P\u00fctten, A. M., Kr\u00e4mer, N. C.,\nHoffmann, L., Sobieraj, S., &amp; Eimler, S. C., 2013. \u201cAn\nexperimental study on emotional reactions towards a robot,\u201d\n<em>International Journal of Social Robotics</em>, 5(1):\n17\u201334.", "Sack, W., 1997. \u201cArtificial Human Nature,\u201d <em>Design\nIssues</em>, 13: 55\u201364.", "Santoni de Sio, F. and Van den Hoven, J., 2018. \u201cMeaningful\nhuman control over autonomous systems: A philosophical account,\u201d\n<em>Frontiers in Robotics and AI</em>, 5, first online 28 February\n2018. doi:10.3389/frobt.2018.00015", "Santoni de Sio, F., &amp; Mecacci, G., 2021. \u201cFour\nresponsibility gaps with artificial intelligence: Why they matter and\nhow to address them,\u201d <em>Philosophy &amp; Technology</em>, 34:\n1057\u20131084.", "Sartor, G. and M. Viola de Azevedo Cunha, 2010. \u201cThe Italian\nGoogle-Case: Privacy, Freedom of Speech and Responsibility of\nProviders for User-Generated Contents,\u201d <em>International\nJournal of Law and Information Technology</em>, 18(4):\n356\u2013378.", "Searle, J. R., 1980. \u201cMinds, brains, and programs\u201d\n<em>Behavioral and Brain Sciences</em>, 3(3): 417\u2013457.", "Singel, R., 2010. \u201cDoes Italy\u2019s Google Conviction\nPortend More Censorship?\u201d <em>Wired</em> (February 24th, 2010),\n <a href=\"http://www.wired.com/threatlevel/2010/02/italy-google-analysis/\" target=\"other\">Singel 2010 available online</a>.", "Sparrow, R., 2007. \u201cKiller Robots,\u201d <em>Journal of\nApplied Philosophy</em>, 24(1): 62\u201377.", "Stahl, B. C., 2004. \u201cInformation, Ethics, and Computers: The\nProblem of Autonomous Moral Agents,\u201d <em>Minds and\nMachines</em>, 14: 67\u201383.", "\u2013\u2013\u2013, 2006. \u201cResponsible Computers? A Case\nfor Ascribing Quasi-Responsibility to Computers Independent of\nPersonhood or Agency,\u201d <em>Ethics and Information\nTechnology</em>, 8: 205\u2013213.", "Stieb, J. A., 2008. \u201cA Critique of Positive Responsibility\nin Computing,\u201d <em>Science and Engineering Ethics</em>, 14(2):\n219\u2013233.", "Strawson, P., 1962. \u201cFreedom and Resentment,\u201d in\n<em>Proceedings of the British Academy</em>, 48: 1-25.", "Suchman, L., 1998. \u201cHuman/machine reconsidered,\u201d\n<em>Cognitive Studies</em>, 5(1): 5\u201313.", "Sullins, J. P., 2006. \u201cWhen is a Robot a Moral Agent?\u201d\n<em>International Review of</em> <em>Information Ethics</em>, 6(12):\n23\u201329.", "Swierstra, T., Waelbers, K., 2012. \u201cDesigning a Good Life: A\nMatrix for the Technological Mediation of Morality,\u201d <em>Science\nand Engineering Ethics</em>, 18: 157\u2013172.\ndoi:10.1007/s11948-010-9251-1", "Taddeo, M. and L. Floridi, 2015. \u201cThe Debate on the Moral\nResponsibilities of Online Service Providers,\u201d <em>Science and\nEngineering Ethics</em>, 22(6): 1575\u20131603.", "Talbert, Matthew, 2022. \u201cMoral Responsibility,\u201d\n<em>The Stanford Encyclopedia of Philosophy</em> (Fall 2022 Edition),\nEdward N. Zalta &amp; Uri Nodelman (eds.) URL = &lt;<a>\nhref=\u201chttps://plato.stanford.edu/archives/win2016/entries/moral-responsibility/\u201d&gt;https://plato.stanford.edu/archives/win2016/entries/moral-responsibility/</a>&gt;.", "Tigard, D. W., 2021a. \u201cResponsible AI and moral\nresponsibility: a common appreciation,\u201d <em>AI and Ethics</em>,\n1(2): 113-117.", "\u2013\u2013\u2013, 2021b. \u201cArtificial moral\nresponsibility: How we can and cannot hold machines\nresponsible,\u201d <em>Cambridge Quarterly of Healthcare Ethics</em>,\n30(3): 435-447.", "U.S. Department of Defense, 2009. \u201cFY2009\u20132034\nUnmanned Systems Integrated Roadmap,\u201d\n <a href=\"https://www.globalsecurity.org/intell/library/reports/2009/dod-unmanned-systems-roadmap_2009-2034.pdf\" target=\"other\">available online </a>.", "Van den Hoven, J., 2002. \u201cWadlopen bij Opkomend Tij: Denken\nover Ethiek en Informatiemaatschappij,\u201d in J. de Mul (ed.),\n<em>Filosofie in Cyberspace</em>, Kampen: Uitgeverij Klement, pp.\n47\u201365.", "V\u00e9liz, C., 2021. \u201cMoral zombies: why algorithms are\nnot moral agents,\u201d <em>AI &amp; Society</em>, 36: 487\u2013497.\ndoi:10.1007/s00146-021-01189-x", "Verbeek, P. P., 2006. \u201cMaterializing Morality: Design Ethics\nand Technological Mediation,\u201d <em>Science, Technology and Human\nValues</em>, 31(3): 361\u2013380.", "Verbeek, P. P., 2021. <em>What Things Do</em>, University Park,\nPA: Pennsylvania State University Press.", "Vidal, J., 2004. \u201cThe alco-lock is claimed to foil\ndrink-drivers. Then the man from the Guardian had a go\n\u2026,\u201d <em>The Guardian</em>, August 5th, 2004.", "Waelbers, K., 2009. \u201cTechnological Delegation:\nResponsibility for the Unintended,\u201d <em>Science &amp;\nEngineering Ethics</em>, 15(1): 51\u201368.", "Wallach, W. and C. Allen, 2009. <em>Moral Machines. Teaching\nRobots Right from Wrong</em>, Oxford: Oxford University Press.", "Whitby, B., 2008. \u201cSometimes it\u2019s hard to be a robot.\nA call for action on the ethics of abusing artificial agents,\u201d\n<em>Interacting with Computers</em>, 20(3): 326\u2013333.", "John Zerilli; John Danaher; James Maclaurin; Colin Gavaghan;\nAlistair Knott; Joy Liddicoat; Merel Noorman, 2021. \u201c7\nAutonomy,\u201d in <em>A Citizens Guide to Artificial\nIntelligence</em>, Cambridge, MA: MIT Press, pp. 107-126.", "Zuboff, S., 1982. \u201cAutomate/Informate: The Two Faces of\nIntelligent Technology,\u201d <em>Organizational Dynamics</em>,\n14(2):5\u201318"]}, "raw_text": "<div id=\"bibliography\">\n<h2><a name=\"Bib\">Bibliography</a></h2>\n<ul class=\"hanging\">\n<li>Allen, C. &amp; W. Wallach, 2012. \u201cMoral Machines.\nContradiction in Terms or Abdication of Human Responsibility?\u201d\nin P. Lin, K. Abney, and G. Bekey (eds.), <em>Robot ethics. The ethics\nand social implications of robotics.</em> Cambridge, Massachusetts:\nMIT Press.</li>\n<li>Allen, C., G. Varner &amp; J. Zinser, 2000. \u201cProlegomena to\nany Future Artificial Moral Agent,\u201d <em>Journal of Experimental\nand Theoretical Artificial Intelligence</em>, 12: 251\u2013261.</li>\n<li>Allen, C. W. Wallach &amp; I. Smit, 2006. \u201cWhy Machine\nEthics?\u201d <em>Intelligent Systems, IEEE ,</em>\n21(4):12\u201317.</li>\n<li>Angwin, J., J. Larson, S. Mattu &amp; L. Kirchner, 2016.\n\u201cMachine Bias. There is software that is used across the county\nto predict future criminals. And it is biased against blacks\u201d,\n<em>ProPublica</em>, May 23,\n 2016,<a href=\"https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\" target=\"other\">Angwin et al. 2016 available online</a></li>\n<li>Asaro, P., 2011. \u201cA Body to Kick, But Still No Soul to Damn:\nLegal Perspectives on Robotics,\u201d in P. Lin, K. Abney, and G.\nBekey (eds.) <em>Robot Ethics: The Ethical and Social Implications of\nRobotics</em>, Cambridge, MA: MIT Press.</li>\n<li>Bechtel, W., 1985. \u201cAttributing Responsibility to Computer\nSystems,\u201d <em>Metaphilosophy</em>, 16(4): 296\u2013306</li>\n<li>Bijker, W. E., T. P. Hughes, &amp; T. Pinch, 1987. <em>The Social\nConstruction of Technological Systems: New Directions in the Sociology\nand History of Technology</em>, London, UK: The MIT Press.</li>\n<li>Bovens, M. &amp; S. Zouridis, 2002. \u201cFrom street-level to\nsystem-level bureaucracies: how information and communication\ntechnology is transforming administrative discretion and\nconstitutional control,\u201d <em>Public Administration Review</em>,\n62(2):174\u2013184.</li>\n<li>Boyd, D. &amp; K. Crawford, 2012. \u201cCritical Questions for\nBig Data: Provocations for a Cultural, Technological, and Scholarly\nPhenomenon,\u201d <em>Information, Communication, &amp; Society</em>\n15(5): 662\u2013679.</li>\n<li>Bryson, J.J., 2018. \u201cPatiency is not a virtue: the design of\nintelligent systems and systems of ethics,\u201d <em>Ethics and\nInformation Technology</em>, 20(1): 15-26.</li>\n<li>Cervantes, J.A., L\u00f3pez, S., Rodr\u00edguez, L.F.,\nCervantes, S., Cervantes, F. and Ramos, F., 2020.\u201cArtificial\nmoral agents: A survey of the current status,\u201d <em>Science and\nEngineering Ethics</em>, 26(2): 501-532.</li>\n<li>Coeckelbergh, M., 2009. \u201cVirtual moral agency, virtual moral\nresponsibility: on the moral significance of the appearance,\nperception, and performance of artificial agents,\u201d <em>AI &amp;\nSociety</em>, 24: 181\u2013189.</li>\n<li>\u2013\u2013\u2013, 2012. \u201cMoral responsibility,\ntechnology, and experiences of the tragic: From Kierkegaard to\noffshore engineering,\u201d <em>Science and Engineering Ethics</em>,\n18(1): 35-48.</li>\n<li>\u2013\u2013\u2013, 2013. \u201cDrones, information\ntechnology, and distance: mapping the moral epistemology of remote\nfighting,\u201d <em>Ethics and Information Technology</em>, 15(2):\n87-98.</li>\n<li>Coeckelbergh, M., 2020. \u201cArtificial intelligence,\nresponsibility attribution, and a relational justification of\nexplainability,\u201d <em>Science and Engineering Ethics</em>, 26(4):\n2051-2068.</li>\n<li>Coeckelbergh, M. &amp; R. Wackers, 2007. \u201cImagination,\nDistributed Responsibility and Vulnerable Technological Systems: the\nCase of Snorre A,\u201d <em>Science and Engineering Ethics</em>,\n13(2): 235\u2013248.</li>\n<li>Cummings, M. L., 2004. \u201cAutomation Bias in Intelligent Time\nCritical Decision Support Systems,\u201d published online: 19 Jun 2012,\nAmerican Institute of Aeronautics and Astronautics.\ndoi:10.2514/6.2004-6313</li>\n<li>Diakopoulos, N., 2020. \u201cTransparency\u201d. In M. Dubber,\nF. Pasquale, &amp; S. Das (Eds.), <em>Oxford handbook of ethics and\nAI</em> (pp. 197\u2013214). Oxford University Press.</li>\n<li>Dennett, D. C., 1997. \u201cWhen HAL Kills, Who\u2019s to Blame?\nComputer Ethics,\u201d in <em>HAL\u2019s Legacy: 2001\u2019s\nComputer as Dream and Reality</em>, D. G. Stork (ed.), Cambridge, MA:\nMIT Press.</li>\n<li>Denning, P. J., 1989. \u201cThe Science of Computing: The\nInternet Worm,\u201d <em>American Scientist</em>, 77(2):\n126\u2013128.</li>\n<li>Dignum, V., 2020. \u201cResponsibility and artificial\nintelligence,\u201d <em>The Oxford Handbook of Ethics of AI</em>,\nMarkus D. Drubber et al. (eds.), Oxford: Oxford University Press, pp.\n214-231.</li>\n<li>Doorn, N. &amp; van de Poel, I., 2012. \u201cEditors Overview:\nMoral Responsibility in Technology and Engineering,\u201d <em>Science\nand Engineering Ethics</em>, 18: 1\u201311.</li>\n<li>Ekelhof, M., 2019. \u201cMoving beyond semantics on autonomous\nweapons: Meaningful human control in operation,\u201d <em>Global\nPolicy</em>, 10(3): 343-348.</li>\n<li>Elish, M.C., 2019. \u201cMoral crumple zones: Cautionary tales in\nhuman-robot interaction,\u201d <em>Engaging Science, Technology, and\nSociety</em>, 5: 40-60.</li>\n<li>Eshleman, A., 2016. \u201cMoral Responsibility,\u201d in <em>The\nStanford Encyclopedia of Philosophy</em> (Winter 2016 Edition), E. N.\nZalta (ed.), URL =\n &lt;<a href=\"https://plato.stanford.edu/archives/win2016/entries/moral-responsibility/\">https://plato.stanford.edu/archives/win2016/entries/moral-responsibility/</a>&gt;</li>\n<li>Eubanks, V., 2018. \u201cAutomating inequality: How high-tech\ntools profile, police, and punish the poor,\u201d New York: St.\nMartin\u2019s Press.</li>\n<li>Felt, U., Fouch\u00e9, R., Miller, C. A., &amp; Smith-Doerr, L.,\n2016. <em>The Handbook of Science and Technology Studies</em>,\nCambridge, MA: MIT Press.</li>\n<li>Fisher, J. M., 1999. \u201cRecent work on moral\nresponsibility,\u201d <em>Ethics</em>, 110(1): 93\u2013139.&gt;</li>\n<li>Floridi, L., &amp; J. Sanders, 2004. \u201cOn the Morality of\nArtificial Agents,\u201d <em>Minds and Machines</em>, 14(3):\n349\u2013379.</li>\n<li>Floridi, L., 2013. \u201cDistributed morality in an information\nsociety,\u201d <em>Science and Engineering Ethics</em>, 19(3):\n727\u2013743.</li>\n<li>\u2013\u2013\u2013, 2016. \u201cFaultless responsibility: on\nthe nature and allocation of moral responsibility for distributed\nmoral actions,\u201d <em>Philosophical Transactions of the Royal\nSociety A</em> (Mathematical Physical and Engineering Sciences),\n374(2083); doi: 10.1098/rsta.2016.0112</li>\n<li>Friedman, B., 1990. \u201cMoral Responsibility and Computer\nTechnology,\u201d Institute of Education Sciences ERIC Number ED321737, \n [<a href=\"https://files.eric.ed.gov/fulltext/ED321737.pdf\" target=\"other\">Friedman 1990 available online</a>].</li>\n<li>\u2013\u2013\u2013 (ed.), 1997. <em>Human Values and the Design\nof Computer Technology</em>, Stanford: CSLI Publications; New York:\nCambridge University Press.</li>\n<li>Gorwa, R., Binns, R., &amp; Katzenbach, C., 2020.\n\u201cAlgorithmic content moderation: Technical and political\nchallenges in the automation of platform governance,\u201d <em>Big\nData &amp; Society</em>, 7(1); first online 28 February 2020.\ndoi:10.1177/2053951719897945</li>\n<li>Gotterbarn D., 2001. \u201cInformatics and professional\nresponsibility,\u201d <em>Science and Engineering Ethics</em>, 7(2):\n221\u2013230.</li>\n<li>Graubard, S. R., 1988. <em>The Artificial Intelligence Debate:\nFalse Starts, Real Foundations</em>, Cambridge, MA: MIT Press.</li>\n<li>Gray, C. H., 1997. \u201cAI at War: The Aegis System in\nCombat,\u201d <em>Directions and Implications of Advanced\nComputing</em>, D. Schuler, (ed.), New York: Ablex, pp.\n62\u201379.</li>\n<li>Gunkel, D. J., 2020. \u201cA vindication of the rights of\nmachines,\u201d in <em>Machine Ethics and Robot Ethics</em>, W.\nWallach and P. Asaro (eds.), London: Routledge, pp. 511-530.</li>\n<li>Hakli, R. and M\u00e4kel\u00e4, P., 2019. \u201cMoral\nresponsibility of robots and hybrid agents,\u201d <em>The\nMonist</em>, 102(2): 259-275.</li>\n<li>Hart, H. L. A., 1968. <em>Punishment and Responsibility</em>,\nOxford: Oxford University Press.</li>\n<li>Herkert, J., Borenstein, J., &amp; Miller, K., 2020. \u201cThe\nBoeing 737 MAX: Lessons for engineering ethics\u201d. <em>Science and\nengineering ethics</em>, <em>26</em>, 2957-2974.</li>\n<li>Hughes, T.P., 1987. \u201cThe evolution of Large Technological\nSystem,\u201d in W. E. Bijker, T. P. Hughes, &amp; T. Pinch (eds.),\n<em>The Social Construction of Technological Systems</em>, Cambridge,\nMA: The MIT Press, pp. 51\u201382.</li>\n<li>IJsselsteijn, W., Y. de Korte, C. Midden, B. Eggen, &amp; E. Hoven\n(eds.), 2006. <em>Persuasive Technology</em>, Berlin:\nSpringer-Verlag.</li>\n<li>Johnson, D. G., 2001. <em>Computer Ethics</em>, 3rd edition, Upper\nSaddle River, New Jersey: Prentice Hall.</li>\n<li>\u2013\u2013\u2013, 2006. \u201cComputer Systems: Moral\nEntities but not Moral Agents,\u201d <em>Ethics and Information\nTechnology</em>, 8: 195\u2013204.</li>\n<li>Johnson, D. G. &amp; T. M. Power, 2005. \u201cComputer systems\nand responsibility: A normative look at technological\ncomplexity,\u201d <em>Ethics and Information Technology</em>, 7:\n99\u2013107.</li>\n<li>Jonas, H., 1984. <em>The Imperative of Responsibility. In search\nof an Ethics for the Technological Age</em>, Chicago: The Chicago\nUniversity Press.</li>\n<li>Kroes, P.&amp; P.P. Verbeek (eds.), 2014. <em>The Moral Status of\nTechnical Artefacts</em>, Dordrecht: Springer</li>\n<li>Kuflik, A., 1999. \u201cComputers in Control: Rational Transfer\nof Authority or Irresponsible Abdication of Authority?\u201d\n<em>Ethics and Information Technology</em>, 1: 173\u2013184.</li>\n<li>Ladd. J., 1989. \u201cComputers and Moral Responsibility. A\nFramework for an Ethical Analysis,\u201d in C.C. Gould (ed.), <em>The\nInformation Web. Ethical and Social Implications of Computer\nNetworking</em>, Boulder, Colorado: Westview Press, pp.\n207\u2013228.</li>\n<li>Latour, B., 1992. \u201cWhere are the Missing Masses? The\nSociology of a Few Mundane Artefacts,\u201d in W. Bijker &amp; J. Law\n(eds.), <em>Shaping Technology/Building Society: Studies in\nSocio-Technical Change</em>, Cambridge, Massachusetts: The MIT press,\npp. 225\u2013258.</li>\n<li>Leveson, N. G. &amp; C. S. Turner, 1993. \u201cAn Investigation\nof the Therac-25 Accidents,\u201d <em>Computer</em>, 26(7):\n18\u201341.</li>\n<li>Leveson, N., 1995. \u201cMedical Devices: The Therac-25,\u201d\nin N. Leveson, <em>Safeware. System, Safety and Computers</em>,\nBoston: Addison-Wesley.</li>\n<li>Martin, K., 2019. \u201cEthical implications and accountability\nof algorithms,\u201d <em>Journal of Business Ethics</em>, 160(4):\n835-850.</li>\n<li>Matthias, A., 2004. \u201cThe responsibility gap: Ascribing\nresponsibility for the actions of learning automata,\u201d <em>Ethics\nand Information Technology</em>, 6: 175\u2013183.</li>\n<li>McCorduck, P., 1979. <em>Machines Who Think</em>, San Francisco:\nW.H. Freeman and Company.</li>\n<li>Miller, K. W., 2008. \u201cCritiquing a critique,\u201d\n<em>Science and Engineering Ethics</em>, 14(2): 245\u2013249.</li>\n<li>Moor, J.H., 2006. \u201cThe Nature, Importance, and Difficulty of\nMachine Ethics,\u201d <em>Intelligent Systems</em> (IEEE), 21(4):\n18\u201321.</li>\n<li>Nissenbaum, H., 1994. \u201cComputing and Accountability,\u201d\n<em>Communications of the Association for Computing Machinery</em>,\n37(1): 72\u201380.</li>\n<li>\u2013\u2013\u2013, 1997. \u201cAccountability in a\nComputerized Society,\u201d in B. Friedman (ed.), <em>Human Values\nand the Design of Computer Technology</em>, Cambridge: Cambridge\nUniversity Press, pp. 41\u201364.</li>\n<li>Nass, C. &amp; Y. Moon, 2000. \u201cMachines and mindlessness:\nSocial responses to computers,\u201d <em>Journal of Social\nIssues</em>, 56(1): 81\u2013103.</li>\n<li>Noorman, M., 2009. <em>Mind the Gap: A Critique of\nHuman/Technology Analogies in Artificial Agents Discourse</em>,\nMaastricht: Universitaire Pers Maastricht.</li>\n<li>\u2013\u2013\u2013, 2012. \u201cResponsibility Practices and\nUnmanned Military Technologies,\u201d <em>Science and Engineering\nEthics</em>, 20(3): 809\u2013826.</li>\n<li>Nihl\u00e9n Fahlquist, J., Doorn, N., &amp;Van de Poel, I.,\n2015. \u201cDesign for the value of responsibility,\u201d in\n<em>Handbook of ethics, values and technological design\u202f\u202f:\nSources, Theory, Values and Application Domains</em>, Jeroen van den\nHoven, Ibo van de Poel and Pieter Vermaas (eds.), Dordrecht:\nSpringer.</li>\n<li>Nyholm, S., 2018. \u201cAttributing agency to automated systems:\nReflections on human-robot collaborations and\nresponsibility-loci,\u201d <em>Science and engineering ethics</em>,\n24(4): 1201-1219.</li>\n<li>Parasuraman, R. &amp; V. Riley, 1997. \u201cHumans and\nAutomation: Use, Misuse, Disuse, Abuse,\u201d <em>Human Factors: the\nJournal of the Human Factors Society</em>, 39(2): 230\u2013253.</li>\n<li>Pasquale, F., 2015. <em>The black box society: The secret\nalgorithms that control money and information</em>. Cambridge, MA:\nHarvard University Press.</li>\n<li>Polder-Verkiel, S. E., 2012. \u201cOnline responsibility: Bad\nsamaritanism and the influence of internet mediation,\u201d\n<em>Science and engineering ethics</em>, 18(1): 117-141.</li>\n<li>Ravenscraft, E., (2020). \u201cHow to Spot\u2014and\nAvoid\u2014Dark Patterns on the Web\u201d, <em>Wired</em>, July 29,\n <a href=\"https://www.wired.com/story/how-to-spot-avoid-dark-patterns/\" target=\"other\">Ravenscraft 2020 available online</a>.</li>\n<li>Reeves, B. &amp; C. Nass, 1996. <em>The Media Equation: How People\nTreat Computers, Television, and New Media Like Real People and\nPlaces</em>, Cambridge: Cambridge University Press.</li>\n<li>Rosenthal-von der P\u00fctten, A. M., Kr\u00e4mer, N. C.,\nHoffmann, L., Sobieraj, S., &amp; Eimler, S. C., 2013. \u201cAn\nexperimental study on emotional reactions towards a robot,\u201d\n<em>International Journal of Social Robotics</em>, 5(1):\n17\u201334.</li>\n<li>Sack, W., 1997. \u201cArtificial Human Nature,\u201d <em>Design\nIssues</em>, 13: 55\u201364.</li>\n<li>Santoni de Sio, F. and Van den Hoven, J., 2018. \u201cMeaningful\nhuman control over autonomous systems: A philosophical account,\u201d\n<em>Frontiers in Robotics and AI</em>, 5, first online 28 February\n2018. doi:10.3389/frobt.2018.00015</li>\n<li>Santoni de Sio, F., &amp; Mecacci, G., 2021. \u201cFour\nresponsibility gaps with artificial intelligence: Why they matter and\nhow to address them,\u201d <em>Philosophy &amp; Technology</em>, 34:\n1057\u20131084.</li>\n<li>Sartor, G. and M. Viola de Azevedo Cunha, 2010. \u201cThe Italian\nGoogle-Case: Privacy, Freedom of Speech and Responsibility of\nProviders for User-Generated Contents,\u201d <em>International\nJournal of Law and Information Technology</em>, 18(4):\n356\u2013378.</li>\n<li>Searle, J. R., 1980. \u201cMinds, brains, and programs\u201d\n<em>Behavioral and Brain Sciences</em>, 3(3): 417\u2013457.</li>\n<li>Singel, R., 2010. \u201cDoes Italy\u2019s Google Conviction\nPortend More Censorship?\u201d <em>Wired</em> (February 24th, 2010),\n <a href=\"http://www.wired.com/threatlevel/2010/02/italy-google-analysis/\" target=\"other\">Singel 2010 available online</a>.</li>\n<li>Sparrow, R., 2007. \u201cKiller Robots,\u201d <em>Journal of\nApplied Philosophy</em>, 24(1): 62\u201377.</li>\n<li>Stahl, B. C., 2004. \u201cInformation, Ethics, and Computers: The\nProblem of Autonomous Moral Agents,\u201d <em>Minds and\nMachines</em>, 14: 67\u201383.</li>\n<li>\u2013\u2013\u2013, 2006. \u201cResponsible Computers? A Case\nfor Ascribing Quasi-Responsibility to Computers Independent of\nPersonhood or Agency,\u201d <em>Ethics and Information\nTechnology</em>, 8: 205\u2013213.</li>\n<li>Stieb, J. A., 2008. \u201cA Critique of Positive Responsibility\nin Computing,\u201d <em>Science and Engineering Ethics</em>, 14(2):\n219\u2013233.</li>\n<li>Strawson, P., 1962. \u201cFreedom and Resentment,\u201d in\n<em>Proceedings of the British Academy</em>, 48: 1-25.</li>\n<li>Suchman, L., 1998. \u201cHuman/machine reconsidered,\u201d\n<em>Cognitive Studies</em>, 5(1): 5\u201313.</li>\n<li>Sullins, J. P., 2006. \u201cWhen is a Robot a Moral Agent?\u201d\n<em>International Review of</em> <em>Information Ethics</em>, 6(12):\n23\u201329.</li>\n<li>Swierstra, T., Waelbers, K., 2012. \u201cDesigning a Good Life: A\nMatrix for the Technological Mediation of Morality,\u201d <em>Science\nand Engineering Ethics</em>, 18: 157\u2013172.\ndoi:10.1007/s11948-010-9251-1</li>\n<li>Taddeo, M. and L. Floridi, 2015. \u201cThe Debate on the Moral\nResponsibilities of Online Service Providers,\u201d <em>Science and\nEngineering Ethics</em>, 22(6): 1575\u20131603.</li>\n<li>Talbert, Matthew, 2022. \u201cMoral Responsibility,\u201d\n<em>The Stanford Encyclopedia of Philosophy</em> (Fall 2022 Edition),\nEdward N. Zalta &amp; Uri Nodelman (eds.) URL = &lt;<a>\nhref=\u201chttps://plato.stanford.edu/archives/win2016/entries/moral-responsibility/\u201d&gt;https://plato.stanford.edu/archives/win2016/entries/moral-responsibility/</a>&gt;.</li>\n<li>Tigard, D. W., 2021a. \u201cResponsible AI and moral\nresponsibility: a common appreciation,\u201d <em>AI and Ethics</em>,\n1(2): 113-117.</li>\n<li>\u2013\u2013\u2013, 2021b. \u201cArtificial moral\nresponsibility: How we can and cannot hold machines\nresponsible,\u201d <em>Cambridge Quarterly of Healthcare Ethics</em>,\n30(3): 435-447.</li>\n<li>U.S. Department of Defense, 2009. \u201cFY2009\u20132034\nUnmanned Systems Integrated Roadmap,\u201d\n <a href=\"https://www.globalsecurity.org/intell/library/reports/2009/dod-unmanned-systems-roadmap_2009-2034.pdf\" target=\"other\">available online </a>.</li>\n<li>Van den Hoven, J., 2002. \u201cWadlopen bij Opkomend Tij: Denken\nover Ethiek en Informatiemaatschappij,\u201d in J. de Mul (ed.),\n<em>Filosofie in Cyberspace</em>, Kampen: Uitgeverij Klement, pp.\n47\u201365.</li>\n<li>V\u00e9liz, C., 2021. \u201cMoral zombies: why algorithms are\nnot moral agents,\u201d <em>AI &amp; Society</em>, 36: 487\u2013497.\ndoi:10.1007/s00146-021-01189-x</li>\n<li>Verbeek, P. P., 2006. \u201cMaterializing Morality: Design Ethics\nand Technological Mediation,\u201d <em>Science, Technology and Human\nValues</em>, 31(3): 361\u2013380.</li>\n<li>Verbeek, P. P., 2021. <em>What Things Do</em>, University Park,\nPA: Pennsylvania State University Press.</li>\n<li>Vidal, J., 2004. \u201cThe alco-lock is claimed to foil\ndrink-drivers. Then the man from the Guardian had a go\n\u2026,\u201d <em>The Guardian</em>, August 5th, 2004.</li>\n<li>Waelbers, K., 2009. \u201cTechnological Delegation:\nResponsibility for the Unintended,\u201d <em>Science &amp;\nEngineering Ethics</em>, 15(1): 51\u201368.</li>\n<li>Wallach, W. and C. Allen, 2009. <em>Moral Machines. Teaching\nRobots Right from Wrong</em>, Oxford: Oxford University Press.</li>\n<li>Whitby, B., 2008. \u201cSometimes it\u2019s hard to be a robot.\nA call for action on the ethics of abusing artificial agents,\u201d\n<em>Interacting with Computers</em>, 20(3): 326\u2013333.</li>\n<li>John Zerilli; John Danaher; James Maclaurin; Colin Gavaghan;\nAlistair Knott; Joy Liddicoat; Merel Noorman, 2021. \u201c7\nAutonomy,\u201d in <em>A Citizens Guide to Artificial\nIntelligence</em>, Cambridge, MA: MIT Press, pp. 107-126.</li>\n<li>Zuboff, S., 1982. \u201cAutomate/Informate: The Two Faces of\nIntelligent Technology,\u201d <em>Organizational Dynamics</em>,\n14(2):5\u201318</li>\n</ul>\n</div>"}, "related_entries": {"entry_list": ["information technology: and moral values", "information technology: phenomenological approaches to ethics and", "moral responsibility", "technology, philosophy of"], "entry_link": [{"../it-moral-values/": "information technology: and moral values"}, {"../ethics-it-phenomenology/": "information technology: phenomenological approaches to ethics and"}, {"../moral-responsibility/": "moral responsibility"}, {"../technology/": "technology, philosophy of"}]}, "academic_tools": {"listed_text": ["<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>", "<a href=\"https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=computing-responsibility\" target=\"other\">How to cite this entry</a>.", "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>", "<a href=\"https://leibniz.stanford.edu/friends/preview/computing-responsibility/\" target=\"other\">Preview the PDF version of this entry</a> at the\n <a href=\"https://leibniz.stanford.edu/friends/\" target=\"other\">Friends of the SEP Society</a>.", "<img alt=\"inpho icon\" src=\"../../symbols/inpho.png\"/>", "<a href=\"https://www.inphoproject.org/entity?sep=computing-responsibility&amp;redirect=True\" target=\"other\">Look up topics and thinkers related to this entry</a>\n at the Internet Philosophy Ontology Project (InPhO).", "<img alt=\"phil papers icon\" src=\"../../symbols/pp.gif\"/>", "<a href=\"https://philpapers.org/sep/computing-responsibility/\" target=\"other\">Enhanced bibliography for this entry</a>\nat <a href=\"https://philpapers.org/\" target=\"other\">PhilPapers</a>, with links to its database."], "listed_links": [{"https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=computing-responsibility": "How to cite this entry"}, {"https://leibniz.stanford.edu/friends/preview/computing-responsibility/": "Preview the PDF version of this entry"}, {"https://leibniz.stanford.edu/friends/": "Friends of the SEP Society"}, {"https://www.inphoproject.org/entity?sep=computing-responsibility&redirect=True": "Look up topics and thinkers related to this entry"}, {"https://philpapers.org/sep/computing-responsibility/": "Enhanced bibliography for this entry"}, {"https://philpapers.org/": "PhilPapers"}]}, "other_internet_resources": {"listed_text": ["<a href=\"http://www.springer.com/computer/swe/journal/10676\" target=\"other\">Ethics and Information Technology</a>:\n A peer-reviewed journal dedicated to advancing the dialogue between\nmoral philosophy and the field of information and communication\ntechnology (ICT).", "<a href=\"https://link.springer.com/journal/11948\" target=\"other\">Science and Engineering Ethics</a>:\n Science and Engineering Ethics is a multi-disciplinary journal that\nexplores ethical issues of direct concern to scientists and\nengineers.", "<a href=\"https://www.springer.com/journal/13347\">Philosophy and Technology: </a>A\n journal that addresses the expanding scope and unprecedented impact\nof technologies, in order to improve the critical understanding of the\nconceptual nature and practical consequences, and hence provide the\nconceptual foundations for their fruitful and sustainable\ndevelopments.", "<a href=\"https://journals.sagepub.com/home/bds\">Big Data and Society</a>:\n A peer-reviewed scholarly journal that publishes interdisciplinary\nwork principally in the social sciences, humanities and computing and\ntheir intersections with the arts and natural sciences about the\nimplications of Big Data for societies.", "<a href=\"http://ethicsandtechnology.eu/\" target=\"other\">4TU Centre for Ethics and Technology </a>.", "<a href=\"http://cpsr.org/\" target=\"other\">Computer Professionals for Social Responsibility (CPSR)</a>", "<a href=\"http://www.iacap.org/\" target=\"other\">IACAP: International Association for Computing and Philosophy</a>:\n concerned with computing and philosophy broadly construed, including\nthe use of computers to teach philosophy, the use of computers to\nmodel philosophical theory, as well as philosophical concerns raised\nby computing.", "<a href=\"http://responsiblerobotics.org/\" target=\"other\">Responsible Robotics </a>:\n Organization to promote the responsible design, development,\nimplementation, and policy of robots embedded in society.", "<a href=\"http://moralmachine.mit.edu/\" target=\"other\">Moral Machine </a>:\n A platform for gathering a human perspective on moral decisions made\nby machine intelligence, such as self-driving cars.", "<a href=\"http://moralmachines.blogspot.com/\" target=\"other\">Moral machines</a>:\n blog on the theory and development of artificial moral agents and\ncomputational ethics."], "listed_links": [{"http://www.springer.com/computer/swe/journal/10676": "Ethics and Information Technology"}, {"https://link.springer.com/journal/11948": "Science and Engineering Ethics"}, {"https://www.springer.com/journal/13347": "Philosophy and Technology: "}, {"https://journals.sagepub.com/home/bds": "Big Data and Society"}, {"http://ethicsandtechnology.eu/": "4TU Centre for Ethics and Technology "}, {"http://cpsr.org/": "Computer Professionals for Social Responsibility (CPSR)"}, {"http://www.iacap.org/": "IACAP: International Association for Computing and Philosophy"}, {"http://responsiblerobotics.org/": "Responsible Robotics "}, {"http://moralmachine.mit.edu/": "Moral Machine "}, {"http://moralmachines.blogspot.com/": "Moral machines"}]}, "tokenized_text": ["1", "challenge", "moral", "responsibility", "moral", "responsibility", "human", "action", "intention", "consequence", "fisher", "1999", "eshleman", "2016", "talbert", "2022", "generally", "speaking", "person", "group", "people", "morally", "responsible", "voluntary", "action", "morally", "significant", "outcome", "would", "make", "appropriate", "blame", "praise", "thus", "may", "consider", "person", "moral", "responsibility", "jump", "water", "try", "rescue", "another", "person", "see", "person", "drowning", "manages", "pull", "person", "water", "likely", "praise", "whereas", "refuse", "help", "may", "blame", "ascribing", "moral", "responsibility", "establishes", "link", "person", "group", "people", "someone", "something", "affected", "action", "person", "group", "person", "group", "performs", "action", "cause", "something", "happen", "often", "referred", "agent", "person", "group", "thing", "affected", "action", "referred", "patient", "establishing", "link", "term", "moral", "responsibility", "agent", "patient", "done", "retrospectively", "well", "prospectively", "sometimes", "ascription", "responsibility", "involve", "giving", "account", "fault", "accident", "punished", "also", "prospectively", "determining", "obligation", "duty", "person", "fulfill", "future", "ought", "however", "circumstance", "appropriate", "ascribe", "moral", "responsibility", "always", "clear", "one", "hand", "concept", "varying", "meaning", "debate", "continue", "set", "moral", "responsibility", "apart", "kind", "responsibility", "hart", "1968", "talbert", "2022", "tigard", "2021a", "concept", "intertwined", "sometimes", "overlap", "notion", "accountability", "liability", "blameworthiness", "roleresponsibility", "causality", "opinion", "also", "differ", "condition", "warrant", "attribution", "moral", "responsibility", "whether", "requires", "agent", "free", "whether", "human", "entity", "moral", "responsibility", "attributed", "see", "entry", "moral", "responsibility", "hand", "difficult", "establish", "direct", "link", "agent", "patient", "complexity", "involved", "human", "activity", "particular", "today", "technological", "society", "individual", "institution", "generally", "act", "sociotechnical", "system", "task", "distributed", "among", "human", "technological", "component", "mutually", "affect", "different", "way", "depending", "context", "bijker", "hughes", "pinch", "1987", "felt", "et", "al", "2016", "increasingly", "complex", "technology", "exacerbate", "difficulty", "identifying", "responsible", "something", "go", "wrong", "retrospective", "account", "happened", "expected", "complex", "system", "challenging", "task", "ascribing", "responsibility", "johnson", "power", "2005", "indeed", "matthias", "argues", "growing", "responsibility", "gap", "complex", "computer", "technology", "become", "le", "human", "being", "directly", "control", "intervene", "behavior", "technology", "le", "reasonably", "hold", "human", "being", "responsible", "technology", "matthias", "2004", "increasing", "pervasiveness", "computer", "technology", "pose", "various", "challenge", "figuring", "moral", "responsibility", "entail", "properly", "ascribed", "explain", "computing", "complicates", "ascription", "responsibility", "consider", "condition", "make", "sense", "hold", "someone", "responsible", "despite", "ongoing", "philosophical", "debate", "issue", "analysis", "moral", "responsibility", "share", "least", "following", "three", "condition", "eshleman", "2016", "jonas", "1984", "causal", "connection", "person", "outcome", "action", "person", "usually", "held", "responsible", "control", "outcome", "event", "subject", "knowledge", "able", "consider", "possible", "consequence", "action", "tend", "excuse", "someone", "blame", "could", "known", "action", "would", "lead", "harmful", "event", "subject", "able", "freely", "choose", "act", "certain", "way", "make", "sense", "hold", "someone", "responsible", "harmful", "event", "action", "completely", "determined", "outside", "force", "closer", "look", "three", "condition", "show", "computing", "complicate", "applicability", "condition", "11", "causal", "contribution", "order", "person", "held", "morally", "responsible", "particular", "event", "able", "exert", "kind", "influence", "event", "make", "sense", "blame", "someone", "accident", "could", "avoided", "acting", "differently", "control", "event", "leading", "accident", "however", "computer", "technology", "obscure", "causal", "connection", "person", "action", "eventual", "consequence", "tracing", "sequence", "event", "led", "computerrelated", "catastrophic", "incident", "plane", "crash", "usually", "lead", "many", "direction", "incident", "seldom", "result", "single", "error", "mishap", "technological", "accident", "commonly", "product", "accumulation", "mistake", "misunderstanding", "negligent", "behavior", "various", "individual", "involved", "development", "use", "maintenance", "computer", "system", "including", "designer", "engineer", "technician", "regulator", "manager", "user", "manufacturer", "seller", "resellers", "even", "policy", "maker", "involvement", "multiple", "actor", "development", "deployment", "technology", "give", "rise", "known", "problem", "many", "hand", "difficult", "determine", "responsible", "multiple", "individual", "contributed", "outcome", "event", "jonas", "1984", "friedman", "1990", "nissenbaum", "1994", "van", "de", "poel", "et", "al", "2015", "one", "classic", "example", "problem", "many", "hand", "computing", "case", "malfunctioning", "radiation", "treatment", "machine", "therac25", "leveson", "turner", "1993", "leveson", "1995", "computercontrolled", "machine", "designed", "radiation", "treatment", "cancer", "patient", "well", "xray", "twoyear", "period", "1980s", "machine", "massively", "overdosed", "six", "patient", "contributing", "eventual", "death", "three", "incident", "result", "combination", "number", "factor", "including", "software", "error", "inadequate", "testing", "quality", "assurance", "exaggerated", "claim", "reliability", "bad", "interface", "design", "overconfidence", "software", "design", "inadequate", "investigation", "followup", "accident", "report", "nevertheless", "analysis", "event", "leveson", "turner", "conclude", "hard", "place", "blame", "single", "person", "action", "negligence", "involved", "might", "proven", "fatal", "contributing", "event", "recent", "example", "problem", "many", "hand", "crash", "two", "737", "max", "passenger", "aircraft", "late", "2018", "early", "2019", "crash", "led", "mulitple", "investigation", "highlighted", "various", "factor", "contributed", "tragic", "outcome", "include", "design", "human", "error", "well", "organizational", "culture", "lack", "training", "heckert", "et", "al", "2020", "say", "moral", "responsibility", "case", "nissenbaum", "1994", "gotterbarn", "2001", "coeckelbergh", "2012", "floridi", "2013", "santonio", "de", "sio", "et", "al", "2021", "many", "actor", "could", "acted", "differently", "make", "difficult", "retrospectively", "identify", "appropriate", "person", "called", "upon", "answer", "make", "amends", "outcome", "adding", "problem", "many", "hand", "temporal", "physical", "distance", "computing", "creates", "person", "consequence", "action", "distance", "blur", "causal", "connection", "action", "event", "friedman", "1990", "computational", "technology", "extend", "reach", "human", "activity", "time", "space", "help", "social", "medium", "communication", "technology", "people", "interact", "others", "side", "world", "satellite", "advanced", "communication", "technology", "allow", "pilot", "fly", "remotely", "controlled", "drone", "groundcontrol", "station", "half", "way", "across", "world", "technology", "enable", "people", "act", "greater", "distance", "remoteness", "dissociate", "original", "action", "eventual", "consequence", "waelbers", "2009", "polderverkiel", "2012", "coeckelbergh", "2013", "person", "us", "technological", "artifact", "perform", "action", "thousand", "mile", "way", "person", "might", "know", "people", "affected", "might", "directly", "partially", "experience", "consequence", "reduce", "sense", "responsibility", "person", "feel", "may", "interfere", "ability", "fully", "comprehend", "significance", "action", "similarly", "designer", "automated", "decisionmaking", "system", "determine", "ahead", "time", "decision", "made", "rarely", "see", "decision", "impact", "individual", "affect", "original", "action", "programming", "system", "may", "effect", "people", "year", "later", "problem", "many", "hand", "distancing", "effect", "use", "technology", "illustrate", "mediating", "role", "technological", "artifact", "confusion", "moral", "responsibility", "technological", "artifact", "bring", "together", "various", "different", "intention", "creator", "user", "people", "create", "deploy", "technology", "objective", "producing", "effect", "world", "software", "developer", "develop", "automated", "content", "moderation", "tool", "often", "request", "manager", "client", "aim", "shielding", "particular", "content", "user", "influencing", "user", "read", "software", "inscribed", "design", "various", "intention", "developer", "manager", "client", "poised", "behave", "given", "particular", "input", "according", "idea", "information", "appropriate", "friedman", "1997", "gorwa", "binns", "katzenbach", "2020", "moral", "responsibility", "therefore", "attributed", "without", "looking", "causal", "efficacy", "artifact", "constrain", "enable", "particular", "human", "activity", "however", "although", "technological", "artefact", "may", "influence", "shape", "human", "action", "determine", "isolated", "instrument", "mean", "work", "regardless", "context", "used", "interpretive", "flexibility", "bijker", "et", "al", "1987", "multistability", "ihde", "1990", "2", "although", "design", "technology", "provides", "set", "condition", "action", "form", "meaning", "action", "result", "human", "agent", "choose", "use", "technology", "particular", "context", "people", "often", "use", "technology", "way", "unforeseen", "designer", "interpretive", "flexibility", "make", "difficult", "designer", "anticipate", "possible", "outcome", "use", "technology", "mediating", "role", "computer", "technology", "complicates", "effort", "retrospectively", "tracing", "back", "causal", "connection", "action", "outcome", "also", "complicates", "forwardlooking", "responsibility", "12", "considering", "consequence", "computer", "technology", "shape", "people", "perceive", "experience", "world", "affect", "second", "condition", "attributing", "moral", "responsibility", "order", "make", "appropriate", "decision", "person", "able", "consider", "deliberate", "consequence", "action", "aware", "possible", "risk", "harm", "action", "might", "cause", "unfair", "hold", "someone", "responsible", "something", "could", "reasonably", "known", "action", "might", "lead", "harm", "one", "hand", "computer", "technology", "help", "user", "think", "action", "choice", "may", "lead", "help", "user", "capture", "store", "organize", "analyze", "data", "information", "zuboff", "1982", "example", "one", "oftennamed", "advantage", "remotecontrolled", "robot", "used", "armed", "force", "rescue", "worker", "enable", "operator", "acquire", "information", "would", "able", "available", "without", "allow", "operator", "look", "beyond", "next", "hill", "around", "next", "corner", "thus", "help", "operator", "reflect", "consequence", "particular", "tactical", "decision", "might", "u", "department", "defense", "2009", "similarly", "data", "analysis", "tool", "find", "pattern", "large", "volume", "data", "human", "data", "analyst", "manually", "process", "boyd", "crawford", "2012", "hand", "use", "computer", "constrain", "ability", "user", "understand", "consider", "outcome", "action", "complex", "technology", "never", "fully", "free", "error", "increasingly", "hide", "automated", "process", "behind", "interface", "van", "den", "hoven", "2002", "example", "illustrates", "computer", "technology", "limit", "understanding", "outcome", "controversial", "risk", "assessment", "tool", "used", "judge", "several", "state", "us", "parole", "decision", "sentencing", "2016", "civil", "society", "organization", "found", "based", "analysis", "risk", "score", "7000", "defendant", "produced", "one", "particular", "algorithm", "score", "poorly", "reflected", "actual", "recidivism", "rate", "seemed", "racial", "bias", "angwin", "et", "al", "2016", "regardless", "whether", "finding", "correct", "particularly", "relevant", "investigation", "also", "showed", "judge", "full", "understanding", "probability", "calculated", "part", "algorithm", "proprietary", "judge", "basing", "sentencing", "suggestion", "algorithm", "fully", "understand", "case", "computer", "technology", "today", "user", "see", "part", "many", "computation", "computer", "performs", "part", "unaware", "performs", "usually", "partial", "understanding", "assumption", "model", "theory", "information", "computer", "screen", "based", "increasing", "complexity", "computer", "system", "reliance", "opaque", "machine", "learning", "algorithm", "make", "even", "difficult", "understand", "happening", "behind", "interface", "pasquale", "2015", "diakopoulos", "2020", "opacity", "many", "computer", "system", "get", "way", "assessing", "validity", "relevance", "information", "prevent", "user", "making", "appropriate", "decision", "people", "tendency", "either", "rely", "much", "enough", "accuracy", "automated", "system", "cummings", "2004", "parasuraman", "riley", "1997", "tendency", "called", "automation", "bias", "person", "ability", "act", "responsibly", "example", "suffer", "distrust", "automation", "result", "high", "rate", "false", "alarm", "therac", "25", "case", "one", "machine", "operator", "testified", "become", "used", "many", "cryptic", "error", "message", "machine", "gave", "involve", "patient", "safety", "leveson", "turner", "1993", "p24", "tended", "ignore", "therefore", "failed", "notice", "machine", "set", "overdose", "patient", "much", "reliance", "automated", "system", "equally", "disastrous", "consequence", "1988", "missile", "cruiser", "us", "vincennes", "shot", "iranian", "civilian", "jet", "airliner", "killing", "290", "passenger", "onboard", "mistakenly", "identified", "airliner", "attacking", "military", "aircraft", "gray", "1997", "cruiser", "equipped", "aegis", "defensive", "system", "could", "automatically", "track", "target", "incoming", "missile", "enemy", "aircraft", "analysis", "event", "leading", "incident", "showed", "overconfidence", "ability", "aegis", "system", "prevented", "others", "intervening", "could", "two", "warship", "nearby", "correctly", "identified", "aircraft", "civilian", "yet", "dispute", "vincennes", "identification", "aircraft", "military", "aircraft", "later", "explanation", "lt", "richard", "thomas", "one", "nearby", "ship", "stated", "called", "robocruiser", "always", "seemed", "picture", "always", "seemed", "telling", "everybody", "get", "link", "though", "picture", "better", "quoted", "gray", "1997", "p", "34", "captain", "ship", "thought", "sophisticated", "aegis", "system", "provided", "crew", "vincennes", "information", "considering", "possible", "consequence", "one", "action", "complicated", "computer", "technology", "make", "possible", "human", "thing", "could", "several", "decade", "ago", "philosopher", "ladd", "pointed", "c", "omputer", "technology", "created", "new", "mode", "conduct", "new", "social", "institution", "new", "vice", "new", "virtue", "new", "way", "helping", "new", "way", "abusing", "people", "ladd", "1989", "p", "21011", "computer", "technology", "today", "similar", "effect", "social", "legal", "convention", "govern", "technology", "take", "time", "emerge", "initial", "absence", "convention", "contributes", "confusion", "responsibility", "taddeo", "floridi", "2015", "example", "ability", "user", "upload", "share", "text", "video", "image", "publicly", "internet", "raised", "whole", "set", "question", "responsible", "content", "uploaded", "material", "question", "heart", "debate", "conviction", "three", "google", "executive", "italy", "violation", "data", "protection", "act", "sartor", "viola", "de", "azevedo", "cunha", "2010", "case", "concerned", "video", "youtube", "four", "student", "assaulting", "disabled", "person", "response", "request", "italian", "postal", "police", "google", "owner", "youtube", "took", "video", "two", "month", "student", "uploaded", "judge", "nonetheless", "ruled", "google", "criminally", "liable", "processing", "video", "without", "taking", "adequate", "precautionary", "measure", "avoid", "privacy", "violation", "judge", "also", "held", "google", "liable", "failing", "adequately", "inform", "student", "uploaded", "video", "data", "protection", "obligation", "p", "367", "ensuing", "debate", "verdict", "critical", "ruling", "insisted", "threatened", "freedom", "expression", "internet", "set", "dangerous", "precedent", "used", "authoritarian", "regime", "justify", "web", "censorship", "see", "also", "singel", "2010", "moreover", "claimed", "platform", "provider", "could", "held", "responsible", "action", "user", "could", "realistically", "approve", "every", "upload", "job", "censure", "yet", "others", "instead", "argued", "would", "immoral", "google", "exempt", "liability", "damage", "others", "suffered", "due", "google", "profitable", "commercial", "activity", "case", "like", "one", "show", "confusion", "possibility", "limitation", "new", "technology", "difficult", "determine", "one", "moral", "obligation", "others", "lack", "experience", "new", "technological", "innovation", "also", "affect", "count", "negligent", "use", "technology", "order", "operate", "new", "computer", "system", "user", "typically", "go", "process", "training", "familiarization", "system", "requires", "skill", "experience", "understand", "imagine", "system", "behave", "coeckelbergh", "wackers", "2007", "friedman", "describes", "case", "programmer", "invented", "experimenting", "computer", "worm", "piece", "code", "replicate", "time", "relatively", "new", "computational", "entity", "1990", "programmer", "released", "worm", "internet", "experiment", "quickly", "got", "control", "code", "replicated", "much", "faster", "expected", "see", "also", "denning", "1989", "today", "would", "find", "satisfactory", "excuse", "familiar", "become", "computer", "worm", "virus", "form", "malware", "however", "friedman", "pose", "question", "whether", "programmer", "really", "acted", "negligent", "way", "consequence", "truly", "unanticipated", "computer", "community", "lack", "experience", "particular", "type", "computational", "entity", "influence", "judge", "negligent", "behavior", "13", "free", "act", "freedom", "act", "probably", "important", "condition", "attributing", "moral", "responsibility", "also", "one", "contested", "talbert", "2022", "tend", "excuse", "people", "moral", "blame", "choice", "act", "way", "typically", "hold", "people", "responsible", "coerced", "forced", "take", "particular", "action", "moral", "philosophy", "freedom", "act", "also", "mean", "person", "free", "autonomy", "fisher", "1999", "someone", "held", "morally", "responsible", "act", "basis", "authentic", "thought", "motivation", "capacity", "control", "behavior", "johnson", "2001", "note", "conception", "autonomy", "different", "way", "term", "autonomy", "often", "used", "computer", "science", "tends", "refer", "ability", "robot", "computer", "system", "independently", "perform", "ie", "without", "human", "loop", "complex", "task", "unpredictable", "environment", "extended", "period", "time", "noorman", "2009", "zerilli", "et", "al", "2021", "nevertheless", "little", "consensus", "capacity", "human", "being", "entity", "enables", "act", "freely", "see", "entry", "free", "autonomy", "moral", "political", "philosophy", "personal", "autonomy", "compatibilism", "require", "rationality", "emotion", "intentionality", "cognition", "indeed", "one", "important", "debate", "moral", "philosophy", "center", "question", "whether", "human", "being", "really", "autonomy", "free", "moral", "responsibility", "still", "attributed", "talbert", "2022", "practice", "attributing", "autonomy", "free", "human", "basis", "fulfillment", "set", "condition", "turn", "le", "straightforward", "endeavor", "attribute", "autonomy", "person", "degree", "adult", "generally", "considered", "autonomous", "child", "individual", "society", "autonomy", "thought", "vary", "manipulated", "controlled", "influenced", "force", "outside", "parent", "peer", "pressure", "moreover", "internal", "physical", "psychological", "influence", "addiction", "mental", "problem", "perceived", "constraining", "autonomy", "person", "computing", "like", "technology", "add", "additional", "layer", "complexity", "determining", "whether", "someone", "free", "act", "affect", "choice", "human", "make", "one", "biggest", "application", "area", "computing", "automation", "decisionmaking", "process", "control", "automation", "help", "centralize", "increase", "control", "multiple", "process", "charge", "limit", "discretionary", "power", "human", "operator", "lowerend", "decisionmaking", "chain", "example", "provided", "automation", "decisionmaking", "public", "administration", "bovens", "zouridis", "2002", "large", "public", "sector", "organization", "last", "decade", "progressively", "standardized", "formalized", "production", "process", "number", "country", "process", "issuing", "decision", "student", "loan", "social", "benefit", "speeding", "ticket", "tax", "return", "carried", "significant", "extent", "computer", "system", "reduced", "scope", "administrative", "discretion", "many", "official", "tax", "inspector", "welfare", "worker", "policy", "officer", "deciding", "apply", "formal", "policy", "rule", "individual", "case", "eubanks", "2018", "case", "citizen", "longer", "interact", "official", "significant", "responsibility", "applying", "knowledge", "rule", "regulation", "decide", "appropriate", "eg", "would", "better", "let", "someone", "warning", "speeding", "ticket", "required", "rather", "decision", "preprogrammed", "algorithm", "apply", "measure", "rule", "regardless", "person", "context", "eg", "speeding", "camera", "care", "context", "personal", "circumstance", "human", "being", "citizen", "interact", "little", "opportunity", "interrogate", "change", "decision", "dignum", "2020", "responsibility", "decision", "made", "case", "moved", "streetlevel", "bureaucrat", "systemlevel", "bureaucrat", "manager", "computer", "expert", "decide", "convert", "policy", "legal", "framework", "algorithm", "decisiontrees", "bovens", "zouridis", "2002", "automation", "bureaucratic", "process", "illustrates", "computer", "technology", "intentionally", "designed", "limit", "discretion", "human", "being", "example", "antialcohol", "lock", "already", "use", "number", "country", "including", "usa", "canada", "sweden", "uk", "requires", "driver", "pas", "breathing", "test", "start", "car", "technology", "force", "particular", "kind", "action", "leaf", "driver", "hardly", "choice", "technology", "might", "subtle", "way", "steering", "behavior", "either", "persuading", "nudging", "user", "verbeek", "2016", "example", "onboard", "computer", "device", "car", "show", "realtime", "information", "fuel", "consumption", "encourage", "driver", "optimize", "fuel", "efficiency", "technology", "designed", "explicit", "aim", "making", "human", "behave", "responsibly", "limiting", "option", "persuading", "choose", "certain", "way", "technology", "designed", "stimulate", "morally", "good", "behavior", "yeung", "note", "kind", "decisionguidance", "technique", "become", "key", "element", "current", "day", "bigdata", "analytic", "technique", "used", "social", "medium", "advertising", "argues", "hyper", "nudge", "extremely", "powerful", "technique", "manipulate", "behaviour", "internet", "user", "user", "internet", "thing", "iot", "device", "due", "networked", "continuously", "updated", "dynamic", "pervasive", "nature", "gather", "data", "wide", "range", "source", "user", "continuously", "make", "prediction", "realtime", "habit", "preference", "user", "target", "advertisement", "information", "price", "incentive", "gently", "unobtrusively", "nudge", "user", "direction", "preferred", "control", "algorithm", "yeung", "2017", "nudge", "hardly", "noticeable", "powerful", "effect", "one", "wonder", "autonomous", "decision", "making", "user", "case", "example", "dark", "pattern", "use", "interface", "website", "apps", "designed", "trick", "user", "thing", "intend", "purchasing", "additional", "expensive", "insurance", "ravenscraft", "2020", "verbeek", "note", "critic", "idea", "intentionally", "developing", "technology", "enforce", "morally", "desirable", "behavior", "argued", "jettisons", "democratic", "principle", "society", "threatens", "human", "dignity", "argue", "deprives", "human", "ability", "right", "make", "deliberate", "decision", "act", "voluntarily", "addition", "critic", "claimed", "human", "acting", "freely", "action", "considered", "moral", "objection", "countered", "verbeek", "argues", "pointing", "rule", "norm", "regulation", "host", "technological", "artifact", "already", "set", "condition", "action", "human", "able", "allowed", "perform", "moreover", "note", "technological", "artifact", "active", "mediator", "affect", "action", "experience", "human", "determine", "people", "creatively", "circumvented", "strict", "morality", "earlier", "version", "alcohol", "lock", "air", "pump", "car", "vidal", "2004", "nevertheless", "critique", "underline", "issue", "stake", "automating", "decisionmaking", "process", "computing", "set", "constraint", "freedom", "person", "act", "thus", "affect", "extent", "held", "morally", "responsible", "challenge", "computer", "technology", "present", "regard", "condition", "ascribing", "responsibility", "indicate", "limitation", "conventional", "ethical", "framework", "dealing", "question", "moral", "responsibility", "traditional", "model", "moral", "responsibility", "seem", "developed", "kind", "action", "performed", "individual", "directly", "visible", "consequence", "waelbers", "2009", "coeckelbergh", "2009", "however", "today", "society", "attribution", "responsibility", "individual", "group", "individual", "intertwined", "artifact", "interact", "well", "intention", "action", "human", "agent", "artifact", "mediate", "acting", "computer", "technology", "may", "require", "different", "kind", "analysis", "held", "responsible", "mean", "morally", "responsible", "discus", "two", "way", "scholar", "taken", "challenge", "1", "reconsidering", "mean", "moral", "agent", "2", "reconsidering", "concept", "moral", "responsibility", "2", "computer", "moral", "agent", "moral", "responsibility", "generally", "attributed", "moral", "agent", "least", "western", "philosophical", "tradition", "moral", "agency", "concept", "exclusively", "reserved", "human", "being", "johnson", "2001", "doorn", "van", "de", "poel", "2012", "unlike", "animal", "natural", "disaster", "human", "being", "tradition", "originator", "morally", "significant", "action", "freely", "choose", "act", "one", "way", "rather", "another", "way", "deliberate", "consequence", "choice", "although", "people", "inclined", "anthropomorphize", "computer", "treat", "moral", "agent", "reef", "na", "1996", "na", "moon", "2000", "rosenthalvon", "der", "p\u00fctten", "2013", "philosopher", "agree", "current", "computer", "technology", "called", "moral", "agent", "would", "mean", "could", "held", "morally", "responsible", "however", "limitation", "traditional", "ethical", "vocabulary", "thinking", "moral", "dimension", "computing", "led", "author", "rethink", "concept", "moral", "agency", "noted", "author", "also", "argued", "reconsideration", "western", "philosophical", "anthropocentric", "conception", "moral", "patiency", "particular", "regard", "question", "concerning", "moral", "standing", "artificial", "agent", "robot", "floridi", "2016", "gunkel", "2020", "coeckelbergh", "2020", "following", "nevertheless", "focus", "moral", "agency", "reflection", "moral", "patiency", "tend", "address", "challenge", "moral", "responsibility", "21", "computer", "morally", "responsible", "agent", "increasing", "complexity", "computer", "technology", "advance", "artificial", "intelligence", "ai", "challenge", "idea", "human", "being", "entity", "moral", "responsibility", "ascribed", "bechtel", "1985", "kroes", "verbeek", "2014", "dennett", "example", "suggested", "holding", "computer", "morally", "responsible", "possible", "concerned", "higherorder", "intentional", "computer", "system", "1997", "intentional", "system", "according", "one", "predicted", "explained", "attributing", "belief", "desire", "well", "rationality", "word", "behavior", "described", "assuming", "system", "mental", "state", "act", "according", "think", "ought", "given", "belief", "desire", "time", "dennett", "noted", "many", "computer", "already", "intentional", "system", "lacked", "higherorder", "ability", "reflect", "reason", "mental", "state", "belief", "belief", "thought", "desire", "dennett", "suggested", "fictional", "hal", "9000", "featured", "movie", "2001", "space", "odyssey", "would", "qualify", "higherorder", "intentional", "system", "held", "morally", "responsible", "although", "advance", "ai", "might", "lead", "hal", "see", "development", "computer", "system", "higherorder", "intentionality", "real", "possibility", "sullins", "argues", "line", "dennett", "moral", "agency", "restricted", "human", "being", "2006", "proposes", "computer", "system", "specifically", "robot", "moral", "agent", "significant", "level", "autonomy", "regarded", "appropriate", "level", "abstraction", "exhibiting", "intentional", "behavior", "robot", "according", "sullins", "would", "significantly", "autonomous", "direct", "control", "agent", "performing", "task", "note", "sullins", "interprets", "autonomy", "narrow", "sense", "comparison", "conception", "autonomy", "moral", "philosophy", "property", "human", "being", "add", "third", "condition", "robot", "also", "position", "responsibility", "moral", "agent", "robot", "performs", "social", "role", "carry", "responsibility", "performing", "role", "robot", "appears", "belief", "understanding", "duty", "towards", "moral", "agent", "p", "28", "illustrate", "kind", "capability", "required", "full", "moral", "agency", "draw", "analogy", "human", "nurse", "argues", "robot", "autonomous", "enough", "carry", "duty", "human", "nurse", "understanding", "role", "responsibility", "health", "care", "system", "would", "full", "moral", "agent", "sullins", "maintains", "time", "machine", "kind", "capability", "available", "even", "modest", "robot", "today", "seen", "moral", "agent", "sort", "certain", "level", "abstraction", "deserving", "moral", "consideration", "p", "29", "echoing", "objection", "early", "project", "strong", "ai", "sack", "1997", "3", "critic", "analysis", "presented", "dennett", "sullins", "objected", "idea", "computer", "technology", "capacity", "make", "human", "being", "moral", "agent", "mental", "state", "intentionality", "common", "sense", "emotion", "empathy", "johnson", "2006", "kuflik", "1999", "nyholm", "2018", "instance", "point", "make", "sense", "treat", "computer", "system", "moral", "agent", "held", "responsible", "suffer", "thus", "punished", "sparrow", "2007", "asaro", "2011", "veliz", "argues", "computer", "may", "act", "like", "moral", "agent", "lack", "sentience", "therefore", "moral", "zombie", "2021", "hakli", "makela", "argue", "computer", "kind", "autonomy", "required", "moral", "agency", "capacity", "result", "engineering", "programming", "undermines", "autonomy", "robot", "disqualifies", "moral", "agent", "2019", "argue", "stahl", "computer", "capable", "moral", "reasoning", "capacity", "understand", "meaning", "information", "process", "2006", "order", "comprehend", "meaning", "moral", "statement", "agent", "part", "form", "life", "statement", "meaningful", "able", "take", "part", "moral", "discourse", "similar", "debate", "ai", "critic", "continue", "draw", "distinction", "human", "computer", "noting", "various", "capacity", "computer", "would", "justify", "attribution", "moral", "agency", "critic", "contest", "human", "being", "might", "able", "build", "computer", "system", "required", "capacity", "moral", "agency", "question", "whether", "ethically", "appropriate", "bryson", "instance", "argues", "even", "possible", "create", "artifact", "capacity", "assumes", "might", "well", "possible", "human", "being", "choice", "matter", "2018", "defines", "moral", "agent", "something", "deemed", "responsible", "society", "action", "p", "16", "society", "thus", "one", "point", "deem", "appropriate", "view", "certain", "computer", "system", "moral", "agent", "instance", "would", "provide", "short", "cut", "figuring", "responsibility", "distributed", "however", "argues", "necessary", "predetermined", "position", "technology", "society", "note", "computer", "technology", "ethical", "framework", "artefact", "society", "therefore", "subject", "human", "control", "p", "15", "choose", "capacity", "equip", "artefact", "see", "coherent", "reason", "creating", "artificial", "agent", "human", "being", "compete", "term", "moral", "agency", "patiency", "22", "creating", "autonomous", "moral", "agent", "absence", "definitive", "argument", "possibility", "future", "computer", "system", "morally", "responsible", "researcher", "within", "field", "machine", "ethic", "aim", "develop", "discussion", "focusing", "instead", "creating", "computer", "system", "behave", "moral", "agent", "moor", "2006", "cervantes", "et", "al", "2019", "zoshak", "dew", "2021", "research", "within", "field", "concerned", "design", "development", "computer", "system", "independently", "determine", "right", "thing", "would", "given", "situation", "according", "allen", "wallach", "autonomous", "moral", "agent", "amas", "would", "capable", "reasoning", "moral", "social", "significance", "behavior", "use", "assessment", "effect", "behavior", "sentient", "being", "make", "appropriate", "choice", "2012", "see", "also", "wallach", "allen", "2009", "allen", "et", "al", "2000", "ability", "needed", "argue", "computer", "becoming", "complex", "capable", "operating", "without", "direct", "human", "control", "different", "context", "environment", "progressively", "autonomous", "technology", "already", "development", "military", "robot", "driverless", "car", "train", "service", "robot", "home", "healthcare", "involved", "moral", "situation", "directly", "affect", "safety", "wellbeing", "human", "autonomous", "bomb", "disposal", "robot", "might", "future", "faced", "decision", "bomb", "defuse", "first", "order", "minimize", "casualty", "similarly", "moral", "decision", "driverless", "car", "might", "make", "whether", "break", "crossing", "dog", "avoid", "risk", "causing", "injury", "driver", "behind", "decision", "require", "judgment", "currently", "operator", "make", "moral", "decision", "decision", "already", "inscribed", "design", "computer", "system", "machine", "ethic", "wallach", "allen", "argue", "go", "one", "step", "beyond", "making", "engineer", "aware", "value", "build", "design", "product", "seek", "build", "ethical", "decisionmaking", "machine", "specify", "mean", "computer", "make", "ethical", "decision", "put", "ethic", "machine", "moor", "distinguished", "three", "different", "kind", "ethical", "agent", "implicit", "ethical", "agent", "explicit", "ethical", "agent", "full", "ethical", "agent", "2006", "first", "kind", "agent", "computer", "ethic", "developer", "inscribed", "design", "agent", "constructed", "adhere", "norm", "value", "context", "developed", "used", "thus", "atm", "teller", "designed", "high", "level", "security", "prevent", "unauthorized", "people", "drawing", "money", "account", "explicit", "ethical", "agent", "computer", "ethic", "word", "basis", "ethical", "model", "determine", "would", "right", "thing", "given", "certain", "input", "ethical", "model", "based", "ethical", "tradition", "kantian", "confucianism", "ubuntu", "utilitarian", "ethicsdepending", "preference", "creator", "agent", "would", "make", "ethical", "decision", "behalf", "human", "user", "developer", "agent", "akin", "autonomous", "moral", "agent", "described", "allen", "wallach", "finally", "moor", "defined", "full", "ethical", "agent", "entity", "make", "ethical", "judgment", "justify", "much", "like", "human", "being", "claimed", "although", "time", "writing", "computer", "technology", "could", "called", "fully", "ethical", "empirical", "question", "whether", "would", "possible", "future", "philosopher", "today", "would", "argue", "question", "answered", "postive", "effort", "build", "amas", "raise", "question", "effort", "affect", "ascription", "moral", "responsibility", "human", "being", "would", "design", "artificial", "agent", "behave", "within", "prespecified", "formalized", "ethical", "framework", "likely", "responsibility", "still", "ascribed", "human", "actor", "deploy", "technology", "however", "allen", "wallach", "acknowledge", "danger", "exclusively", "focusing", "equipping", "robot", "moral", "decisionmaking", "ability", "rather", "also", "looking", "sociotechnical", "system", "robot", "embedded", "may", "cause", "confusion", "distribution", "responsibility", "2012", "robot", "moral", "decisionmaking", "capability", "may", "present", "similar", "challenge", "ascribing", "responsibility", "technology", "introduce", "new", "complexity", "obfuscate", "causal", "connection", "lead", "back", "creator", "user", "23", "expanding", "concept", "moral", "agency", "prospect", "increasingly", "autonomous", "intelligent", "computer", "technology", "growing", "difficulty", "finding", "responsible", "human", "agent", "lead", "floridi", "sander", "take", "different", "approach", "2004", "propose", "extend", "class", "moral", "agent", "include", "artificial", "agent", "disconnecting", "moral", "agency", "moral", "accountability", "notion", "moral", "responsibility", "contend", "insurmountable", "difficulty", "traditional", "rather", "outdated", "view", "human", "found", "accountable", "certain", "kind", "software", "even", "hardware", "demand", "different", "approach", "p", "372", "instead", "suggest", "artificial", "agent", "acknowledged", "moral", "agent", "held", "accountable", "responsible", "illustrate", "draw", "comparison", "artificial", "agent", "dog", "source", "moral", "action", "dog", "cause", "morally", "charged", "action", "like", "damaging", "property", "helping", "save", "person", "life", "case", "searchandrescue", "dog", "identify", "moral", "agent", "even", "though", "generally", "hold", "morally", "responsible", "according", "floridi", "sander", "source", "moral", "action", "held", "morally", "accountable", "correcting", "punishing", "like", "animal", "floridi", "sander", "argue", "artificial", "agent", "seen", "source", "moral", "action", "thus", "held", "morally", "accountable", "conceived", "behaving", "like", "moral", "agent", "appropriate", "level", "abstraction", "notion", "level", "abstraction", "refers", "stance", "one", "adopts", "towards", "entity", "predict", "explain", "behavior", "low", "level", "abstraction", "would", "explain", "behavior", "system", "term", "mechanical", "biological", "process", "higher", "level", "abstraction", "help", "describe", "behavior", "system", "term", "belief", "desire", "thought", "high", "enough", "level", "computational", "system", "effectively", "described", "interactive", "autonomous", "adaptive", "held", "accountable", "according", "floridi", "sander", "p", "352", "thus", "require", "personhood", "free", "agent", "morally", "accountable", "rather", "agent", "act", "intention", "able", "make", "choice", "advantage", "disconnecting", "accountability", "responsibility", "according", "floridi", "sander", "place", "focus", "moral", "agenthood", "accountability", "censure", "instead", "figuring", "human", "agent", "responsible", "le", "likely", "assign", "responsibility", "cost", "forced", "necessity", "identify", "human", "moral", "agent", "liberate", "technological", "development", "aa", "artificial", "agent", "bound", "standard", "limiting", "view", "p", "376", "artificial", "agent", "behave", "badly", "dealt", "directly", "autonomous", "behavior", "complexity", "make", "difficult", "distribute", "responsibility", "among", "human", "agent", "immoral", "agent", "modified", "deleted", "possible", "attribute", "moral", "accountability", "even", "moral", "responsibility", "determined", "critic", "floridi", "sander", "view", "accountability", "moral", "agency", "argue", "placing", "focus", "analysis", "computational", "artifact", "treating", "moral", "agent", "draw", "attention", "away", "human", "deploy", "develop", "johnson", "instance", "make", "case", "computer", "technology", "remain", "connected", "intentionality", "creator", "user", "2006", "argues", "although", "computational", "artifact", "part", "moral", "world", "recognized", "entity", "moral", "relevance", "moral", "agent", "intentional", "intentional", "mental", "state", "purpose", "come", "freedom", "act", "emphasizes", "although", "artifact", "intentional", "intentionality", "intentionality", "related", "functionality", "humanmade", "artifact", "design", "use", "reflect", "intention", "designer", "user", "human", "user", "turn", "use", "intentionality", "interact", "software", "interacting", "artifact", "activate", "inscribed", "intention", "designer", "developer", "human", "activity", "computer", "technology", "designed", "developed", "tested", "installed", "initiated", "provided", "input", "instruction", "perform", "specified", "task", "without", "human", "activity", "computer", "would", "nothing", "attributing", "independent", "moral", "agency", "computer", "johnson", "claim", "disconnect", "human", "behavior", "creates", "deploys", "us", "turn", "attention", "away", "force", "shape", "technological", "development", "limit", "possibility", "intervention", "instance", "leaf", "issue", "sorting", "responsible", "dealing", "malfunctioning", "immoral", "artificial", "agent", "make", "amends", "harmful", "event", "may", "cause", "postpones", "question", "account", "condition", "artificial", "agent", "allowed", "operate", "noorman", "2009", "yet", "technology", "still", "part", "moral", "action", "without", "moral", "agent", "several", "philosopher", "stressed", "moral", "responsibility", "properly", "understood", "without", "recognizing", "active", "role", "technology", "shaping", "human", "action", "jonas", "1984", "verbeek", "2006", "johnson", "power", "2005", "nyholm", "2018", "johnson", "instance", "claim", "although", "computer", "moral", "agent", "artifact", "designer", "artifact", "artifact", "user", "focus", "moral", "evaluation", "work", "action", "johnson", "2006", "human", "create", "artifact", "inscribe", "particular", "value", "intention", "achieve", "particular", "effect", "world", "turn", "technological", "artifact", "influence", "human", "being", "affect", "perceive", "interpret", "world", "similarly", "verbeek", "maintains", "technological", "artifact", "alone", "moral", "agency", "moral", "agency", "hardly", "ever", "purely", "human", "moral", "agency", "generally", "involves", "mediating", "artifact", "shape", "human", "behavior", "often", "way", "anticipated", "designer", "2008", "moral", "decision", "action", "coshaped", "technological", "artifact", "suggests", "form", "human", "action", "three", "form", "agency", "work", "1", "agency", "human", "performing", "action", "2", "agency", "designer", "helped", "shaped", "mediating", "role", "artifact", "3", "artifact", "mediating", "human", "action", "agency", "artifact", "inextricably", "linked", "agency", "designer", "user", "reduced", "either", "subject", "act", "make", "moral", "decision", "composite", "human", "technological", "component", "moral", "agency", "merely", "located", "human", "complex", "blend", "human", "technology", "later", "paper", "floridi", "explores", "concept", "distributed", "moral", "action", "2013", "2016", "argues", "moral", "significant", "outcome", "reduced", "moral", "significant", "action", "individual", "morally", "neutral", "action", "several", "individual", "still", "result", "morally", "significant", "event", "individual", "might", "intended", "cause", "harm", "nevertheless", "combined", "action", "may", "still", "result", "moral", "harm", "someone", "something", "order", "deal", "problem", "subsequently", "assigning", "moral", "responsibility", "distributed", "moral", "action", "argues", "focus", "analysis", "shift", "agent", "patient", "moral", "action", "moral", "action", "evaluated", "term", "harm", "patient", "regardless", "intention", "agent", "involved", "assigning", "responsibility", "focus", "whether", "agent", "causally", "accountable", "outcome", "adjusting", "behavior", "prevent", "harm", "agent", "causally", "accountable", "artificial", "biological", "autonomous", "interact", "environment", "learn", "interaction", "held", "responsible", "distributed", "moral", "action", "according", "floridi", "2016", "3", "rethinking", "concept", "moral", "responsibility", "light", "noted", "difficulty", "ascribing", "moral", "responsibility", "several", "author", "critiqued", "way", "concept", "used", "interpreted", "relation", "computing", "claim", "traditional", "model", "framework", "dealing", "moral", "responsibility", "fall", "short", "propose", "different", "perspective", "interpretation", "address", "difficulty", "discussed", "section", "31", "assigning", "responsibility", "one", "approach", "rethink", "moral", "responsibility", "assigned", "gotterbarn", "2001", "waelbers", "2009", "come", "computing", "practitioner", "gotterbarn", "identifies", "potential", "sidestep", "avoid", "responsibility", "looking", "someone", "else", "blame", "attribute", "potential", "two", "pervasive", "misconception", "responsibility", "first", "misconception", "computing", "ethically", "neutral", "practice", "according", "gotterbarn", "misplaced", "belief", "technological", "artifact", "practice", "building", "ethically", "neutral", "often", "used", "justify", "narrow", "technologycentered", "focus", "development", "computer", "system", "without", "taking", "broader", "context", "technology", "operate", "account", "narrow", "focus", "detrimental", "consequence", "gotterbarn", "give", "tragic", "case", "patient", "death", "result", "faulty", "xray", "device", "example", "programmer", "given", "assignment", "write", "program", "could", "lower", "raise", "xray", "device", "pole", "xray", "technician", "set", "required", "height", "programmer", "focused", "solving", "given", "puzzle", "failed", "take", "account", "circumstance", "device", "would", "used", "contingency", "might", "occur", "thus", "consider", "possibility", "patient", "could", "accidentally", "way", "device", "moving", "pole", "oversight", "eventually", "resulted", "tragic", "accident", "patient", "crushed", "device", "technician", "set", "device", "tabletop", "height", "realizing", "patient", "still", "underneath", "according", "gotterbarn", "computer", "practitioner", "moral", "responsibility", "consider", "contingency", "even", "though", "may", "legally", "required", "design", "use", "technological", "artifact", "moral", "activity", "choice", "one", "particular", "design", "solution", "another", "real", "material", "consequence", "second", "misconception", "responsibility", "determining", "blame", "something", "go", "wrong", "computer", "practitioner", "according", "gotterbarn", "conventionally", "adopted", "malpractice", "model", "responsibility", "focus", "determining", "appropriate", "person", "blame", "harmful", "incident", "2001", "malpractice", "model", "lead", "sort", "excuse", "shirk", "responsibility", "particular", "complexity", "computer", "technology", "introduce", "allow", "computer", "practitioner", "sidestep", "responsibility", "distance", "developer", "effect", "use", "technology", "create", "instance", "used", "claim", "direct", "immediate", "causal", "link", "would", "tie", "developer", "malfunction", "developer", "argue", "contribution", "chain", "event", "negligible", "part", "team", "larger", "organization", "limited", "opportunity", "otherwise", "malpractice", "model", "according", "gotterbarn", "entices", "computer", "practitioner", "distance", "accountability", "blame", "two", "misconception", "based", "particular", "retrospective", "view", "responsibility", "place", "focus", "exempts", "one", "blame", "liability", "reference", "ladd", "gotterbarn", "call", "negative", "responsibility", "distinguishes", "positive", "responsibility", "see", "also", "ladd", "1989", "positive", "responsibility", "emphasizes", "virtue", "obliged", "regard", "consequence", "action", "others", "gotterbarn", "2001", "p", "227", "positive", "responsibility", "entail", "part", "professionalism", "computer", "expert", "strive", "minimize", "foreseeable", "undesirable", "event", "focus", "ought", "done", "rather", "blaming", "punishing", "others", "irresponsible", "behavior", "gotterbarn", "argues", "computing", "profession", "adopt", "positive", "concept", "responsibility", "emphasizes", "obligation", "duty", "computer", "practitioner", "regard", "consequence", "one", "action", "minimize", "possibility", "causing", "harm", "computer", "practitioner", "moral", "responsibility", "avoid", "harm", "deliver", "properly", "working", "product", "according", "regardless", "whether", "held", "accountable", "thing", "turn", "differently", "emphasis", "positive", "moral", "responsibility", "computer", "practitioner", "raise", "question", "far", "responsibility", "reach", "particular", "light", "system", "many", "hand", "help", "create", "difficulty", "involved", "anticipating", "contingency", "might", "cause", "system", "malfunction", "stieb", "2008", "miller", "2008", "extent", "developer", "manufacturer", "expected", "exert", "anticipate", "prevent", "consequence", "use", "technology", "possible", "bug", "code", "computer", "system", "today", "generally", "incomprehensible", "single", "programmer", "seems", "unlikely", "complex", "computer", "system", "completely", "error", "free", "martin", "argues", "respect", "developer", "company", "decide", "sell", "computer", "technology", "particular", "context", "responsible", "ethical", "implication", "use", "technology", "context", "2019", "responsible", "implication", "knowledgeable", "design", "decision", "unique", "position", "inscribe", "technology", "particular", "idea", "bias", "technology", "thus", "company", "creates", "sell", "riskassessment", "system", "context", "judicial", "decisionmaking", "responsible", "ethical", "implication", "bias", "resulting", "use", "opaqueness", "company", "willingly", "take", "obligation", "understand", "value", "decision", "ensure", "algorithm", "ethical", "implication", "congruent", "context", "p", "10", "however", "leaf", "open", "question", "responsibility", "outside", "context", "manufacturer", "mobile", "phone", "anticipated", "product", "would", "used", "roadside", "bomb", "manufacturer", "designer", "engineer", "foresee", "possible", "condition", "product", "eventually", "operate", "moreover", "much", "control", "person", "feel", "responsible", "outcome", "event", "question", "speak", "santonio", "de", "sio", "meccaci", "2020", "call", "active", "responsibility", "gap", "described", "active", "responsibility", "much", "way", "positive", "responsibility", "relates", "moral", "obligation", "person", "ensure", "behavior", "system", "design", "control", "use", "minimizes", "harm", "gap", "responsibility", "according", "result", "person", "sufficiently", "aware", "capable", "motivated", "see", "act", "according", "obligation", "p", "1059", "address", "gap", "active", "responsibility", "well", "backwardlooking", "gap", "responsibility", "santonio", "de", "sio", "meccaci", "suggest", "approach", "underline", "need", "look", "broader", "sociotechnical", "system", "human", "agent", "technology", "assigning", "responsibility", "requires", "looking", "whole", "chain", "design", "development", "use", "social", "technical", "well", "organizational", "perspective", "element", "change", "adjusted", "effort", "address", "responsibility", "gap", "including", "design", "computer", "system", "well", "organization", "us", "base", "approach", "idea", "designing", "sociotechnial", "system", "meaningful", "human", "control", "developed", "santonio", "de", "sio", "van", "den", "hoven", "2018", "meaningful", "human", "control", "concept", "originally", "gained", "currency", "context", "autonomous", "lethal", "weapon", "approach", "addressing", "responsibility", "gap", "question", "mean", "control", "technological", "system", "becomes", "particularly", "pertinent", "situation", "weapon", "system", "delegated", "task", "involved", "target", "selection", "engagement", "ekelhof", "2019", "santoni", "di", "sio", "van", "den", "hoven", "2018", "developed", "conception", "meaningful", "human", "control", "get", "actionable", "analysis", "control", "help", "engineer", "computing", "professional", "policy", "maker", "designer", "think", "design", "sociotechnical", "system", "responsibility", "mind", "santonio", "de", "sio", "van", "den", "hoven", "assume", "technology", "part", "decisional", "mechanism", "human", "agent", "carry", "action", "world", "mechanism", "responsive", "moral", "reason", "agent", "control", "meaningful", "human", "control", "thus", "conditional", "extent", "outcome", "connected", "decisional", "mechanism", "human", "agent", "elucidate", "connection", "formulate", "two", "necessary", "condition", "meaningful", "control", "called", "tracking", "tracing", "tracking", "requires", "whole", "sociotechnical", "system", "technical", "human", "organizational", "element", "responsive", "moral", "reason", "relevant", "agent", "relevant", "fact", "circumstance", "behaviour", "system", "reflect", "reason", "value", "intention", "actor", "given", "particular", "circumstance", "example", "machine", "learning", "system", "trained", "distinguish", "husky", "wolf", "system", "act", "according", "relevant", "reason", "eg", "alarming", "farmer", "presence", "wolf", "machine", "learning", "system", "trained", "make", "distinction", "shown", "picture", "wolf", "snow", "husky", "urban", "environment", "may", "deduce", "relevant", "distinguishing", "feature", "snow", "shown", "picture", "wolf", "urban", "environment", "might", "subsequently", "misclassify", "wolf", "husky", "case", "system", "properly", "track", "reason", "relevant", "human", "agent", "fact", "environment", "note", "tracking", "relation", "involve", "multiple", "human", "agent", "along", "chain", "moral", "reason", "necessarily", "come", "operator", "user", "also", "come", "policy", "maker", "designer", "programmer", "tracing", "requires", "outcome", "traced", "back", "earlier", "decision", "made", "human", "agent", "place", "position", "resulting", "outcome", "example", "author", "give", "drunk", "driver", "causing", "serious", "accident", "even", "driver", "fulfill", "condition", "responsibility", "time", "accident", "mental", "incapacitation", "driver", "make", "earlier", "decision", "drinking", "much", "tracing", "condition", "santoni", "di", "sio", "van", "den", "hoven", "formulate", "assumes", "possible", "one", "human", "agent", "involved", "action", "led", "outcome", "action", "mediated", "nonhuman", "system", "according", "tracing", "condition", "requires", "whole", "sociotechnical", "system", "designed", "least", "one", "human", "agent", "sufficient", "knowledge", "moral", "awareness", "potential", "target", "legitimate", "response", "behavior", "system", "santonio", "de", "sio", "van", "den", "hoven", "understanding", "meaningful", "human", "control", "implication", "design", "computer", "system", "also", "design", "environment", "social", "institutional", "practice", "tracking", "tracing", "relevant", "human", "moral", "reason", "occurs", "level", "design", "32", "responsibility", "practice", "santonio", "de", "sio", "van", "den", "hoven", "analysis", "meaningful", "human", "control", "draw", "attention", "social", "function", "moral", "responsibility", "provides", "yet", "another", "perspective", "issue", "stahl", "2006", "tigard", "2021b", "prospectively", "retrospectively", "responsibility", "work", "organize", "social", "relation", "people", "people", "institution", "set", "expectation", "people", "fulfillment", "certain", "obligation", "duty", "provides", "mean", "correct", "encourage", "certain", "behavior", "instance", "robotics", "company", "expected", "build", "safeguard", "prevent", "robot", "harming", "human", "company", "fails", "live", "expectation", "held", "accountable", "case", "pay", "damage", "undergo", "kind", "punishment", "punishment", "prospect", "punishment", "encourage", "company", "regard", "system", "safety", "reliability", "sound", "design", "risk", "involved", "production", "robot", "might", "trigger", "company", "take", "action", "prevent", "future", "accident", "yet", "might", "also", "encourage", "find", "way", "shift", "blame", "idea", "responsibility", "interpersonal", "relationship", "expectation", "duty", "obligation", "place", "focus", "practice", "holding", "someone", "responsible", "strawson", "1962", "talbert", "2022", "particular", "practice", "social", "structure", "place", "ascribe", "responsibility", "hold", "people", "accountable", "influence", "relate", "technology", "turn", "century", "nissenbaum", "already", "noted", "difficulty", "attributing", "moral", "responsibility", "large", "extent", "traced", "back", "particular", "characteristic", "organizational", "cultural", "context", "computer", "technology", "embedded", "argued", "conceive", "nature", "capacity", "limitation", "computing", "influence", "answerability", "develop", "use", "computer", "technology", "1997", "time", "observed", "systematic", "erosion", "accountability", "increasingly", "computerized", "society", "conceived", "accountability", "value", "practice", "place", "emphasis", "preventing", "harm", "risk", "accountability", "mean", "someone", "several", "people", "answer", "malfunction", "lifecritical", "system", "cause", "risk", "grave", "injury", "cause", "infrastructure", "large", "monetary", "loss", "even", "malfunction", "cause", "individual", "loss", "time", "convenience", "contentment", "1994", "p", "74", "used", "powerful", "tool", "motivating", "better", "practice", "consequently", "reliable", "trustworthy", "system", "1997", "p", "43", "holding", "people", "accountable", "harm", "risk", "caused", "computer", "system", "provides", "strong", "incentive", "minimize", "provide", "starting", "point", "assigning", "punishment", "cultural", "organizational", "practice", "time", "however", "seemed", "opposite", "due", "condition", "computer", "technology", "commonly", "developed", "deployed", "coupled", "popular", "conception", "nature", "capacity", "limitation", "computing", "p", "43", "nissenbaum", "identified", "four", "barrier", "accountability", "society", "1", "problem", "many", "hand", "2", "acceptance", "computer", "bug", "inherent", "element", "large", "software", "system", "3", "using", "computer", "scapegoat", "4", "ownership", "without", "liability", "according", "nissenbaum", "people", "tendency", "shirk", "responsibility", "shift", "blame", "others", "accident", "occur", "problem", "many", "hand", "idea", "software", "bug", "inevitable", "byproduct", "complex", "computer", "system", "easily", "accepted", "excuse", "answering", "harmful", "outcome", "people", "also", "inclined", "point", "finger", "complexity", "computer", "argue", "computer", "fault", "thing", "go", "wrong", "finally", "perceived", "tendency", "company", "claim", "ownership", "software", "developed", "dismiss", "responsibility", "come", "ownership", "illustrate", "pointed", "extended", "license", "agreement", "assert", "manufacturer", "ownership", "software", "disclaim", "accountability", "quality", "performance", "product", "four", "barrier", "nissenbaum", "argued", "stand", "way", "culture", "accountability", "aimed", "maintaining", "clear", "line", "accountability", "culture", "foster", "strong", "sense", "responsibility", "virtue", "encouraged", "everyone", "connected", "outcome", "particular", "action", "answerable", "accountability", "according", "nissenbaum", "different", "liability", "liability", "looking", "person", "blame", "compensate", "damage", "suffered", "event", "person", "found", "others", "let", "hook", "may", "encourage", "people", "look", "excuse", "blaming", "computer", "accountability", "however", "applies", "involved", "requires", "particular", "kind", "organizational", "context", "one", "answerability", "work", "entice", "people", "pay", "greater", "attention", "system", "safety", "reliability", "sound", "design", "order", "establish", "culture", "accountability", "see", "also", "martin", "2019", "herkert", "et", "al", "2020", "organization", "place", "le", "value", "accountability", "little", "regard", "responsibility", "organizing", "production", "process", "likely", "allow", "technological", "product", "become", "incomprehensible", "nissenbaum", "analysis", "illustrates", "practice", "holding", "someone", "responsible", "established", "way", "holding", "people", "account", "conveying", "expectation", "duty", "obligation", "continuously", "changing", "negotiated", "partly", "response", "introduction", "new", "technology", "see", "also", "noorman", "2012", "lack", "culture", "accountability", "lead", "responsibility", "attributed", "wrong", "people", "people", "become", "madeleine", "clare", "elish", "call", "moral", "crumple", "zone", "2019", "human", "actor", "absorb", "responsibility", "even", "though", "limited", "control", "system", "work", "elish", "argues", "given", "complexity", "technological", "system", "medium", "public", "tend", "blame", "accident", "human", "error", "misattribute", "responsibility", "nearest", "human", "operator", "pilot", "maintenance", "personnel", "rather", "technological", "system", "decisionmakers", "higher", "chain", "nissenbaum", "argued", "context", "technology", "developed", "used", "significant", "influence", "ascription", "moral", "responsibility", "several", "author", "stressed", "moral", "responsibility", "properly", "understood", "without", "recognizing", "active", "role", "technology", "shaping", "human", "action", "jonas", "1984", "verbeek", "2006", "johnson", "power", "2005", "waelbers", "2009", "according", "johnson", "power", "enough", "look", "human", "intend", "ascribing", "responsibility", "person", "act", "technology", "requires", "coming", "grip", "behavior", "technology", "2005", "p", "107", "one", "consider", "various", "way", "technological", "artifact", "mediate", "human", "action", "moral", "responsibility", "thus", "action", "person", "group", "people", "affect", "others", "morally", "significant", "way", "also", "action", "shaped", "technology", "moral", "responsibility", "perspective", "located", "individual", "interpersonal", "relationship", "distributed", "among", "human", "technology", "4", "conclusion", "computer", "technology", "challenged", "conventional", "conception", "moral", "responsibility", "raised", "question", "distribute", "responsibility", "appropriately", "human", "being", "still", "held", "responsible", "behavior", "complex", "computer", "technology", "limited", "control", "understanding", "human", "being", "agent", "held", "morally", "responsible", "concept", "moral", "agent", "extended", "include", "artificial", "computational", "entity", "response", "question", "philosopher", "reexamined", "concept", "moral", "agency", "moral", "responsibility", "although", "clear", "consensus", "concept", "entail", "increasingly", "digital", "society", "clear", "discussion", "reflection", "concept", "need", "address", "technology", "affect", "human", "action", "responsibility", "action", "begin", "end"]}