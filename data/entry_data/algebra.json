{
    "url": "algebra",
    "title": "Algebra",
    "authorship": {
        "year": "Copyright \u00a9 2022",
        "author_text": "Vaughan Pratt\n<pratt@cs.stanford.edu>",
        "author_links": [
            {
                "mailto:pratt%40cs%2estanford%2eedu": "pratt@cs.stanford.edu"
            }
        ],
        "raw_html": "<div id=\"article-copyright\">\n<p>\n<a href=\"../../info.html#c\">Copyright \u00a9 2022</a> by\n\n<br/>\nVaughan Pratt\n&lt;<a href=\"mailto:pratt%40cs%2estanford%2eedu\"><em>pratt<abbr title=\" at \">@</abbr>cs<abbr title=\" dot \">.</abbr>stanford<abbr title=\" dot \">.</abbr>edu</em></a>&gt;\n    </p>\n</div>"
    },
    "pubinfo": [
        "First published Tue May 29, 2007",
        "substantive revision Thu Nov 3, 2022"
    ],
    "preamble": "\n\nAlgebra is a branch of mathematics sibling to geometry, analysis\n(calculus), number theory, combinatorics, etc. Although algebra has\nits roots in numerical domains such as the reals and the complex\nnumbers, in its full generality it differs from its siblings in\nserving no specific mathematical domain. Whereas geometry treats\nspatial entities, analysis continuous variation, number theory integer\narithmetic, and combinatorics discrete structures, algebra is equally\napplicable to all these and other mathematical domains.\n\nElementary algebra, in use for centuries and taught in\nsecondary school, is the arithmetic of indefinite quantities or\nvariables \\(x, y,\\ldots\\). Whereas the definite sum \\(3+4\\) evaluates\nto the definite quantity 7, the indefinite sum \\(x+y\\) has no definite\nvalue, yet we can still say that it is always equal to \\(y+x\\), or to\n\\(x^2 -y^2\\) if and only if \\(x\\) is either \\(-y\\) or \\(y+1\\).\n\nElementary algebra provides finite ways of managing the infinite. A\nformula such as \\(\\pi r^2\\) for the area of a circle of radius \\(r\\)\ndescribes infinitely many possible computations, one for each possible\nvaluation of its variables. A universally true law expresses\ninfinitely many cases, for example the single equation \\(x+y = y+x\\)\nsummarizes the infinitely many facts \\(1+2 = 2+1, 3+7 = 7+3\\), etc.\nThe equation \\(2x = 4\\) selects one number from an infinite set of\npossibilities. And \\(y = 2x+3\\) expresses the infinitely many points\nof the line with slope 2 passing through \\((0, 3)\\) with a finite\nequation whose solutions are exactly those points.\n\nElementary algebra ordinarily works with real or complex values.\nHowever its general methods, if not always its specific operations and\nlaws, are equally applicable to other numeric domains such as the\nnatural numbers, the integers, the integers modulo some integer \\(n\\),\nthe rationals, the quaternions, the Gaussian integers, the \\(p\\)-adic\nnumbers, and so on. They are also applicable to many nonnumeric\ndomains such as the subsets of a given set under the operations of\nunion and intersection, the words over a given alphabet under the\noperations of concatenation and reversal, the permutations of a given\nset under the operations of composition and inverse, etc. Each such\nalgebraic structure, or simply algebra, consists of\nthe set of its elements and operations on those elements obeying the\nlaws holding in that domain, such as the set \\(Z = \\{0, \\pm 1, \\pm 2,\n\\ldots \\}\\) of integers under the integer operations \\(x+y\\) of\naddition, \\(xy\\) of multiplication, and \\(-x\\), negation, or the set\n\\(2^X\\) of subsets of a set \\(X\\) under the set operations \\(X\\cup Y\\)\nof union, \\(X\\cap Y\\) of intersection, and \\(X'\\), complement relative\nto \\(X\\).\n\nThe laws are often similar but not identical. For example integer\nmultiplication distributes over addition, \\(x(y+z) = xy+xz\\), but not\nconversely, for example \\(2+(3\\times 5) = 17\\) but \\((2+3)\\times(2+5)\n= 35\\). In the analogy that makes intersection the set theoretic\ncounterpart of multiplication and union that of addition, intersection\ndistributes over union, \n\\[ X\\cap(Y\\cup Z) = (X\\cap Y)\\cup(X\\cap Z), \\]\n\n\nas for the integers, but unlike the integers union also distributes\nover intersection: \n\\[ X\\cup(Y\\cap Z) = (X\\cup Y)\\cap(X\\cup Z). \\]\n\n\nWhereas elementary algebra is conducted in a fixed algebra,\nabstract or modern algebra treats classes of\nalgebras having certain properties in common, typically those\nexpressible as equations. The subject, which emerged during the 19th\ncentury, is traditionally introduced via the classes of groups, rings,\nand fields. For example any number system under the operations of\naddition and subtraction forms an abelian (commutative) group; one\nthen passes to rings by bringing in multiplication, and further to\nfields with division. The common four-function calculator provides the\nfour functions of the field of reals.\n\nThe abstract concept of group in full generality is defined not in\nterms of a set of numbers but rather as an arbitrary set equipped with\na binary operation \\(xy\\), a unary inverse \\(x^{-1}\\) of that\noperation, and a unit \\(e\\) satisfying certain equations\ncharacteristic of groups. One striking novelty with groups not\nencountered in everyday elementary algebra is that their\nmultiplication need not be abelian: \\(xy\\) and \\(yx\\) can be\ndifferent! For example the group \\(S_3\\) of the six possible\npermutations of three things is not abelian, as can be seen by\nexchanging adjacent pairs of letters in the word dan. If you\nexchange the two letters on the left before the two on the right you\nget adn and then and, but if you perform these\nexchanges in the other order you get dna and then\nnda instead of and. Likewise the group of\n43,252,003,274,489,856,000 operations on Rubik\u2019s cube and the\ninfinite group \\(SO(3)\\) of rotations of the sphere are not abelian,\nthough the infinite group \\(SO(2)\\) of rotations of the circle is\nabelian. Quaternion multiplication and matrix multiplication is also\nnoncommutative. Abelian groups are often called additive groups and\ntheir group operation is referred to as addition \\(x+y\\) rather than\nmultiplication \\(xy\\).\n\nGroups, rings and fields only scratch the surface of abstract algebra.\nVector spaces and more generally modules are restricted forms of rings\nin which the operands of multiplication are required to be a scalar\nand a vector. Monoids generalize groups by dropping inverse; for\nexample the natural numbers form a monoid but not a group for want of\nnegation. Boolean algebras abstract the algebra of sets. Lattices\ngeneralize Boolean algebras by dropping complement and the\ndistributivity laws.\n\nA number of branches of mathematics have found algebra such an\neffective tool that they have spawned algebraic subbranches. Algebraic\nlogic, algebraic number theory, and algebraic topology are all heavily\nstudied, while algebraic geometry and algebraic combinatorics have\nentire journals devoted to them.\n\nAlgebra is of philosophical interest for at least two reasons. From\nthe perspective of foundations of mathematics, algebra is strikingly\ndifferent from other branches of mathematics in both its domain\nindependence and its close affinity to formal logic. Furthermore the\ndichotomy between elementary and abstract algebra reflects a certain\nduality in reasoning that Descartes, the inventor of Cartesian\nDualism, would have appreciated, wherein the former deals with the\nreasoning process and the latter that which is reasoned about, as\nrespectively the mind and body of mathematics.\n\nAlgebra has also played a significant role in clarifying and\nhighlighting notions of logic, at the core of exact philosophy for\nmillennia. The first step away from the Aristotelian logic of\nsyllogisms towards a more algebraic form of logic was taken by Boole\nin an 1847 pamphlet and subsequently in a more detailed treatise,\nThe Laws of Thought, in 1854. The dichotomy between\nelementary algebra and modern algebra then started to appear in the\nsubsequent development of logic, with logicians strongly divided\nbetween the formalistic approach as espoused by Frege, Peano, and\nRussell, and the algebraic approach followed by C. S. Peirce,\nSchroeder, and Tarski.\n",
    "toc": [
        {
            "#Int": "1. Elementary Algebra"
        },
        {
            "#Formulas": "1.1 Formulas"
        },
        {
            "#Laws": "1.2 Laws"
        },
        {
            "#Word": "1.3 Word problems"
        },
        {
            "#Cart": "1.4 Cartesian geometry"
        },
        {
            "#AbsAlg": "2. Abstract Algebra"
        },
        {
            "#Sgrp": "2.1 Semigroups"
        },
        {
            "#Grp": "2.2 Groups"
        },
        {
            "#Rng": "2.3 Rings"
        },
        {
            "#Fld": "2.4 Fields"
        },
        {
            "#Apps": "2.5 Applications"
        },
        {
            "#UnivAlg": "3. Universal Algebra"
        },
        {
            "#UAConcepts": "3.1 Concepts"
        },
        {
            "#EqLog": "3.2 Equational Logic"
        },
        {
            "#Birk": "3.3 Birkhoff\u2019s Theorem"
        },
        {
            "#Lin": "4. Linear Algebra"
        },
        {
            "#Vct": "4.1 Vector Spaces"
        },
        {
            "#Ass": "4.2 Associative Algebras"
        },
        {
            "#Algzn": "5. Algebraization of mathematics"
        },
        {
            "#AlgGeom": "5.1 Algebraic geometry"
        },
        {
            "#AlgNum": "5.2 Algebraic number theory"
        },
        {
            "#AlgTop": "5.3 Algebraic topology"
        },
        {
            "#AlgLog": "5.4 Algebraic logic"
        },
        {
            "#Free": "6. Free algebras"
        },
        {
            "#FMon": "6.1 Free monoids and groups"
        },
        {
            "#FRng": "6.2 Free rings"
        },
        {
            "#FComb": "6.3 Free combinatorial structures"
        },
        {
            "#FLog": "6.4 Free logical structures"
        },
        {
            "#FCat": "6.5 Free algebras categorially"
        },
        {
            "#Bib": "Bibliography"
        },
        {
            "#Aca": "Academic Tools"
        },
        {
            "#Oth": "Other Internet Resources"
        },
        {
            "#Rel": "Related Entries"
        }
    ],
    "main_text": "\n1. Elementary algebra\n\nElementary algebra deals with numerical terms, namely\nconstants 0, 1, 1.5, \\(\\pi\\), variables \\(x,\ny,\\ldots\\), and combinations thereof built with operations\nsuch as \\(+\\), \\(-\\), \\(\\times\\) , \\(\\div\\) , \\(\\sqrt{\\phantom{x}}\\),\netc. to form such terms as \\(x+1, x\\times y\\) (standardly abbreviated\n\\(xy\\)), \\(x + 3y\\), and \\(\\sqrt{x}\\).\n\nTerms may be used on their own in formulas such as \\(\\pi\nr^2\\), or in equations serving as laws such as \\(x+y = y+x\\),\nor as constraints such as \\(2x^2 -x+3 = 5x+1\\) or \\(x^2 + y^2\n= 1\\).\n\nLaws are always true; while they have the same form as constraints\nthey constrain only vacuously in that every valuation of their\nvariables is a solution. The constraint \\(x^2 +y^2 = 1\\) has a\ncontinuum of solutions forming a shape, in this case a circle\nof radius 1. The constraint \\(2x^2 -x+3 = 5x-1\\) has two solutions,\n\\(x = 1\\) or 2, and may be encountered in the solution of word\nproblems, or in the determination of the points of intersection\nof two curves such as the parabola \\(y = 2x^2 -x+3\\) and the line \\(y\n= 5x-1\\).\n1.1 Formulas\n\nA formula is a term used in the computation of values by hand or\nmachine. Although some attributes of physical objects lend themselves\nto direct measurement such as length and mass, others such as area,\nvolume, and density do not and must be computed from more readily\nobserved values with the help of the appropriate formula. For example\nthe area of a rectangle \\(L\\) inches long by \\(W\\) inches wide is\ngiven by the formula \\(LW\\) in units of square inches, the volume of a\nball of radius \\(r\\) is \\(4\\pi r^3 /3\\), and the density of a solid of\nmass \\(M\\) and volume \\(V\\) is given by \\(M/V\\).\n\nFormulas may be combined to give yet more formulas. For example the\ndensity of a ball of mass \\(M\\) and radius \\(r\\) can be obtained by\nsubstituting the above formula for the volume of a ball for \\(V\\) in\nthe above formula for the density of a solid. The resulting formula\n\\(M/(4\\pi r^3 /3)\\) is then the desired density formula.\n1.2 Laws\n\nLaws or identities are equations that hold for all applicable values\nof their variables. For example the commutativity law \n\\[ x+y = y+x \\]\n\n\nholds for all real values of \\(x\\) and \\(y\\). Likewise the\nassociativity law \n\\[ x+(y+z) = (x+y)+z \\]\n\n\nholds for all real values of \\(x, y\\) and \\(z\\). On the other hand,\nwhile the law \\(x/(y/z) = zx/y\\) holds for all numerical values of\n\\(x\\), it holds only for nonzero values of \\(y\\) and \\(z\\) in order to\navoid the illegal operation of division by zero.\n\nWhen a law holds for all numerical values of its variables, it also\nholds for all expression values of those variables. Setting \\(x = M, y\n= 4\\pi r^3\\), and \\(z = 3\\) in the last law of the preceding paragraph\nyields \\(M/(4\\pi r^3 /3) = 3M/(4\\pi r^3)\\). The left hand side being\nour density formula from the preceding section, it follows from this\ninstance of the above law that its right hand side is an equivalent\nformula for density in the sense that it gives the same answers as the\nleft hand side. This new density formula replaces one of the two\ndivisions by a multiplication.\n1.3 Word problems\n\nIf Xavier will be three times his present age in four years time, how\nold is he? We can solve this word problem using algebra by\nformalizing it as the equation \\(3x = x + 4\\) where \\(x\\) is\nXavier\u2019s present age. The left hand side expresses three times\nXavier\u2019s present age, while the right hand side expresses his\nage in four years\u2019 time.\n\nA general rule for solving such equations is that any solution to it\nis also a solution to the equation obtained by applying some operation\nto both sides. In this case we can simplify the equation by\nsubtracting \\(x\\) from both sides to give \\(2x = 4\\), and then\ndividing both sides by 2 to give \\(x = 2\\). So Xavier is now two years\nold.\n\nIf Xavier is twice as old as Yvonne and half the square of her age,\nhow old is each? This is more complicated than the previous example in\nthree respects: it has more unknowns, more equations, and terms of\nhigher degree. We may take \\(x\\) for Xavier\u2019s age and \\(y\\) for\nYvonne\u2019s age. The two constraints may be formalized as the\nequations \\(x = 2y\\) and \\(x = y^2 /2\\), the latter being of degree 2\nor quadratic.\n\nSince both right hand sides are equal to \\(x\\) we can infer \\(2y = y^2\n/2\\). It is tempting to divide both sides by \\(y\\), but what if \\(y =\n0\\)? In fact \\(y = 0\\) is one solution, for which \\(x = 2y = 0\\) as\nwell, corresponding to Xavier and Yvonne both being newborns. Setting\nthat solution to one side we can now look for solutions in which \\(y\\)\nis not zero by dividing both sides by \\(y\\). This yields \\(y = 4\\), in\nwhich case \\(x = 2y = 8\\). So now we have a second solution in which\nXavier is eight years old and Yvonne four.\n\nIn the absence of any other information, both solutions are\nlegitimate. Had the problem further specified that Yvonne was a\ntoddler, or that Xavier was older than Yvonne, we could have ruled out\nthe first solution.\n1.4 Cartesian geometry\n\nLines, circles, and other curves in the plane can be expressed\nalgebraically using Cartesian coordinates, named for its\ninventor Rene Descartes. These are defined with respect to a\ndistinguished point in the plane called the origin, denoted\n\\(O\\). Each point is specified by how far it is to the right of and\nabove \\(O\\), written as a pair of numbers. For example the pair (2.1,\n3.56) specifies the point 2.1 units to the right of \\(O\\), measured\nhorizontally, and 3.56 units above it, measured vertically; we call\n2.1 the \\(x\\) coordinate and 3.56 the \\(y\\) coordinate of that point.\nEither coordinate can be negative: the pair \\((-5, -1)\\) corresponds\nto the point 5 units to the left of \\(O\\) and 1 unit below it. The\npoint \\(O\\) itself is coordinatized as (0, 0).\n\nLines. Given an equation in variables \\(x\\) and \\(y\\), a\npoint such as (2, 7) is said to be a solution to that\nequation when setting \\(x\\) to 2 and \\(y\\) to 7 makes the equation\ntrue. For example the equation \\(y = 3x+5\\) has as solutions the\npoints (0, 5), (1, 8), (2, 11), and so on. Other solutions include\n(.5, 6.5), (1.5, 9.5), and so on. The set of all solutions constitutes\nthe unique straight line passing through (0, 5) and (1, 8). We then\ncall \\(y = 3x+5\\) the equation of that line.\n\nCircles. By Pythagoras\u2019s Theorem the square of the\ndistance between two points \\((x, y)\\) and \\((x', y')\\) is given by\n\\((x'-x)^2 +(y'-y)^2\\). As a special case of this, the square of the\ndistance of the point \\((x, y)\\) to the origin is \\(x^2 +y^2\\). It\nfollows that those point at distance \\(r\\) from the origin are the\nsolutions in \\(x\\) and \\(y\\) to the equation \\(x^2 +y^2 = r^2\\). But\nthese points are exactly those forming the circle of radius \\(r\\)\ncentered on \\(O\\). We identify this equation with this circle.\n\nVarieties The roots of any polynomial in \\(x\\) and \\(y\\) form\na curve in the plane called a one-dimensional variety of\ndegree that of the polynomial. Thus lines are of degree 1,\nbeing expressed as polynomials \\(ax+by+c\\), while circles centered on\n\\((x', y')\\) are of degree 2, being expressed as polynomials\n\\((x-x')^2 +(y-y')^2 -r^2\\). Some varieties may contain no points, for\nexample \\(x^2 +y^2 +1\\), while others may contain one point, for\nexample \\(x^2 +y^2\\) having the origin as its one root. In general\nhowever a two-dimensional variety will be a curve. Such a curve may\ncross itself, or have a cusp, or even separate into two or more\ncomponents not connected to each other.\n\nSpace The two-dimensional plane is generalized to\nthree-dimensional space by adding to the variables \\(x\\) and \\(y\\) a\nthird variable \\(z\\) corresponding to the third dimension. The\nconventional orientation takes the first dimension to run from west to\neast, the second from south to north, and the third from below to\nabove. Points are then triples, for example the point \\((2, 5, -3)\\)\nis 2 units to the east of the origin, 5 units to the north of it, and\n3 units below it.\n\nPlanes and spheres. These are the counterparts in space of\nlines and circles in the plane. An equation such as \\(z = 3x + 2y\\)\ndefines not a straight line but rather a flat plane, in this case the\nunique plane passing through the points (0, 1, 2), (1, 0, 3), and (1,\n1, 5). And the sphere of radius \\(r\\) centered on the origin is given\nby \\(x^2 +y^2 +z^2 = r^2\\). The roots of a polynomial in \\(x, y\\) and\n\\(z\\) form a surface in space called a two-dimensional variety, of\ndegree that of the polynomial, just as for one-dimensional varieties.\nThus planes are of degree 1 and spheres of degree 2.\n\nThese methods generalize to yet higher dimensions by adding yet more\nvariables. Although the geometric space we experience physically is\nlimited to three dimensions, conceptually there is no limit to the\nnumber of dimensions of abstract mathematical space. Just as a line is\na one-dimensional subspace of the two-dimensional plane, and a plane\nis a two-dimensional subspace of three-dimensional space, each\nspecifiable with an equation, so is a hyperplane a\nthree-dimensional subspace of four-dimensional space, also specifiable\nwith an equation such as \\(w = 2x - 7y + z\\).\n2. Abstract Algebra\n\nElementary algebra fixes some domain, typically the reals or complex\nnumbers, and works with the equations holding within that domain.\nAbstract or modern algebra reverses this picture by\nfixing some set \\(A\\) of equations and studying those domains for\nwhich those equations are identities. For example if we take the set\nof all identities expressible with the operations of addition,\nsubtraction, and multiplication and constants 0 and 1 that hold for\nthe integers, then the algebras in which those equations hold\nidentically are exactly the commutative rings with identity.\n\nHistorically the term modern algebra came from the title of the first\nthree editions of van der Waerden\u2019s classic text of that name,\nrenamed simply \u201cAlgebra\u201d for its fourth edition in 1955.\nVolume 1 treated groups, rings, general fields, vector spaces, well\norderings, and real fields, while Volume 2 considered mainly linear\nalgebra, algebras (as vector spaces with a compatible multiplication),\nrepresentation theory, ideal theory, integral algebraic elements,\nalgebraic functions, and topological algebra. On the one hand modern\nalgebra has since gone far beyond this curriculum, on the other this\nconsiderable body of material is already more than what can be assumed\nas common knowledge among graduating Ph.D. students in mathematics,\nfor whom the typical program is too short to permit mastering all this\nmaterial in parallel with focusing on their area of\nspecialization.\n\nA core feature of abstract algebra is the existence of domains where\nfamiliar laws fail to hold. A striking example is commutativity of\nmultiplication, which as we noted in the introduction need not hold\nfor the multiplication of an arbitrary group, even so simple a group\nas the six permutations of three letters.\n2.1 Semigroups\n\nWe begin with the concept of a binary operation on a set \\(X\\), namely\na function \\(f: X^2 \\rightarrow X\\) such that \\(f(x, y)\\) is an\nelement of \\(X\\) for all elements \\(x, y\\) of \\(X\\). Such an operation\nis said to be associative when it satisfies \\(f(f(x, y), z) =\nf(x, f(y, z))\\) for all \\(x, y, z\\) in \\(X\\).\n\nA semigroup is a set together with an associative operation,\ncalled the multiplication of the semigroup and notated \\(xy\\)\nrather than \\(f(x, y)\\).\n\nThe product \\(xx\\) of an element with itself is denoted \\(x^2\\).\nLikewise \\(xxx\\) is denoted \\(x^3\\) and so on.\n\nExamples\n\nThe set of all nonempty words over a given alphabet under the\noperation of concatenation.\nThe set of all functions \\(f: X \\rightarrow X\\) on a set \\(X\\)\nunder the operation of function composition.\nThe set of integer \\(n\\times n\\) matrices under matrix\nmultiplication, for a fixed positive integer \\(n\\).\n\n\n\nConcatenation \\(uv\\) of words \\(u, v\\) is associative because when a\nword is cut into two, the concatenation of the two parts is the\noriginal word regardless of where the cut is made. The concatenation\nof al and gebra is the same as that\nof algeb and ra, illustrating\nassociativity of concatenation for the case \\(x = \\)\nal, \\(y =\\) geb, \\(z =\\)\nra.\n\nComposition \\(f\\cdot g\\) of two functions \\(f\\) and \\(g\\) is\nassociative via the reasoning \n\\[\\begin{align*}\n(f\\cdot(g\\cdot h))(x) &= f((g\\cdot h)(x)) \\\\\n &= f(g(h(x))) \\\\\n &=(f\\cdot g)(h(x)) \\\\\n &=((f\\cdot g)\\cdot h)(x) \n\\end{align*}\\]\n\n\nfor all \\(x\\) in \\(X\\), whence \\(f\\cdot(g\\cdot h) = (f\\cdot g)\\cdot\nh\\).\n\nA semigroup \\(H\\) is a subsemigroup of a semigroup \\(G\\) when\n\\(H\\) is a subset of \\(G\\) and the multiplication of \\(G\\) restricted\nto \\(H\\) coincides with that of \\(H\\). Equivalently a subsemigroup of\n\\(G\\) is a subset \\(H\\) of \\(G\\) such that for all \\(x, y\\) in \\(H,\nxy\\) is in \\(H\\).\n\nExamples\n\nThe semigroup of all nonempty words over a given alphabet has as a\nsubsemigroup the words of even length; however the words of odd length\ndo not form a subsemigroup because the concatenation of two odd-length\nwords is not of odd length.\nThe semigroup of all functions \\(f: X \\rightarrow X\\) on a set\n\\(X\\) under function composition has as subsemigroups the injective or\none-to-one functions, the surjective or onto functions, and the\nbijections or permutations.\n\n\n\nA binary operation is called commutative when it satisfies\n\\(f(x, y) = f(y, x)\\) for all \\(x, y\\) in \\(X\\). A commutative\nsemigroup is a semigroup whose operation is commutative. All the\nexamples so far have been of noncommutative semigroups. The following\nillustrate the commutative case.\n\nExamples\n\nThe set of positive integers under addition.\nThe set of all integers under addition.\nThe set of words on a one-letter alphabet under\nconcatenation.\nThe set \\(\\{0, 1\\}\\) of bits (binary digits) under any of the\noperations AND, OR, XOR.\nThe set \\(2^X\\) of subsets of a fixed set \\(X\\) under any of the\nset theoretic operations intersection, union, symmetric\ndifference.\nThe set of vectors in the upper right quadrant of the plane under\nvector addition.\nThe same but omitting the origin.\nThe set of all three-dimensional vectors under vector\naddition.\nThe set of polynomials in one variable \\(x\\) with integer\ncoefficients under polynomial addition.\nThe set of integer \\(m\\times n\\) matrices under matrix addition,\nfor fixed positive integers \\(m,n\\).\n\n\n\nAn element \\(x\\) of \\(X\\) is a left identity for \\(f\\) when\n\\(f(x, y) = y\\) for all \\(y\\) in \\(X\\), and a right identity\nwhen \\(f(y, x) = y\\) for all \\(y\\) in \\(X\\). An identity for\n\\(f\\) is an element that is both a left identity and a right identity\nfor \\(f\\). An operation \\(f\\) can have only one identity, because when\n\\(x\\) and \\(y\\) are identities they are both equal to \\(f(x,y)\\).\n\nA monoid is a semigroup containing an identity for the\nmultiplication of the semigroup, notated 1.\n\nExamples\n\nThe identity for concatenation is the empty word. Hence words\nunder concatenation form a monoid when the empty word is allowed.\nThe identity for addition is zero, or the origin in the case of\nvector addition. Hence any of the above examples of semigroups for\nwhich the operation is addition forms a monoid if and only if it\ncontains zero.\nThe identity for composition is the identity function \\(1_X : X\n\\rightarrow X\\) defined as \\(1_X (x) = x\\) for all \\(x\\) in \\(X\\),\nwhence the semigroup of all functions on a set \\(X\\) forms a\nmonoid.\n\n\n\nA monoid \\(H\\) is a submonoid of a monoid \\(G\\) when it is a\nsubsemigroup of \\(G\\) that includes the identity of \\(G\\).\n2.2 Groups\n\nWhen two elements \\(x, y\\) of a monoid satisfy \\(xy = 1\\) we say that\n\\(x\\) is the left inverse of \\(y\\) and \\(y\\) is the right inverse of\n\\(x\\). An element \\(y\\) that is both a left and right inverse of \\(x\\)\nis called simply an inverse of \\(x\\).\n\nA group is a monoid every element of which has an\ninverse. The cardinality of a group is traditionally referred to\nas its order. A group element \\(g\\) is said to be of order\n\\(n\\) when \\(n\\) is the least positive integer for which \\(g^n = 1\\).\n\n\nExamples\n\nThe monoid of integers under addition, because every integer \\(x\\)\nhas inverse \\(-x\\).\nThe set \\(\\{0, 1\\}\\) of bits (binary digits) under the operation\n\\(\\XOR\\), because each bit is its own inverse: \\(0 \\XOR 0 = 1 \\XOR 1 =\n0\\). This is just the case \\(n = 2\\) of the monoid\n\\(\\{0,1,2,\\ldots,n-1\\}\\) of integers mod(ulo) \\(n\\) under addition mod\n\\(n\\) (e.g. \\(3 + 4 = 2\\) (mod 5)). Here 0 is its own inverse and the\ninverse of \\(m\\) when nonzero is \\(n - m\\).\nThe monoid \\(S_X\\) of bijections or permutations \\(f: X\n\\rightarrow X\\) under composition, because every permutation has an\ninverse \\(f^{-1}\\). When \\(X\\) has \\(n\\) elements \\(S_n\\) is of order\n\\(n!\\). \\(S_n\\) is abelian if and only if \\(n \\le 2\\).\nThe monoid of rotations of the plane about a point under\ncomposition, because every rotation can be reversed. This group is\ncalled the circle group, denoted SO(2).\nThe monoid of rotations of three-dimensional space about a point\nunder composition, again because every rotation can be reversed. This\ngroup is denoted SO(3).\nThe monoid of symmetries (rotations and reflections) of the\nregular \\(n\\)-gon about its center that carry the \\(n\\)-gon into\nitself, again by reversibility. This group is called the dihedral\ngroup \\(D_n\\), and is of order \\(2n\\). Like \\(S_n, D_n\\) is\nabelian if and only if \\(n \\le 2\\); in particular \\(D_3 = S_3\\).\n\n\n\nA subgroup of a group \\(G\\) is a submonoid of \\(G\\) closed\nunder inverses. The monoids of natural numbers and of even integers\nare both submonoids of the monoid of integers under addition, but only\nthe latter submonoid is a subgroup, being closed under negation,\nunlike the natural numbers.\n\nAn abelian group is a group whose operation is commutative.\nThe group operation of an abelian group is conventionally referred to\nas addition rather than multiplication, and abelian groups are\nsometimes called additive groups.\n\nA cyclic group is a group \\(G\\) with an element \\(g\\) such\nthat every element of \\(G\\) is of the form \\(g^i\\) for some positive\ninteger \\(i\\). Cyclic groups are abelian because \\(g^{i}g^j = g^{i +\nj} = g^j g^{i}\\). The group of integers under addition, and the groups\nof integers mod \\(n\\) for any positive integer \\(n\\), all form cyclic\ngroups, with 1 as a generator in every case. All cyclic groups are\nisomorphic to one of these. There are always other generators when the\ngroup is of order 3 or more, for example \\(-1\\), and for groups of\nprime order every nonzero element is a generator.\n2.3 Rings\n\nA ring is an abelian group that is also a monoid by virtue of\nhaving a second operation, called the multiplication of the\nring. Zero annihilates, meaning that \\(0x = x0 = 0\\).\nFurthermore multiplication distributes over addition (the group\noperation) in both arguments. That is, \\(x(y+z) = xy + xz\\) and\n\\((x+y)z = xz + yz\\).\n\nExamples\n\nThe additive group of integers expanded with the operation of\ninteger multiplication.\nThe additive group of polynomials in one variable \\(x\\) with\ninteger coefficients expanded with\u00a0 polynomial\nmultiplication.\nThe additive group of integer \\(n\\times n\\) matrices, for a fixed\npositive integer n, expanded with matrix\nmultiplication.\nThe group of numbers of the form \\(a+b\\sqrt{2}\\) where \\(a\\) and\n\\(b\\) are integer, because \\((a+b\\sqrt{2})(c+d\\sqrt{2}) =\nac+2bd+(bc+ad)\\sqrt{2}\\).\nThe group of integers mod \\(n\\) for \\(n \\ge 2\\), because\n\\((x+an)(y+bn) = xy + (xb + ya + abn)n\\).\n\n\n\nIn all but the last example, the integers (other than the integer\n\\(n\\) giving the size of the matrices) may be replaced by any of the\nrationals, the reals, or the complex numbers. When replacing the\nintegers with the reals the fourth example becomes simply the ring of\nreals because even if \\(b\\) is zero \\(a\\) can be any real. However\nwhen replacing with the rational numbers the ring includes the\nrationals, but is more than that because \\(\\sqrt{2}\\) is irrational,\nyet it does not contain for example \\(\\sqrt{3}\\).\n2.4 Fields\n\nA field is a ring for which the multiplicative monoid of\nnonzero ring elements is an abelian group. That is, multiplication\nmust be commutative, and every nonzero element \\(x\\) must have a\nreciprocal \\(1/x\\).\n\nExamples\n\nThe ring of rationals, because rational multiplication is\ncommutative and every nonzero rational \\(m/n\\) has reciprocal\n\\(n/m\\).\nThe ring of reals, and the ring of complex numbers, for the\nanalogous reasons.\nThe ring of numbers of the form \\(a+b\\sqrt{2}\\) where \\(a\\) and\n\\(b\\) are rational, because \\(a+b\\sqrt{2}\\) is zero only when\n\\(a=b=0\\), and otherwise has reciprocal \\((a-b\\sqrt{2})/(a^2 -2b^2\n)\\); called the field of quadratic irrationals.\nThe ring \\(Z_p\\) of integers modulo a prime \\(p\\), because the\nmultiplicative monoid of nonzero numbers includes a number \\(g\\) such\nthat \\(g^{p-1} = 1\\) and every nonzero number has the form \\(g^i\\) for\nsome integer \\(i\\), and hence has an inverse, namely\n\\(g^{p-1-i}\\).\n\n\n\nThe last example does not generalize directly to other moduli. However\nfor any modulus that is a power \\(p^n\\) of a prime, it can be shown\nthat there exists a unique multiplication making the group \\(Z_{p^n}\\)\na ring in a way that makes the nonzero elements of the ring a cyclic\n(and therefore abelian) group under the multiplication, and hence\nmaking the ring a field. The fields constructed in this way are the\nonly finite fields.\n2.5 Applications\n\nWhy study entire classes? Well, consider for example the set \\(Z\\) of\nintegers along with the binary operation of addition \\(x+y\\), the\nunary operation of negation \\(-x\\), and the constant 0. These\noperations and the constant satisfy various laws such as \\(x+(y+z) =\n(x+y)+z, x+y = y+x, x+0 = x\\), and \\(x+(-x) = 0\\). Now consider any\nother algebra with operations that not only have the same names but\nalso satisfy the same laws (and possibly more), called a\nmodel of those laws. Such an algebra could serve any of the\nfollowing purposes.\n\n(i) It could tell us to what extent the equational laws holding of the\nintegers characterize the integers. Since the set \\(\\{0, 1\\}\\) of\nintegers mod 2 under addition and negation satisfies all the laws that\nthe integers do, we immediately see that no single equational property\nof the integers tells us that there are infinitely many integers. On\nthe other hand any finite model of the equational theory of the\nintegers necessarily satisfies some law that the integers don\u2019t\nsatisfy, in particular the law \\(x+x+\\ldots +x = 0\\) where the number\nof \\(x\\)\\(s\\) on the left hand side is the size of the\nmodel. Since the equational theory of the integers contains no such\nlaw we can tell from its theory as a whole that the integers must be\nan infinite set. On the other hand the rational numbers under addition\nand negation satisfy exactly the same equational properties as the\nintegers, so this theory does not characterize the algebra of integers\nunder addition and subtraction with sufficient precision to\ndistinguish it from the rationals.\n\n(ii) It could provide us with a useful new domain that can be\nsubstituted for the integers in any application depending only on\nequational properties of the integers, but which differs from the\nintegers in other (necessarily nonequational) useful respects. For\nexample the rationals, which satisfy the same laws as we just noted,\ndiffer in having the density property, that between any two\nrationals there lies another rational. Another difference is that it\nsupports division: whereas the ratio of two integers is usually not an\ninteger, the ratio of two rationals is always a rational. The reals\nalso satisfy the same equations, and like the rationals are dense and\nsupport division. Unlike the rationals however the reals have the\ncompleteness property, that the set of all upper bounds of\nany nonempty set of reals is either empty or has a least member,\nneeded for convergent sequences to have a limit to converge to.\n\nThis idea extends to other operations such as multiplication and\ndivision, as with fields. A particularly useful case of such a\ngeneralization is given by the use of complex numbers in Cartesian\ngeometry. When \\(x\\) and \\(y\\) range over the field of reals, \\(x^2\n+y^2 =1\\) describes the ordinary Euclidean circle in two dimensions,\nbut when the variables range over the complex numbers this equation\ndescribes the complex counterpart of the circle, visualizable as a\ntwo-dimensional surface embedded in four real dimensions (regarding\nthe complex plane as having two real dimensions). Or if the variables\nrange over the integers mod 7, which form a field under the usual\narithmetic operations mod 7, the circle consists of eight points,\nnamely \\((\\pm 1, 0), (0, \\pm 1)\\), and \\((\\pm 2, \\pm 2)\\). Certain\ntheorems about the Euclidean circle provable purely algebraically\nremain provable about these other kinds of circles because all the\nequations on which the proof depends continue to hold in these other\nfields, for example the theorem that a line intersects a circle in at\nmost two points.\n\n(iii) It could help us decide whether some list of equational laws\nintended to axiomatize the integers is complete in the sense\nthat any equation holding of the integers follows from the laws in\nthat list. If some structure satisfies all the axioms in the list, but\nnot some other equation that holds of the integers, then we have a\nwitness to the incompleteness of the axiomatization. If on the other\nhand we can show how to construct any algebra satisfying the axioms\nfrom the algebra of integers, limiting ourselves only to certain\nalgebraic constructions, then by a theorem of Birkhoff applicable to\nthose constructions we can infer that the axiomatization is\ncomplete.\n\n(iv) It could give another of way of defining a class, besides the\nstandard way of listing axioms. In the case at hand, the class of all\nalgebras with a constant, a unary operation, and a binary operation,\nsatisfying all the laws satisfied by the integers, is exactly the\nclass of abelian groups. \n3. Universal Algebra\n\nUniversal algebra is the next level of abstraction after abstract\nalgebra. Whereas elementary algebra treats equational reasoning in a\nparticular algebra such as the field of reals or the field of complex\nnumbers, and abstract algebra studies particular classes of algebras\nsuch as groups, rings, or fields, universal algebra studies classes of\nclasses of algebras. Much as abstract algebra numbers groups, rings,\nand fields among its basic classes, so does universal algebra count\nvarieties, quasivarieties, and elementary classes among its basic\nclasses of classes.\n\nA model of a theory is a structure for which all the\nequations of that theory are identities. Terms are built up from\nvariables and constants using the operations of the theory. An\nequation is a pair of terms; it is satisfied by an algebra when the\ntwo terms are equal under all valuations of (assignments of values to)\nthe \\(n\\) variables appearing in the terms, equivalently when they\ndenote the same \\(n\\)-ary operation. A quasiequation is a pair\nconsisting of a finite set of equations, called the premises or\nantecedents, and another equation, the conclusion; it is satisfied by\nan algebra when the two terms of the conclusion are equal under all\nvaluations of the \\(n\\) variables appearing in the terms satisfying\nthe premises. A first order formula is a quantified Boolean\ncombination of relational terms.\n\nA variety is the class of all models of a set of equations. A\nquasivariety is the class of all models of a set of\nquasiequations. An elementary class is the class of all\nmodels of a set of first-order formulas.\n\nQuasivarieties have received much less attention than either varieties\nor elementary classes, and we accordingly say little about them here.\nElementary classes are treated in sufficient depth elsewhere in this\nencyclopedia that we need not consider them here. We therefore focus\nin this section on varieties.\n\nAbelian groups, groups, rings, and vector spaces over a given field\nall form varieties.\n\nA central result in this area is the theorem that a lattice arises as\nthe lattice of subalgebras of some algebra if and only if it arises as\nthe lattice of congruences on some algebra. Lattices of this sort are\ncalled algebraic lattices. When the congruences of an algebra\npermute, its congruence lattice is modular, a strong condition\nfacilitating the analysis of finite algebras in particular.\n3.1 Concepts\n\nFamiliar theorems of number theory emerge in algebraic form for\nalgebras. An algebra \\(A\\) is called directly irreducible or\nsimple when its lattice of congruences is the two-element\nlattice consisting of \\(A\\) and the one-element algebra, paralleling\nthe notion of prime number \\(p\\) as a number whose lattice of divisors\nhas two elements \\(p\\) and 1. However the counterpart of the\nfundamental theorem of arithmetic, that every positive integer factors\nuniquely as a product of primes, requires a more delicate kind of\nproduct than direct product. Birkhoff\u2019s notion of subdirect\nproduct enabled him to prove the Subdirect Representation Theorem,\nthat every algebra arises as the subdirect product of its subdirectly\nirreducible quotients. Whereas there are many subdirectly irreducible\ngroups, the only subdirectly irreducible Boolean algebra is the\ninitial or two-element one, while the subdirectly irreducible rings\nsatisfying \\(x^n = x\\) for some \\(n \\gt 1\\) are exactly the finite\nfields.\n\nAnother central topic is duality: Boolean algebras are dual to Stone\nspaces, complete atomic Boolean algebras are dual to sets,\ndistributive lattices with top and bottom are dual to partially\nordered sets, algebraic lattices are dual to semilattices, and so on.\nDuality provides two ways of looking at an algebra, one of which may\nturn out to be more insightful or easier to work with than the other\ndepending on the application.\n\nThe structure of varieties as classes of all models of some equational\ntheory is also of great interest. The earliest result in this area is\nBirkhoff\u2019s theorem that a class of algebras is a variety if and\nonly if it is closed under formation of quotients (homomorphic\nimages), subalgebras, and arbitrary (including empty and infinite)\ndirect products. This \u201cmodern algebra\u201d result constitutes\na completeness theorem for equational logic in terms of its models.\nIts elementary counterpart is the theorem that the equational theories\non a free algebra \\(F(V)\\), defined as the deductively closed sets of\nequations that use variables from \\(V\\), are exactly its substitutive\ncongruences.\n\nA locally finite variety is one whose finitely generated free algebras\nare finite, such as pointed sets, graphs (whether of the directed or\nundirected variety), and distributive lattices. A congruence\npermutable variety is a variety all of whose algebras are congruence\npermutable. Maltsev characterized these in terms of a necessary and\nsufficient condition on their theories, namely that \\(F\\)(3) contain\nan operation \\(t(x, y, z)\\) for which \\(t(x, x, y) = t(y, x, x) = y\\)\nare in the theory. Analogous notions are congruence distributivity and\ncongruence modularity, for which there exist analogous syntactic\ncharacterizations of varieties of algebras with these properties. A\nmore recently developed power tool for this area is McKenzie\u2019s\nnotion of tame congruences, facilitating the study of the structure of\nfinite algebras.\n\nWithin the algebraic school, varieties have been defined with the\nunderstanding that the operations of a signature form a set. Insights\nfrom category theory, in particular the expression of a variety as a\nmonad, defined as a monoid object in the category \\(C^C\\) of\nendofunctors of a category \\(C\\) (Set in the case of ordinary\nuniversal algebra) indicate that a cleaner and more general notion of\nvariety is obtained when the operations can form a proper class. For\nexample the important classes of complete semilattices, CSLat, and\ncomplete atomic\n Boolean algebras,\n CABA, form varieties only with this broader notion of signature. In\nthe narrow algebraic sense of variety, the dual of a variety can never\nbe a variety, whereas in the broader monadic notion of variety, the\nvariety Set of sets is dual to CABA while CSLat is self-dual.\n3.2 Equational Logic\n\nAxiom systems. Identities can also be used to transform\nequations to equivalent equations. When those equations are themselves\nidentities for some domain, the equations they are transformed into\nremain identities for that domain. One can therefore start from some\nfinite set of identities and manufacture an unlimited number of new\nidentities from them.\n\nFor example if we start from just the two identities \\((x+y)+z =\nx+(y+z)\\) and \\(x+y = y+x\\), we can obtain the identity \\((w+x)+(y+z)\n= (w+y)+(x+z)\\) via the following series of transformations.\n\n\\[\\begin{align*}\n(w+x)+(y+z) &= ((w+x)+y)+z \\\\\n &=(w+(x+y))+z \\\\\n &=(w+(y+x))+z \\\\\n &=((w+y)+x)+z \\\\\n &=(w+y)+(x+z) \n\\end{align*}\\]\n\n\nThis process of manufacturing new identities from old is called\ndeduction. Any identity that can be generated by deduction\nstarting from a given set \\(A\\) of identities is called a\nconsequence of \\(A\\). The set of all consequences of \\(A\\) is\ncalled the deductive closure of \\(A\\). We refer to \\(A\\) as\nan axiomatization of its deductive closure. A set that is its\nown deductive closure is said to be deductively closed. It is\nstraightforward to show that a set is deductively closed if and\nonly if it is the deductive closure of some set.\n\nAn equational theory is a deductively closed set of\nequations, equivalently the set of all consequences of some set \\(A\\)\nof equations. Every theory always has itself as its own\naxiomatization, but it will usually also have smaller axiomatizations.\nA theory that has a finite axiomatization is said to be finitely\nbased or finitely axiomatizable.\n\nEffectiveness. Finitely based theories can\nbe effectively enumerated. That is, given a finite set \\(A\\) of\nequations, one can write a computer program that prints consequences\nof \\(A\\) for ever in such a way that every consequence of \\(A\\) will\nappear at some finite position in the infinite list of all\nconsequences. The same conclusion obtains when we weaken the\nrequirement that \\(A\\) be finite to merely that it can be effectively\nenumerated. That is, if the axiomatization is effectively enumerable\nso is its deductive closure.\n\n(In reconciling the finite with the infinite, bear in mind that if we\nlist all the natural numbers 0, 1, 2, \u2026 in order, we obtain an\ninfinite list every member of which is only finitely far from the\nbeginning, and also has a well-defined predecessor (except for 0) and\nsuccessor. Only if we attempt to pad this list out at the\n\u201cend\u201d with infinite numbers does this principle break\ndown.\n\nOne way to visualize there being an \u201cend\u201d that could have\nmore elements beyond it is to consider the rationals of the form\n\\(1/n\\) for all nonzero integers \\(n\\), in increasing order. This list\nstarts out \\(-1/1, -1/2, -1/3,\\ldots\\) and after listing infinitely\nmany negative rationals of that form, with no greatest such, switches\nover to positive rationals, with no first such, finally ending with\n1/3, 1/2, 1/1. The entire list is discrete in the sense that every\nrational except the endpoints \\(-1/1\\) and 1/1 has a well-defined\npredecessor and successor in this subset of the rationals, unlike the\nsituation for the set of all rationals between \\(-1/1\\) and \\(1/1\\).\nThis would no longer be the case were we to introduce the rational 0\n\u201cin the middle\u201d, which would have neither a predecessor\nnor a successor.)\n\nEquational Logic. Our informal account of\ndeduction can be formalized in terms of five rules for producing new\nidentities from old. In the following, \\(s\\) and \\(t\\) denote\narbitrary terms.\n\n(R1)\nFrom nothing infer \\(t = t\\).\n(R2)\nFrom \\(s = t\\) infer \\(t = s\\).\n(R3)\nFrom \\(s = t\\) and \\(t = u\\) infer \\(s = u\\).\n(R4)\nFrom \\(s_1 = t_1, s_2 = t_2 , \\ldots ,s_n = t_n\\) infer \\(f(s_1,\ns_2 , \\ldots ,s_n)= f(t_1, t_2 , \\ldots ,t_n)\\), where \\(f\\) is an\n\\(n\\)-ary operation.\n(R5)\nFrom \\(s = t\\) infer \\(s' = t'\\) where \\(s'\\) and \\(t'\\) are the\nterms resulting from consistently substituting terms for variables in\n\\(s\\) and \\(t\\) respectively.\n\n\n\u201cConsistently\u201d in this context means that if a term is\nsubstituted for one occurrence of a given variable, the same term must\nbe substituted for all occurrences of that variable in both \\(s\\) and\n\\(t\\). We could not for example appeal solely to R5 to\njustify substituting \\(u+v\\) for \\(x\\) in the left hand side of \\(x+y\n= y+x\\) and \\(v+u\\) for \\(x\\) in the right hand side, though some\nother rule might permit it.\n\nAn equational theory as a set of pairs of terms amounts to a binary\nrelation on the set of all terms. Rules R1\u2013R3\ncorrespond to respectively reflexivity, symmetry, and transitivity of\nthis binary relation, \\(i.e\\). these three rules assert that an\nequational theory is an equivalence relation. Rule\nR4 expresses the further property that this binary relation\nis a congruence. Rule R5 further asserts that the\nrelation is a substitutive congruence. It can be shown that a binary\nrelation on the set of terms is an equational theory if and only if it\nis a substitutive congruence. These five rules therefore completely\naxiomatize equational logic in the sense that every consequence of a\nset \\(A\\) of equations can be produced from \\(A\\) via finitely many\napplications of these five rules.\n3.3 Birkhoff\u2019s Theorem\n\nA variety is by definition the class of models of some equational\ntheory. In 1935 Birkhoff provided an equivalent characterization of\nvarieties as any class closed under quotients (homomorphic images),\ndirect products, and subalgebras. These notions are defined as\nfollows.\n\nGiven two algebras \\((X, f_1 , \\ldots f_k)\\) and \\((Y, g_1 , \\ldots\ng_k)\\), a homomorphism \\(h: (X, f_1 , \\ldots f_k) \\rightarrow\n(Y, g_1 , \\ldots g_k)\\) is a function \\(h: X \\rightarrow Y\\)\nsatisfying \\(h(f_i (x_0 , \\ldots ,x_{n_{ i}-1 })) = g_i (h(x_0),\n\\ldots ,h(x_{n_{ i}-1 })))\\) for each \\(i\\) from 1 to \\(k\\) where\n\\(n_i\\) is the arity of both \\(f_i\\) and \\(g_i\\).\n\nA subalgebra of an algebra is a set of elements of the\nalgebra closed under the operations of the algebra.\n\nLet \\(I\\) be an arbitrary set, which may be empty, finite, or\ninfinite. A family \\(\\langle A_{i}\\rangle_{i\\in I}\\) of\nalgebras \\((X_i, f_{1}^i,\\ldots, f_k^i)\\) indexed by \\(I\\) consists of\none algebra \\(A_i\\) for each element \\(i\\) of \\(I\\). We define the\ndirect product \\(\\Pi A_i\\) (or \\(\\Pi_{i\\in I} A_i\\) in full)\nof such a family as follows.\n\nThe underlying set of \\(\\Pi A_i\\) is the cartesian product \\(\\Pi X_i\\)\nof the underlying sets \\(X_i\\), and consists of those \\(I\\)-tuples\nwhose \\(i\\)-th element is some element of \\(X_i\\). (\\(I\\) may even be\nuncountable, but in this case the nonemptiness of \\(\\Pi X_i\\) as a\nconsequence of the nonemptiness of the individual \\(X_i\\)\u2019s is\nequivalent to the axiom of choice. This should be kept in mind for any\nconstructive applications of Birkhoff\u2019s theorem.)\n\nThe \\(j\\)-th operation of \\(\\Pi A_i\\), of arity \\(n_j\\), takes an\n\\(n_j\\)-tuple \\(t\\) of elements of \\(\\Pi X_i\\) and produces the\n\\(I\\)-tuple \\(\\langle f_{j}^i(t_{1}^i , \\ldots t_{n_{ j}\n}^i)\\rangle_{i\\in I}\\) where \\(t_k^i\\) is the \\(i\\)-th component of\nthe \\(k\\)-th component of \\(t\\) for \\(k\\) from 1 to \\(n_j\\).\n\nGiven two algebras \\(A\\), \\(B\\) and a homomorphism \\(h: A \\rightarrow\nB\\), the homomorphic image \\(h(A)\\) is the subalgebra of\n\\(B\\) consisting of elements of the form \\(h(a)\\) for \\(a\\) in\n\\(A\\).\n\nGiven a class \\(C\\) of algebras, we write \\(P(C)\\) for the class of\nall algebras formed as direct products of families of algebras of \\(C,\nS(C)\\) for the class of all subalgebras of algebras of \\(C\\), and\n\\(H(C)\\) for the class of all homomorphic images of algebras of\n\\(C\\).\n\nIt is relatively straightforward to show that any equation satisfied\nby all the members of \\(C\\) is also satisfied by all the members of\n\\(P(C), S(C)\\), and \\(H(C)\\). Hence for a variety \\(V, P(V) = S(V) =\nH(V)\\).\n\nBirkhoff\u2019s theorem is the converse: for any class \\(C\\) such\nthat \\(P(C) = S(C) = H(C), C\\) is a variety. In fact the theorem is\nslightly stronger: for any class \\(C\\), HSP\\((C)\\) is a\nvariety. That is, to construct all the models of the theory of \\(C\\)\nit suffices to close \\(C\\) first under direct products, then under\nsubalgebras, and finally under homomorphic images; that is, later\nclosures do not compromise earlier ones provided \\(P, S\\), and \\(H\\)\nare performed in that order.\n\nA basic application of Birkhoff\u2019s theorem is in proving the\ncompleteness of a proposed axiomatization of a class \\(C\\). Given an\narbitrary model of the axioms, it suffices to show that the model can\nbe constructed as the homomorphic image of a subalgebra of a direct\nproduct of algebras of \\(C\\).\n\nThis completeness technique complements the completeness observed in\nthe previous section for the rules of equational logic.\n4. Linear Algebra\n4.1 Vector Spaces\n\nSibling to groups, rings, and fields is the class of vector\nspaces over any given field, constituting the universes of linear\nalgebra. Vector spaces lend themselves to two opposite approaches:\naxiomatic or abstract, and synthetic or concrete. The axiomatic\napproach takes fields (whence rings, whence groups) as a prerequisite;\nit first defines a notion of \\(R\\)-module as an abelian group with a\nscalar multiplication over a given ring \\(R\\), and then defines a\nvector space to be an \\(R\\)-module for which \\(R\\) is a field. The\nsynthetic approach proceeds via the familiar representation of vector\nspaces over the reals as \\(n\\)-tuples of reals, and of linear\ntransformations from \\(m\\)-dimensional to \\(n\\)-dimensional vector\nspaces as \\(m\\times n\\) matrices of reals. For the full generality of\nvector spaces including those of infinite dimension, \\(n\\) need not be\nlimited to finite numbers but can be any cardinal.\n\nThe abstract approach, as adopted by such classical texts as Mac Lane\nand Birkhoff, has a certain purist appeal and is ideally suited to\nmathematics majors. The concrete approach has the benefit of being\nable to substitute calculus or less for groups-rings-fields as a\nprerequisite, suiting it to service courses for scientists and\nengineers needing only finite-dimensional matrix algebra, which enjoys\nenormous practical applicability. Linear algebra over other fields, in\nparticular finite fields, is used in coding theory, quantum computing,\netc., for which the abstract approach tends to be better suited.\n\nFor any field \\(F\\), up to isomorphism there is exactly one vector\nspace over \\(F\\) of any given finite dimension. This is a theorem in\nthe abstract approach, but is an immediate consequence of the\nrepresentation in the concrete approach (the theorem is used in\nrelating the two approaches).\n\nAnother immediate consequence of the concrete approach is duality for\nfinite-dimensional vector spaces over \\(F\\). To every vector space\n\\(V\\), of any dimension, corresponds its dual space \\(V^*\\) comprised\nof the functionals on \\(V\\), defined as the linear\ntransformations \\(f: V\\rightarrow F\\), viewing the field \\(F\\) as the\none-dimensional vector space. The functionals form a vector space\nunder coordinatewise addition \\((f+g)(u) = f(u)+g(u)\\) and\nmultiplication \\((xf)(u) = x(f(u))\\) by any scalar \\(x\\) in \\(F\\), and\nwe take \\(V^*\\) to be that space. This operation on vector spaces\nextends to the linear transformations \\(f: U\\rightarrow V\\) as \\(f^* :\nV^*\\rightarrow U^*\\) defined such that \\(f\\) maps each functional \\(g:\nV\\rightarrow F\\) to \\(g\\cdot f: U\\rightarrow F\\). Repeating this\noperation produces a vector space that, in the finite-dimensional\ncase, is isomorphic to \\(V\\), that is, \\(V \\cong V^{**}\\), making the\noperation an involution. The essence of duality for finite-dimensional\nvector spaces resides in its involutary nature along with the reversal\nof the linear transformations.\n\nThis duality is easily visualized in the concrete approach by viewing\nlinear transformations from \\(U\\) to \\(V\\) as \\(m\\times n\\) matrices.\nThe duality simply transposes the matrices while leaving the machinery\nof matrix multiplication itself unchanged. It is then immediate that\nthis operation is an involution that reverses maps\u2014the \\(m\\times\nn\\) matrix linearly transforming an \\(n\\)-dimensional space \\(U\\) to\nan \\(m\\)-dimensional one \\(V\\) transposes to an \\(n\\times m\\) matrix\nlinearly transforming the \\(m\\)-dimensional space \\(V^*\\) to the\n\\(n\\)-dimensional space \\(U^*\\).\n4.2 Associative Algebras\n\nThe linear transformations \\(f: V\\rightarrow V\\) on a vector space\n\\(V\\) can be added, subtracted, and multiplied by scalars, pointwise\nin each case, and hence form a vector space. When the space has finite\ndimension \\(n\\), the linear transformations are representable as\n\\(n\\times n\\) matrices.\n\nIn addition they can be composed, whence they form a vector space\nequipped with a bilinear associative operation, namely composition. In\nthe finite-dimensional case, composition is just the usual matrix\nproduct. Vector spaces furnished with such a product constitute\nassociative algebras. Up to isomorphism, all associative\nalgebras arise in this way whether of finite or infinite dimension,\nproviding a satisfactory and insightful characterization of the notion\nin lieu of an axiomatic characterization, not given here.\n\nWell-known examples of associative algebras are the reals, the complex\nnumbers, and the quaternions. Unlike vector spaces, many nonisomorphic\nassociative algebras of any given dimension greater than one are\npossible.\n\nA class of associative algebras of interest to physicists is that of\nthe Clifford algebras. Clifford algebras over the reals (which as\nvector spaces are Euclidean spaces) generalize complex numbers and\nquaternions by permitting any number of formal quantities \\(e\\)\nanalogous to \\(i = \\sqrt{-1}\\) to be adjoined to the field of reals.\nThe common feature of these quantities is that each satisfies either\n\\(e^2 = -1\\) or \\(e^2 = 1\\). Whereas there are a great many\nassociative algebras of low dimension, only a few of them arise as\nClifford algebras. The reals form the only one-dimensional Clifford\nalgebra, while the hyperbolic plane, defined by \\(e^2 = 1\\), and the\ncomplex plane, defined by \\(e^2 = -1\\), are the two two-dimensional\nClifford algebras. The hyperbolic plane is just the direct square of\nthe real field, meaning that its product is coordinatewise, \\((a,\nb)(c, d) = (ac, bd)\\), unlike that of the complex plane where it\ndefined by \\((a, b)(c, d) = (ac - bd, ad+bc)\\). The two\nfour-dimensional Clifford algebras are the \\(2\\times 2\\) matrices and\nthe quaternions. Whereas the \\(2\\times 2\\) matrices contain zero\ndivisors (nonzero matrices whose product is zero), and so form only a\nring, the quaternions contain no zero divisors and so form a division\nring. Unlike the complex numbers however, the quaternions do not form\na field because their multiplication is not commutative. Complex\nmultiplication however makes the complex plane a commutative division\nring, that is, a field.\n5. Algebraization of mathematics\n\nA number of branches of mathematics have benefited from the\nperspective of algebra. Each of algebraic geometry and algebraic\ncombinatorics has an entire journal devoted to it, while algebraic\ntopology, algebraic logic, and algebraic number theory all have strong\nfollowings. Many other more specialized areas of mathematics have\nsimilarly benefited.\n5.1 Algebraic geometry\n\nAlgebraic geometry begins with what we referred to in the introduction\nas shapes, for example lines \\(y = ax +b\\), circles \\(x^2 +y^2 =\nr^2\\), spheres \\(x^2 +y^2 +z^2 = r^2\\), conic sections \\(f(x, y) = 0\\)\nwhere \\(f\\) is a quadratic polynomial in \\(x\\) and \\(y\\), quadric\nsurfaces \\(f(x, y, z) = 0\\) with \\(f\\) again quadratic, and so on.\n\nIt is convenient to collect the two sides of these equations on the\nleft so that the right side is always zero. We may then define a shape\nor variety to consist of the roots or zeros of a\npolynomial, or more generally the common zeros of a set of\npolynomials.\n\nOrdinary analytical or Cartesian geometry is conducted over the reals.\nAlgebraic geometry is more commonly conducted over the complex\nnumbers, or more generally over any algebraically closed field. The\nvarieties definable in this way are called affine\nvarieties.\n\nSometimes however algebraic closure is not desirable, for example when\nworking at the boundary of algebraic geometry and number theory where\nthe field may be finite, or the rationals.\n\nMany kinds of objects are characterized by what structure their maps\nhold invariant. Posets transform via monotone functions, leaving order\ninvariant. Algebras transform via homomorphisms, leaving the algebraic\nstructure invariant. In algebraic geometry varieties transform via\nregular \\(n\\)-ary functions \\(f: A^n \\rightarrow\nA\\), defined as functions that are locally rational polynomials in\n\\(n\\) variables. Locally rational means that at each point of the\ndomain of \\(f\\) there exists a neighborhood on which \\(f\\) is the\nratio of two polynomials, the denominator of which is nonzero in that\nneighborhood.\n\nThis notion generalizes to regular functions \\(f: A^n \\rightarrow\nA^m\\) defined as \\(m\\)-tuples of regular \\(n\\)-ary functions.\n\nGiven two varieties \\(V, V'\\) in \\(A^n\\) and \\(A^m\\) respectively, a\nregular function from \\(A^n\\) to \\(A^m\\) whose restriction to \\(V\\) is\na function from \\(V\\) to \\(V'\\) is called a regular function of\nvarieties. The category of affine varieties is then defined to\nhave as its objects all affine varieties and as its morphisms all\nregular functions thereof.\n\nPolynomials being continuous, one would expect regular functions\nbetween varieties to be continuous also. A difficulty arises with the\nshapes of varieties, where there can be cusps, crossings, and other\nsymptoms of singularity. What is needed here is a suitable topology by\nwhich to judge continuity.\n\nThe trick is to work not in affine space but its projective\nspace. To illustrate with Euclidean three-space, its associated\nprojective space is the unit sphere with antipodal points identified,\nforming a two-dimensional manifold. Equivalently this is the space of\nall (unoriented) lines through the origin. Given an arbitrary affine\nspace, its associated projective space is the space of all such lines,\nunderstood as a manifold.\n\nThe topology on projective space appropriate for algebraic geometry is\nthe Zariski topology, defined not by its open sets but rather\nby its closed sets, which are taken to be the algebraic sets, namely\nthose sets constituting the common zeros of a set of homogeneous\npolynomials. The crucial theorem is then that regular maps between\naffine varieties are continuous with respect to the Zariski\ntopology.\n5.2 Algebraic number theory\n\nAlgebraic number theory has adopted these generalizations of algebraic\ngeometry. One class of varieties in particular that has been of great\nimportance to number theory is that of elliptical curves.\n\nA celebrated success of algebraic number theory has been Andrew\nWiles\u2019 proof of Fermat\u2019s so-called \u201clast\ntheorem.\u201d This had remained an open problem for over three and a\nhalf centuries.\n5.3 Algebraic topology\n\nAlgebraic topology analyzes the holes and obstructions in connected\ntopological spaces. A topologist is someone who imagines all objects\nto be made of unbreakable but very pliable playdough, and therefore\ndoes not see the need to distinguish between a coffee cup and doughnut\nbecause either can be turned into the other. Topology is concerned\nwith the similarities and differences between coffee cups with \\(n\\)\nhandles, surfaces with \\(n\\) holes, and more complicated shapes.\nAlgebraic topology expresses the invariants of such shapes in terms of\ntheir homotopy groups and homology groups.\n5.4 Algebraic logic\n\nAlgebraic logic got off to an early start with Boole\u2019s\nintroduction of Boolean algebra in an 1847 pamphlet. The methods of\nmodern algebra began to be applied to Boolean algebra in the 20th\ncentury. Algebraic logic then broadened its interests to first order\nlogic and modal logic. Central algebraic notions in first order logic\nare ultraproducts, elementary equivalence, and elementary and\npseudoelementary varieties. Tarski\u2019s cylindric algebras\nconstitute a particular abstract formulation of first order logic in\nterms of diagonal relations coding equality and substitution relations\nencoding variables. Modal logic as a fragment of first order logic is\nmade algebraic via Boolean modules.\n6. Free Algebras\n\nGiven any system such as integer arithmetic or real arithmetic, we can\nwrite \\(T\\) for the set of all definite terms such as \\(1 + (2/3)\\)\nbuilt from constants and constituting the definite language, and\n\\(T[V]\\) for the larger indefinite language permitting variables drawn\nfrom a set \\(V\\) in place of some of the constant symbols, with terms\nsuch as \\(x + (2/y)\\). When \\(V\\) contains only a single variable\n\\(\u201cx\u201d\\), \\(T[\\{\u201cx\u201d\\}]\\) is usually abbreviated\nto \\(T[\u201cx\u201d]\\) or just \\(T[x\\)] which is usually\nunambiguous. This convention extends to the algebra \\(\\Phi\\) of terms\nof \\(T\\) together with its list of operation symbols viewed as\noperations for combining terms; we write \\(\\Phi[V\\)] and call it the\nterm algebra on \\(V\\). \n\nThis notion of term algebra is a purely syntactic one involving only\nthe operation symbols, constants, and variables of some language. The\nterms \\(2 + 3\\) and \\(3 + 2\\) are distinct; likewise \\(x + y\\) and \\(y\n+ x\\) are distinct terms. As such they can be considered concrete\nterms.\n\nNow in a universe such as the integers certain concrete terms are\nequivalent in the sense that they always evaluate to the same element\nof the universe regardless of the values of their variable, for\nexample \\(x + y\\) and \\(y + x\\). It is convenient to collect\nequivalent concrete terms into equivalence classes each of which is to\nbe thought of as an abstract term.\n\nAs a simple example of abstract terms consider linear polynomials of\nthe form \\(ax + by\\) where \\(a\\) and \\(b\\) are nonnegative integers,\nfor example \\(7x + 3y\\). The set of all such polynomials includes 0\nand is closed under polynomial addition, an associative and\ncommutative operation. This set together with the operation of\naddition and the zero polynomial therefore constitutes a commutative\nmonoid.\n\nThis monoid is an example of a free algebra, namely the free\ncommutative monoid on two generators \\(x\\) and \\(y\\). What makes it\nfree is that it satisfies no laws other than those of a commutative\nmonoid. It is not however a free monoid because it satisfies the\ncommutative law. The free monoid on two generators \\(x\\) and \\(y\\) is\ninstead the set of all finite strings over the two-letter alphabet\n\\(\\{x,y\\}\\).\n\nWhen commutativity is introduced as a law, it identifies the\npreviously distinct strings \\(xy\\) and \\(yx\\) as a single polynomial;\nmore generally any two strings with the same number of \\(x\\)s and\n\\(y\\)s are identified.\n\nFree monoids and free commutative monoids are examples of free\n\\(C\\)-algebras where \\(C\\) is a class of algebras. In these two\nexamples the class \\(C\\) is respectively that of monoids and\ncommutative monoids.\n\nA free \\(C\\)-algebra is an algebra that lives at the frontier of\nsyntax and semantics. On the semantic side it is a member of \\(C\\). On\nthe syntactic side its elements behave like terms subject to the laws\nof \\(C\\), but no other laws expressible with its generators.\nCommutativity \\(xy = yx\\) is expressible with two generators and so a\nfree monoid on two or more generators cannot be commutative, though\nthe free monoid on one generator, namely the set of all finite strings\nover a one-letter alphabet does form a commutative monoid on one\ngenerator.\n\nOn the syntactic side, the free \\(C\\)-algebra \\(B\\) on a set \\(X\\)\narises as a quotient of the term algebra formed from \\(X\\) (viewed as\na set of variables) using the operation symbols and constants common\nto the algebras of \\(C\\). The quotient identifies those terms that\nhave the same value for all algebras \\(A\\) of \\(C\\) and all valuations\nassigning values in \\(A\\) to the variables of \\(X\\). This performs\njust enough identifications to satisfy every law of \\(C\\) (thereby\nmaking this quotient a \\(C\\)-algebra) while still retaining the\nsyntactic essence of the original term algebra in a sense made more\nprecise by the following paragraph.\n\n(Since the concept of a term algebra can seem a little circular in\nplaces, a more detailed account may clarify the concept. Given the\nlanguage of \\(C\\), meaning the operation symbols and constant symbols\ncommon to the algebras of \\(C\\), along with a set \\(X\\) of variables,\nwe first form the underlying set of the algebra, and then interpret\nthe symbols of the language as operations on and values in that set.\nThe set itself consists of the terms built in the usual way from those\nvariables and constant symbols using the operation symbols; in that\nsense these elements are syntactic. But now we change our point of\nview by treating those elements as semantic, and we look to the\nconstant symbols and operation symbols of the language as syntactic\nentities needing to be interpreted in this semantic domain (albeit of\nterms) in order to turn this set of terms into an algebra of terms. We\ninterpret each constant symbol as itself. And we interpret each\n\\(n\\)-ary operation symbol \\(f\\) as the \\(n\\)-ary operation that takes\nany \\(n\\) terms \\(t_1 , \\ldots ,t_n\\) as its \\(n\\) arguments and\nreturns the single term \\(f(t_1 , \\ldots ,t_n)\\). Note that this\ninterpretation of \\(f\\) only returns a term, it does not\nactually build it. All term building was completed when we\nproduced the underlying set of the algebra.)\n\nFrom the semantic side, a \\(C\\)-algebra \\(B\\) together with a subset\n\\(X\\) of \\(B\\) thought of as variables is said to be a free\n\\(C\\)-algebra on \\(X\\), or is freely generated by \\(X\\), when, given\nany \\(C\\)-algebra \\(A\\), any valuation in \\(A\\) of the variables in\n\\(X\\) (that is, any function \\(f: X\\rightarrow A\\)) uniquely extends\nto a homomorphism \\(h: B\\rightarrow A\\). (We say that \\(h:\nB\\rightarrow A\\) extends \\(f: X\\rightarrow A\\) when the restriction of\n\\(h\\) to \\(X\\) is \\(f\\).)\n\nAs a convenient shorthand a free \\(C\\)-algebra on no generators can\nalso be called an initial \\(C\\)-algebra. An initial \\(C\\)-algebra has\nexactly one homomorphism to every \\(C\\)-algebra.\n\nBefore proceeding to the examples it is worthwhile pointing out an\nimportant basic property of free algebras as defined from the semantic\nside.\n\nTwo free algebras \\(B, B'\\) on respective generator sets \\(X, Y\\)\nhaving the same cardinality are isomorphic.\n\nBy way of proof, pick any bijection \\(f: X\\rightarrow Y\\). This, its\ninverse \\(f': Y\\rightarrow X\\), and the two identity functions on\nrespectively \\(X\\) and \\(Y\\), form a system of four functions closed\nunder composition. Each of these functions is from a generator set to\nan algebra and therefore has a unique extension to a homomorphism.\nThese four homomorphisms are also closed under composition. The one\nfrom \\(B\\) to itself extends the identity function on \\(X\\) and\ntherefore must be the identity homomorphism on \\(B\\) (since the latter\nexists and its restriction to \\(X\\) is the identity function on\n\\(X)\\). Likewise the homomorphism from \\(G\\) to \\(G\\) is an identity\nfunction. Hence the homomorphisms between \\(B\\) and \\(G\\) compose in\neither order to identities, which makes them isomorphisms. But this is\nwhat it means for \\(B\\) and \\(B'\\) to be isomorphic.\n\nThis fact allows us to say the free algebra on a given set,\nthinking of isomorphic algebras as being \u201cmorally\u201d the\nsame. Were this not the case, our quotient construction would be\nincomplete as it produces a unique free algebra, whereas the above\ndefinition of free algebra allows any algebra isomorphic to that\nproduced by the quotient construction to be considered free. Since all\nfree algebras on \\(X\\) are isomorphic, the quotient construction is as\ngood as any, and is furthermore one way of proving that they exist. It\nalso establishes that the choice of set of variables is irrelevant\nexcept for its cardinality, as intuition would suggest.\n6.1 Free monoids and groups\n\nTake \\(C\\) to be the class of monoids. The term algebra determined by\nthe binary operation symbol and the constant symbol for identity can\nbe viewed as binary trees with variables and copies of the constant\nsymbol at the leaves. Identifying trees according to associativity has\nthe effect of flattening the trees into words that ignore the order in\nwhich the operation was applied (without however reversing the order\nof any arguments). This produces words over the alphabet \\(X\\)\ntogether with the identity. The identity laws then erase the\nidentities, except in the case of a word consisting only of the\nidentity symbol, which we take to be the empty word.\n\nThus the monoid of finite words over an alphabet \\(X\\) is the free\nmonoid on \\(X\\).\n\nAnother representation of the free monoid on \\(n\\) generators is as an\ninfinite tree, every vertex of which has \\(n\\) descendants, one for\neach letter of the alphabet, with each edge labeled by the\ncorresponding letter. Each vertex \\(v\\) represents the word consisting\nof the letters encountered along the path from the root to \\(v\\). The\nconcatenation of \\(u\\) and \\(v\\) is the vertex arrived at by taking\nthe subtree whose root is the vertex \\(u\\), noticing that this tree is\nisomorphic to the full tree, and locating \\(v\\) in this subtree as\nthough it were the full tree.\n\nIf we ignore the direction and labels of the edges in this tree we can\nstill identify the root: it is the only vertex with \\(n\\) edges\nincident on it, all other vertices have \\(n+1\\), namely the one\nincoming edge and the \\(n\\) outgoing ones.\n\nThe free commutative monoid on a set is that monoid whose\ngenerators behave like letters just as for free monoids (in particular\nthey are still atoms), but which satisfy the additional law \\(uv =\nvu\\). We make further identifications, e.g. of\n\u201cdog\u201d and \u201cdgo\u201d. Order of letters in a word is\nnow immaterial, all that matters is how many copies there are of each\nletter. This information can be represented as an \\(n\\)-tuple of\nnatural numbers where \\(n\\) is the size of the alphabet. Thus the free\ncommutative monoid on \\(n\\) generators is \\(N^n\\), the algebra of\n\\(n\\)-tuples of natural numbers under addition.\n\nIt can also be obtained from the tree representation of the free\nmonoid by identifying vertices. Consider the case \\(n = 2\\) of two\nletters. Since the identifications do not change word length, all\nidentifications are of vertices at the same depth from the root. We\nperform all identifications simultaneously as follows. At every vertex\n\\(v\\), identify \\(v_{01}\\) and \\(v_{10}\\) and their subtrees. Whereas\nbefore there were \\(2^n\\) vertices at depth \\(n\\), now there are\n\\(n+1\\). Furthermore instead of a tree we have the upper right\nquadrant of the plane, that is, \\(N^2\\), rotated 135 degrees\nclockwise, with every vertex \\(v\\) at the top of a diamond whose other\nvertices are \\(v_0\\) and \\(v_1\\) at the next level down, and the\nidentified pair \\(v_{01} = v_{10}\\) below both.\n\nTo form the free group on \\(n\\) generators, first form the free monoid\non \\(2n\\) generators, with generators organized into complementary\npairs each the inverse of the other, and then delete all adjacent\ncomplementary pairs from all words.\n\nThis view is not particularly insightful. The group counterpart of the\ntree representation does a better job of presenting a free group.\nConsider the free group on \\(n = 2\\) generators \\(A\\) and \\(B\\). We\nstart with the free monoid on 4 generators \\(A, B, a, b\\) where \\(a\\)\nis the inverse of \\(A\\) and \\(b\\) that of \\(B\\). Every vertex of this\ntree has 4 descendants. So the root has degree 4 and the remaining\nvertices have degree 5: every vertex except the root has one edge\ngoing in, say the generator \\(a\\), and four out. Consider any nonroot\nvertex \\(v\\). The effect of deleting adjacent complementary pairs is\nto identify the immediate ancestor of \\(v\\) with one of the four\ndescendants of \\(v\\), namely the one that makes the path from the\nancestor to the descendant a complementary pair. For every nonroot\nvertex \\(v\\) these identifications reduce the degree of \\(v\\) from 5\nto 4. The root remains at degree 4.\n\nSo now we have an infinite graph every vertex of which has degree 4.\nUnlike the tree for the free monoid on 2 generators, where the root is\ntopologically different from the other vertices, the tree for the free\ngroup on 2 generators is entirely homogeneous. Thus if we throw away\nthe vertex labels and rely only on the edge labels to navigate, any\nvertex can be taken as the identity of the group.\n\nThis homogeneity remains the case for the free abelian group on 2\ngenerators, whose vertices are still of degree 4. However the\nadditional identifications turns it from a tree (a graph with no\ncycles) to a grid whose vertices are the lattice points of the plane.\nThat is, the free abelian group on 2 generators is \\(Z^2\\), and on\n\\(n\\) generators \\(Z^n\\). The edges are the line segments joining\nadjacent lattice points.\n6.2 Free rings\n\nWith no generators the free monoid, free group, and free ring are all\nthe one-element algebra consisting of just the additive identity 0. A\nring with identity means having a multiplicative identity, that is, a\nword \\(\\varepsilon\\). But this makes \\(\\varepsilon\\) a generator for\nthe additive group of the ring, and the free abelian group on one\ngenerator is the integers. So the free ring with identity on no\ngenerators is the integers under addition and now multiplication.\n\nThe free ring on one generator \\(x\\) must include \\(x^2 , x^3\\), etc.\nby multiplication, but these can be added and subtracted resulting in\npolynomials such as \\(7x^3 -3x^2 +2x\\) but without a constant term,\nwith the exception of 0 itself. The distributivity law for rings means\nthat a term such as \\((7x+x^2 )(2x^3 +x)\\) can be expanded as \\(7x^2\n+x^3 +14x^4 +2x^5\\). It should now be clear that these are just\nordinary polynomials with no constant term; in particular we are\nmissing the zero-degree polynomial 1 and so this ring has no\nmultiplicative identity. However it is a commutative ring even though\nwe did not specify this. The free ring with identity on one generator\nintroduces 1 as the multiplicative identity and becomes the ordinary\none-variable polynomials since now we can form all the integers. Just\nas with monoids, the free ring with identity on two generators is not\ncommutative, the polynomials \\(xy\\) and \\(yx\\) being distinct. The\nfree commutative ring with identity on two generators however\nconsists of the ordinary two-variable polynomials over the\nintegers.\n6.3 Free combinatorial structures\n\nFrom the examples so far one might conclude that all free algebras on\none or more generators are infinite. This is by no means always the\ncase; as counterexamples we may point to a number of classes: sets,\npointed sets, bipointed sets, graphs, undirected graphs, Boolean\nalgebras, distributive lattices, etc. Each of these forms a locally\nfinite variety as defined earlier.\n\nA pointed set is an algebra with one constant, say \\(c\\). The free\npointed set on \\(x\\) and \\(y\\) has three elements, \\(x, y\\), and\n\\(c\\). A bipointed set is an algebra with two constants \\(c\\) and\n\\(d\\), and the free bipointed set on \\(x\\) and \\(y\\) then has four\nelements, \\(x, y, c\\), and \\(d\\).\n\nGraphs, of the oriented kind arising in say automata theory where\nmultiple edges may connect the same two vertices, can be organized as\nalgebras having two unary operations \\(s\\) and \\(t\\) satisfying\n\\(s(s(x)) = t(s(x)) = s(x)\\) and \\(t(t(x)) = s(t(x)) = t(x)\\). The\nfree graph on one generator \\(x\\) has three elements, \\(x, s(x)\\), and\n\\(t(x)\\), constituting respectively an edge and its two endpoints or\nvertices. In this framework the vertices are the elements\nsatisfying \\(s(x) = x\\) (and hence \\(t(x) = x\\) since \\(x = s(x) =\nt(s(x)) = t(x)\\)); all other elements constitute edges. The free graph\non \\(n\\) generators consists of \\(n\\) such edges, all independent.\nOther graphs arise by identifying elements. There is no point\nidentifying an edge with either another edge or a vertex since that\nsimply absorbs the first edge into the second entity. This leaves only\nvertices; identifying two vertices yields a single vertex common to\ntwo edges, or to the same edge in the identification \\(s(x) = t(x)\\)\ncreating a self-loop.\n\nThe term \u201coriented\u201d is to be preferred to\n\u201cdirected\u201d because a directed graph as understood in\ncombinatorics is an oriented graph with the additional property that\nif \\(s(x) = s(y)\\) and \\(t(x) = t(y)\\) then \\(x = y\\); that is, only\none edge is permitted between two vertices in a given direction.\n\nUnoriented graphs are defined as for graphs with an additional unary\noperation \\(g\\) satisfying \\(g(g(x)) = x\\) and \\(s(g(x)) = t(x)\\)\n(whence \\(s(x) = s(g(g(x))) = t(g(x)))\\). The free undirected graph on\n\\(x\\) consists of \\(x, s(x), t(x)\\), and \\(g(x)\\), with the pair \\(x,\ng(x)\\) constituting the two one-way lanes of a two-lane highway\nbetween \\(s(x) = t(g(x))\\) and \\(t(x) = s(g(x)\\)). Identification of\nelements of undirected graphs works as for their oriented\ncounterparts: it is only worth identifying vertices. However there is\none interesting twist here: vertices can be of two kinds, those\nsatisfying \\(x = g(x)\\) and those not. The latter kind of vertex is\nnow asymmetric: one direction of the bidirectional edge is identified\nwith its vertices while the other one forms an oriented loop in the\nsense that its other direction is a vertex. This phenomenon does not\narise for undirected graphs defined as those satisfying \u201cif\n\\(s(x) = s(y)\\) and \\(t(x) = t(y)\\) then \\(x = y\\).\u201d\n6.4 Free logical structures\n\nBoolean algebras are traditionally defined axiomatically as\ncomplemented distributive lattices, which has the benefit of showing\nthat they form a variety, and furthermore a finitely axiomatizable\none. However Boolean algebras are so fundamental in their own right\nthat, rather than go to the trouble of defining lattice, distributive,\nand complemented just for this purpose, it is easier as well as more\ninsightful to obtain them from the initial Boolean algebra. It\nsuffices to define this as the two-element set \\(\\{0, 1\\}\\), the\nconstants (zeroary operations) 0 and 1, and the \\(2^{2^2} = 16\\)\nbinary operations. A Boolean algebra is then any algebra with those 16\noperations and two constants satisfying the equations satisfied by the\ninitial Boolean algebra.\n\nAn almost-definitive property of the class of Boolean algebras is that\ntheir polynomials in the initial Boolean algebra are all the\noperations on that algebra. The catch is that the inconsistent class\nconsisting of only the one-element or inconsistent algebra also has\nthis property. This class is easily ruled out however by adding that\nBoolean algebra is consistent. But just barely\u2014adding any new\nequation to Boolean algebra (without introducing new operations)\naxiomatizes the inconsistent algebra.\n\nSheffer has shown that the constants and the 16 operations can be\ngenerated as polynomials in just one constant, which can be 0 or 1,\nand one binary operation, which can be NAND, \\(\\neg(x\\wedge y)\\), or\nNOR, \\(\\neg(x\\vee y)\\). Any such sufficient set is called a\nbasis. Along the same lines Stone has shown that conjunction,\nexclusive-or, and the constant 1 form a basis. The significance of\nStone\u2019s basis over Sheffer\u2019s is that Boolean algebras\norganized with those operations satisfy all the axioms for a\ncommutative ring with identity with conjunction as multiplication and\nexclusive-or as addition, as well as the law \\(x^2 = 1\\). Any ring\nsatisfying this last condition is called a Boolean ring.\nBoolean rings are equivalent to Boolean algebras in the sense that\nthey have the same polynomials.\n\nAn atom of a Boolean algebra is an element \\(x\\) such that for all\n\\(y, x\\wedge y\\) is either \\(x\\) or 0. An atomless Boolean algebra is\none with no atoms.\n\nThere is exactly one Boolean algebra of cardinality every finite power\nof 2, and it is isomorphic to the Boolean algebra of a power set\n\\(2^X\\) of that cardinality under the set operations of union,\nintersection, and complement relative to \\(X\\). Hence all finite\nBoolean algebras have cardinality a power of 2. This situation changes\nwith infinite Boolean algebras; in particular countable Boolean\nalgebras exist. One such is the free Boolean algebra on countably many\ngenerators, which is the only countable atomless Boolean algebra. The\nfinite and cofinite (complement of a finite set) subsets of the set\n\\(N\\) of natural numbers form a subalgebra of the powerset Boolean\nalgebra \\(2^N\\) not isomorphic to the free Boolean algebra, but it has\natoms, namely the singleton sets.\n\nThe free Boolean algebra \\(F(n)\\) on \\(n\\) generators consists of all\n\\(2^2 n\\) \\(n\\)-ary operations on the two-element Boolean algebra.\nBoolean algebras therefore form a locally finite variety.\n\nThe equational theory of distributive lattices is obtained from that\nof Boolean algebras by selecting as its operations just the monotone\nbinary operations on the two-element algebra, omitting the constants.\nThese are the operations with the property that if either argument is\nchanged from 0 to 1, the result does not change from 1 to 0. A\ndistributive lattice is any model of those Boolean equations between\nterms built solely with monotone binary operations. Hence every\nBoolean algebra is a distributive lattice.\n\nDistributive lattices can be arbitrarily \u201cthin.\u201d At the\nextreme, any chain (linear or total order, \\(e.g\\). the reals\nstandardly ordered) under the usual operations of max and min forms a\ndistributive lattice. Since we have omitted the constants this\nincludes the empty lattice, which we have not excluded here as an\nalgebra. (Some authors disallow the empty set as an algebra but this\nproscription spoils many good theorems without gaining any useful\nones.) Hence there exist distributive lattices of every possible\ncardinality.\n\nEvery finite-dimensional vector space is free, being generated by any\nchoice of basis. This extends to infinite-dimensional vector spaces\nprovided we accept the Axiom of Choice. Vector spaces over a finite\nfield therefore form a locally finite variety when scalar\nmultiplication is organized as one unary operation for each field\nelement.\n6.5 Free algebras categorially\n\nWe now consider how free algebras are organized from the perspective\nof category theory. We defined the free algebra \\(B\\) generated by a\nsubset \\(X\\) of \\(B\\) as having the property that for every algebra\n\\(A\\) and every valuation \\(f: X\\rightarrow A\\), there exists a unique\nhomomorphism \\(h: B\\rightarrow A\\). Now every homomorphism \\(h:\nB\\rightarrow A\\) necessarily arises in this way, since its restriction\nto \\(X\\), as a function from \\(X\\) to \\(A\\), is a valuation.\nFurthermore every function \\(f: X\\rightarrow A\\) arises as the\nrestriction to \\(X\\) of its extension to a homomorphism. Hence we have\na bijection between the functions from \\(X\\) to \\(A\\) and the\nhomomorphisms from \\(B\\) to \\(A\\).\n\nNow the typing here is a little casual, so let us clean it up. Since\n\\(X\\) is a set while \\(A\\) is an algebra, \\(f\\) is better typed as\n\\(f: X\\rightarrow U(A)\\) where \\(U(A)\\) denotes the underlying set of\n\\(A\\). And the relationship of \\(X\\) to \\(B\\) is better understood\nwith the notation \\(B = F(X)\\) denoting the free algebra generated by\nthe set \\(X\\). So \\(U\\) maps algebras to sets while \\(F\\) maps sets to\nalgebras. \\(F\\) and \\(U\\) are not in general inverses of each other,\nbut they are nonetheless related in a way we now make precise.\n\nFor any category \\(\\mathbf{C}\\), the notation \\(\\mathbf{C}(A, B)\\) is\ngenerally used to denote the set of all morphisms from object \\(A\\) to\nobject \\(B\\) in category \\(\\mathbf{C}\\). And the set of all functions\nfrom the set \\(X\\) to the set \\(Y\\) can be understood as the\nparticular case \\(\\mathbf{Set}(X, Y)\\) of this convention where\n\\(\\mathbf{C}\\) is taken to be the class \\(\\mathbf{Set}\\) of all sets,\nwhich we can think of as discrete algebras, that is, algebras\nwith no structure. A class of algebras along with a specified set of\nhomomorphisms between any two of its members is an instance of a\ncategory. The members of the class are called the\nobjects of the category while the homomorphisms are called\nthe morphisms.\n\nThe bijection we have just observed can now be stated as\n\n\\[ \\mathbf{C}(F(X), A) \\cong \\mathbf{Set}(X, U(A)) \\]\n\n\nSuch a bijection is called an adjunction between\n\\(\\mathbf{Set}\\) and \\(\\mathbf{C}\\). Here \\(F: \\mathbf{Set}\\rightarrow\n\\mathbf{C}\\) and \\(U: \\mathbf{C}\\rightarrow \\mathbf{Set}\\) are\nrespectively the left and right adjoints of this adjunction;\nwe say that \\(F\\) is left adjoint to (or of) \\(U\\) and \\(U\\) right\nadjoint to \\(F\\).\n\nWe have only described how \\(F\\) maps sets to algebras, and \\(U\\) maps\nalgebras to sets. However \\(F\\) also maps functions to homomorphisms,\nmapping each function \\(f\\) to its unique extension as a homomorphism,\nwhile \\(U\\) maps homomorphisms to functions, namely the homomorphism\nitself as a function. Such maps between categories are instances of\nfunctors.\n\nIn general a category \\(C\\) consists of objects \\(a, b, c\\) and\nmorphisms \\(f: a\\rightarrow b\\), together with an associative\ncomposition law for \u201ccomposable\u201d morphisms \\(f:\nb\\rightarrow c\\), \\(g: a\\rightarrow b\\) yielding the morphism \\(fg:\na\\rightarrow c\\). Furthermore every object a has an identity element\n\\(1_a : a\\rightarrow a\\) which whenever composable with a morphism\n\\(f\\) (on one side or the other) composes with it to yield \\(f\\). A\nfunctor \\(F: C\\rightarrow D\\) maps objects of \\(C\\) to objects of\n\\(D\\) and morphisms of \\(C\\) to morphisms of \\(D\\), such that \\(F(fg)\n= F(f)F(g)\\) and \\(F(1_a) = 1_{F(a)}\\). That is, functors are\n\u201chomomorphisms of categories,\u201d preserving composition and\nidentities.\n\nWith no further qualification such a category is considered an\nabstract category. The categories we have been working with\nare concrete in the sense that they come with a given\nunderlying set or forgetful functor \\(U: C\\rightarrow\\)Set.\nThat is, algebras are based on sets, homomorphisms are certain\nfunctions between these sets, and \\(U\\) simply \u201cforgets\u201d\nthe algebraic structure. Such forgetful functors are faithful\nin the sense that for any two morphisms \\(f, g: a\\rightarrow b\\) of\n\\(C\\), if \\(U(f) = U(g)\\) then \\(f = g,\\) i.e. \\(U\\) does not identify\ndistinct homomorphisms. In general a concrete category is defined as a\ncategory \\(C\\) together with a faithful forgetful functor \\(U:\nC\\rightarrow \\textrm{Set}\\).\n\nCategories themselves admit a further generalization to 2-categories\nas algebras over two-dimensional graphs, with associative composition\nof 1-cells generalized to the 2-associative pasting of 2-cells. A\nfurther simplification of the free-algebra machinery then obtains,\nnamely via abstract adjunctions as the natural 2-dimensional\ncounterpart of isomorphisms in a category, which in turn is the\nnatural 1-dimensional counterpart of equality of elements in a set,\nthe 0-dimensional idea that two points can turn out to be one. This\nleads to a notion of abstract monad as simply the composition of an\nadjoint pair of 1-cells, one of which is the 1-cell abstracting the\nfunctor \\(F\\) that manufactures the free algebra \\(F(V)\\) from \\(V\\).\nOrdinary or concrete monads arise as the composition of functors as\nconcrete 1-cells of a 2-category of categories.\n",
    "bibliography": {
        "categories": [],
        "cat_ref_text": {
            "ref_list": [
                "Harold R. Jacobs, <em>Elementary Algebra</em>, 876pp, W.H.\nFreeman, 1979.",
                "Bartel Leendert van der Waerden, <em>Moderne Algebra</em> (2\nvolumes), Springer, Vol. 1 1930, Vol. 2 1931.",
                "I.N. Herstein, <em>Topics in Algebra</em>, 2nd ed, 400pp, Wiley,\n1975.",
                "Saunders Mac Lane, <em>Categories for the Working\nMathematician</em>, Springer-Verlag, 1978."
            ]
        },
        "raw_text": "<div id=\"bibliography\">\n<h2><a name=\"Bib\">Bibliography</a></h2>\n<ul>\n<li>Harold R. Jacobs, <em>Elementary Algebra</em>, 876pp, W.H.\nFreeman, 1979.</li>\n<li>Bartel Leendert van der Waerden, <em>Moderne Algebra</em> (2\nvolumes), Springer, Vol. 1 1930, Vol. 2 1931.</li>\n<li>I.N. Herstein, <em>Topics in Algebra</em>, 2nd ed, 400pp, Wiley,\n1975.</li>\n<li>Saunders Mac Lane, <em>Categories for the Working\nMathematician</em>, Springer-Verlag, 1978.</li>\n</ul>\n</div>"
    },
    "related_entries": {
        "entry_list": [
            "Boolean algebra: the mathematics of",
            "category theory"
        ],
        "entry_link": [
            {
                "../boolalg-math/": "Boolean algebra: the mathematics of"
            },
            {
                "../category-theory/": "category theory"
            }
        ]
    },
    "academic_tools": {
        "listed_text": [
            "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=algebra\" target=\"other\">How to cite this entry</a>.",
            "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://leibniz.stanford.edu/friends/preview/algebra/\" target=\"other\">Preview the PDF version of this entry</a> at the\n <a href=\"https://leibniz.stanford.edu/friends/\" target=\"other\">Friends of the SEP Society</a>.",
            "<img alt=\"inpho icon\" src=\"../../symbols/inpho.png\"/>",
            "<a href=\"https://www.inphoproject.org/entity?sep=algebra&amp;redirect=True\" target=\"other\">Look up topics and thinkers related to this entry</a>\n at the Internet Philosophy Ontology Project (InPhO).",
            "<img alt=\"phil papers icon\" src=\"../../symbols/pp.gif\"/>",
            "<a href=\"https://philpapers.org/sep/algebra/\" target=\"other\">Enhanced bibliography for this entry</a>\nat <a href=\"https://philpapers.org/\" target=\"other\">PhilPapers</a>, with links to its database."
        ],
        "listed_links": [
            {
                "https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=algebra": "How to cite this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/preview/algebra/": "Preview the PDF version of this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/": "Friends of the SEP Society"
            },
            {
                "https://www.inphoproject.org/entity?sep=algebra&redirect=True": "Look up topics and thinkers related to this entry"
            },
            {
                "https://philpapers.org/sep/algebra/": "Enhanced bibliography for this entry"
            },
            {
                "https://philpapers.org/": "PhilPapers"
            }
        ]
    },
    "other_internet_resources": {
        "listed_text": [
            "<a href=\"http://mathworld.wolfram.com/topics/Algebra.html\" target=\"other\">Many algebraic topics</a>,\n at Mathworld, Wolfram Research.",
            "<a href=\"http://www.answers.com/topic/linear-algebra\" target=\"other\">Linear algebra</a>,\n at Answers.com"
        ],
        "listed_links": [
            {
                "http://mathworld.wolfram.com/topics/Algebra.html": "Many algebraic topics"
            },
            {
                "http://www.answers.com/topic/linear-algebra": "Linear algebra"
            }
        ]
    }
}