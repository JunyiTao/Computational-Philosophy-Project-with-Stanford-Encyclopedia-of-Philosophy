{
    "url": "ethics-search",
    "title": "Search Engines and Ethics",
    "authorship": {
        "year": "Copyright \u00a9 2020",
        "author_text": "Herman Tavani\n<htavani@rivier.edu>\nMichael Zimmer\n<michael.zimmer@marquette.edu>",
        "author_links": [
            {
                "https://www2.rivier.edu/faculty/htavani/": "Herman Tavani"
            },
            {
                "mailto:htavani%40rivier%2eedu": "htavani@rivier.edu"
            },
            {
                "http://michaelzimmer.org": "Michael Zimmer"
            },
            {
                "mailto:michael%2ezimmer%40marquette%2eedu": "michael.zimmer@marquette.edu"
            }
        ],
        "raw_html": "<div id=\"article-copyright\">\n<p>\n<a href=\"../../info.html#c\">Copyright \u00a9 2020</a> by\n\n<br/>\n<a href=\"https://www2.rivier.edu/faculty/htavani/\" target=\"other\">Herman Tavani</a>\n&lt;<a href=\"mailto:htavani%40rivier%2eedu\"><em>htavani<abbr title=\" at \">@</abbr>rivier<abbr title=\" dot \">.</abbr>edu</em></a>&gt;<br/>\n<a href=\"http://michaelzimmer.org\" target=\"other\">Michael Zimmer</a>\n&lt;<a href=\"mailto:michael%2ezimmer%40marquette%2eedu\"><em>michael<abbr title=\" dot \">.</abbr>zimmer<abbr title=\" at \">@</abbr>marquette<abbr title=\" dot \">.</abbr>edu</em></a>&gt;\n    </p>\n</div>"
    },
    "pubinfo": [
        "First published Mon Aug 27, 2012",
        "substantive revision Tue Aug 11, 2020"
    ],
    "preamble": "\n\n\nWhat is an Internet search engine? Why are search engines\nproblematic from an ethical perspective? In this entry, the available\nphilosophical literature on this topic will be critically reviewed.\nHowever, relatively few academic works on the topic of search engines\nhave been written from a philosophical perspective. And only a handful\nof the existing publications that focus specifically on the\nethical aspects of search engines have been contributed by\nphilosophers (see, for example, Nagenborg 2005).\n",
    "toc": [
        {
            "#IntrOver": "1. Introduction and Overview"
        },
        {
            "#SearEngiDeveEvolShorHist": "2. Search Engine Development and Evolution: A Short History"
        },
        {
            "#PreInteEraCompInfoRetr1940": "2.1 The Pre-Internet Era of Computing and Information Retrieval (1940s\u20131970s)"
        },
        {
            "#EarlIntePreWebEra1980": "2.2 The Early Internet (pre-Web) Era (1980s)"
        },
        {
            "#EarlWebEra1990": "2.3 The (Early) Web Era (1990s)"
        },
        {
            "#Web20Era2000": "2.4 The \u201cWeb 2.0\u201d Era (2000\u2013Present)"
        },
        {
            "#EthiImpl": "3. Ethical Implications and Core Ethical Issues"
        },
        {
            "#SearEngiBiasProbOpac": "3.1 Search Engine Bias and the Problem of Opacity/Nontransparency"
        },
        {
            "#PrivConsNonVoluDiscPersInfo": "3.2 Privacy, Consent, and Non-voluntary Disclosure of Personal Information"
        },
        {
            "#MoniSurv": "3.3 Monitoring and Surveillance"
        },
        {
            "#DemoCensThreLibeFree": "3.4 Democracy, Censorship, and the Threat to Liberty and Freedom"
        },
        {
            "#CybeSecuInteThin": "3.5 (Cyber)Security and the Internet of Things"
        },
        {
            "#MoraAcct": "4. Moral Accountability and Social-Responsibility Issues for Search Engine Companies"
        },
        {
            "#ComRelCon": "4.1 Commerce-Related Conflicts for SECs"
        },
        {
            "#LegLiaRig": "4.2 Legal Liability, a \"Right to erasure,\" and Fake News in the Digital Era"
        },
        {
            "#SomQueAff": "4.3 Some Questions Affecting Trust"
        },
        {
            "#Conc": "5. Conclusion"
        },
        {
            "#Bib": "Bibliography"
        },
        {
            "#Aca": "Academic Tools"
        },
        {
            "#Oth": "Other Internet Resources"
        },
        {
            "#Rel": "Related Entries"
        }
    ],
    "main_text": "\n1. Introduction and Overview\n\n\nIt may be difficult to imagine today\u2019s world without search\nengines. Which high school student has not used a Web search engine to query\nabout some topic or subject? Of course, it is quite possible that many\nInternet users, both young and old, do not consciously distinguish\nbetween the search engines they use and Web browsers that now also\ntypically include search engines as a feature within their user\ninterface. But virtually all Internet users have come to expect and\ndepend on the instantaneous results they receive in response to their\nvarious search queries. While there is no shortage of definitions of\n\u201csearch engine,\u201d none has been accepted as the\nstandard or universally agreed upon definition. For purposes of this\nentry, however, the definition of a (Web) search engine, put forth by\nHalavais (2009, 5\u20136), is \u201can information retrieval system\nthat allows for keyword searches of distributed digital text.\u201d\nWe note that this definition includes some important technical terms\nand concepts that, in turn, need defining and further elucidation. Our\nexamination of key technical concepts underlying search engines is\nintended to provide a useful context for our analysis of the ethical\nimplications. In this sense, Blanke (2005, 34) is correct that an\nadequate analysis of the ethical aspects of search engines\n\u201crequires knowledge about the technology and its\nfunctioning.\u201d\n\n\nWe begin with a brief sketch of the history and evolution of search\nengines, from their conception in the pre-Internet era to the\ndevelopment and implementation of contemporary (\u201cWeb 2.0\u201d\nera) search engines such as Google. Our examination of important\nhistorical developments of this technology is intended to address our\nfirst question, noted above: \u201cWhat is a search engine?\u201d It\nalso provides a backdrop for analyzing our second major question,\n\u201cWhy are search engines problematic from an ethical\nperspective?\u201d where a cluster of ethical concerns involving\nsearch engine technology is examined. These include issues ranging\nfrom search engine bias and the problem of opacity/non-transparency,\nto concerns affecting privacy and surveillance, to a set of issues\ninvolving censorship and democracy. We also describe some emerging\nethical concerns affecting (cyber)security\u2014at the data, system,\nand (inter)national levels\u2014generated by the use of\n\u201cdiscoverable search engines\u201d in the context of the\nInternet of Things. Additionally, we question whether search engine\ncompanies have any special moral obligations, e.g., in light of their\n\u201cprivileged\u201d placed in society as \u201cgatekeepers of\nthe Web\u201d (Hinman, 2005, 21), for which they should be held\naccountable. In analyzing this question, we also examine a key role\nthat trust now plays for users who have come to depend on search\nengines, and the companies that develop them, for access to accurate\ninformation.\n\n\nFinally, in the concluding\nsection, we briefly mention some impacts that search engines have for\nbroader philosophical issues (especially in the area of epistemology)\nthat may not be solely or mainly ethical in nature. However, an\nadequate analysis of these issues is beyond the scope of this\nentry.\n2. Search Engine Development and Evolution: A Short History\n\n\nBecause search engines provide Internet users with access to important\ninformation by directing them to links to available online resources\non a plethora of topics, many are inclined to see search engine\ntechnology in a positive light; some might also assume, as Noble\n(2018) and others note, that this technology is\n\u201cvalue-neutral.\u201d However, search engines can raise a\nnumber of ethical controversies. Before examining these controversies,\nhowever, we first briefly discuss the history of search engine\ntechnology via categories that, for our purposes, reflect four\ndistinct eras: (i) Pre-Internet, (ii) Internet (pre-Web), (iii) early\nWeb, and (iv) Web 2.0. We will see how technical developments in each\nera have had some implications for the cluster of ethical issues\nexamined in\nSection 3.\n2.1 The Pre-Internet Era of Computing and Information Retrieval (1940s\u20131970s)\n\n\nToday, we tend to associate search engines with computer technology,\nand perhaps more specifically with Internet-based computing and\nelectronic devices. Yet, the early work in search/information\nretrieval systems was carried out independently of developments in\nelectronic computing. Whereas the first (general purpose) electronic\ncomputer\u2014the ENIAC (Electronic Numerical Integrator And\nComputer)\u2014was completed in November 1945 and announced in\nFebruary 1946 (Palfreman and Swade, 1991), several decades would pass\nbefore Internet search engines became available. Because ENIAC and\nother early computers were designed primarily to \u201ccrunch\nnumbers,\u201d relatively little thought had been given to the kinds\nof information-retrieval systems that could be used to search\nthrough the large amount of data that those non-networked (or\n\u201cstand-alone\u201d) computers were capable of storing. However,\nsome information-retrieval theorists, as we shall see, had already\nbegun to worry about the amount of information that was becoming\navailable during this period and that, in all likelihood, would\nproliferate with the advent of computers. In particular, they were\nconcerned about how an ever-expanding repository of information could\nbe organized and retrieved in a practical way.  Halavais (2009, 13)\nnotes that early computers \u201cdrew on the ideas of librarians and\nfiling clerks\u201d for arranging the stored information that would\nbe retrieved. But some of the leading thinkers in the emerging field\nof information retrieval (or IR), which Van Couvering (2008) describes\nas a \u201chybrid\u201d academic discipline combining elements of\ninformation science and computer science, saw that traditional methods\nfor retrieving information would not be effective in the era of\nelectronic computer systems.\n\n\nOne visionary who saw the need for a new kind of organizing and\nretrieval scheme to manage the expanding volume of information was\nVannevar Bush, perhaps the most important figure in the history of\ninformation-retrieval/search-engine theory in the pre-Internet era. In\nhis classic article, \u201cAs We May Think\u201d (Atlantic\nMonthly, July 1945), published approximately six months before\nENIAC\u2019s official announcement, Bush remarked,\n\nThe summation of human experience is being expanded at a\nprodigious rate, and the means we use for threading through the\nconsequent maze to the momentarily important item is the same as was\nused in the days of square-rigged ships.\n\n\n\nHowever, Bush believed that a technological solution to this problem\nwas possible through a system he called memex, which he\ndescribed as a\n\ndevice in which an individual stores all his\nbooks, records, and communications, and which is mechanized so that it\ncan be consulted with exceeding speed and flexibility.\n\n\n Bush\nenvisioned the memex behaving like an \u201cintricate web of\ntrails\u201d similar to the function of the human mind, which he\nbelieved works by a method of \u201cassociation\u201d and not via an\nalphabetical index (of the kind typically used in libraries and other\ncataloging schemes). According to Levy (2008, 508), the most\n\u201cinnovative feature\u201d of Bush\u2019s memex system was the\nestablishing of\n\nassociative indices between portions of\nmicrofilmed text\u2014what we now call hypertext links\u2014so\nthat researchers could follow trails of useful information through\nmasses of literature.\n\n Via Bush\u2019s \u201cassociative\nindexing\u201d scheme, different pieces of information could be linked\nor tied together, \u201cas any single item may be caused at will to\nselect immediately and automatically another.\u201d Thus, Bush is\noften credited with having anticipated the kinds of search engine\nfunctions that would eventually be used on the Internet and the World\nWide Web.\n\n\nTwo other important figures in the history of search engine theory\nwho made significant contributions during the pre-Internet era\nwere Gerald Salton and Ted Nelson. Salton, who some consider the\n\u201cfather of modern search technology,\u201d developed the SMART\n(Salton\u2019s Magic Automatic Retriever of Text) information\nretrieval system. And Nelson, who developed hypertext in 1963,\nsignificantly influenced search engine theory through his Project\nXanadu (Wall 2011). Although several years passed before Salton\u2019s\nand Nelson\u2019s contributions could be incorporated into modern\nsearch engines, it is worth noting that some very\n\u201cprimitive\u201d search functions had been built into the\noperating systems for some pre-Internet-era computers. For example,\nHalavais points out that the UNIX operating system supported a search\nutility called \u201cFinger.\u201d Via the Finger command, a UNIX\nuser could search for one or more users who also had active accounts on\na particular UNIX system. To inquire about a UNIX user named\n\u201cJones,\u201d for example, one could simply enter the command\n\u201cFinger Jones\u201d at the user prompt on the command line.\nHowever, this search function was very limited, since the only kind of\ninformation that could be retrieved was information about whether one\nor more users were currently logged into the system and about which\ntime those users logged in/out. But, as Halavais points out, this\nrudimentary search facility also enabled UNIX users to arrange limited\nsocial gatherings\u2014e.g., users could \u201cFinger\u201d one\nanother to set up a time to play tennis after work (provided, of\ncourse, that the users were logged into their UNIX accounts at that\ntime).\n\n\nSome of the conceptual/technological breakthroughs that occurred\nduring the pre-Internet era of search engine development made possible\ntwo kinds of ethical issues examined in Section 3. \nFirst,\nBush\u2019s \u201cassociative indexing\u201d scheme for retrieving\ninformation, as opposed to more traditional cataloging schemes based\non straight-forward inferential rules and techniques, enabled (even if\nunintentionally) some of the kinds of \u201cbias\u201d and\nobjectivity-related concerns affecting users\u2019 search\nresults that we examine in Sections 3.1 and 4.1. \nSecond, the kind of\nsearch function made possible by the UNIX \u201cFinger\u201d utility,\nenabling UNIX users to retrieve information about the availability of\nfellow UNIX users and to acquire information about which times those\nusers logged into and logged out from the system, generated some\nprivacy-and-monitoring-related concerns that are included among the\nethical issues we examine in Sections 3.2 and \n3.3.\n2.2 The Early Internet (pre-Web) Era (1980s)\n\n\nBy the 1960s, plans for developing a vast network of computer networks\n(i.e., what was eventually to become the Internet) were well\nunderway. And by 1970, work had begun on the ARPANET (Advanced\nResearch Projects Agency Network), which is commonly viewed as the\npredecessor of the Internet. This US-based project was funded by DARPA\n(Defense Advanced Research Projects Agency) into the late 1980s, when\nthe National Science Foundation Network (NSFnet) took over the project\n(Spinello 2011). Although multiple computer networks existed during\nthis period, they were not easily able to communicate and exchange\ndata with one another; a common protocol was needed for the various\nnetworks to exchange data between systems. The Transmission Control\nProtocol/Internet Protocol (TCP/IP) architecture was eventually\nselected as the standard protocol for the newly emerging\nInternet. With the implementation of this new standard, there was\nconsiderable optimism (especially among many in the academic research\ncommunity) about the potential for sharing the data that resided in\nthe various computers systems comprising the fledgling\nInternet. However, one very important challenge still remained: How\ncould Internet users locate the rich resources potentially available\nto them? To do this, a sophisticated search program/utility,\nwith a robust indexing system, was needed to point to the available\ncomputer databases that existed and to identify the content that\nresided in those databases. The first indexes on the Internet were\nfairly primitive, and as Halavais (2009) points out, \u201chad to be\ncreated by hand.\u201d\n\n\nWith TCP/IP now in place, privately owned computer\nnetworks\u2014including LANs (local area networks) and WANs (wide\narea networks)\u2014were able to communicate with one another and, in\nprinciple at least, also able to exchange vast amounts of information\nover the network. However, another protocol\u2014one that would be\nlayered on top of TCP/IP\u2014was needed to accomplish this\nobjective. So, FTP (File Transfer Protocol), a client/server-based\nsystem, was developed and implemented in response to this need. To\nexchange or share files with a fellow Internet user in this scheme,\none first had to set up an FTP\n server.[1]\n Users could then upload files to and\nretrieve them from an FTP server, via an FTP client. Perhaps more\nimportantly, they could also now effectively\nsearch for files with one of the newly developed search\nengines, the first of which was called ARCHIE.\n\n\nThe ARCHIE search engine enabled users to enter queries based on a\nlimited set of features\u2014mainly \u201cfile names.\u201d\nARCHIE\u2019s searchable database of file names was comprised of the\nfile directory listings of hundreds of systems available to public FTP\nservers (and eventually to \u201canonymous\u201d FTP servers as\nwell). In the early 1990s, two other search engines were also fairly\nprominent: VERONICA (Very Easy Rodent-Oriented Net-Wide Index to\nComputer Archives) and JUGHEAD (Jonzy\u2019s Universal Gopher\nHierarchy Excavation and Display). Both VERONICA and JUGHEAD had an\nadvantage over the ARCHIE search engine in that they were able to\nsearch for plain-text files, in addition to searching for file names.\nThese two search engines also worked in connection with a system called\nGOPHER. According to Halavais (2009, 22), GOPHER\u2019s\n\u201cmenu-driven approach\u201d to search helped to bring\n\u201corder to the Internet,\u201d since users \u201ccould now\nnavigate through menus that organized documents.\u201d\n\n\nSome of the technological breakthroughs that occurred during the\nearly-Internet era of search engine development exacerbated a\nprivacy-related ethical issue examined in Section 3. \nSpecifically,\nInternet-wide search functions, enabled by compliance with the TCP/IP\nand FTP protocols, dramatically increased the scope of the\nprivacy-and-monitoring concerns (initially generated in the\npre-Internet era via applications such as the UNIX \u201cFinger\u201d\nutility) that we examine in Sections 3.2\n and\n 3.3. Additionally,\n\u201canonymous\u201d FTP servers, also developed in this period,\nmade it possible for technically-savvy users to upload proprietary\nfiles, such as copyrighted software applications, on to the Internet\n(with anonymity). And the indexing schemes supported by the ARCHIE and\nGOPHER search systems enabled users to search for and download/share\nthose proprietary files with relative anonymity. Although intellectual\nproperty issues are not included among the ethical concerns examined in\nSection 3, it is worth noting that the\ndevelopment of some search-engine-related applications during this era\npaved the way for the kinds of illegal file-sharing practices\ninvolving copyrighted music that arose in connection with the Napster\nsite in the late 1990s. (The controversial Napster web site was one of\nthe first, as well as the most popular, sites used by many young\npeople at that time to exchange proprietary music online with their\nfriends).\n2.3 The (Early) Web Era (1990s)\n\n\nThe first Web site was developed in 1991 (at the CERN European\nlaboratory for particle physics) by Tim Berners-Lee, who also founded\nthe World Wide Web Consortium (W3C) at MIT in 1994. The World Wide Web\nwas based on the Hyper Text Transfer Protocol (HTTP) and used a format\ncalled the Hyper Text Markup Language (HTML) for designing and\ndelivering documents; many non-technical users found navigating the\nWeb to be much more friendly and versatile than using GOPHER and FTP\nto exchange files. For the (HTTP-based) Web to realize its full\npotential and to become attractive to non-technical users, however, a\nmore intuitive user interface was needed. The Mosaic Web browser\n(later called Netscape Navigator) became available in 1993 and was the\nfirst Internet application to include a graphical user interface\n(GUI); this interface, with its intuitive features that enabled users\nto click on hyperlinks, made navigating the Web much easier for\nnon-technical users. Although Netscape Navigator was a Web browser,\nand not a search engine, it provided a forum in which many specialized\nWeb search engine companies were able to flourish. A host of search\nengines, most of which were dedicated to specific areas or specific\nkinds of searches, soon became available. Some search engines that\nwere especially popular during this period were Excite (introduced in\n1993) and Lycos and Infoseek (both of which were available in\n1994). Others included Looksmart and Alta Vista, introduced in 1995,\nand Ask.com (originally called AskJeeves) in 1997 (Wall 2011).\n\n\nAlthough the internal structure of a search engine is fairly\ncomplex\u2014comprising, among other components, programs called\n\u201cspiders\u201d that \u201ccrawl\u201d the Web\u2014the\nuser-interface portion of the search process is quite straightforward\nand can be summarized in terms of two steps: (1) a user enters search\nterm/phrase or \u201ckeyword\u201d in a \u201csearch box\u201d;\nand (2) the search engine returns a list of relevant Web\n\u201cpages\u201d that typically include hyperlinks to the pages\nlisted. Many of the early Web search engines were highly specialized\nand thus could be viewed as \u201cvertical\u201d (i.e., in current\ntechnical parlance regarding search engine technology) in terms of\ntheir scope. For example, Ask.com was designed to accept queries in\nthe form of specific questions and thus could be viewed as a vertical\nsearch engine. Halavais defines a vertical search engine as one that\nlimits itself \u201cin terms of topic, medium, region, language, or\nsome other set of constraints, covering that area in greater\ndepth.\u201d (In this sense, vertical search engines are far more\ncapable of drilling down into particular topics than expanding out\ninto associated subjects.) A few of the popular search engines that\nflourished during the early Web period, however, were more general, or\n\u201chorizontal,\u201d in nature. Alta Vista, for instance, was one\nof the first search engines to fit into this category. Today, most of\nthe major search engines are horizontal, and Google is arguably the\nbest known horizontal search engine. We should note, however, that\nvertical search engines still play an important role today. Consider\nan example where one uses Google, or an alternative horizontal search\nengine such as Yahoo! or (Microsoft\u2019s) Bing, to locate the Web site\nfor Bates College.\n Once the user has successfully accessed the main page on the Bates site \nshe can then use Bates\u2019 local\nsearch facility, a vertical search engine, to retrieve information\nabout faculty and staff who work at that college, or retrieve\ninformation about various academic programs and co-curricular\nactivities sponsored by that college, and so forth.  Within that\nvertical search engine, however, the user cannot retrieve broader\ninformation about faculty and academic programs at related colleges\nand universities or about related topics in general (as they could\nwhen using a horizontal search engine).\n\n\nAnother type of Web search engine is a meta search\nengine, which, as its name suggests, draws from the results of\nmultiple (specialized) search engines and then combines and re-ranks\nthe results. One of the first, and perhaps most popular, meta search\nengines in the mid-to-late 1990s was HotBot (Wall 2011). Meta search\nengines had a much more important role to play during the early years\nof the Web. As search engines improved and became more sophisticated,\nthe need for meta search dramatically declined. Today, most general\npurpose (horizontal) search engines, such as Google and Bing, are able\nto return the same level of ranked results (as meta search engines once\ndid), via their aggregation schemes. In fact, the founders of Google\nhave described their search engine as an \u201caggregator of\ninformation\u201d (Brin and Page 1998).\n\n\nSome of the technological breakthroughs that occurred during the\n\u201cearly Web\u201d era of search engine development helped to make\npossible two kinds of privacy-related ethical issues examined in\nSection 3. First, the vast amount of online \ninformation about ordinary\npeople that became accessible to Web-based search engines during this\nera made it possible for those people to become the\n\u201ctargets\u201d of online searches conducted by anyone who had\naccess to the Internet; this concern is examined in\n Section 3.2.\nSecond, the practice of aggregating personal information, which was\nbeing routinely collected by major search engine companies and their\nadvertisers, contributed significantly to the data-mining-related\nprivacy issues that are examined in Section 3.3.\n2.4 The \u201cWeb 2.0\u201d Era (2000\u2013Present)\n\n\nAlthough the expression \u201cWeb 2.0\u201d is now commonly used\nto differentiate the current Web environment from the early Web,\ncritics point out that this expression is somewhat vague or imprecise.\nWhereas the early Web (sometimes referred to as \u201cWeb 1.0\u201d)\nhas been described as an online environment that was essentially\npassive or static, in so far as one could simply view the contents of a\nWeb site that had been set up by an organization or an individual\n(e.g., when one visited someone\u2019s \u201chome page\u201d), Web\n2.0 is more dynamic in that it supports many interactive or\n\u201cparticipatory\u201d features. In a Web 2.0 environment, for\nexample, users can interact and collaborate with others in ways that\nwere not previously possible. These collaborative features include\nwikis (with Wikipedia being the best know example), as well as blogs\nand social networking applications (such as Facebook and Twitter). Of\ncourse, the relevant question for us to consider is whether the Web 2.0\nenvironment itself either changes or significantly affects the\nfunctions of search engines and, more importantly, the ethical issues\nthey generate.\n\n\nIt is not clear whether we can accurately label current search\nengines as \u201cWeb 2.0 search engines,\u201d even though they\noperate in a Web 2.0 environment. For example, many of the\nparticipatory tools and functions that apply to applications such as\nsocial networks, blogs, and wikis do not necessarily apply\nto contemporary search engines. So, it may be more appropriate to use\nHinman\u2019s phrase \u201csecond-generation search engines.\u201d\nHowever, O\u2019Reilly (2005) suggests that Google\u2019s practice of\nincorporating user-generated content to provide a \u201cbetter\u201d\nWeb search environment for users is compatible with interactive\ndimensions and objectives of Web 2.0. But despite\nO\u2019Reilly\u2019s interpretation of Google\u2019s practices, \none might still question whether the phrase \u201cWeb 2.0\nsearch engines\u201d is warranted; so, we will instead refer to\ncontemporary (or second-generation) search engines as \u201cWeb\n2.0-era search engines.\u201d\n\n\nWhat, exactly, distinguishes a Web 2.0-era search engine from the\nearlier ones? Hinman notes that the traditional criteria Web search\nengine companies used to rank sites was based on two factors: (1) the\nnumber of visits to a page (i.e., \u201cpopularity\u201d), and (2)\nthe \u201cnumber of other pages that link to a given page.\u201d With\nrespect to the second criterion, Diaz (2008) and others point to an\nanalogy used in ranking the importance of academic papers. They note,\nfor example, that an academic paper is generally viewed to be important\nif it is cited by many other papers. And that paper is perhaps viewed\nas even more important if it is cited by highly cited works. Hinman\nbelieves that the shift to (what we call) Web 2.0-era search engines\noccurred when companies, such as Google, \u201clooked more closely at\nwhat users wanted to find\u201d (which, as he also points out, is not\nalways the most popular site). He notes, for example, that\nGoogle\u2019s formula employs the following strategy:\n\u201cUsers\u2019 needs \u2192 Search terms \u2192 Desired\nsite\u201d (Hinman 2005, 22). He also notes that in this scheme,\n\n\nwhat the user wants becomes an integral part of the formula, as\ndoes the set of search terms commonly used to express what the user\nwants.\n\n Hinman and others credit Google\u2019s success as the\npremier contemporary Web search engine to the company\u2019s\nproprietary algorithm, called PageRank.\n\n\nZimmer (2008, 77) believes that Google\u2019s ultimate goal is\nto \u201ccreate \u2018the perfect search engine\u2019 that will\nprovide only intuitive, personalized, and relevant results.\u201d\nHalpern (2011) points out that the search process has already\n\u201cbecome personalized\u201d\u2014i.e., \u201cinstead of being\nuniversal, it is idiosyncratic and oddly peremptory.\u201d And Pariser\n(2011), who asserts that \u201cthere is no standard Google\nanymore,\u201d also notes that with \u201cpersonalized\nsearch,\u201d the result(s) suggested by Google\u2019s\nalgorithm is probably the best match for the search query.\nSome ethical implications affecting the personalization of search\nalgorithms are examined in Section 3.4.\n\nBefore concluding this section, we should briefly mention two\nadditional kinds of contemporary search\nengines. First, DuckDuckGo (an\nalternative to Google and Bing) which prides itself on not profiling\nits users, claims that it does not personalize search results. So,\nthis search engine\u2019s users will typically get the same search results,\nas well as the same ranking of results, for a given query. Second, a\nnew type of search engine, capable of \u201cdiscovering\u201d objects or\n\u201cthings\u201d on the Internet is now also available. Two examples of this\nkind of \u201cdiscoverable search engine,\u201d Shodan and Thingful, are\nexamined in the final section of this entry.\n3. Ethical Implications and Core Ethical Issues\n\n\nMost Internet users are well aware of the virtues of search engines.\nAs we noted earlier, many of us now depend on them to direct us to\ninformation that affects nearly all facets of our day-to-day\nlives\u2014information about work, travel, recreation, entertainment,\nfinances, politics, news, sports, music, education, and so\nforth. However, as we also noted earlier, the use of search engines\nhas generated a cluster of ethical concerns. In Section 3, we organize\nthese concerns into five broad categories: (i) search-engine bias and\nthe problem of opacity/non-transparency, (ii) personal privacy and\ninformed consent, (iii) monitoring and surveillance, (iv) censorship\nand democracy, (v) (cyber)security issues, \u201cdiscoverable search\nengines,\u201d and the Internet of Things. A different, but also\nrelated, cluster of ethical issues \u2014 viz, questions concerning\nmoral accountability and social responsibilities for search engine\ncompanies \u2014 are examined in Section 4.\n3.1 Search Engine Bias and the Problem of Opacity/Nontransparency\n\n\nWhat is search-engine bias, and why is it controversial? In\nreviewing the literature on this topic, it would seem that the phrase\n\u201csearch-engine bias\u201d has been used to describe at least\nthree distinct, albeit sometimes overlapping, concerns: (1)\nsearch-engine technology is not neutral, but instead has embedded\nfeatures in its design that favor some values over others; (2) major\nsearch engines systematically favor some sites (and some kinds of sites)\nover others in the lists of results they return in response to user\nsearch queries; and (3) search algorithms do not use objective criteria\nin generating their lists of results for search queries.\n3.1.1 The Non-Neutrality of Search Engines\n\n\nWhile some users may assume that search engines are\n\u201cneutral\u201d or value-free, critics argue that search engine\ntechnology, as well as computer technology in general, is value-laden\nand thus biased because of the kinds of features typically included in\ntheir design. For example, Brey (1998, 2004) and others (see, for\ninstance, Friedman and Nissenbaum 1996) have argued\nthat computer technology has certain built-in features that tend to\nfavor some values over others. Brey worries that some of these\ntechnological features have embedded values that are \u201cmorally\nopaque.\u201d Because the values embedded in these features are not\nalways apparent to the technical experts who develop computer systems,\nBrey believes that a methodological framework, which expands upon what\nhe calls the \u201cstandard\u201d applied-ethics model typically used\nin \u201cmainstream computer ethics,\u201d is needed to identify or\ndisclose the \u201chidden\u201d values at the design stage. He refers\nto this model as \u201cdisclosive computer ethics\u201d (Brey 2004, 55\u201356).\n\n\nIdentifying the human values embedded in technological design and\ndevelopment has been the main objective of a movement called Value\nSensitive Design or VSD, which Friedman, Kahn and Borning\n(2008, 70) define as a\n\ntheoretically grounded approach to the\ndesign of technology that accounts for human values in a principled and\ncomprehensive manner throughout the design process.\n\n Friedman et al. illustrate their model using the example of\nInternet cookies\u2014i.e., small text files that a Web browser\nplaces on a user\u2019s computer system for the purposes of tracking and\nrecording that user\u2019s activities on a Web site. In particular, they\nexamine the design of cookies in connection with the informed-consent\nprocess vis-\u00e0-vis Web browsers. They further argue that embedded\nfeatures in the design of cookies challenge the value of informed\nconsent and that this value is important because it protects other\nvalues such as privacy, autonomy, and trust.\n\n\nCookies technology is not only embedded in the design of contemporary\nWeb browsers, it is also used by major search engine companies to\nacquire information about users. In so far as these companies place\ncookies on users\u2019 computer systems, without first getting their\nconsent, they also seem to contribute to, and perhaps even exacerbate,\nat least one kind of technology-related bias\u2014i.e., one that\nthreatens values such as privacy and autonomy, while favoring values\nassociated with surveillance and monitoring. However, since this kind\nof bias also applies to design issues affecting Web browsers, it is\nnot peculiar to search engines per se.\n3.1.2 The Manipulation of Search Results\n\n\nSome critics, including Introna and Nissenbaum (2000), tend to view\nthe schemes used to manipulate search results as the paradigm case of\nbias in the context of search engines.  In their highly influential\npaper on this topic, Introna and Nissenbaum argued that search\nengines\n\nsystematically exclude certain sites\nand certain types of sites, in favor of others, systematically giving\nprominence to some at the expense of others.\n\n There has been\nconsiderable speculation as to why this is the case, but we briefly\nexamine two reasons that have been prominent in discussions in the\nliterature: (a) the interests of advertisers who sponsor search\nengines; and (b) schemes used by technically-savvy individuals and\norganizations to manipulate the ordered ranking of sites returned by\nsearch engines. (A third reason that is also sometimes suggested has to\ndo with the nature of the algorithms used by major search engine\ncompanies; that issue is examined in this Section 3.1.3).\n3.1.2.1 Online Advertising Strategies and Search Bias\n\n\nSome critics, including Hinman (2005) and Noble (2018), point\nout that search engine companies are \u201canswerable\u201d to the\npaid advertisers who sponsor them. So, for many of these critics,\nbias-related concerns affecting the inclusion/exclusion of certain\nsites can be attributable mainly to the interests of paid advertisers.\nGoogle founders Brin and Page (1998, 18), who initially opposed\nthe idea of paid advertisement on search engines, noted that it would\nseem reasonable to \n\nexpect that advertising funded search engines\nwill be inherently biased towards the advertisers and away from the\nneeds of consumers\u2026 Since it is very difficult even for experts\nto evaluate search engines, search engine bias is particularly\ninsidious\u2026[and] less blatant bias are likely to be tolerated by\nthe market.\n\n\n\nIt is worth noting that advertising schemes used by search engines\nhave evolved over time. For example, Diaz (2008, 21) points out that\nbanner ads, which were common on the Internet during the Web 1.0 era,\nhave been replaced by \u201cpaid placement of ads.\u201d He also\nnotes that \u201cpaid listings\u201d (unlike the earlier banner ads)\ndo not always look very different from the normal results returned to\nusers. Elgesem (2008) points out that search engines such as GoTo,\nwhose lists of search results were based entirely on \u201cpaid\nhits\u201d from advertisers, allegedly failed because of user\ndissatisfaction with the results they received. Eventually, however,\nGoTo was taken over by Google, which continued to use GoTo\u2019s\nmethod for generating paid-ad-based search results but physically\nseparated those results from the \u201corganic\u201d results that\nappear on the center of its pages (Elgesem 2008); this scheme, which\ncontrasts the two different kinds of results, seems to have been\naccepted by Google users.\n\n\nDiaz describes two other kinds of bias-related concerns that affect\nadvertising schemes used by search engine companies: (i) the arbitrary\n(and seemingly inconsistent) criteria used by these companies in\naccepting advertisements, and (ii) the imprecise (and sometimes\nconfusing) criteria used to separate editorials (approved by a search\nengine company) from their paid advertisements. (Although the kind of\ndiscrimination with regard to the ads that Google accepts or rejects\nmay seem arbitrary in an innocuous sense, Diaz notes that Google has\nalso refused to accept ads from some organizations that have been\ncritical of corporations that were already sponsors for Google.)\nRegarding (ii), Diaz explains how bias-related concerns affecting paid\nadvertisements can sometimes be confused with editorials that are also\ndisplayed by search engine companies on their Web pages. Whereas\neditorials and ads may look very different in newspapers, Diaz notes\nthat this is not always the case in search engines. (We also examine\nsome aspects of search engine bias in the context of online\nadvertisements in our analysis of moral-accountability issues for\nsearch engine companies in Section 4.\n\n\nSome critics assume that as conflicts affecting online\nadvertisements in the context of search engines are eventually resolved, the amount of\nbias in search engine results will also decline or perhaps disappear\naltogether. However, other schemes can also be used to influence the\nprivileging of some sites over others, in terms of both their inclusion\n(vs. exclusion) and their ranking.\n3.1.2.2 Technological Schemes Used to Manipulate Search Results\n\nWe have already noted that some technically-savvy\nindividuals and organizations have figured out various strategies for\n\u201cgaming the system\u201d\u2014i.e., positioning their sites\nhigher in the schemes used by search engine companies to generate\nresults (see, for example, Goodwin 2018). These schemes are commonly referred to by\n\u201cinsiders\u201d as instances of SEO, or Search Engine\nOptimization, and we briefly consider what can now be regarded as a\nclassic SEO ploy. Some organizations and individuals had effectively\nused HTML meta tags and keywords (embedded in HTML source code) to\ninfluence search engine companies to give their sites higher rankings\nin the ordering schemes for their respective categories of search.\nEventually, however, search engine companies recognized the\nmanipulative aspects of these HTML features and began to disregard them\nin their ranking algorithms (Goldman 2008).\n\n\nMany organizations now use a different kind of strategy to achieve a\nhigher ranking for their sites\u2014one that takes advantage of the\n(general) formulas currently used by major search engine companies.\nBlanke (2005, 34) notes that in Google\u2019s algorithm, Web pages\n\u201cachieve a better ranking if they optimize their relationship\nwithin the system of hubs and authorities.\u201d Whereas\n\u201cauthorities\u201d are Web pages that are \u201clinked by many\nothers,\u201d hubs \u201clink themselves to many pages.\u201d Diaz\n(2008) points out that highly referenced hubs will have the highest Page\nRanks. So Web site owners and designers who know how to exploit these\nfactors (as well as how to use various SEO-related schemes) to\nmanipulate ranking in search engine results will have the highest\nranked sites. Diaz also notes that these sites \u201ctend to belong to\nlarge, well-known technology companies such as Amazon and eBay,\u201d\nwhile \u201cmillions of typical pages\u2026will have the lowest\nranking.\u201d Thus, it would seem that the kind of search engine bias\nidentified by Introna and Nissenbaum that, whereby certain Web sites\nare systematically included/excluded in favor of others, will not\nnecessarily be eliminated simply by resolving conflicts related to paid\nadvertising in the context of search engine companies. In all\nlikelihood, organizations will continue to figure out ways to use\nSEO-related techniques to achieve a higher ranking for their sites and\nthus gain better exposure on search engines, especially on Google. As\nHinman (2005) puts it, \u201cEsse est indicato in Google (to be is to be\nindexed on Google).\u201d\n3.1.3 The Problem of Objectivity\n\n\nConcerns affecting search engine bias vis-\u00e0-vis questions\nhaving to do with \u201cobjectivity\u201d have two distinct aspects:\n(A) objectivity regarding criteria used in search algorithms; and (B)\nobjectivity with respect to the results returned by a particular search\nengine in its responses to multiple users entering the same search\nquery. With regard to (A), we saw that the traditional criteria used by Web search engine\ncompanies to rank sites was based on two factors: the number of visits\nto a page, and the number of other pages that link to a given page;\nHinman (2005) believes that this technique would seem to give the user\n\u201csome semblance of objective criteria.\u201d He points out, for\nexample, that even if search engines were to \u201cget it wrong\u201d\nin returning the best results for a particular search query, there was\nan \u201cobjective fact of the matter to be gotten wrong.\u201d And\neven though the early search engines ranked their sites in terms of\n\u201cpopularity,\u201d there was, as Hinman (2005, 22) puts it, a\n\u201ctechnical and objective meaning\u201d of popularity. But we\nalso saw that this formula has changed dramatically in the case of Web\n2.0-era search engines, where increasingly \u201cpersonalized\nalgorithms\u201d tend to tailor search results to fit the profile of\nthe user entering the query.\n\n\nThis trend toward the \u201cpersonalizing\u201d of algorithms feeds\ndirectly into concerns affecting (B). Even though many sophisticated\nusers might suspect that the lists of returns for their search queries\nare biased, for any number of possible reasons\u2014e.g., paid\nadvertising, unfair influence by large corporations, editorial\ncontrol, and so forth\u2014many search engine users still tend to\nassume, perhaps na\u00efvely, that when any two users enter the exact\nsame search query in a major search engine such as Google, they would\nreceive identical lists of responses. In other words, even if the\nformula used is skewed, or biased in a way that favors some sites over\nothers, the search algorithm would nonetheless return results based on\na formula that is internally consistent, and thus standard or\n\u201cobjective\u201d in some sense. But, this is no longer the case\nin an era where formulas based on \u201cpersonalization\u201d\ngenerate search results tailored to a user\u2019s profile. For example, if\nI enter the term \u201ceagles\u201d in a search box, the list and\nthe order of returns that I receive will likely depend on the profile\nthat the search engine company has constructed about me. If the\ncompany determines that I am interested in biology, for instance, I\nmay be directed to a site sponsored by the Audubon Society. But, if\ninstead, it determines that I am a sports enthusiast living in the\nPhiladelphia area, I may be directed first to the Philadelphia Eagles\nWeb site. On the\ncontrary, if my profile suggests that I like rock/pop music, I may be\ndirected first to the site for the Eagles music group. So there would\nnot appear to be any overall \u201cobjective\u201d formula used by\nthe search engine in question.\n\n\nSome question whether a lack of objectivity with respect to the\nresults returned to users in response to their search queries is\nnecessarily a problem. For example, Blanke (2005, 34) believes that\nwe should \u201cneither demand nor expect to receive information from\nsearch engines that is objective\u201d (i.e., information that is\neither \u201cneutral or complete\u201d). So, he argues that any\ncritique of search engines as being biased \u201cmisses its\ntarget,\u201d because one should not expect search engines to deliver\nonly neutral and objective results. His rationale for this claim,\nhowever, seems to be based on the view that search engine technology\n\u201cwas not designed to do this.\u201d But this analysis seems to\nbeg the question, unless, of course, Blanke means that search engine\ntechnology could not, in principle, be designed to deliver neutral and\nobjective results.\n\n\nQuestions concerning objectivity in the context of search engines\nare also examined by Goldman (2008), who seems to defend\u2014and,\nat times, perhaps even applaud\u2014search engine bias in that\nrespect. First, he notes that search engine companies \u201cmake\neditorial judgments about what data to collect and how to present that\ndata\u201d (2008, 122). However, he also believes that search\nengine bias is \u201cnecessary and desirable\u201d\u2014i.e., it\nis \u201cthe unavoidable consequence of search engines exercising\neditorial control over their databases\u201d (p. 127). So he is\nwilling to concede that \u201csearch engine companies, like all other\nmedia companies, skew results.\u201d But while many assume that search\nengine bias is undesirable, Goldman sees it as a \u201cbeneficial\nconsequence of search engines optimizing content for their\nusers.\u201d He also believes that \u201cthe \u2018winner takes\nall\u2019 effect caused by top placement in search results will be\nmooted by emerging personalized search technology\u201d (p. 121). He\nfurther argues that \u201cpersonalized ranking algorithms\u201d will\n\u201creduce the effect of search engine bias because there will\nlikely be multiple \u2018top\u2019 search results of a particular\nsearch term instead of a single winner [and] personalized algorithms\nwill eliminate many of the current concerns about search engine\nbias\u201d (p. 130). Thus Goldman seems to suggest, paradoxically\nperhaps, that any problems affecting objectivity will be solved by\nincreased subjectivity in the form of personalization of\nresults achieved by personalized search algorithms. However, this\ndirection in search engine evolution can have serious negative effects\nfor democracy and democratic ideals (discussed further in\n Section 3.4).\n\n\nConcerns affecting objectivity and bias in the context of search\nengines are also closely related to controversies pertaining to the\nlack of \u201copenness\u201d or \u201ctransparency\u201d (see,\nfor example, Noble 2018). Some critics point out that search engine\ncompanies are not fully open, or transparent, both with respect\nto why they (a) include some sites and not others (in their\nlists of results for users\u2019 queries), and (b) rank some pages in\ntheir list of search results higher than others. These kinds of\nopacity/non-transparency-related concerns tend to fall under a\ndescription that Hinman (2005) calls \u201cthe problem of\nalgorithm.\u201d Hinman notes that the algorithms that govern\nsearches are well-kept secrets, which he also believes is\nappropriate. Because Google\u2019s PageRank algorithm is \u201ca\npatented and closely guarded piece of intellectual property\u201d\n(Halpern 2011, 4), we don\u2019t know the algorithm\u2019s\nformulas. And this factor, of course, makes it difficult to comment on\nan algorithm\u2019s objectivity or lack thereof.\n\n\nAnother set of worries affecting opacity/non-transparency arise\nbecause search engine companies do not always disclose, either fully or\nclearly, their practices with respect to two important points: (i)\nwhether (and to what extent) they collect information about users; and\n(ii) what they do with that information once it has been collected.\nThese kinds of opacity/non-transparency concerns involving search\nengine companies, which are also related to privacy issues affecting\nmonitoring and surveillance, are examined in detail in\n Section 3.3.\n3.2 Privacy, Consent, and Non-voluntary Disclosure of Personal Information\n\n\nAt least two distinct kinds of privacy concerns arise in the context\nof search engines. One set of privacy issues emerges because search\nengine companies can collect personal information about search engine\nusers; in this scheme, the users are, in effect, \u201cdata\nsubjects\u201d for the search engine companies and their advertisers.\nHowever, search engine users themselves\u2014whether acting on their\nown behalf or on behalf of organizations that hire them\u2014can use\nthe technology to conduct online searches about people. In this case,\nthe targeted people (some of whom may never have used or possibly have\nnever even heard of a search engine) are the subjects of search engine\nusers. In both cases, privacy concerns arise in connection with\nquestions about fairness for the data subjects involved. Consider that\nmany of those who become the subjects of the search queries have not\nexplicitly consented either to having certain kinds of personal\ninformation about them collected or having personal information about\nthem (that has been collected in some other context) also being made\navailable on the Web, or both.\n\n\nIn this section, we examine search-engine-related privacy concerns\naffecting people who have become the subjects, or\n\u201ctargets,\u201d of queries by search engine users. This kind of\nprivacy concern is exacerbated by the ever expanding amount of personal\ninformation about ordinary people that is currently discoverable by\nsearch engines and thus accessible to Internet users. But why, exactly,\nis this problematic from the vantage point of privacy? For one thing,\nit is not clear that most people have voluntarily consented to having\ninformation about them placed in databases or in online forums that are\naccessible to search engines (and thus potentially available to any\nInternet user). And we noted that search engine users, whether they are\nacting simply on their own, or as representatives of business and\ncorporations, can and often do access a wealth of information about\nmany of us via search engines. Privacy advocates question whether this\npractice is fair, especially to people who have not explicitly\nconsented to having personal information about them included in the\nonline forums and databases that are now so easily searchable because\nof sophisticated search engine technology.\n\n\nPrivacy concerns that arise in contexts in which people are the subjects\nof search queries can be further differentiated in terms of two\nseparate categories, i.e., where search engines are used to: (i) track\nthe location of individuals, sometimes for the purpose of harassing or\nstalking them; and (ii) acquire personal information about people. We\nbriefly examine each practice.\n\n\nRegarding (i), one might ask why using search engines to track and\nlocate persons is controversial from a privacy perspective. First,\nconsider that some organizations have developed specialized search\nengines for questionable purposes, such as stalking people. For\nexample, one search facility (Gawker-Stalker Maps, introduced in 2006)\nwas designed specifically for the purpose of stalking famous people,\nincluding celebrities.  Imagine a case in which a celebrity has been\nspotted while dining at an up-scale restaurant in San Francisco. The\nperson who spots the celebrity can send a \u201ctip\u201d via text\nmessage or e-mail to Gawker-Stalker, informing the site\u2019s users\nof her whereabouts. The Gawker site then provides its users, via\nprecise GPS software, with information about exactly where, and at\nwhat time, she was last seen. Users interested in stalking that\ncelebrity can then follow her movements electronically, via the Gawker\nsite, or they can locate and follow her in physical space, if they\nhappen to be in the same geographical vicinity as the celebrity at\nthat time (Tavani 2016). Currently, Gawker-Stalker seems to be in a\nstate of transition. Although gawker.com, a subsidiary of Gawker\nMedia, ceased operations following a bankruptcy suit in 2016, the\ncontroversial Gawker site was acquired by the Bustle Digital Group in\n2018 (Kelly 2019).\n\n\nSecond, we note that it is not only celebrities and \u201chigh-profile\u201d\npublic figures that are vulnerable to being stalked, as well as to\nhaving personal information about them accessed, via search engines.\nConsider the case of Amy Boyer, a twenty-year old resident of New\nHampshire, who was stalked online by a former \u201cadmirer\u201d\nnamed Liam Youens and who was eventually murdered by him in 1999. Using\nstandard Internet search facilities, Youens was able to get all of the\ninformation about Boyer that he needed to stalk her\u2014i.e.,\ninformation about where she lived, worked, and so forth (see, for\nexample, Tavani and Gridzinsky, 2002). Incidents such as the Boyer case\ninvite us to question current policies\u2014or, in lieu of clear and\nexplicit policies, our default positions and assumptions\u2014with\nregard to the amount and the kind of personal information about\nordinary persons that is currently accessible to search engine users.\nIt now appears likely that Amy Boyer had no idea that so much personal\ninformation about her was so easily accessible online via search\nengines.\n\n\nWe next examine (ii), the use of search engines to find information\nabout people\u2014not about their location, but about their\nactivities, interests, and backgrounds. As in the Amy Boyer case, these\nsearch-engine-related privacy issues also arise when ordinary people\nbecome the subjects of search queries. Consider that, increasingly,\nemployers use online search techniques to acquire information about\nprospective and current employees. It is well known that many employers\ntry to access information on the Facebook, Twitter, and Instagram accounts of job applicants\nthey are considering. This kind of information has, in certain\ninstances, been used by employers in determining whether or not to hire\nparticular applicants (and possibly also used in deciding whether or\nnot to promote current employees). So, for example, a college student\nwho posts on Facebook one or more pictures of himself drinking\nalcoholic beverages, or perhaps behaving wildly at a party, can\npotentially jeopardize his future employment opportunity with a company\nthat might otherwise hire him upon graduating from college. In defense\nof this kind of \u201cscreening\u201d practice used by companies in\nhiring employees, one could argue that the company has merely elected\nto use currently available tools to search for information about\npersons who voluntarily posted material (e.g., in the form of\nphotos, etc.) about themselves on Facebook.\n\n\nOur primary concern here is with personal information that has not\nbeen voluntarily disclosed by persons, but is nonetheless accessible\nonline via search engines. This kind of concern involving access to\npersonal information in online forums is by no means new or recent.\nConsider that in the decade preceding Facebook, employers had been\nable to access information about job applicants via online search\ntools\u2014e.g., they could (and did) use search engines to\naccomplish this task, simply by entering the name of the individual in\na search engine box. Imagine a hypothetical scenario in which a\nperson, Lee, applies for a full-time employment position at\nCorporation X. Also, imagine that someone on the\ncorporation\u2019s search committee for this position decides to conduct an\nonline search about Lee, shortly after receiving her\napplication. Further imagine that in response to the query about Lee,\nthree results are returned by the search engine.  One result includes\na link to a gay/lesbian organization in which Lee is identified as\nsomeone who contributed to a recent event hosted by that\norganization. Next, imagine that Lee is turned down for the job at\nCorporation X. Further imagine that Lee becomes curious as to\nwhy she might not have been selected for that job and she decides to\ndo an Internet search of her name for the first time. Lee then\ndiscovers the search result linking her to the gay/lesbian\norganization (Tavani 1998). Should Lee infer that she was denied the\njob because of her apparent association with this organization? Is\nthat a reasonable inference? Maybe not. Nevertheless, an important\nquestion arises: Is it fair that someone has posted this information\nabout Lee online, without her consent, to a source that is accessible\nto one or more search engines? Is that information about Lee now\n\u201cfair game,\u201d and should it be viewed simply as information\nthat is \u201cup for grabs\u201d (Nissenbaum 2004) and thus\nappropriate for use by prospective employers?\n\n\nHow is the scenario involving Lee different from a case in which an\nemployer uses information on a Facebook account to screen job\napplicants? For one thing, Facebook users typically post information\nabout themselves that can be seen by others and thus have voluntarily\nconsented to have that information available for others to access\n(assuming that they have not specified their Facebook privacy\nsettings). Furthermore, they are also aware that such information about\nthem exists on that online forum. But what about job applicants who do\nnot have accounts on Facebook, or on any other social networking site?\nAre they less vulnerable to online scrutiny by potential employers?\nMany people may have no idea about either the kind or amount of online\npersonal information about them that is accessible to an employer or to\nanyone using a search engine.\n\n\nNext consider a hypothetical scenario similar to Lee\u2019s, where\nPhil, who recently earned a Ph.D., is applying for a faculty position\nat University X. But suppose that a few disgruntled former students\nhave posted some highly critical and negative remarks about Phil on\nRateMyProfessor.com. Next, suppose that a member of the faculty search\ncommittee at University X conducts an online search on Phil and\ndiscovers the disparaging remarks made by the students. Finally, Phil\nis informed that he has not been selected for the faculty position.\nShortly after receiving his letter of rejection, Phil happens to\ndiscover the comments about him made by the disgruntled students, by\nconducting an online search of his name. Would it be unreasonable for\nPhil to infer that the remarks by these students on RateMyProfessor.com\ninfluenced the hiring committee\u2019s decision not to select him?\n\n\nIn one sense, Phil\u2019s predicament is very similar to\nLee\u2019s\u2014viz., neither job applicant had any kind of control\nover what people other than themselves had posted about them in online\nforums accessible to search engines. However, the negative information\nposted about Phil was directly related to the kind of criteria that\ntypically would be used in considering an applicant for a faculty\nposition. The information posted about Lee, while not directly related\nto the job for which she was applying, could nonetheless also harm her\nchances of gaining that position. In neither case, however, did Lee or\nPhil have any say about the kind of information about them, or about\nthe accuracy of that information, that could be so easily retrieved\nonline and used by a prospective employer in making a decision about\nwhether or not to hire them.\n\n\nOn the one hand, we can ask what kind of recourse people like Phil\nand Lee could expect to have in situations such as this\u2014e.g.,\ncan they reasonably expect to have control over any kind of information\nabout them that is currently accessible to search engines? But, on the\nother hand, it may not seem totally unreasonable for them to have some\nexpectation of limited control over their personal information, even if\nonly to be able to challenge the legitimacy of inaccurate information,\nespecially when they had not consented to having it included in online\nforums and databases accessible to search\n engines.[2]\nThere is also an aspect of this kind of personal information that\noverlaps with \u201cpublic\u201d information. So, perhaps the\ntension that arises in these scenarios can be viewed as a contemporary\nvariation of the age-old debate about the private vs. public nature of\npersonal information. This tension is further complicated by the fact\nthat in the U.S. most people, as the subjects of online searches,\nenjoy little, if any, normative protection regarding personal\ninformation about them that is now available online\u2014mainly\nbecause of the presumed \u201cpublic nature\u201d of this personal\ninformation involved (Tavani 2005). (As we will see in Section 4.2.1,\nhowever, citizens in EU countries enjoy much more normative protection\nof their personal data in online contexts than U.S. citizens.)\n\n\nSome forms of personal information enjoy normative protection via\nspecific privacy policies and laws, because they qualify as\ninformation about persons that is considered either sensitive or\nintimate, or both.  We can refer to this kind of personal information\nas Non-Public Personal Information (or NPI). However, many privacy\nanalysts now worry about the ways in which a different kind of\npersonal information\u2014Public Personal Information (or PPI), which\nis non-confidential and non-intimate in character\u2014is easily\ncollected and exchanged over the Internet. How can PPI and NPI be\ndistinguished? NPI, which as noted above, is viewed as information\nabout persons that is essentially confidential or sensitive in nature,\nincludes information about a person\u2019s finances and medical\nhistory. PPI, although also understood as information that is personal\nin nature, is different from NPI in at least one important respect: it\nis neither sensitive nor confidential. For example, information about\nwhere an individual works or attends school, as well as what kind of\nautomobile he or she owns, can be considered personal information in\nthe sense that it is information about some individual as a\nparticular person. However, this kind of personal information\ntypically does not enjoy the same kinds of privacy protection that has\nbeen granted to NPI (Tavani 2016).\n\n\nInitially, concerns about personal information that can be gathered\nand exchanged electronically focused mainly on NPI. In response to\nthese concerns, some specific privacy laws and policies were\nestablished to protect NPI. But many privacy advocates now also worry\nabout the ways in which PPI is routinely collected and analyzed via\ndigital technologies. They have argued that PPI deserves greater legal\nand normative protection than it currently has. Nissenbaum (1997,\n1998) has referred to the challenge that we face with regard to\nprotecting (the kind of information that we refer to as) PPI as the\n\u201cproblem of protecting privacy in public.\u201d Some privacy\nadvocates argue that our earlier assumptions about what kinds of\npublicly available information about us need explicit legal protection\n(or perhaps some kind of less formal \u201cnormative\u201d\nprotection, at the level of policies) are no longer adequate because\nof the way much of that information can now be processed via digital\ntechnologies, especially in the commercial sphere. For example,\nseemingly innocuous information about persons, based on their\nactivities in the public sphere, can be \u201cmined\u201d to create\nuser profiles based on implicit patterns in the data and those\nprofiles (whether accurate or not) can be used to make important\ndecisions affecting people.\n3.3 Monitoring and Surveillance\n\n\nWe next examine privacy concerns in which search engine users\nthemselves are the data subjects (i.e., for search engine companies).\nZimmer (2008, 83) notes that personal information about users is\n\u201croutinely collected\u201d when they use search engines for\ntheir \u201cinformation-seeking activities.\u201d But why, exactly,\nis this problematic from the perspective of privacy? For one thing,\nsearch engine companies such as Google create a record of every search\nmade by users, and these records are also archived. The topic searched\nfor, as well as the date and time the specific search request is made\nby a user, are included in the record. Until recently, many people had\nbeen unaware that their search queries were being recorded and\ntracked.\n\n\nIn January 2006, many Google users learned that the search engine\ncompany had kept a log of all of their previous searches. At least\nfour major search engine companies had been subpoenaed by the Bush\nAdministration in 2005 for search records based on one week\u2019s of\nsearches during the summer of 2005 (see, for example, Nissenbaum\n2010). They were Google, Yahoo, AOL, and Microsoft (MSN). Google\nrefused to turn over the information. The other search engine\ncompanies would not say how they responded; many assumed, however,\nthat those companies complied with the government\u2019s\nsubpoena. Google was requested to turn over two items: (1) the results\nof search queries/requests it received during a one-week period, and\n(2) a random list of approximately one million URLs searched. Google\nargued that turning over this information would: (a) violate the\nprivacy of its users (and undermine their trust in the search engine\ncompany), and (b) reveal information about the (proprietary) algorithm\nand the processes that Google uses and that this would potentially\nharm its competitive edge as a search service. A court ruled that\nGoogle did not have to comply with (1), but it reached a compromise\nregarding (2), ruling that Google turn over 50,000 URLs to the\ngovernment (Nissenbaum 2010, 29\u201330).\n\n\nThe information collected about a user\u2019s search queries might\nseem relatively innocuous\u2014after all, who would be interested in\nknowing about the kinds of searches we conduct on the Internet, and who\nwould want to use this information against us? On the other hand,\nhowever, seemingly innocuous personal information can be mined by\ninformation merchants and used to construct personal profiles about us,\nand that these profiles, in turn, can be based on information that is\nnot accurate and can be used to make decisions about us that are not\nfair. For example, imagine a case in which a student happens to be\nwriting a paper on Internet pornography and uses a search engine to\nacquire some references for her research. Records of this user\u2019s\nsearch requests could reveal several queries that individual made about\npornographic Web sites, which in turn might suggest that this user is\ninterested in viewing pornography. So individual searches made by a\nparticular user could theoretically be analyzed in ways to construct a\nprofile of that user that is inaccurate. And, records of the searches\nmade by this and other users could later be subpoenaed in court\ncases (Tavani 2016).\n\n\nAs already noted, information about a user\u2019s search queries is\ncollected by search engine companies as well as by many different kinds\nof \u201cinformation merchants\u201d in the commercial sphere.\nHalpern (2011, 8) notes that there are approximately 500 companies\nthat are able to track all of our online movements, thereby\n\u201cmining the raw material of the Web and selling it to\u2026data\nmining companies.\u201d Pariser (2011) points out that in tracking our\nactivities, Google\u2019s uses fifty-seven signals\u2014\n\neverything from where you were logged in, from what browser you\nwere using, to what you had searched for before to make queries about,\nto who you were and what kinds of sites you\u2019d like.\n\n And\nZimmer (2008, 77) notes that Google integrates information gathered\nfrom\n\nWeb cookies, detailed server logs, and user\naccounts\u2026 [from Google applications such as Gmail, Google +, and\nGoogle Chrome]\u2026 which provides a powerful infrastructure of\ndataveillance to monitor, record, and aggregate users\u2019 online\nactivities.\n\n Furthermore, Pariser notes that Google and other\nmajor search engine companies use \u201cprediction engines\u201d to\nconstruct and refine theories about who we are (and what we want to do\nnext). He also notes that many information merchants regard every\n\u201cclick signal\u201d a user creates as a \u201ccommodity\u201d\nthat can be \u201cauctioned off within microseconds to the highest\nbidding consumer.\u201d Pariser points out that one information\nmerchant, a company called Acxiom, has\n\n accumulated an average of\n1500 pieces of data on every person in its database\u2014personal\ndata that ranges from credit scores to medications used.\n\n\n\nOf course, some users might respond that they do not feel threatened\nby this practice; for example, they might be inclined to feel safe\nfrom a loss of personal privacy because they assume that the data\ncollected about them is anonymous in the sense that it is identifiable\nonly as an IP address, as opposed to a person\u2019s name. However, Zimmer\n(2008, 83) notes that in 2005, one AOL user was able to be identified\nby name \u201cbecause the Web searches she performed on various\ntopics were recorded and later released by AOL.\u201d It turns out\nthat AOL Research had released over three months worth of personal\nsearch data involving 650,000 users (Wall 2011, 18). Nissenbaum (2010,\n30) points out that in this case, AOL used a process in which\n\n certain\nidentities could be extracted from massive records of anonymized\nsearch-query data that AOL regularly posted on the Internet for use by\nthe scientific research community.\n\n Zimmer believes that the\nincident involving AOL is not unique, but is instead one more use of\ndata surveillance or \u201cdataveillance\u201d (a term coined by\nRoger Clarke in 1988)\u2014i.e., one applied in the context of\nsearch queries.\n\n\nIt is also important to consider whether a meaningful distinction\ncan be drawn between monitoring and surveillance in this context.\nNoting that the two terms are often used interchangeably, Nissenbaum\n(2010, 22) differentiates between them in the following way. Whereas\nsurveillance is a \u201cform of monitoring \u2018from above\u2019 by\npolitical regimes and those in authority,\u201d monitoring is used in\nbroader social and \u201csocio-technical\u201d contexts. In\nNissenbaum\u2019s scheme, both monitoring and surveillance are\nexamples of what she calls \u201csocio-technical contexts,\u201d but\nthey are usually put to different uses. For example, Nissenbaum points\nout that monitoring can be done by systems \u201cwhose explicit\npurpose is to monitor\u201d (e.g., CCTVs). But she also notes that\ninformation itself can constitute a \u201cmodality of\nmonitoring.\u201d For example, she points out that Clarke\u2019s\nnotion of \u201cdataveillance\u201d includes monitoring practices\nthat involve both interactions and transactions. However, we can\ngenerally regard the kinds of practices carried out by information\nmerchants in the consumer sphere as instances of monitoring (rather\nthan surveillance) in Nissenbaum\u2019s sense of that term.\n\n\nWe next shift our focus away from privacy concerns about monitoring\nin the commercial sector to worries about surveillance by\ngovernment actors with respect to information acquired as a result of\nusers\u2019 search queries. Earlier in this section we noted that in\n2005, the Bush Administration informed Google that it must turn over a\nlist of all users\u2019 queries entered into its search engine during\na one week period (the exact dates were not specified by Google). The\nBush Administration\u2019s decision to seek information about the\nsearch requests of ordinary users triggered significant criticism from\nmany privacy advocates. Although the Bush Administration claimed that\nit had the authority to seek electronic information in order to fight\nthe \u201cwar on terror\u201d and to prevent another September\n11-like attack, some critics worried that the government was trying to\nuse the subpoenaed information, not for national defense or\nanti-terrorism purposes, but rather to gain data to support its stance\non the Child Online Protection Act, which had been challenged in a U.S.\nDistrict Court and was being revisited by Congress (Nissenbaum 2010, 29).\nThese critics also worried about the implications this has for privacy\n(as an important human value) in the ongoing tension involving security\nvs. privacy interests. And even if privacy is not an absolute value but\nis sometimes outweighed by security concerns, as Himma (2007) argues,\nsome critics question the rationale used for obtaining records of\nsearch requests made by ordinary citizens.\n\n\nHinman (2005) notes that the Patriot Act, passed in the aftermath of\n9/11, allowed U.S. government officials to get information from\nlibraries about which books members had borrowed. He then shows how the\nreasoning used in the case of libraries could easily be extended to\nsearch engines\u2014for example,\n\n if the government could see\nwhich books someone was taking out of a library, why couldn\u2019t it\nalso see which searches we made on search engines?\n\n Hinman also points out that there are several other ways in which\na user\u2019s search requests can be disclosed because of practices\nused by major search engine companies such as Google. He also worries\nthat such practices could eventually lead to surveillance and to\nsuppressing political dissent (as is it has in China). Hinman\nquestions whether Google might have been under political pressure from\noutside interests (e.g., the Bush Administration) to take down\nphotographs of tortured prisoners in the controversial Abu Ghraib\ndetention center (used by the U.S. during the Iraq War in 2004), which\nwere posted but then soon removed with no apparent explanation by that\nsearch engine company. (Some related, as well as additional, questions\nconcerning moral-responsibility-related issues for search engine\ncompanies are examined in detail in Section 4.)\n3.4 Democracy, Censorship, and the Threat to Liberty and Freedom\n\n\nIn this section, we consider some implications that the surveillance\nof users\u2019 queries by search-engine companies can have for a free\nand open society. In the early days of the Internet, many people\nassumed that search engine technology favored democracy and democratic\nideals. For example, Introna and Nissenbaum (2000, 169) note that\nsearch engines were viewed as a technology that would\n\n\u2026give voice to diverse social, economic, and cultural\ngroups, to members of society not frequently heard in the public sphere\n[and] empower the traditionally disempowered, giving them access both\nto typically unreachable modes of power and to previously unavailable\ntroves of information.\n\n\n\nHowever, Introna and Nissenbaum also describe what can be viewed as an\n\u201canti-democratic\u201d aspect of contemporary search technology\nwhen they note that search engines \u201csystematically\nexclude\u201d certain Web sites, as well as \u201ccertain types of\nsites, \u201d over others. And Diaz (2008, 11) echoes this concern\nwhen he notes that that major search engine companies such as Google\ndirect \u201chundreds of millions of users towards some content and\nnot others, towards some sources and not others.\u201d So, following\nDiaz (p. 15), we can ask whether the kinds of \u201cindependent\nvoices and diverse viewpoints\u201d that are essential for a\ndemocracy are capable of being \u201cheard through the filter of\nsearch engines.\u201d\n\n\nSearch engines have often been described as the \u201cgatekeepers\nof cyberspace,\u201d and some critics note that this has significant\nimplications for democracy. For example, Diaz (2008, 11) points out that\n\n if\nwe believe in the principles of deliberative democracy\u2014and\nespecially if we believe that that the Web is an open\n\u2018democratic\u2019 medium\u2014then we should expect our\nsearch engines to disseminate a broad spectrum of information on any\ngiven topic.\n\n Hinman (2005, 25) makes a similar point, when he\nnotes that \u201cthe flourishing of deliberative democracy is\ndependent on the free and undistorted access to information.\u201d And\nbecause search engines are \u201cincreasingly the principal\ngatekeepers of knowledge,\u201d Hinman argues that \u201cwe find\nourselves moving in a philosophically dangerous position.\u201d (We\nbriefly return to Hinman\u2019s point in the\n concluding section of this entry.)\n\n\nMorozov (2011) also describes some concerns for democracy\nvis-\u00e0-vis contemporary search engines by calling attention to\nthe filtering of information that search engines make possible. For one\nthing, he agrees with Sunstein (2001) who worries that the kind of\nselectivity made possible by Internet filtering can easily trap us\ninside our \u201cinformation cocoons.\u201d And Lessig (2000)\nsuggests that any kind of filtering on the Internet is equivalent to\ncensorship because it blocks out some forms of expression. Morozov\npoints out that whereas Sunstein worries that people could use\nInternet technology to \u201coverly customize what they read,\u201d\nthe reality is that contemporary search engine companies have already\nsilently done this for them. Morozov\u2019s concerns about what search\nengine companies are now doing through filtering and customization\nschemes, and why this is problematic for a democracy, are echoed by\nPariser (2011, 13) who points out that \u201cpersonalization\nfilters serve up a kind of invisible autopropaganda,\nindoctrinating us with our own ideas, amplifying our desire for things\nthat are familiar and leaving us oblivious to the dangers lurking in\nthe dark territory of the unknown\u201d.\n\n\nPariser notes that while democracy \u201crequires citizens to see\nthings from one another\u2019s point of view,\u201d we are instead\nincreasingly \u201cmore enclosed in our own bubbles.\u201d He goes\non to note that democracy also \u201crequires a reliance on shared\nfacts,\u201d but instead we are being presented with \u201cparallel\nbut separate universes.\u201d To illustrate how this trend away from\ncitizens having shared facts can be so dangerous for a democracy,\nPariser uses the example of the debate about climate change in the U.S\nduring the past decade. He points out that studies have shown that\nbetween 2001 and 2010 many people\u2019s beliefs about whether the climate\nwas warming shifted significantly, based on one\u2019s affiliation with a\nmajor political party. Pariser notes that a Web search for\n\u201cclimate change\u201d will yield very different results for a\nperson whom the search algorithm determines to be a Democrat than for\nsomeone it determines to be a Republican. He also notes that the\nsearch algorithm will generate different results for someone it\ndetermines to be an oil company executive vs. an environmental\nactivist.\n\n\nAlong lines similar to Pariser\u2019s, Halpern (2011, 5\u20136) notes that\nsearch engines like Google direct us to material that is most likely\nto reinforce our own \u201cworldview, ideology, and\nassumptions\u201d and thus \u201ccut us off from dissenting opinion\nand conflicting points of view.\u201d Pariser points out that with\nGoogle, a user now gets the results that the search engine company\nbelieves are best for that particular user.  He describes a case where\ntwo people entered in the keyword \u201cBP\u201d (for British\nPetroleum) during the time period of the accident involving the Deep\nWater Horizon oil rig in the Gulf of Mexico. In response to one user\u2019s\nquery, investment information about BP was returned as the lead\nresult, while the other user received information about the oil\nspill.\n\n\nNot only do some practices by search engine companies pose a threat\nfor democracy and democratic ideals, other practices (in which search\nengine companies are arguably complicit) reinforce censorship schemes\ncurrently used by non-democratic nations. For example, it is well know\nthat China has succeeded in blocking access to political sites that it\nregards as threatening. Consider that it blocks ordinary Chinese users\nfrom access to sites such as \u201cTiananmen Square,\u201d\n\u201cFree Tibet,\u201d and \u201cDalai Lama.\u201d Critics note\nthat Google agreed to comply with China\u2019s censorship laws when\nthe search engine company entered the Chinese market in 2006. Spinello\n(2012) believes that this agreement violated Google\u2019s\n\u201cdon\u2019t be evil\u201d principle\u2014a core principle of\nthe search engine company\u2014because Google \u201cfacilitated and\nsupported\u201d China\u2019s censorship regime. But some of\nGoogle\u2019s defenders have argued that the search engine\ncompany\u2019s entry into China made it possible for China\u2019s\nresidents to have greater access to information, overall (i.e., beyond\nwhat would otherwise be accessible through Baidu (www.Baidu.com), a\nChinese-owned search engine service founded in 2000). Other defenders\nof Google point out that the search engine giant did not act alone\nbecause major U.S. companies, such as Yahoo and MSN, also complied\nwith China\u2019s censorship laws. And Hinman notes that the Chinese\ngovernment also received cooperation from other American companies,\nsuch as Cisco Systems, in establishing the infrastructure or backbone\ncomponents of its Internet firewall.\n\n\nIn 2010, Google changed its policy for operating in China and directed\nits Google.cn users to a site in Hong Kong that was then uncensored.\nHowever, Hinman believes that we should still worry about\nGoogle\u2019s willingness to comply with the Chinese\ngovernment\u2019s strict censorship laws when initially setting up\nits business operations in China. He reasons that if this search\nengine giant could be so easily influenced by a government that has a\nrelatively low economic impact on its business overall, it could be\nmuch more influenced by the U.S. Government, where the political and\neconomic impact would be far more significant. Some worry that this\nfactor has also given Google considerable power over companies that\nrely on it for their Internet traffic (Spinello 2012). They also worry\nthat Google, in an effort to retain that economic power in the U.S. in\nthe future, could conceivably cave into pressure to comply with\ngovernment policies (in the U.S. and possibly other democratic nations\nas well) that might support censorship at some level.\n\n\nWhile many initially believed that that the Internet would promote\ndemocracy by weeding out totalitarian societies, because they are\n\u201cinefficient and wasteful\u201d (Chorost 2011), Berners-Lee\n(2010) believes that the Web \u201cwe have come to know\u201d is now\nthreatened because both totalitarian and democratic governments alike\nare \u201cmonitoring people\u2019s online habits, endangering important\nhuman rights.\u201d Perhaps Hinman (2005, 25) best sums up this\nworry, when he remarks,\n\n\nWe risk having  our access to information controlled by ever-powerful,\nincreasingly opaque, and almost completely unregulated search engines\nthat could shape and distort our future largely without our knowledge.\nFor the sake of a free society, we must pursue the development of\nstructures of accountability for search engines.\n\n\nIf Berners-Lee, Hinman, and others are correct, it would seem that we\nhave much to worry about indeed as we go forward trying to preserve\nour basic freedoms in a democracy, while at the same time taking\nadvantage of many of the personalizing- and customizing-based features\nthat search engines such as Google now offer us. It would also seem\nthat increased transparency on the part of search engine companies\ncould would be an important step in helping us to alleviate some of\nthese concerns. But who, exactly, should be responsible for regulating\nsearch engine companies and for holding them accountable? We examine\nthese and related questions in Section 4. First, however, we briefly\nconsider some ethical issues, particularly cyber-security related\nconcerns, that arise because of a relatively new kind of search engine\nthat is capable of \u201cdiscovering\u201d the location of \u201cthings\u201d (objects)\nvia the Internet.\n3.5 (Cyber)Security and the Internet of Things\n\n\nIn Sections 3.1 through 3.4, we examined a wide range of ethical\nconcerns affecting the use of search engines, focusing mainly on what\ncan be viewed as the \u201cstandard\u201d or received ethical\nconcerns that have come to be associated with search engines during\nthe past three decades. One topic that arguably has been neglected, or\nperhaps considered only indirectly (e.g., in our earlier discussion of\nprivacy-related issues in Sections 3.2 and 3.3), has to do with\nethical concerns affecting (cyber)security. In addition to\ndata security, which is closely related to data privacy (described\nearlier), issues associated with system and network security, as well\nas with national and international security, also now arise in light\nof a specific kind of search search engine that is capable of locating\n\u201cthings.\u201d In this section, we briefly consider some\nemerging security-related challenges generated by the use of this type\nof search engine in the context of the Internet of Things (IoT).\n\n\nWhat is IoT, and how does it raise security-related ethical concerns\naffecting search engines? IoT generally refers to the interconnection\nof \u201cobjects\u201d on the Internet; these (networked and\n\u201cintelligent\u201d) objects can include webcams, printers, and\nother devices that are not easily identifiable, locatable, or\naccessible to users via conventional search engines (see, for example,\nBurgess 2018). So users might assume that since information about\nthese objects would not be accessible via typical Internet searches,\nthe objects would be fairly secure from network hackers. Many users\nmight also be enamored by the kinds of conveniences they could enjoy\nif their \u201cintelligent\u201d objects were networked\ntogether. Consider, for example, a \u201csmart home\u201d where\nthese objects could communicate with one another as well as with that\nhomeowner\u2019s mobile devices (and possibly also communicate\ndirectly with networked devices and applications embedded in the\nhomeowner\u2019s automobile). On the one hand, we can imagine that\nthe owner of this home would be delighted if its\n\u201cintelligent\u201d refrigerator communicated with her while\ndriving home to alert her that the refrigerator\u2019s supply of milk\nis low. The refrigerator might also communicate directly with the\nhomeowner\u2019s smart phone to trigger an app designed to display\nthe lowest prices for milk at nearby grocery stores. So, ordinary\nusers could easily come to enjoy the kinds of conveniences made\npossible via their interconnected \u201cintelligent\u201d\nobjects. On the downside, however, it is likely that these objects may\nnot have an appropriate level of network security. Thus users could be\nvulnerable to having their objects hacked and also be subject to\nhaving personal information about them (vis-\u00e0-vis their\ninteractions with these objects) acquired by unauthorized individuals\nand organizations, if search engines were indeed capable of\ndiscovering and locating their (non-sufficiently-secure) objects and\ndevices.\n\n\nWhereas traditional search engines have assisted users in locating\nonline information pertaining to the names of people and places,\ncommercial/governmental/educational organizations and institutions,\netc. (via a standard protocol involving HTTP addresses and HTML\nprogramming code), some search engines are now also capable of\nsearching the Internet for certain kinds of \u201cthings\u201d (or\nobjects). Two of these search engines are Thingful and\nShodan. Thingful describes itself as a \u201cdiscoverable\nsearch engine\u201d that provides users with \u201ca unique\ngeographical index of connected objects around the world\u201d\n(https://thingful.net/). As such, Thingful boasts that it can index\nacross multiple IoT networks and infrastructures. Because this search\nengine can locate the geographical position of objects and devices,\nsome critics worry that Thingful can also easily access personal data\nregarding the ways in which users interact and communicate with their\nconnected devices and objects (as in the case of a homeowner\ncommunicating with the \u201cintelligent\u201d objects in her smart\nhome).\n\n\nShodan, a controversial search engine created by John Matherly in\n2009, enables users to find specific types of (Internet-connected)\ncomputers, devices, and systems (including servers and routers), via\nglobally located servers that index the internet continuously.  Shodan\nhas been described both as a \u201cdark Google\u201d and the\n\u201cscariest search engine ever\u201d because it could be used to\nlocate components in a nation\u2019s critical infrastructure as well\nas its military defense system, including the \u201ccommand and\ncontrol systems for nuclear power plants and particle-accelerating\ncyclotrons\u201d (Goldman 2013). Some have even suggested that Shodan\nwas used to locate and monitor major components in Iran\u2019s\ncontroversial nuclear program (Charette 2012). This has also led to\nspeculation that Shodan may have assisted the (then secret)\n\u201cOlympic Games Operation\u201d allegedly carried out by the\nU.S. and Israel to cause Iran\u2019s centrifuges\u2014i.e.,\nfast-spinning machines that enrich uranium\u2014to spin out of\ncontrol (O\u2019Harrow 2012).  While some might be inclined to argue that nations\nsuch as the U.S and Israel would have been justified in using whatever\ncyber-related means they had at their disposal to disrupt the progress\nof Iran\u2019s nuclear program, we can clearly see the downside of\nsuch a practice; i.e., it could also be adopted by so-called\n\u201crogue nations\u201d and used against \u201clegitimate\u201d\nnation states.\n\nSo it would seem that \u201cdiscoverable search engines\u201d like\nShodan and Thingful pose threats not only to our personal privacy and\nsecurity, but also to the security of a nation\u2019s critical\ninfrastructure and military defense systems as well. It will be\ninteresting to see whether this relatively new kind of search engine\nwill be used by nation states, and possibly even by terrorist groups\nas well, in their future strategies and practices affecting\ncyberwarfare, cyberterrorism, and cyberespionage. It will also be\ninteresting to see whether these search engines, and the companies and\norganizations that develop them, will need to be closely regulated\nbecause of the significant societal threats they now pose. We next\nexamine a related question as to whether search engine companies in\ngeneral have some special social/moral responsibilities, in light of\ntheir important societal roles.\n4. Moral Accountability and Social-Responsibility Issues for Search Engine Companies\n\n\nThus far, issues from the perspectives of business ethics and\nprofessional responsibility have not been directly addressed in this\nentry; our main focus in Section 3 has been on the kinds of moral\nimpacts that search engines have for ordinary users, especially with\nrespect to privacy, surveillance, and freedom. In Section 3.1, however,\nwe hinted that search engine companies (SECs) might be held morally\naccountable for their practices involving bias in the ranking of search\nresults. And in concluding Section 3.4, we briefly entertained the\nnotion that major SECs might be held accountable for policies and\npractices that either favor or directly support censorship. In this\nsection, we consider whether SECs have any special social\nresponsibilities and moral obligations because of their\n\u201cprivileged place\u201d in our society. We begin by briefly\ndescribing a key societal role that search engines play in providing\naccess to information and knowledge.\n\n\nElgesem (2008, 241) notes that search engines have an important\nsocietal role as \u201ccontributors to the public use of reason\n\u201d, and Noble (2018, 148) argues that search results can actually\n\u201cstructure knowledge\u201d for the societies within which they\noperate. As we already noted in Section 3, Hinman (2005, 2008) and\nDiaz (2008) view search engines as \u201cgatekeepers\u201d of\nknowledge on the Web. Taddeo and Floridi (2016, 1575), who also\ndescribe the important \u201cgatekeeping function\u201d that SECs\nlike Google now have because of their \u201ccentrality\u201d in\ninformation societies, worry about the lack of consensus thus far with\nregard to which \u201cprinciples should shape\u2026[an\nSEC\u2019s]\u2026moral responsibilities and practices.\u201d The\nauthors argue that we need an \u201cethical framework\u201d both to\n\u201cdefine\u201d an SEC\u2019s responsibilities and\n\u201cprovide the fundamental sharable principles\u201d necessary to\nguide an SEC\u2019s conduct \u201cwithin the multicultural and\ninternational context in which they operate.\u201d But Taddeo and\nFloridi also believe that this problem applies to other kinds of major\n\u201conline service providers,\u201d which the authors refer to as\n\u201cOSPs\u201d; they include Facebook and Twitter as well as\nGoogle under that category. However, we limit our focus on\nmoral-responsibility issues in this section to SECs per se.\n\n\nDoes the central role that major SECs like Google and Bing now play\nas \u201cgatekeepers\u201d of knowledge in our information society\nentail some special responsibilities for them? Hinman (2005, 21) lists\nfour reasons why these companies should shoulder significant social\nresponsibility. First, he notes that search engines \u201cplay an\nabsolutely crucial role\u201d in accessing information and that\nwithout them, the Web would \u201csimply be inaccessible to us\u201d\nand thus \u201calmost useless.\u201d Second, he points out that\naccess to information is \u201ccrucial for responsible\ncitizenship,\u201d also noting that \u201ccitizens in a democracy\ncannot make informed decisions without access to accurate\ninformation.\u201d Third, Hinman notes that search engines have become\n\u201ccentral to education,\u201d and he points out that students now\nsearch on Google and other major search engines more frequently than\nthey visit libraries. Fourth, he points out that major search engines\nare owned by private corporations \u2013 i.e., by businesses that are\nmainly, and \u201cquite properly,\u201d interested in making a\nprofit. Regarding Hinman\u2019s fourth point, it would seem that\nconflicts can easily arise because of an SEC\u2019s mission to be\nprofitable and its broader societal role in providing access to\ninformation in a manner that is fair, accurate, and unbiased.\n4.1 Commerce-Related Conflicts for SECs\n\n\nConsider one kind of conflict involving bias and profit\nfor SECs. Nicas (2011, 1) points out that while many SECs were\ninitially \u201ccontent to simply produce search results,\u201d some\nare now very much involved in diverse markets, \u201coffering\neverything from online music to local coupons to mobile phones.\u201d\nNicas also describes a bias-related concern affecting this trend by\nnoting that when Google entered into the online travel business, it\n\u201cbegan placing its new flight-search service atop general search\nresults\u201d \u2013 i.e., above those of other major players in the\nonline travel business such as Orbitz and Expedia. Because Google also\nengages in these kinds of practices in EU countries, where an estimated\n85% of users select Google either as their main or sole search engine,\nit has and continues to face charges of anti-trust violations. In\nEurope, Google has been formally charged with \u201csystematically\nfavoring its own comparison shopping product\u201d by prominently\ndisplaying that product/service in its search returns,\n\u201cirrespective of its merits\u201d and at the expense of its\ncompetitors (Chappell 2016). If the accounts provided by Nicas,\nChappell, and others are correct, it would seem that there are good\nreasons to be concerned about commercial conflicts involving major SECs such as Google.\n\n\nA relatively new bias-related conflict for SECs has recently surfaced\nin connection with \u201cvoice search,\u201d which is becoming\nincreasingly popular with users of mobile devices. Barysevich (2016)\nnotes that voice search is the \u201cfastest growing type of\nsearch,\u201d pointing to statistics in the U.S.showing that 55% of\nteenagers and approximately 40% of adults use this mode of search on a\ndaily basis.  Why are users\u2019 searches trending in the direction\nof voice versus (\u201ctraditional\u201d) text? Hawkins (2017, Other\nInternet Resources) offers two reasons: (a) voice search is faster\nthan text, since most people also tend to speak much more quickly than\nthey type; and (b) voice search is hands-free, which is convenient for\npeople using mobile devices. Given Google\u2019s preeminence \u2013\nand some might even say its \u201cdominance\u201d \u2013 in the\nonline \u201csearch industry\u201d to date, one might assume that\nGoogle has also been the lead player in the emerging area of voice\nsearch. But Hawkins points out that Bing has had an edge over Google\nin the voice-search market thus far. He attributes this to the fact\nthat Apple\u2019s Siri, Microsoft\u2019s Cortana, and Amazon\u2019s\nAlexa \u2013 three of the four major \u201cvirtual assistants\u201d\non the market \u2013 use Bing for their searches. Of course, one\nmight well expect that Microsoft\u2019s voice-assistant applications\nwould direct its users\u2019 searches to Bing, and that\nGoogle\u2019s voice assistants would do likewise in directing users\nto its search engine. But one could also question whether voice-search\nusers, especially of products/services other than Microsoft\u2019s\nand Google\u2019s, should be given an explicit option to set default\nvoice-search requests on their devices to a specific search engine of\ntheir choice?  Otherwise, companies like Amazon and Apple could be\nperceived as biased because of their tilting, even if unintentionally,\nthe voice-search market toward Bing (and away from Google)? However,\nsince Google has already cornered so much of the existing search and\nsearch-related markets, it might be difficult for us to view that SEC\nas somehow itself a victim of (search) bias.\n\n\nSome critics have pointed out that conflicts of interest involving\nSECs are not limited to bias and unfair business practices in the\ncommercial sector, but can also affect the free flow of knowledge in\nsociety. For example, Carr (2011, 163) worries that the commercial\ncontrol that Google and other major SECs now have over the\n\u201cdistribution of digital information\u201d could ultimately lead\nto \u201crestrictions on the flow of knowledge.\u201d And Hinman\n(2008, 67) believes that the control of knowledge that SECs have is\n\u201cin a very fundamental sense, a public trust, yet it remains\nfirmly ensconced in private hands and behind a veil of corporate\nsecrecy.\u201d Much of this secrecy, as we have seen, is closely tied\nto the proprietary search algorithms that major search engines use,\nwhich also raises the question of whether aspects of these algorithms\nshould be more transparent to the general public. But Elgesem (p. 241)\nbelieves that SECs should not be required to disclose information about\ntheir proprietary search algorithms, arguing instead that they should\nbe required to (a) make their policies known to users and (b) follow\nthose policies as closely as possible.\n\n\nWe conclude this section by noting that SECs continue to face some\ncritical challenges with respect to fulfilling their \u201cgatekeeper\nrole\u201d in a socially responsible way, while simultaneously\nprotecting both their proprietary algorithms and the interests of their\nshareholders to whom they also have legal and moral obligations. In the\nremainder of Section 4, we focus our attention on two different kinds\nof accountability-related ethical concerns affecting SECs: (i)\nmoral responsibility/legal liability issues arising\nin cases where search engines have provided links to Web sites whose\ncontent can be considered morally controversial; and (ii) questions\ninvolving trust. We begin with an analysis of (i).\n4.2 Legal Liability, a \u201cRight to Erasure,\u201d and Fake News in the Digital Era\n\n\nShould SECs be held accountable for directing users, even if\nunintentionally, to some Web sites whose content is either illegal or\nmorally controversial, or both? We should note that American SECs,\nincluding Google, claim that they provide only a service, and\nthus are not \u201ccontent providers.\u201d These companies believe\nthat it is not reasonable to hold them responsible/liable for the\ncontent on sites that they merely identify or provide links to in their\nlists of search returns. They also correctly point out that in the\nU.S., providers of online services \u2013 including major ISPs like\nVerizon and Comcast \u2013 are not held legally liable for the online\ncontent accessible through their services, provided they comply with\nofficial legal requests to remove content that explicitly violate U.S.\nlaws. Analogously, SECs believe that if they comply with official legal\nrequests to \u201cde-index,\u201d or remove links to, sites that\nwillingly and intentionally violate U.S. laws \u2013 e.g., laws\nviolating copyright, child pornography, and so forth \u2013 they too\nshould not be held legally liable for any content that users happen to\naccess via their services. However, many American-owned SECs operate\ninternationally, where laws and regulatory schemes differ from those in\nthe U.S. (as we saw in the case of Google in China).\n\n\nIt is worth noting that in Europe, SECs are viewed as\n\u201ccontrollers of personal data,\u201d as opposed to mere\nproviders of a service (see, for example, Google Spain SL, Google\nInc. 2013); as such, these companies, as well as ISPs that operate in\nEU countries, can be held responsible/liable for the content that is\naccessible through their services. One major difference regarding how\nSECs are viewed in the U.S. versus Europe came to the fore in the\nrelatively recent debate about whether users should have the\n\u201cRight to Be Forgotten,\u201d now commonly referred to as the\nRight to Erasure.\n4.2.1 The Right to Erasure (RtE)\n\n\nWhat is RtE, and why has it been controversial from the point of view\nof moral accountability issues for SECs? We can trace the origin of\nwhat eventually became the RtE debate to 2010, when Mario Costeja\nGonz\u00e1lez requested that Google remove a link included in its\nlist of returns for online searches of his name (see, for example,\nGoogle Spain SL, Google Inc. 2014). The link in question was to an\narticle in a Spanish newspaper (La Vanguardia) about a home\nforeclosure that occurred 16 years earlier. Gonz\u00e1lez, a Spanish\ncitizen, petitioned Spain\u2019s National Data Protection Agency, to\nhave the link removed. He argued that the information about his\nforeclosure, which was still prominently displayed in Google\u2019s\nlist of search returns for \u201cMario Gonz\u00e1lez,\u201d was no\nlonger \u201crelevant.\u201d Although the Spanish court ruled in\nGonz\u00e1lez\u2019s favor in 2010, many critics were unsure that\nthis court\u2019s ruling would also hold in other EU countries. So,\nthese critics were not surprised when Google appealed the Spanish\ncourt\u2019s ruling.\n\n\nAs the debate in Europe escalated, many RtE advocates argued that\nthe (former) EU Directive on Data Protection (Directive 95/46/EC),\nwhich protects the privacy rights citizens of all EU countries,\nincludes language that indirectly supported RtE. For example, they\nnoted that Article 12 (\u201cRight of Access\u201d) of that directive\ngrants \u201cthe rectification, erasure, or blocking of data the\nprocessing of which does not comply with the provisions of the\nDirective because the information is\u2026inaccurate\u201d (see the\nEuropean Commission\u2019s \u201cFact Sheet on \u2018The Right to Be\nForgotten\u2019 Ruling\u201d). So, citizens of EU countries already\nhad an explicit legal right to have inaccurate personal\ninformation about them removed/erased from the Internet, or at least\nde-linked or de-indexed from search engines. Some RtE supporters also\nbelieved that Article 12 could be interpreted to imply a right to the\nerasure/de-linking of online personal information that is no longer\ndeemed to be relevant.\n\n\nSECs and other RtE opponents, including publishers and journalists,\nhave viewed RtE from a very different perspective. They argued that\nRtE would censor information, thereby limiting freedom of expression,\nand would degrade the overall quality of the Internet by limiting the\namount of online information available to users (see, for example,\nTavani 2018). Because of these and related concerns, they concluded\nthat the public would be\nharmed by RtE. However, some RtE supporters, including Bottis\n(2014, 2), suggested that RtE-related data protection was needed\nprecisely because it would help to eliminate harm \u2013 viz.,\n\u201cpsychological harm\u201d (and possibly even physical harm in\nsome cases) \u2013 that users might otherwise experience. Consider,\nfor example, that victims of \u201crevenge porn\u201d sites would\nhave explicit legal recourse regarding their requests to\nremove/de-index links that associated their names with those\ncontroversial sites (see the account of revenge porn vis-\u00e0-vis\nthe Right to Be Forgotten, now RtE, in Kritikos 2018).\n\n\nIn 2012, the European Commission (EC) proposed a revised\nData-Protection Regulation (Article 17) for the (former) EU Directive,\nwhich included specific provisions for the removal (i.e., the\n\u201crectification, erasure, or blocking\u201d) of personal data\nthat was irrelevant (as well as personal data that was\n\u201cinaccurate,\u201d \u201cinadequate,\u201d or\n\u201cexcessive\u201d). The EU Parliament approved the EC\u2019s\nrecommendations in 2013, after making a few minor modifications to\nArticle 17 and renaming the \u201cright\u201d in question the\n\u201cRight to Erasure.\u201d Perhaps not surprisingly, the newly\nrevised RtE regulation was challenged by Google and other corporations\noperating in Europe. However, in May 2014, the Court of Justice of the\nEuropean Union (CJEU) upheld the Spanish court\u2019s (2010) ruling in\nthe Gonz\u00e1lez case, also confirming Article 17 of the updated EU\nDirective on Data Protection. The high court\u2019s ruling confirmed\nthat citizens residing in all EU nations have the right, under certain\nconditions, to request that search engine operators remove links to\nsome kinds of personal information about them that are \u201cno longer\nrelevant.\u201d The CJEU also noted, however, that RtE is not an\nabsolute right; for example, this right would have to be balanced\nagainst other rights, including \u201cfreedom of expression\u201d\n(see the EU\u2019s \u201cFact Sheet on the \u2018Right to Be\nForgotten\u2019 Ruling\u201d).\n\n\nAlthough Google announced that it would comply with the CJEU\u2019s\nruling, it also worried that it would not be able to respond to all of\nthe users\u2019 requests that it would likely receive to\nremove/de-index links; doing so, it suggested, would not only be an\nonerous task but might also be a practical impossibility. Kelion (2019)\nnotes that in the time period between the CJEU\u2019s May 2014 ruling\nand September 2019, Google had received \u201cmore than 845,000\nrequests to remove a total of 3.3 million web addresses,\u201d which\nresulted in approximately \u201c45% of the links ultimately getting\ndelisted.\u201d So it would seem that Google and its supporters indeed\nhave a reasonable concern, given the magnitude of removal requests it\nhas received. But one can still reasonably ask, as Bottis has, whether\nthe sheer volume of these requests constitutes sufficient grounds for\nrepealing RtE.\n\n\nBottis points out we do not cease to enact and to comply with laws\nthat apply in physical space simply because their enforcement could not\npossibly eliminate crimes such as prostitution, drug dealing, etc. She\nfurther notes that in the digital world, protecting privacy and\ncopyright has sometimes seemed impossible to achieve, but we still\npropose, enact, and enforce laws to protect both as best we can. So on\nthis view, we could not justify \u201cde-legislating\u201d RtE solely\non the grounds that it is difficult to enforce. But it is also worth\nnoting that as in the case of Google\u2019s arguments for repealing\nRtE, many of the arguments advanced by RtE\u2019s supporters also\ninclude one or more logical fallacies; an analysis of some of those\nfallacious arguments is included in Tavani (2018).\n\n\nIn one sense, the debate about RtE (formerly referred to as the RTBF\ndebate) would now seem moot, in Europe at least, following the\nCJEU\u2019s 2014 ruling and its later approval of the General Data\nProtection Regulation (GDPR), which was adopted by the EU in April 2016\nand went into effect in May 2018. Yet, a new and different kind of\ndebate concerning RtE has since replaced the original \u2013 viz., a\ndispute about (i) how RtE should be implemented in the EU, especially\nin light of what some have criticized as the \u201cvague\u201d\nguidelines given to SECs; and (ii) whether RtE needs to be universally\napplied to be effective. With respect to (i), Google and other SECs\nclaimed that the CJEU did not provide sufficiently clear and precise\ncriteria for guiding them in determining which requests for\nremoval/deletion merit serious consideration and which do not. So,\nGoogle and other SECs continue to examine RtE requests on a\ncase-by-case basis.\n\n\nRegarding (ii), many privacy advocates argue that while RtE applies\nin all EU countries, it cannot be an effective law without strong\ninternational support; so, many members of the EC, as well as the\nprivacy regulators in the 28 EU nations (including the UK which will\nsoon exit the EU but will likely continue to support the principles\ncomprising RtE), have argued that RtE should apply beyond Europe. Even\nthough SECs operating in Europe are required to comply with GDPR (and\nits RtE provisions) in all EU countries, RtE can be very easily\ncircumvented both within and outside Europe. For example, a European\nuser who enters the keyword \u201cMario Gonz\u00e1lez\u201d on\nGoogle.com (or on non-local (European) versions of Bing, Yahoo,\nDuckDuckGo, etc.) will still find the link to the story about that\nperson\u2019s home foreclosure prominently listed, even though users\nof Google.es (in Spain), or Google users in other EU countries such as\nGermany (Google.de), will not see that link in their list of search\nreturns. So RtE supporters have petitioned for broader application of\nthe new law, while Google and other SECs have challenged that view in\ncourt. In September 2019, the EU\u2019s highest court ruled that\nGoogle did not have to apply RtE globally (Kelion 2019). So, many\nprivacy advocates in Europe and elsewhere believe that a universal or\nglobal version of RtE is still needed for it to be an effective\nlaw (see, for example, Global Partners Digital 2018).\n4.2.2 The Challenge of Fake News in the Digital Era\n\n\nNot only are SECs now required to comply with requests (from European\ncitizens living in EU countries) to de-index and remove links to\npersonal data that is deemed to be \u201cirrelevant,\u201d they\ncurrently face a somewhat similar challenge with respect to what to do\nabout indexing and providing links to false and/or misleading\ninformation included in online forums. This is especially apparent in\nthe case of Fake News (FN). What is FN, and why is it a challenge for\nSECs? Although there is no single, universally agreed upon definition\nof FN, it is generally understood to mean \u201cfabricated\nnews,\u201d \u201cfalse news stories,\u201d and so forth. According\nto the Cambridge English Dictionary, FN is defined as\n\u201cfalse stories that appear to be news, spread on the internet or\nother media, usually created to influence political views\u201d (see\nthe link in Other Internet Resources).  The prevalence of FN in online\nforums, and especially on social media sites and blogs, raises\nquestions about whether SECs are complicit in furthering the\ndissemination of this false information.\n\n\nIn the U.S., concerns about FN received considerable media attention\nfollowing the 2016 presidential election, when it was reported that\nactors in nations outside the US were deliberately posting false and\nmisleading information on social media sites such as Facebook to\ninfluence the outcome of the election against then-candidate Hilary\nClinton. In one sense, it would seem that the specific details of this\ncontroversy affect mainly, or possibly even solely, social media sites\nsuch as Facebook and Twitter as well as blogs, but not necessarily\nSECs. But we can also question whether SECs might share some\nresponsibility for exacerbating concerns about FN by providing users\nwith search results that \u2013 even if unintentionally \u2013 link\nto various FN stories included on social media sites and elsewhere on\nthe Internet.\n\n\nOne might be inclined to argue that since FN deliberately promotes\nerroneous and inaccurate information as truthful content, it should be\nmore closely regulated by governmental agencies. But as Lipinski\n(2018, 69) points out, \u201cwhile all Fake News possesses an element\nof untruth, not all Fake News is defamatory.\u201d He goes on to note\nthat in the U.S., the purveyor of non-defamatory FN is protected by\nthe First Amendment; so there is no legal recourse against the\n\u201cspeaker\u201d of non-defamatory FN or against the media\noutlets that post it. Thus, Lipinski\u2019s description of the law\ncould be interpreted to suggest that SECs, along with social media\nsites, should be exonerated from any legal liability when it comes to\nthe dissemination of FN. Yet, even if social media sites continue to\npermit the posting of FN on their platforms, we can still ask whether\nSECs might be held to a higher standard, given their privileged role\nas gatekeepers of knowledge on the Web to ensure that search engine\nusers users are directed to information that is\n\u201caccurate\u201d.  \n\n\nShould SECs be responsible for de-indexing links that happen to direct\ntheir users to some of the original sources of FN posts. An SEC, in\nits defense, could argue that it would be unfair to be held\nresponsible for pointing users to information that is legal and that\nis already freely available on the Internet. However, a critic might\nrespond that SECs operating in Europe have already agreed \u2013\nalbeit under the threat of law \u2013 to remove links to information\nthat has been deemed \u201cirrelevant,\u201d even though that\ninformation\u2019s content is truthful. So, a\nfortiori, the critic might ask: why shouldn\u2019t those\ncompanies also be required to remove links to information that is\nblatantly false and/or deliberately misleading? Perhaps the EU\ncountries \u2013 building on the rationale used in RtE (and also\nincluded in the broader GDPR) for allowing the erasure of personal\ninformation that is \u201cinaccurate,\u201d as well as no longer\n\u201crelevant\u201d \u2013 will take the lead in the future in\nrequiring SECs to respond to requests to remove links to at least some\nkinds of FN.\n\n\nIt is also worth noting that some major SECs have codes of ethics or\ncodes of conduct containing specific principles that can be\ninterpreted as conflicting with practices that contribute to or\nperpetuate the dissemination of FN, or any kind of information that\nhas been shown to be false or misleading. For example, Google\u2019s\nCode of Conduct (see link in Other Internet Resources), Section I\n(titled \u201cServe Our Users\u201d) states that because Google\ndelivers \u201cgreat products and services,\u201d it holds itself to\n\u201ca higher standard.\u201d And in Section I.1 (titled\n\u201cIntegrity\u201d), the Google Code states:\n\nOur reputation as a company that our users can trust is our most\nvaluable asset, and it is up to all of us to make sure that we\ncontinually earn that trust. All of our communications and other\ninteractions with our users should increase their trust in us.\n\n\n\nDoes Google\u2019s commitment to hold itself to a higher standard\n\u2013 i.e., to more than what is legally required of that corporation\nin a minimalist sense \u2013 imply that it has an obligation to avoid\ndirecting its users to sites that traffic in FN? And if Google is seen\nas enabling increased access to false and misleading information, could\nthat lead to an erosion of the \u201ctrust\u201d that this SEC now\nseems to enjoy? In its code of conduct, Google claims that it strives\n\u201ccontinually\u201d to \u201cearn\u201d the trust of its users.\nBut in an era of FN, we can wonder whether the sense of trust that\nGoogle, or any other major SEC, seeks to increase might instead begin to erode.\n\n\nIn Section 4.3 we further examine the concept of trust in the\ncontext of SECs. First, however, we conclude this section by noting\nthat as of this date, no clear and explicit policies have been adopted\nby major SECs regarding FN. In fact, there is no evidence to suggest\nthat SECs even see themselves as bearing any level of responsibility\nfor the rapid spread of FN. Many SECs would likely argue that if any\nentity ought to be held responsible for the widespread FN on the\nInternet, it should be the social media sites and the blogs on which\nmost of that misinformation is originally posted. Unfortunately,\nhowever, an adequate analysis of the role of social media sites and\ntheir specific responsibility in the FN controversy is beyond the scope\nof this entry; for more information about ethical issues affecting\nsocial media sites per se, see Vallor (2016).\n4.3 Some Questions Affecting Trust\n\n\nIn the preceding section, we saw how the rapid spread of Fake News\nonline raises one kind of trust-related concern for SECs: the accuracy\nof the information on some sites to which search engine users are\ndirected. We next briefly examine the concept of trust in more general\nterms, before considering whether and how it might be possible for\nusers to enter into trust relationships with SECs. As in the case of\nother key ethical issues affecting SECs, it is not possible to discuss\nhere the many aspects of trust in the detail that they would otherwise\nwarrant. For an excellent overview of trust (in general), see McLeod\n(2015). We limit our analysis to some current trust-related issues\ninvolving SECs, and suggest that the way SECs respond to these issues\ncould affect the sense of trust that users will have in the future for\nmajor SECs such as Google and Bing.\n\n\nWhat is trust, and why is it so important in the context of SECs?\nFollowing Baier (1986), many philosophers view trust as an\n\u201cattitude,\u201d while others see trust either in\n\u201crelational terms\u201d (Robbins 2016) or as a kind of\n\u201cexpectation\u201d (Walker 2006). And as Govier (1997, 35)\nnotes, one reason why trust is important is because \u201cattitudes of\ntrust and distrust affect the nature and quality of our social\nreality.\u201d When discussing the topic of trust, it is important to\nmake some key distinctions at the outset. For example, we need to\ndifferentiate trust from trustworthiness, which is often viewed either\nas a property of the trustee (rather than as an attitude of\nthe trustor) or a relation between trustor and trustee.\nAdditionally, it is important to distinguish between ethical\ntrust and epistemic trust (both of which are described below),\nin the context of SECs.\n\n\nAre contemporary SECs trustworthy organizations? If so, why is it\nthat we claim to trust them? Is it because (a) we perceive them as a\nreliable resource for directing us to the most relevant and\naccurate online information, in response to our search queries (i.e.,\nepistemic trust)? Or is it because (b) SEC\u2019s policies regarding\nwhat they do with our personal information, once collected, are\nperceived as transparent and fair (ethical trust)? Or is it\nbecause of both (a) and (b)? We can also ask whether there might be\nsome alternative/additional reasons why one could claim to trust or\ndistrust an SEC.\n\n\nOf course, one could reasonably question whether it is possible for\nhumans to enter into a trust relationship with an SEC, as a corporate\nentity, or even with a non-human entity of any kind. For example, some might assume\nthat a genuine trust relationship can only exist between humans\n(or perhaps more specifically, between \u201chuman agents\u201d). Others,\nhowever, believe that trust relationships can be extended to include\nsome non-human, or \u201cartificial,\u201d agents\u2014i.e., not\nonly corporations, which are sometimes viewed as aggregates of human\nagents, but also artificial electronic agents such as\n\u201cbots.\u201d Because issues affecting trust and responsibility\nfor SECs are distributed over a vast and diffuse network, many diverse\nkinds of agents can be involved\u2014e.g., from corporate CEOs and\nexecutives/boards, to customer-service representatives, to engineers\nand software developers, to the many (non-human) artificial (AI) agents\nor bots that can also \u201cmake some limited decisions\u201d on\nbehalf of an SEC. So what, exactly, is it that we trust/distrust when\nwe say we trust/distrust an SEC? In claiming to trust Google, for\nexample, we may, in effect, be saying that we trust all of the\ncomponent elements comprising that SEC.\n\n\nOne model that can help us to think about trust in contexts as vast\nand complex as major SECs is Margaret Urban Walker\u2019s framework of\n\u201cdiffuse default trust\u201d (Walker 2006). In Walker\u2019s\nscheme, trust relationships occur in environments (i.e., \u201cspaces\nand circumstances\u201d) that she calls \u201czones of default\ntrust.\u201d Walker differentiates trust from mere reliance by noting\nthat in a trust relationship between A and B, A not only expects B to\ndo X, but A expects it of B (Walker, 79). And because trust\n\u201clinks reliance to responsibility,\u201d Walker (p. 80) believes\nthat A has a \u201cnormative expectation\u201d of B. But she also\nnotes that we are not always conscious of these normative expectations,\nbecause they are typically \u201cunreflective and often\nnonspecific\u201d expectations where \u201cstrangers or unknown\nothers may be relied upon to behave in acceptable ways.\u201d Our\nreliance on the \u201cgood and tolerable behavior of others\u201d is\nat the core of Walker\u2019s notion of \u201cdefault trust\u201d (p.\n85).\n\n\nAn important feature of Walker\u2019s framework is that it allows\nfor trust relations between individuals who have not met, and who will\npossibly never meet, in person. And in her scheme, trust relationships\ncan occur not only between humans who are unknown to one another, but\nalso between humans and non-human entities such as corporations. Walker\nillustrates the latter point in an example where someone encounters\nparticularly bad service on an airplane operated by a major commercial\nairline. She points out that when we have such an experience of bad\nservice, it is appropriate to feel resentment, not necessarily toward\nspecific individuals who work for the airline, but toward the airline\nitself. So the entities comprising a zone of trust need not be limited\nto known (human) individuals.\n\n\nWalker (p. 85) extends her notion of default trust to include zones\nof \u201cdiffuse default trust,\u201d as also illustrated in her\nexample involving the major airline corporation. Consider the many ways\nthat responsibility can become \u201cdiffuse\u201d when distributed\nover a vast zone of trust such as a commercial airline (Walker), or a\nmajor SEC like Google which also qualifies as a zone of diffuse-default\ntrust comprising a diverse range of agents (both human and non-human).\nAmong the non-human entities in that zone are artificial electronic\nagents (bots) who \u201coperate behind the scenes,\u201d both with\nhumans and other artificial agents. The latter kinds of entities also\ninclude multi-agent systems that can \u201cact\u201d in limited ways\non behalf of an SEC. For a detailed description of how Walker\u2019s\nmodel can be applied in digital contexts, including zones of diffuse\ndefault trust comprising both humans and multi-agent systems, see the\nanalysis in Buechner and Tavani (2011).\n\n\nAs noted above, it is important to distinguish between epistemic\ntrust and ethical trust, both of which are essential for trusting SECs.\nWhereas concerns regarding the former sense of trust overlap with\nissues affecting an SEC\u2019s reliability, as well as with the\naccuracy of information contained on the Web sites it indexes and links\nto, ethical trust in the case of SECs can overlap with\nquestions/concerns pertaining to a search organization\u2019s user\npolicies and whether those policies are open, fair, and trustworthy.\nRecognizing and complying with the requirements of both aspects of\ntrust is crucial for an SEC like Google to realize a key objective\nstated in its code of conduct\u2014viz., to \u201cincrease\u201d and\n\u201ccontinually earn\u2026trust\u201d with its users (The Google\nCode of Conduct, 2018).\n\n\nWe conclude this section by acknowledging that much more could be\nsaid about the important role that trust plays in the context of SECs.\nBut we have at least identified some of the reasons why trust is very\nimportant for SECs and also showed how those companies can qualify as\n\u201czones of diffuse default trust\u201d (Walker). Although we have\nexamined only one model of trust \u2013 Walker\u2019s framework\n\u2013 in analyzing some of the many dimensions of trust affecting\nSECs, we have not argued that Walker\u2019s is the only, or even the\nbest, theory for understanding trust issues affecting SECs. Nor have we\nargued that Walker\u2019s theory is without internal problems.\nInstead, we put aside those kinds of questions in favor of using a\nspecific model to analyze trust-related questions affecting SECs in a\nsystematic way.\n\n\nWe began Section 4 by asking whether SECs have any special moral\nobligations because of their \u201cprivileged place\u201d in society\n(i.e., in light of their role as \u201cgatekeepers\u201d of knowledge\non the Web), and in Section 4.1 we examined Hinman\u2019s concern about \nthe kind of control of knowledge that SECs currently have. In Section 4.2, we\nidentified a relatively recent trust-related challenge that SECs now\nface because of the proliferation of Fake News on the Web. And based on\nour further analysis of trust in Section 4.3, it would seem that an\nSEC\u2019s ability to establish and maintain trust with its users\nwill be crucial for that organization to thrive in the future.\n5. Conclusion\n\n\nIn this entry, we have seen how various ethical issues arose during\nkey developmental stages of search technology that eventually led to\ncontemporary \u201cWeb 2.0-era search engines.\u201d Search\ntechnology itself has evolved over the past 75 years\u2014i.e., from\npre-computer-based techniques (such as memex) intended to help\nscientists and professional researchers locate and retrieve important\ninformation in a timely manner, to an Internet-based technology that\nassisted ordinary users in locating (and linking directly to) relevant\nWeb sites in response to their manifold search queries, to a highly\nsophisticated technology that, in its current form, has become\nincreasingly commodified and personalized to the point that it might\nnow be regarded as a threat to some of our basic freedoms and\ndemocratic ideals.\n\n\nThe ethical issues examined in this entry are\nmainly from a deontological perspective, thus reflecting the published\nwork on this topic to date. However, in no way is our analysis of\nethical issues affecting search engines intended to be exhaustive;\nrather it merely reflects the standard or \u201cmainstream\u201d\napproach that applied ethicists and other scholars have taken thus far\nin their analyses of search engine controversies. One could easily\nimagine questions arising from other ethical perspectives as\nwell. From the vantage point of social justice, for example, one could\nreasonably ask whether search engine companies in the U.S. and other\ndeveloped nations are morally obligated to help bridge the\n\u201cinformation divide\u201d by providing easier and more\nubiquitous access (via search technologies) to users in developing\nnations, especially to non-English speaking users. Also, from a\nutilitarian or consequentialist perspective, one might ask whether the\noverall social consequences would be more beneficial if search engine\nusers were legally permitted to retrieve some forms of proprietary\ninformation (e.g. proprietary online information concerning health and\npublic-health studies that were made possible by tax-payer funding)\nfor personal use.\n\nAdditionally, some alternative ethical frameworks could also be used\nin analyzing ethical issues affecting search engines. These include,\nbut are not limited to, Hans Jonas\u2019 \u201cImperative of\nResponsibility\u201d (Jonas 1984), Bernard Gert\u2019s \u201cCommon\nMorality\u201d (Gert 2007), and John Rawls\u2019 \u201cjustice as\nfairness\u201d (Rawls 1999). For a very brief description of how\nethical aspects of search engines might also be approached from the\nperspectives of \u201crights,\u201d \u201cfairness,\u201d\n\u201ccommon good\u201d and \u201cvirtue,\u201d see the Markkula\nCenter for Applied Ethics (in the Other Internet Resources).\n\n\nEven though the primary focus has been on identifying and analyzing\nethical issues affecting search engines, in closing it is\nworth reiterating a point made in the introductory section\u2014viz.,\nthat search engine technology also has implications for some\nother kinds of philosophical issues. This is especially apparent in the\narea of epistemology, where some critics raise concerns related to our\nreceived notions of the nature and justification of knowledge claims,\nin an era of widespread use and dependence on search engines. For\nexample, Hinman (2008, 75) argues that search engines\n\u201ccontribute significantly to the social construction of\nknowledge\u201d\u2014i.e., not only do they provide access to\nknowledge, but they also increasingly \u201cplay a crucial role in\nthe constitution of knowledge itself.\u201d An analysis of this claim, however, as well as\nother epistemological and (broader) philosophical aspects of search engines, is beyond\nthe scope of the present entry.\n",
    "bibliography": {
        "categories": [],
        "cat_ref_text": {
            "ref_list": [
                "Abbate, J., 1999. <em>Inventing the Internet</em>, Cambridge, MA:\nMIT Press.",
                "Baier, A. C., 1986. \u201cTrust and Antitrust,\u201d \n<em>Ethics</em>, 96: 231\u2013260.",
                "Barysevich, A., 2016. \u201cHow Voice Search Will Forever Change\nSEO,\u201d <em>Search Engine Journal</em>, June 14. \n <a href=\"https://www.searchenginejournal.com/voice-search-will-forever-change-seo/164189/\" target=\"other\">available online</a>.",
                "Berners-Lee, T., 2010. \u201cLong Live the Web: A Call for\nContinued Open Standards and Neutrality,\u201d <em>Scientific\nAmerican</em>, November. \n<a href=\"http://www.scientificamerican.com/article.cfm?id=long-live-the-web\" target=\"other\">available online</a>.",
                "Blanke, T., 2005. \u201cEthical Subjectification and Search\nEngines: Ethics Reconsidered,\u201d <em>International Review of\nInformation Ethics</em>,  3: 33\u201338.",
                "Bottis, M., 2014. \u201cAllow Me to Live the Good Life, Let Me\nForget: Legal and Psychological Foundations of the Right to Be\nForgotten and the New Developments in the European Union Laws,\u201d\nin <em>Well-Being, Flourishing, and ICTs</em>: <em>Proceedings of the\nEleventh International Conference on Computer\nEthics\u2013Philosophical Enquiry</em>, Menomonie, WI: INSEIT, Article\n10.",
                "Brey, P., 1998. \u201cThe Politics of Computer Systems and the\nEthics of Design,\u201d in <em>Computer Ethics: Philosophical\nEnquiry</em>, M. J. van den Hoven (ed.),  Rotterdam:\nErasmus University Press, pp. 64\u201375.",
                "\u2013\u2013\u2013, 2004. \u201cDisclosive Computer\nEthics,\u201d in\n<em>Readings in CyberEthics</em>, 2nd edition,  R. A. Spinello and H. T. \nTavani (eds.),  Sudbury, MA: Jones and Bartlett, pp. 55\u201366.",
                "Brin, S. and Page, L., 1998. \u201cThe Anatomy of a Large-Scale\nHypertextual Web Search Engine,\u201d in <em>Seventh International\nWorld-Wide Web Conference (WWW 7)</em>, Amsterdam: Elsevier.",
                "Buechner, J. and H. T. Tavani, 2011. \u201cTrust and Multi-Agent\nSystems: Applying the \u2018Diffuse, Default Model\u2019 of Trust to\nExperiments Involving Artificial Agents,\u201d <em>Ethics and\nInformation Technology</em>, 13(1): 39\u201351.",
                " Burgess, M., 2018. \u201cWhat is the Internet of Things?\u201d\n<em>Wired</em>, February 16. <a href=\"https://www.wired.co.uk/article/internet-of-things-what-is-explained-iot\" target=\"other\"> available online</a>",
                "Bush, V., 1945. \u201cAs We May Think,\u201d <em>Atlantic\nMonthly</em>, July. \n<a href=\"http://www.theatlantic.com/magazine/archive/1945/07/as-we-may-think/3881/\" target=\"other\">available online</a>.",
                "Carr, N., 2011. <em>The Shallows: What the Internet is Doing to Our\nBrain</em>, New York: Norton.",
                "Chappell, B., 2016. \u201cEU Charges Google with Antitrust\nViolations, Will Also Look at Android,\u201d <em>National \nPublic Radio</em>, April 15,\n <a href=\"http://www.npr.org/sections/thetwo-way/2015/04/15/399788719/eu-charges-google-with-antitrust-violations-will-also-look-at-android\" target=\"other\">available online</a>.",
                "Charette, R. N., 2012. \u201cGone Missing: The Public Policy Debate\non Unleashing the Dogs of Cyberwar,\u201d <em>IEEE Spectrum</em>, June\n4. \n <a href=\"http://spectrum.ieee.org/riskfactor/telecom/security/gone-missing-the-public-policy-debate-on-unleashing-the-dogs-of-cyberwar/?utm_source=techalert&amp;utm_medium=email&amp;utm_campaign=060712\" target=\"other\">available online</a>.",
                "Chorost, M., 2011. <em>World Wide Mind: The Coming Integration of\nHumanity, Machines, and the Internet</em>, New York: Free Press.",
                "Diaz, A., 2008. \u201cThrough the Google Goggles: Sociopolitical\nBias in Search Engine Design,\u201d in <em>Web Search:\nMultidisciplinary Perspectives</em>,  A. Spink and M. Zimmer (eds.), \nBerlin: Springer-Verlag, pp. 11\u201334.",
                "Elgesem, D., 2008. \u201cSearch Engines and the Public Use of\nReason,\u201d <em>Ethics and Information Technology</em>, 10(4):\n233\u2013242.",
                "European Commission, 2014,  \u201cFact Sheet on \u2018The Right to Be\nForgotten\u2019 Ruling C-131/12.\u201d \n <a href=\"http://ec.europa.eu/justice/data-protection/files/factsheets/factsheet_data_protection_en.pdf\" target=\"other\">available online</a>.",
                "Floridi, L., 2014. \u201cThe Right to Be Forgotten\u2014The Road\nAhead,\u201d <em>The Guardian</em>, October 8. \n <a href=\"http://www.theguardian.com/technology/2014/oct/08/the-right-to-be-forgotten-the-road-ahead\" target=\"other\">available online</a>.",
                "Friedman, B., P. Kahn,  and A. Borning, 2008. \u201cValue\nSensitive Design and Information Systems,\u201d in <em>The Handbook of\nInformation and Computer Ethics</em>,  K. E. Himma and H. T. Tavani (eds.),\nHoboken, NJ: John Wiley and Sons, pp. 69\u2013101.",
                "Friedman, B. and H. Nissenbaum, 1996 \u201cBias in Computer\nSystems,\u201d <em>ACM Transactions on Computer Systems</em>,\n14(3): 330\u2013347.",
                "Gert, B., 2007. <em>Common Morality: Deciding What to Do</em>, New\nYork: Oxford University Press.",
                "Global Partners Digital, 2018. <em>Travel Guide to the Digital World: Data Protection for Human Rights Defenders</em>, London: GPD. <a href=\"https://www.gp-digital.org/publication/travel-guide-to-the-digital-world/\" target=\"other\">available online</a>.",
                "Goldman, D., 2013. \u201cShodan: The Scariest Search Engine on the\nInternet,\u201d <em>CNN Money</em> (The Cybercrime Economy), April\n8. <a href=\"http://money.cnn.com/2013/04/08/technology/security/shodan/index.html\" target=\"other\">available online</a>.",
                "Goldman, E., 2008. \u201cSearch Engine Bias and the Demise of\nSearch Engine Utopianism,\u201d in <em>Web Search: Multidisciplinary\nPerspectives</em>, A. Spink and M. Zimmer (eds.), Berlin:\nSpringer-Verlag, pp. 121\u2013134.",
                "Goodwin, D., 2018. \u201cWhat is SEO? Here\u2019s Search Engine Optimization Defined by 60 Experts,\u201d <em>Search Engine Journal</em>, January 2, <a href=\"https://www.searchenginejournal.com/seo-101/what-is-seo/#close\" target=\"other\">available online</a>.",
                " Google Spain SL, 2013. Google Inc. v. Agencia Espa\u00f1ola de\nProtecci\u00f3n de Datos, Mario Costeja Gonz\u00e1lez. Case C 131/12. Opinion of Advocate General, June 25. <a href=\"https://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:62012CC0131:EN:HTML\" target=\"other\">available online</a>.",
                "\u2013\u2013\u2013, 2014. Google Inc. v. Agencia\nEspa\u00f1ola de Protecci\u00f3n de Datos, Mario Costeja Gonz\u00e1lez. Case C\n131/12. Judgment of the Court (Grand Chamber), May 13. <a href=\"https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A62012CJ0131\" target=\"other\">available online</a>.",
                "Govier, T., 1997. <em>Social Trust and Human Communities</em>,\nMontreal and Kingston: McGill-Queen\u2019s University Press.",
                "Halavais, A., 2009. <em>Search Engine Society</em>, Malden, MA:\nPolity.",
                "Halpern, S., 2011. \u201cMind Control and the Internet,\u201d\n<em>New York Review of Books</em>, June 23. <a href=\"http://www.nybooks.com/articles/archives/2011/jun/23/mind-control-and-internet/\" target=\"other\">available online</a>\n",
                "Himma, K. E., 2007. \u201cPrivacy vs. Security: Why Privacy is Not\nan Absolute Value or Right,\u201d <em>University of San Diego Law\nReview</em> (Fourth Annual Editors\u2019 Symposium),  45: 857\u2013921.",
                "Hinman, L. M., 2005. \u201cEsse Est Indicato in Google: Ethical\nand Political Issues in Search Engines,\u201d <em>International Review\nof Information Ethics</em>,  3: 19\u201325.",
                "\u2013\u2013\u2013, 2008. \u201cSearching Ethics: The Role of Search\nEngines in the Construction and Distribution of Knowledge,\u201d in\n<em>Web Search: Multidisciplinary Perspectives</em>, A. Spink and M.\nZimmer (eds.), Berlin: Springer-Verlag, pp. 67\u201376.",
                "Introna, L. and H. Nissenbaum, 2000. \u201cShaping the Web: Why\nThe Politics of Search Engines Matters,\u201d <em>The\nInformation Society</em>, 16(3): 169\u2013185.",
                "Jonas, H., 1984. <em>The Imperative of Moral Responsibility: In\nSearch of an Ethics for the Technological Age</em>, Chicago: University\nof Chicago Press.",
                "Kelion, L., 2019. \u201cGoogle Wins Landmark Right to Be Forgotten\nCase,\u201d <em>BBC News</em>, September 24.\n  <a href=\"https://www.bbc.com/news/technology-49808208\" target=\"other\">available online</a>.",
                "Kelly, K.J., 2019. \u201cGawker Stalker Might Get Another Chance\nto Acquire Website,\u201d <em>New York Post</em>, August 6. <a href=\"https://nypost.com/2019/08/06/gawker-stalker-might-get-another-chance-to-acquire-website/\" target=\"other\"> available online</a>.",
                "Kritikos, K. C., 2018. \u201cThe Right to Forget, Obliterate,\nErase: Defending Personal Data Privacy in the Digital Age,\u201d\n<em>Journal of Information Ethics</em>, 27(2): 47\u201365.",
                "Lessig, L., 2000. <em>Code and Other Laws of Cyberspace</em>, New\nYork: Basic Books.",
                "Levy, D. M., 2008. \u201cInformation Overload,\u201d in <em>The\nHandbook of Information and Computer Ethics</em>, K. E. Himma and\nH. T. Tavani (eds.), Hoboken, NJ: John Wiley &amp; Sons,\npp. 497\u2013515.",
                "Lipinski, T., 2018. \u201cThe Law and Economics of Recognizing the\nRight to Be Forgotten in an Era of Fake News,\u201d <em>Journal of\nInformation Ethics</em>, 27(2): 66\u201380.",
                "Markkula Center for Applied Ethics,  2016. \u201cUnavoidable\nEthical Questions about Search Engines,\u201d Santa Clara University.\n <a href=\"https://www.scu.edu/ethics/focus-areas/internet-ethics/resources/unavoidable-ethical-questions-about-search-engines/\" target=\"other\">available online</a>.",
                "McLeod, C., 2015. \u201cTrust,\u201d <em>The Stanford Encyclopedia\nof Philosophy</em> (Fall 2015 Edition), Edward N. Zalta (ed.), URL =\n &lt;<a href=\"https://plato.stanford.edu/archives/fall2015/entries/trust/\">https://plato.stanford.edu/archives/fall2015/entries/trust/</a>&gt;.",
                "Moor, J. H., 1997. \u201cTowards a Theory of Privacy in the\nInformation Age,\u201d <em>Computers and Society</em>,  27(3): 27\u201332.",
                "Morozov, E., 2011. \u201cYour Own Facts,\u201d <em>New York Times\nSunday Book Review</em>, June 10. \n<a href=\"http://www.nytimes.com/2011/06/12/books/review/book-review-the-filter-bubble-by-eli-pariser.html\" target=\"other\">available online</a>\n",
                "Nagenborg, M. (ed.), 2005. <em>The Ethics of Search Engines</em>,\nSpecial Issue of <em>International Review of Information Ethics</em>,\nVol. 3.",
                "Nicas, J., 2011. \u201cGoogle Roils Travel,\u201d <em>Wall Street\nJournal</em>, 12/27. \n<a href=\"http://online.wsj.com/article/SB10001424052970203686204577116700668483194.html\" target=\"other\">available online</a>.",
                "Nissenbaum, H., 1997. \u201cToward an Approach to Privacy in\nPublic: Challenges of Information Technology,\u201d <em>Ethics and\nBehavior</em>,  7(3): 207\u2013219.",
                "\u2013\u2013\u2013, 1998. \u201cProtecting Privacy in an Information\nAge,\u201d <em>Law and Philosophy</em>,  17: 559\u2013596.",
                "\u2013\u2013\u2013, 2004. \u201cPrivacy as Contextual\nIntegrity,\u201d <em>Washington Law Review</em>,  79(1): 119\u2013157.",
                "\u2013\u2013\u2013, 2010. <em>Privacy in Context: Technology, Policy,\nand the Integrity of Social Life</em>, Stanford: Stanford\nUniversity Press.",
                "O\u2019Harrow, R., 2012. \u201cCyber Search Engine Shodan Exposes\nIndustrial Control Systems to New Risks,\u201d <em>The Washington\nPost</em>, June 3, \n <a href=\"http://www.washingtonpost.com/investigations/cyber-search-engine-exposes-vulnerabilities/2012/06/03/gJQAIK9KCV_story.html\" target=\"other\">available online</a>.",
                "Noble, S.U., 2018. <em>Algorithms of Oppression: How Search\nEngines Reinforce Racism</em>, New York: New York University Press.",
                "O\u2019Reilly, T., 2005. \u201cWhat is Web 2.0? Design Patterns\nand Business Models for the Next Generation of Software,\u201d\n<em>O\u2019Reilly Media</em>, \n<a href=\"http://oreilly.com/web2/archive/what-is-web-20.html\" target=\"other\">available online</a>.",
                "Palfreman, J., and D. Swade, 1991. <em>The Dream Machine:\nExploring the Computer Age</em>, London: BBC Books. ",
                "Pariser, E., 2011. <em>The Filter Bubble: What the Internet is\nHiding from You</em>, New York: Penguin.",
                "Rawls, J., 1999. <em>A Theory of Justice</em>, revised edition, New York:\nOxford University Press.",
                "Robbins, B. G., 2016. \u201cWhat is Trust? A Multidisciplinary\nReview, Critique, and Synthesis.\u201d <em>Sociology Compass</em>,\n10(10): 972\u2013986. doi:10.1111/soc4.12391",
                "Scott, M., 2014. \u201cThe Right to Be Forgotten Should Apply\nWorldwide, Panel Says,\u201d <em>New York Times</em>, November 26.\n<a href=\"http://www.nytimes.com/2014/11/27/technology/right-to-be-forgotten-should-be-extended-beyond-europe-eu-panel-says.html?_r=0\" target=\"other\">available online</a>.",
                "\u2013\u2013\u2013, 2016. \u201cEurope Tried to Rein In Google. It\nBackfired,\u201d <em>New York Times</em>, April 18, \n<a href=\"http://www.nytimes.com/2016/04/19/technology/google-europe-privacy-watchdog.html?_r=2\" target=\"other\">available online</a>.",
                "Spinello, R. A., 2011. <em>CyberEthics: Morality and Law in\nCyberspace</em>, 4th edition, Sudbury, MA: Jones and Bartlett.",
                "\u2013\u2013\u2013, 2012. \u201cGoogle in China: Corporate\nResponsibility on a Censored Internet,\u201d in <em>Investigating\nCyber Law and Cyber Ethics: Issues, Impacts, Practices</em>, \nA. Dudley, J. Braman,  and G. Vincenti (eds.), Hershey, PA: IGI Global,\npp. 239\u2013253.",
                "Sunstein, C., 2001. <em>Republic.com</em>, Princeton, NJ: Princeton\nUniversity Press.",
                "Taddeo, M. and L. Floridi, 2016. \u201cThe Debate on the Moral\nResponsibilities of Online Service Providers,\u201d <em>Science\nand Engineering Ethics</em>, 22: 1575\u20131603.\ndoi:10.1007/s11948-015-9734-1",
                "Tavani, H. T., 1998. \u201cInternet Search Engines and Personal\nPrivacy,\u201d in <em>Computer Ethics: Philosophical Enquiry</em>, \nM. J. van den Hoven (ed.),  Rotterdam: Erasmus University\nPress, pp. 214\u2013223.",
                "\u2013\u2013\u2013, 2005. \u201cSearch Engines, Personal Information,\nand the Problem of Protecting Privacy in Public,\u201d\n<em>International Review of Information Ethics</em>, 3: 39\u201345.",
                "\u2013\u2013\u2013, 2007. \u201cPhilosophical Theories of Privacy:\nImplications for an Adequate Online Privacy Policy,\u201d\n<em>Metaphilosophy</em>,  38(1): 1\u201322.",
                "\u2013\u2013\u2013, 2016. <em>Ethics and Technology: Controversies,\nQuestions, and Strategies for Ethical Computing</em>, 5th edition, Hoboken,\nNJ: John Wiley and Sons.",
                "\u2013\u2013\u2013, 2018. \u201cShould We Have a Right to Be Forgotten?\nA Critique of Key Arguments Underlying This Question,\u201d\n<em>Journal of Information Ethics</em>, 27(2): 26\u201346.",
                "Tavani, H. T. and F. S. Grodzinsky, 2002. \u201cCyberstalking,\nPersonal Privacy, and Moral Responsibility,\u201d <em>Ethics and\nInformation Technology</em>,  4(2): 123\u2013132.",
                "Tavani, H. T. and J. H. Moor, 2001. \u201cPrivacy Protection,\nControl of Information, and Privacy-Enhancing Technologies,\u201d\n<em>Computers and Society</em>, 31(1): 6\u201311.",
                "Vallor, S., 2016. \u201cSocial Networking and Ethics,\u201d <em>The\nStanford Encyclopedia of Philosophy</em> (Winter 2016 Edition),\nEdward N. Zalta (ed.), URL = \n  &lt;<a href=\"https://plato.stanford.edu/archives/win2016/entries/ethics-social-networking/\">https://plato.stanford.edu/archives/win2016/entries/ethics-social-networking/</a>&gt;.",
                "Van Couvering, E., 2008. \u201cThe History of Internet Search\nEngines: Navigational Media,\u201d in <em>Web Search:\nMultidisciplinary Perspectives</em>,  A. Spink  and M.  Zimmer (eds.), \nBerlin: Springer-Verlag, pp. 177\u2013206.",
                "Walker, M. U., 2006. <em>Moral Repair: Reconstructing Moral\nRelations after Wrongdoing</em>, New York: Cambridge University\nPress.",
                "Wall, A., 2011. \u201cHistory of Search Engines: From 1945 to\nGoogle Today,\u201d <em>Atlantic Online</em>, \n<a href=\"http://www.searchenginehistory.com\" target=\"other\">available online</a>.",
                "Zimmer, M., 2008. \u201cThe Gaze of the Perfect Search Engine:\nGoogle as an Institution of Dataveillance,\u201d in <em>Web Search:\nMultidisciplinary Perspectives</em>,  A. Spink and M. Zimmer (eds.),\nBerlin: Springer-Verlag, pp. 77\u201399."
            ]
        },
        "raw_text": "<div id=\"bibliography\">\n<h2 id=\"Bib\">Bibliography</h2>\n<ul class=\"hanging\">\n<li>Abbate, J., 1999. <em>Inventing the Internet</em>, Cambridge, MA:\nMIT Press.</li>\n<li>Baier, A. C., 1986. \u201cTrust and Antitrust,\u201d \n<em>Ethics</em>, 96: 231\u2013260.</li>\n<li>Barysevich, A., 2016. \u201cHow Voice Search Will Forever Change\nSEO,\u201d <em>Search Engine Journal</em>, June 14. \n <a href=\"https://www.searchenginejournal.com/voice-search-will-forever-change-seo/164189/\" target=\"other\">available online</a>.</li>\n<li>Berners-Lee, T., 2010. \u201cLong Live the Web: A Call for\nContinued Open Standards and Neutrality,\u201d <em>Scientific\nAmerican</em>, November. \n<a href=\"http://www.scientificamerican.com/article.cfm?id=long-live-the-web\" target=\"other\">available online</a>.</li>\n<li>Blanke, T., 2005. \u201cEthical Subjectification and Search\nEngines: Ethics Reconsidered,\u201d <em>International Review of\nInformation Ethics</em>,  3: 33\u201338.</li>\n<li>Bottis, M., 2014. \u201cAllow Me to Live the Good Life, Let Me\nForget: Legal and Psychological Foundations of the Right to Be\nForgotten and the New Developments in the European Union Laws,\u201d\nin <em>Well-Being, Flourishing, and ICTs</em>: <em>Proceedings of the\nEleventh International Conference on Computer\nEthics\u2013Philosophical Enquiry</em>, Menomonie, WI: INSEIT, Article\n10.</li>\n<li>Brey, P., 1998. \u201cThe Politics of Computer Systems and the\nEthics of Design,\u201d in <em>Computer Ethics: Philosophical\nEnquiry</em>, M. J. van den Hoven (ed.),  Rotterdam:\nErasmus University Press, pp. 64\u201375.</li>\n<li>\u2013\u2013\u2013, 2004. \u201cDisclosive Computer\nEthics,\u201d in\n<em>Readings in CyberEthics</em>, 2nd edition,  R. A. Spinello and H. T. \nTavani (eds.),  Sudbury, MA: Jones and Bartlett, pp. 55\u201366.</li>\n<li>Brin, S. and Page, L., 1998. \u201cThe Anatomy of a Large-Scale\nHypertextual Web Search Engine,\u201d in <em>Seventh International\nWorld-Wide Web Conference (WWW 7)</em>, Amsterdam: Elsevier.</li>\n<li>Buechner, J. and H. T. Tavani, 2011. \u201cTrust and Multi-Agent\nSystems: Applying the \u2018Diffuse, Default Model\u2019 of Trust to\nExperiments Involving Artificial Agents,\u201d <em>Ethics and\nInformation Technology</em>, 13(1): 39\u201351.</li>\n<li> Burgess, M., 2018. \u201cWhat is the Internet of Things?\u201d\n<em>Wired</em>, February 16. <a href=\"https://www.wired.co.uk/article/internet-of-things-what-is-explained-iot\" target=\"other\"> available online</a></li>\n<li>Bush, V., 1945. \u201cAs We May Think,\u201d <em>Atlantic\nMonthly</em>, July. \n<a href=\"http://www.theatlantic.com/magazine/archive/1945/07/as-we-may-think/3881/\" target=\"other\">available online</a>.</li>\n<li>Carr, N., 2011. <em>The Shallows: What the Internet is Doing to Our\nBrain</em>, New York: Norton.</li>\n<li>Chappell, B., 2016. \u201cEU Charges Google with Antitrust\nViolations, Will Also Look at Android,\u201d <em>National \nPublic Radio</em>, April 15,\n <a href=\"http://www.npr.org/sections/thetwo-way/2015/04/15/399788719/eu-charges-google-with-antitrust-violations-will-also-look-at-android\" target=\"other\">available online</a>.</li>\n<li>Charette, R. N., 2012. \u201cGone Missing: The Public Policy Debate\non Unleashing the Dogs of Cyberwar,\u201d <em>IEEE Spectrum</em>, June\n4. \n <a href=\"http://spectrum.ieee.org/riskfactor/telecom/security/gone-missing-the-public-policy-debate-on-unleashing-the-dogs-of-cyberwar/?utm_source=techalert&amp;utm_medium=email&amp;utm_campaign=060712\" target=\"other\">available online</a>.</li>\n<li>Chorost, M., 2011. <em>World Wide Mind: The Coming Integration of\nHumanity, Machines, and the Internet</em>, New York: Free Press.</li>\n<li>Diaz, A., 2008. \u201cThrough the Google Goggles: Sociopolitical\nBias in Search Engine Design,\u201d in <em>Web Search:\nMultidisciplinary Perspectives</em>,  A. Spink and M. Zimmer (eds.), \nBerlin: Springer-Verlag, pp. 11\u201334.</li>\n<li>Elgesem, D., 2008. \u201cSearch Engines and the Public Use of\nReason,\u201d <em>Ethics and Information Technology</em>, 10(4):\n233\u2013242.</li>\n<li>European Commission, 2014,  \u201cFact Sheet on \u2018The Right to Be\nForgotten\u2019 Ruling C-131/12.\u201d \n <a href=\"http://ec.europa.eu/justice/data-protection/files/factsheets/factsheet_data_protection_en.pdf\" target=\"other\">available online</a>.</li>\n<li>Floridi, L., 2014. \u201cThe Right to Be Forgotten\u2014The Road\nAhead,\u201d <em>The Guardian</em>, October 8. \n <a href=\"http://www.theguardian.com/technology/2014/oct/08/the-right-to-be-forgotten-the-road-ahead\" target=\"other\">available online</a>.</li>\n<li>Friedman, B., P. Kahn,  and A. Borning, 2008. \u201cValue\nSensitive Design and Information Systems,\u201d in <em>The Handbook of\nInformation and Computer Ethics</em>,  K. E. Himma and H. T. Tavani (eds.),\nHoboken, NJ: John Wiley and Sons, pp. 69\u2013101.</li>\n<li>Friedman, B. and H. Nissenbaum, 1996 \u201cBias in Computer\nSystems,\u201d <em>ACM Transactions on Computer Systems</em>,\n14(3): 330\u2013347.</li>\n<li>Gert, B., 2007. <em>Common Morality: Deciding What to Do</em>, New\nYork: Oxford University Press.</li>\n<li>Global Partners Digital, 2018. <em>Travel Guide to the Digital World: Data Protection for Human Rights Defenders</em>, London: GPD. <a href=\"https://www.gp-digital.org/publication/travel-guide-to-the-digital-world/\" target=\"other\">available online</a>.</li>\n<li>Goldman, D., 2013. \u201cShodan: The Scariest Search Engine on the\nInternet,\u201d <em>CNN Money</em> (The Cybercrime Economy), April\n8. <a href=\"http://money.cnn.com/2013/04/08/technology/security/shodan/index.html\" target=\"other\">available online</a>.</li>\n<li>Goldman, E., 2008. \u201cSearch Engine Bias and the Demise of\nSearch Engine Utopianism,\u201d in <em>Web Search: Multidisciplinary\nPerspectives</em>, A. Spink and M. Zimmer (eds.), Berlin:\nSpringer-Verlag, pp. 121\u2013134.</li>\n<li>Goodwin, D., 2018. \u201cWhat is SEO? Here\u2019s Search Engine Optimization Defined by 60 Experts,\u201d <em>Search Engine Journal</em>, January 2, <a href=\"https://www.searchenginejournal.com/seo-101/what-is-seo/#close\" target=\"other\">available online</a>.</li>\n<li> Google Spain SL, 2013. Google Inc. v. Agencia Espa\u00f1ola de\nProtecci\u00f3n de Datos, Mario Costeja Gonz\u00e1lez. Case C 131/12. Opinion of Advocate General, June 25. <a href=\"https://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:62012CC0131:EN:HTML\" target=\"other\">available online</a>.</li>\n<li>\u2013\u2013\u2013, 2014. Google Inc. v. Agencia\nEspa\u00f1ola de Protecci\u00f3n de Datos, Mario Costeja Gonz\u00e1lez. Case C\n131/12. Judgment of the Court (Grand Chamber), May 13. <a href=\"https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A62012CJ0131\" target=\"other\">available online</a>.</li>\n<li>Govier, T., 1997. <em>Social Trust and Human Communities</em>,\nMontreal and Kingston: McGill-Queen\u2019s University Press.</li>\n<li>Halavais, A., 2009. <em>Search Engine Society</em>, Malden, MA:\nPolity.</li>\n<li>Halpern, S., 2011. \u201cMind Control and the Internet,\u201d\n<em>New York Review of Books</em>, June 23. <a href=\"http://www.nybooks.com/articles/archives/2011/jun/23/mind-control-and-internet/\" target=\"other\">available online</a>\n</li>\n<li>Himma, K. E., 2007. \u201cPrivacy vs. Security: Why Privacy is Not\nan Absolute Value or Right,\u201d <em>University of San Diego Law\nReview</em> (Fourth Annual Editors\u2019 Symposium),  45: 857\u2013921.</li>\n<li>Hinman, L. M., 2005. \u201cEsse Est Indicato in Google: Ethical\nand Political Issues in Search Engines,\u201d <em>International Review\nof Information Ethics</em>,  3: 19\u201325.</li>\n<li>\u2013\u2013\u2013, 2008. \u201cSearching Ethics: The Role of Search\nEngines in the Construction and Distribution of Knowledge,\u201d in\n<em>Web Search: Multidisciplinary Perspectives</em>, A. Spink and M.\nZimmer (eds.), Berlin: Springer-Verlag, pp. 67\u201376.</li>\n<li>Introna, L. and H. Nissenbaum, 2000. \u201cShaping the Web: Why\nThe Politics of Search Engines Matters,\u201d <em>The\nInformation Society</em>, 16(3): 169\u2013185.</li>\n<li>Jonas, H., 1984. <em>The Imperative of Moral Responsibility: In\nSearch of an Ethics for the Technological Age</em>, Chicago: University\nof Chicago Press.</li>\n<li>Kelion, L., 2019. \u201cGoogle Wins Landmark Right to Be Forgotten\nCase,\u201d <em>BBC News</em>, September 24.\n  <a href=\"https://www.bbc.com/news/technology-49808208\" target=\"other\">available online</a>.</li>\n<li>Kelly, K.J., 2019. \u201cGawker Stalker Might Get Another Chance\nto Acquire Website,\u201d <em>New York Post</em>, August 6. <a href=\"https://nypost.com/2019/08/06/gawker-stalker-might-get-another-chance-to-acquire-website/\" target=\"other\"> available online</a>.</li>\n<li>Kritikos, K. C., 2018. \u201cThe Right to Forget, Obliterate,\nErase: Defending Personal Data Privacy in the Digital Age,\u201d\n<em>Journal of Information Ethics</em>, 27(2): 47\u201365.</li>\n<li>Lessig, L., 2000. <em>Code and Other Laws of Cyberspace</em>, New\nYork: Basic Books.</li>\n<li>Levy, D. M., 2008. \u201cInformation Overload,\u201d in <em>The\nHandbook of Information and Computer Ethics</em>, K. E. Himma and\nH. T. Tavani (eds.), Hoboken, NJ: John Wiley &amp; Sons,\npp. 497\u2013515.</li>\n<li>Lipinski, T., 2018. \u201cThe Law and Economics of Recognizing the\nRight to Be Forgotten in an Era of Fake News,\u201d <em>Journal of\nInformation Ethics</em>, 27(2): 66\u201380.</li>\n<li>Markkula Center for Applied Ethics,  2016. \u201cUnavoidable\nEthical Questions about Search Engines,\u201d Santa Clara University.\n <a href=\"https://www.scu.edu/ethics/focus-areas/internet-ethics/resources/unavoidable-ethical-questions-about-search-engines/\" target=\"other\">available online</a>.</li>\n<li>McLeod, C., 2015. \u201cTrust,\u201d <em>The Stanford Encyclopedia\nof Philosophy</em> (Fall 2015 Edition), Edward N. Zalta (ed.), URL =\n &lt;<a href=\"https://plato.stanford.edu/archives/fall2015/entries/trust/\">https://plato.stanford.edu/archives/fall2015/entries/trust/</a>&gt;.</li>\n<li>Moor, J. H., 1997. \u201cTowards a Theory of Privacy in the\nInformation Age,\u201d <em>Computers and Society</em>,  27(3): 27\u201332.</li>\n<li>Morozov, E., 2011. \u201cYour Own Facts,\u201d <em>New York Times\nSunday Book Review</em>, June 10. \n<a href=\"http://www.nytimes.com/2011/06/12/books/review/book-review-the-filter-bubble-by-eli-pariser.html\" target=\"other\">available online</a>\n</li>\n<li>Nagenborg, M. (ed.), 2005. <em>The Ethics of Search Engines</em>,\nSpecial Issue of <em>International Review of Information Ethics</em>,\nVol. 3.</li>\n<li>Nicas, J., 2011. \u201cGoogle Roils Travel,\u201d <em>Wall Street\nJournal</em>, 12/27. \n<a href=\"http://online.wsj.com/article/SB10001424052970203686204577116700668483194.html\" target=\"other\">available online</a>.</li>\n<li>Nissenbaum, H., 1997. \u201cToward an Approach to Privacy in\nPublic: Challenges of Information Technology,\u201d <em>Ethics and\nBehavior</em>,  7(3): 207\u2013219.</li>\n<li>\u2013\u2013\u2013, 1998. \u201cProtecting Privacy in an Information\nAge,\u201d <em>Law and Philosophy</em>,  17: 559\u2013596.</li>\n<li>\u2013\u2013\u2013, 2004. \u201cPrivacy as Contextual\nIntegrity,\u201d <em>Washington Law Review</em>,  79(1): 119\u2013157.</li>\n<li>\u2013\u2013\u2013, 2010. <em>Privacy in Context: Technology, Policy,\nand the Integrity of Social Life</em>, Stanford: Stanford\nUniversity Press.</li>\n<li>O\u2019Harrow, R., 2012. \u201cCyber Search Engine Shodan Exposes\nIndustrial Control Systems to New Risks,\u201d <em>The Washington\nPost</em>, June 3, \n <a href=\"http://www.washingtonpost.com/investigations/cyber-search-engine-exposes-vulnerabilities/2012/06/03/gJQAIK9KCV_story.html\" target=\"other\">available online</a>.</li>\n<li>Noble, S.U., 2018. <em>Algorithms of Oppression: How Search\nEngines Reinforce Racism</em>, New York: New York University Press.</li>\n<li>O\u2019Reilly, T., 2005. \u201cWhat is Web 2.0? Design Patterns\nand Business Models for the Next Generation of Software,\u201d\n<em>O\u2019Reilly Media</em>, \n<a href=\"http://oreilly.com/web2/archive/what-is-web-20.html\" target=\"other\">available online</a>.</li>\n<li>Palfreman, J., and D. Swade, 1991. <em>The Dream Machine:\nExploring the Computer Age</em>, London: BBC Books. </li>\n<li>Pariser, E., 2011. <em>The Filter Bubble: What the Internet is\nHiding from You</em>, New York: Penguin.</li>\n<li>Rawls, J., 1999. <em>A Theory of Justice</em>, revised edition, New York:\nOxford University Press.</li>\n<li>Robbins, B. G., 2016. \u201cWhat is Trust? A Multidisciplinary\nReview, Critique, and Synthesis.\u201d <em>Sociology Compass</em>,\n10(10): 972\u2013986. doi:10.1111/soc4.12391</li>\n<li>Scott, M., 2014. \u201cThe Right to Be Forgotten Should Apply\nWorldwide, Panel Says,\u201d <em>New York Times</em>, November 26.\n<a href=\"http://www.nytimes.com/2014/11/27/technology/right-to-be-forgotten-should-be-extended-beyond-europe-eu-panel-says.html?_r=0\" target=\"other\">available online</a>.</li>\n<li>\u2013\u2013\u2013, 2016. \u201cEurope Tried to Rein In Google. It\nBackfired,\u201d <em>New York Times</em>, April 18, \n<a href=\"http://www.nytimes.com/2016/04/19/technology/google-europe-privacy-watchdog.html?_r=2\" target=\"other\">available online</a>.</li>\n<li>Spinello, R. A., 2011. <em>CyberEthics: Morality and Law in\nCyberspace</em>, 4th edition, Sudbury, MA: Jones and Bartlett.</li>\n<li>\u2013\u2013\u2013, 2012. \u201cGoogle in China: Corporate\nResponsibility on a Censored Internet,\u201d in <em>Investigating\nCyber Law and Cyber Ethics: Issues, Impacts, Practices</em>, \nA. Dudley, J. Braman,  and G. Vincenti (eds.), Hershey, PA: IGI Global,\npp. 239\u2013253.</li>\n<li>Sunstein, C., 2001. <em>Republic.com</em>, Princeton, NJ: Princeton\nUniversity Press.</li>\n<li>Taddeo, M. and L. Floridi, 2016. \u201cThe Debate on the Moral\nResponsibilities of Online Service Providers,\u201d <em>Science\nand Engineering Ethics</em>, 22: 1575\u20131603.\ndoi:10.1007/s11948-015-9734-1</li>\n<li>Tavani, H. T., 1998. \u201cInternet Search Engines and Personal\nPrivacy,\u201d in <em>Computer Ethics: Philosophical Enquiry</em>, \nM. J. van den Hoven (ed.),  Rotterdam: Erasmus University\nPress, pp. 214\u2013223.</li>\n<li>\u2013\u2013\u2013, 2005. \u201cSearch Engines, Personal Information,\nand the Problem of Protecting Privacy in Public,\u201d\n<em>International Review of Information Ethics</em>, 3: 39\u201345.</li>\n<li>\u2013\u2013\u2013, 2007. \u201cPhilosophical Theories of Privacy:\nImplications for an Adequate Online Privacy Policy,\u201d\n<em>Metaphilosophy</em>,  38(1): 1\u201322.</li>\n<li>\u2013\u2013\u2013, 2016. <em>Ethics and Technology: Controversies,\nQuestions, and Strategies for Ethical Computing</em>, 5th edition, Hoboken,\nNJ: John Wiley and Sons.</li>\n<li>\u2013\u2013\u2013, 2018. \u201cShould We Have a Right to Be Forgotten?\nA Critique of Key Arguments Underlying This Question,\u201d\n<em>Journal of Information Ethics</em>, 27(2): 26\u201346.</li>\n<li>Tavani, H. T. and F. S. Grodzinsky, 2002. \u201cCyberstalking,\nPersonal Privacy, and Moral Responsibility,\u201d <em>Ethics and\nInformation Technology</em>,  4(2): 123\u2013132.</li>\n<li>Tavani, H. T. and J. H. Moor, 2001. \u201cPrivacy Protection,\nControl of Information, and Privacy-Enhancing Technologies,\u201d\n<em>Computers and Society</em>, 31(1): 6\u201311.</li>\n<li>Vallor, S., 2016. \u201cSocial Networking and Ethics,\u201d <em>The\nStanford Encyclopedia of Philosophy</em> (Winter 2016 Edition),\nEdward N. Zalta (ed.), URL = \n  &lt;<a href=\"https://plato.stanford.edu/archives/win2016/entries/ethics-social-networking/\">https://plato.stanford.edu/archives/win2016/entries/ethics-social-networking/</a>&gt;.</li>\n<li>Van Couvering, E., 2008. \u201cThe History of Internet Search\nEngines: Navigational Media,\u201d in <em>Web Search:\nMultidisciplinary Perspectives</em>,  A. Spink  and M.  Zimmer (eds.), \nBerlin: Springer-Verlag, pp. 177\u2013206.</li>\n<li>Walker, M. U., 2006. <em>Moral Repair: Reconstructing Moral\nRelations after Wrongdoing</em>, New York: Cambridge University\nPress.</li>\n<li>Wall, A., 2011. \u201cHistory of Search Engines: From 1945 to\nGoogle Today,\u201d <em>Atlantic Online</em>, \n<a href=\"http://www.searchenginehistory.com\" target=\"other\">available online</a>.</li>\n<li>Zimmer, M., 2008. \u201cThe Gaze of the Perfect Search Engine:\nGoogle as an Institution of Dataveillance,\u201d in <em>Web Search:\nMultidisciplinary Perspectives</em>,  A. Spink and M. Zimmer (eds.),\nBerlin: Springer-Verlag, pp. 77\u201399.</li>\n</ul>\n</div>"
    },
    "related_entries": {
        "entry_list": [
            "computing: and moral responsibility",
            "ethics: internet research",
            "information technology: and moral values",
            "information technology: and privacy",
            "information technology: phenomenological approaches to ethics and",
            "privacy",
            "social networking and ethics",
            "technology, philosophy of"
        ],
        "entry_link": [
            {
                "../computing-responsibility/": "computing: and moral responsibility"
            },
            {
                "../ethics-internet-research/": "ethics: internet research"
            },
            {
                "../it-moral-values/": "information technology: and moral values"
            },
            {
                "../it-privacy/": "information technology: and privacy"
            },
            {
                "../ethics-it-phenomenology/": "information technology: phenomenological approaches to ethics and"
            },
            {
                "../privacy/": "privacy"
            },
            {
                "../ethics-social-networking/": "social networking and ethics"
            },
            {
                "../technology/": "technology, philosophy of"
            }
        ]
    },
    "academic_tools": {
        "listed_text": [
            "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=ethics-search\" target=\"other\">How to cite this entry</a>.",
            "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://leibniz.stanford.edu/friends/preview/ethics-search/\" target=\"other\">Preview the PDF version of this entry</a> at the\n <a href=\"https://leibniz.stanford.edu/friends/\" target=\"other\">Friends of the SEP Society</a>.",
            "<img alt=\"inpho icon\" src=\"../../symbols/inpho.png\"/>",
            "<a href=\"https://www.inphoproject.org/entity?sep=ethics-search&amp;redirect=True\" target=\"other\">Look up topics and thinkers related to this entry</a>\n at the Internet Philosophy Ontology Project (InPhO).",
            "<img alt=\"phil papers icon\" src=\"../../symbols/pp.gif\"/>",
            "<a href=\"https://philpapers.org/sep/ethics-search/\" target=\"other\">Enhanced bibliography for this entry</a>\nat <a href=\"https://philpapers.org/\" target=\"other\">PhilPapers</a>, with links to its database."
        ],
        "listed_links": [
            {
                "https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=ethics-search": "How to cite this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/preview/ethics-search/": "Preview the PDF version of this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/": "Friends of the SEP Society"
            },
            {
                "https://www.inphoproject.org/entity?sep=ethics-search&redirect=True": "Look up topics and thinkers related to this entry"
            },
            {
                "https://philpapers.org/sep/ethics-search/": "Enhanced bibliography for this entry"
            },
            {
                "https://philpapers.org/": "PhilPapers"
            }
        ]
    },
    "other_internet_resources": {
        "listed_text": [
            "Hawkins, J., 2017. \u201c5 Things to Know About Voice Search and\nBing,\u201d <em>SEMrush Blog</em>, August 4. \n <a href=\"https://www.semrush.com/blog/voice-search-bing/#:~:text=Three%20of%20the%20four%20major,powers%20its%20own%20Google%20Assistant\" target=\"other\">available online</a>.",
            "<a href=\"https://dictionary.cambridge.org/us/dictionary/english/fake-news\" target=\"other\">Fake News</a>,\n  entry in the <em>Cambridge English Dictionary</em>.",
            "<a href=\"https://abc.xyz/investor/other/google-code-of-conduct/\" target=\"other\">Google Code of Conduct</a>, Alphabet Investor Relations, last updated July 31, 2018.",
            "<a href=\"https://www.scu.edu/ethics/focus-areas/internet-ethics/resources/the-ethics-and-politics-of-search-engines/\" target=\"other\">The Ethics and Politics of Search Engines</a>,\n panel discussion, co-sponsored by Santa Clara University Markkula\nCenter for Applied Ethics and the Santa Clara University Center for\nScience, Technology, and Society, on February 27, 2006. ",
            "<a href=\"https://www.scu.edu/ethics/focus-areas/internet-ethics/resources/unavoidable-ethical-questions-about-search-engines/\" target=\"other\">Unavoidable Ethical Questions about Search Engines</a>,\n Markkula Center for Applied Ethics",
            "<a href=\"https://www.acm.org/code-of-ethics/\" target=\"other\">Code of Ethics and Professional Conduct</a>, Association for Computing Machinery.",
            "<a href=\"https://www.bruceclay.com/web_ethics/\" target=\"other\">SEO Code of Ethics</a>,\nBruce Clay, Inc."
        ],
        "listed_links": [
            {
                "https://www.semrush.com/blog/voice-search-bing/#:~:text=Three%20of%20the%20four%20major,powers%20its%20own%20Google%20Assistant": "available online"
            },
            {
                "https://dictionary.cambridge.org/us/dictionary/english/fake-news": "Fake News"
            },
            {
                "https://abc.xyz/investor/other/google-code-of-conduct/": "Google Code of Conduct"
            },
            {
                "https://www.scu.edu/ethics/focus-areas/internet-ethics/resources/the-ethics-and-politics-of-search-engines/": "The Ethics and Politics of Search Engines"
            },
            {
                "https://www.scu.edu/ethics/focus-areas/internet-ethics/resources/unavoidable-ethical-questions-about-search-engines/": "Unavoidable Ethical Questions about Search Engines"
            },
            {
                "https://www.acm.org/code-of-ethics/": "Code of Ethics and Professional Conduct"
            },
            {
                "https://www.bruceclay.com/web_ethics/": "SEO Code of Ethics"
            }
        ]
    }
}