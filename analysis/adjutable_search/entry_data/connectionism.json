{
    "url": "connectionism",
    "title": "Connectionism",
    "authorship": {
        "year": "Copyright \u00a9 2019",
        "author_text": "Cameron Buckner\n<cjbuckner@uh.edu>\nJames Garson\n<JGarson@uh.edu>",
        "author_links": [
            {
                "http://cameronbuckner.net/professional/": "Cameron Buckner"
            },
            {
                "mailto:cjbuckner%40uh%2eedu": "cjbuckner@uh.edu"
            },
            {
                "http://www.hfac.uh.edu/phil/garson/Jim_Garson.htm": "James Garson"
            },
            {
                "mailto:JGarson%40uh%2eedu": "JGarson@uh.edu"
            }
        ],
        "raw_html": "<div id=\"article-copyright\">\n<p>\n<a href=\"../../info.html#c\">Copyright \u00a9 2019</a> by\n\n<br/>\n<a href=\"http://cameronbuckner.net/professional/\" target=\"other\">Cameron Buckner</a>\n&lt;<a href=\"mailto:cjbuckner%40uh%2eedu\"><em>cjbuckner<abbr title=\" at \">@</abbr>uh<abbr title=\" dot \">.</abbr>edu</em></a>&gt;<br/>\n<a href=\"http://www.hfac.uh.edu/phil/garson/Jim_Garson.htm\" target=\"other\">James Garson</a>\n&lt;<a href=\"mailto:JGarson%40uh%2eedu\"><em>JGarson<abbr title=\" at \">@</abbr>uh<abbr title=\" dot \">.</abbr>edu</em></a>&gt;\n    </p>\n</div>"
    },
    "pubinfo": [
        "First published Sun May 18, 1997",
        "substantive revision Fri Aug 16, 2019"
    ],
    "preamble": "\n\nConnectionism is a movement in cognitive science that hopes to explain\nintellectual abilities using artificial neural networks (also known as\n\u201cneural networks\u201d or \u201cneural nets\u201d). Neural\nnetworks are simplified models of the brain composed of large numbers\nof units (the analogs of neurons) together with weights that measure\nthe strength of connections between the units. These weights model the\neffects of the synapses that link one neuron to another. Experiments\non models of this kind have demonstrated an ability to learn such\nskills as face recognition, reading, and the detection of simple\ngrammatical structure.\n\nPhilosophers have become interested in connectionism because it\npromises to provide an alternative to the classical theory of the\nmind: the widely held view that the mind is something akin to a\ndigital computer processing a symbolic language. Exactly how and to\nwhat extent the connectionist paradigm constitutes a challenge to\nclassicism has been a matter of hot debate in recent years.\n",
    "toc": [
        {
            "#DesNeuNet": "1. A Description of Neural Networks"
        },
        {
            "#NeuNetLeaBac": "2. Neural Network Learning and Backpropagation"
        },
        {
            "#SamWhaNeuNetDo": "3. Samples of What Neural Networks Can Do"
        },
        {
            "#StrWeaNeuNetMod": "4. Strengths and Weaknesses of Neural Network Models"
        },
        {
            "#ShaConBetConCla": "5. The Shape of the Controversy between Connectionists and Classicists"
        },
        {
            "#ConRep": "6. Connectionist Representation"
        },
        {
            "#SysDeb": "7. The Systematicity Debate"
        },
        {
            "#ConSemSim": "8. Connectionism and Semantic Similarity"
        },
        {
            "#ConEliFolPsy": "9. Connectionism and the Elimination of Folk Psychology"
        },
        {
            "#PC": "10. Predictive Coding Models of Cognition"
        },
        {
            "#DL": "11. Deep Learning: Connectionism\u2019s New Wave"
        },
        {
            "#Bib": "Bibliography"
        },
        {
            "#Aca": "Academic Tools"
        },
        {
            "#Oth": "Other Internet Resources"
        },
        {
            "#Rel": "Related Entries"
        }
    ],
    "main_text": "\n1. A Description of Neural Networks\n\nA neural network consists of large number of units joined together in\na pattern of connections. Units in a net are usually segregated into\nthree classes: input units, which receive information to be processed,\noutput units where the results of the processing are found, and units\nin between called hidden units. If a neural net were to model the\nwhole human nervous system, the input units would be analogous to the\nsensory neurons, the output units to the motor neurons, and the hidden\nunits to all other neurons. \n\nHere is a simple illustration of a simple neural net:\n\n\n\n\nEach input unit has an activation value that represents some feature\nexternal to the net. An input unit sends its activation value to each\nof the hidden units to which it is connected. Each of these hidden\nunits calculates its own activation value depending on the activation\nvalues it receives from the input units. This signal is then passed on\nto output units or to another layer of hidden units. Those hidden\nunits compute their activation values in the same way, and send them\nalong to their neighbors. Eventually the signal at the input units\npropagates all the way through the net to determine the activation\nvalues at all the output units.\n\nThe pattern of activation set up by a net is determined by the\nweights, or strength of connections between the units. Weights may be\neither positive or negative. A negative weight represents the\ninhibition of the receiving unit by the activity of a sending unit.\nThe activation value for each receiving unit is calculated according a\nsimple activation function. Activation functions vary in detail, but\nthey all conform to the same basic plan. The function sums together\nthe contributions of all sending units, where the contribution of a\nunit is defined as the weight of the connection between the sending\nand receiving units times the sending unit\u2019s activation value.\nThis sum is usually modified further, for example, by adjusting the\nactivation sum to a value between 0 and 1 and/or by setting the\nactivation to zero unless a threshold level for the sum is reached.\nConnectionists presume that cognitive functioning can be explained by\ncollections of units that operate in this way. Since it is assumed\nthat all the units calculate pretty much the same simple activation\nfunction, human intellectual accomplishments must depend primarily on\nthe settings of the weights between the units.\n\nThe kind of net illustrated above is called a feed forward net.\nActivation flows directly from inputs to hidden units and then on to\nthe output units. More realistic models of the brain would include\nmany layers of hidden units, and recurrent connections that send\nsignals back from higher to lower levels. Such recurrence is necessary\nin order to explain such cognitive features as short-term memory. In a\nfeed forward net, repeated presentations of the same input produce the\nsame output every time, but even the simplest organisms habituate to\n(or learn to ignore) repeated presentation of the same stimulus.\nConnectionists tend to avoid recurrent connections because little is\nunderstood about the general problem of training recurrent nets.\nHowever Elman (1991) and others have made some progress with simple\nrecurrent nets, where the recurrence is tightly constrained.\n2. Neural Network Learning and Backpropagation\n\nFinding the right set of weights to accomplish a given task is the\ncentral goal in connectionist research. Luckily, learning algorithms\nhave been devised that can calculate the right weights for carrying\nout many tasks (see Hinton 1992 for an accessible review). These fall\ninto two broad categories: supervised and unsupervised learning.\nHebbian learning is the best known unsupervised form. As each input is\npresented to the net, weights between nodes that are active together\nare increased, while those weights connecting nodes that are not\nactive together are decreased. This form of training is especially\nuseful for building nets that can classify the input into useful\ncategories. The most widely used supervised algorithm is called\nbackpropagation. To use this method, one needs a training set\nconsisting of many examples of inputs and their desired outputs for a\ngiven task. This external set of examples \u201csupervises\u201d the\ntraining process. If, for example, the task is to distinguish male\nfrom female faces, the training set might contain pictures of faces\ntogether with an indication of the sex of the person depicted in each\none. A net that can learn this task might have two output units\n(indicating the categories male and female) and many input units, one\ndevoted to the brightness of each pixel (tiny area) in the picture.\nThe weights of the net to be trained are initially set to random\nvalues, and then members of the training set are repeatedly exposed to\nthe net. The values for the input of a member are placed on the input\nunits and the output of the net is compared with the desired output\nfor this member. Then all the weights in the net are adjusted slightly\nin the direction that would bring the net\u2019s output values closer\nto the values for the desired output. For example, when male\u2019s\nface is presented to the input units the weights are adjusted so that\nthe value of the male output unit is increased and the value of the\nfemale output unit is decreased. After many repetitions of this\nprocess the net may learn to produce the desired output for each input\nin the training set. If the training goes well, the net may also have\nlearned to generalize to the desired behavior for inputs and outputs\nthat were not in the training set. For example, it may do a good job\nof distinguishing males from females in pictures that were never\npresented to it before.\n\nTraining nets to model aspects of human intelligence is a fine art.\nSuccess with backpropagation and other connectionist learning methods\nmay depend on quite subtle adjustment of the algorithm and the\ntraining set. Training typically involves hundreds of thousands of\nrounds of weight adjustment. Given the limitations of computers in the\npast, training a net to perform an interesting task took days or even\nweeks. More recently, the use of massively parallel dedicated\nprocessors (GPUs) has helped relieve these heavy computational\nburdens. But even here, some limitations to connectionist theories of\nlearning will remain to be faced. Humans (and many less intelligent\nanimals) display an ability to learn from single examples; for\nexample, a child shown a novel two-wheeled vehicle and given the name\n\u201cSegway\u201d, knows right away what a Segway is (Lake, Zaremba\net al. 2015). Connectionist learning techniques such as\nbackpropagation are far from explaining this kind of \u201cone\nshot\u201d learning.\n3. Samples of What Neural Networks Can Do\n\nConnectionists have made significant progress in demonstrating the\npower of neural networks to master cognitive tasks. Here are three\nwell-known experiments that have encouraged connectionists to believe\nthat neural networks are good models of human intelligence. One of the\nmost attractive of these efforts is Sejnowski and Rosenberg\u2019s\n1987 work on a net that can read English text called NETtalk. The\ntraining set for NETtalk was a large data base consisting of English\ntext coupled with its corresponding phonetic output, written in a code\nsuitable for use with a speech synthesizer. Tapes of NETtalk\u2019s\nperformance at different stages of its training are very interesting\nlistening. At first the output is random noise. Later, the net sounds\nlike it is babbling, and later still as though it is speaking English\ndouble-talk (speech that is formed of sounds that resemble English\nwords). At the end of training, NETtalk does a fairly good job of\npronouncing the text given to it. Furthermore, this ability\ngeneralizes fairly well to text that was not presented in the training\nset.\n\nAnother influential early connectionist model was a net trained by\nRumelhart and McClelland (1986) to predict the past tense of English\nverbs. The task is interesting because although most of the verbs in\nEnglish (the regular verbs) form the past tense by adding the suffix\n\u201c-ed\u201d, many of the most frequently verbs are irregular\n(\u201cis\u201d / \u201cwas\u201d, \u201ccome\u201d /\n\u201ccame\u201d, \u201cgo\u201d / \u201cwent\u201d). The net\nwas first trained on a set containing a large number of irregular\nverbs, and later on a set of 460 verbs containing mostly regulars. The\nnet learned the past tenses of the 460 verbs in about 200 rounds of\ntraining, and it generalized fairly well to verbs not in the training\nset. It even showed a good appreciation of \u201cregularities\u201d\nto be found among the irregular verbs (\u201csend\u201d /\n\u201csent\u201d, \u201cbuild\u201d / \u201cbuilt\u201d;\n\u201cblow\u201d / \u201cblew\u201d, \u201cfly\u201d /\n\u201cflew\u201d). During learning, as the system was exposed to the\ntraining set containing more regular verbs, it had a tendency to\noverregularize, i.e., to combine both irregular and regular forms:\n(\u201cbreak\u201d / \u201cbroked\u201d, instead of\n\u201cbreak\u201d / \u201cbroke\u201d). This was corrected with\nmore training. It is interesting to note that children are known to\nexhibit the same tendency to overregularize during language learning.\nHowever, there is hot debate over whether Rumelhart and\nMcClelland\u2019s is a good model of how humans actually learn and\nprocess verb endings. For example, Pinker and Prince (1988) point out\nthat the model does a poor job of generalizing to some novel regular\nverbs. They believe that this is a sign of a basic failing in\nconnectionist models. Nets may be good at making associations and\nmatching patterns, but they have fundamental limitations in mastering\ngeneral rules such as the formation of the regular past tense. These\ncomplaints raise an important issue for connectionist modelers, namely\nwhether nets can generalize properly to master cognitive tasks\ninvolving rules. Despite Pinker and Prince\u2019s objections, many\nconnectionists believe that generalization of the right kind is still\npossible (Niklasson & van Gelder 1994).\n\nElman\u2019s 1991 work on nets that can appreciate grammatical\nstructure has important implications for the debate about whether\nneural networks can learn to master rules. Elman trained a simple\nrecurrent network to predict the next word in a large corpus of\nEnglish sentences. The sentences were formed from a simple vocabulary\nof 23 words using a subset of English grammar. The grammar, though\nsimple, posed a hard test for linguistic awareness. It allowed\nunlimited formation of relative clauses while demanding agreement\nbetween the head noun and the verb. So for example, in the\nsentence\n\n\nAny man that chases dogs that chase cats \u2026\nruns.\n\n\nthe singular \u201cman\u201d must agree with the\nverb \u201cruns\u201d despite the intervening\nplural nouns (\u201cdogs\u201d, \u201ccats\u201d) which might\ncause the selection of \u201crun\u201d. One of the important\nfeatures of Elman\u2019s model is the use of recurrent connections.\nThe values at the hidden units are saved in a set of so called context\nunits, to be sent back to the input level for the next round of\nprocessing. This looping back from hidden to input layers provides the\nnet with a rudimentary form of memory of the sequence of words in the\ninput sentence. Elman\u2019s nets displayed an appreciation of the\ngrammatical structure of sentences that were not in the training set.\nThe net\u2019s command of syntax was measured in the following way.\nPredicting the next word in an English sentence is, of course, an\nimpossible task. However, these nets succeeded, at least by the\nfollowing measure. At a given point in an input sentence, the output\nunits for words that are grammatical continuations of the sentence at\nthat point should be active and output units for all other words\nshould be inactive. After intensive training, Elman was able to\nproduce nets that displayed perfect performance on this measure\nincluding sentences not in the training set. The work of Christiansen\nand Chater (1999a) and Morris, Cottrell, and Elman (2000) extends this\nresearch to more complex grammars. For a broader view of progress in\nconnectionist natural language processing see summaries by\nChristiansen and Chater (1999b), and Rohde and Plaut (2003).\n\nAlthough this performance is impressive, there is still a long way to\ngo in training nets that can process a language like English.\nFurthermore, doubts have been raised about the significance of\nElman\u2019s results. For example, Marcus (1998, 2001) argues that\nElman\u2019s nets are not able to generalize this performance to\nsentences formed from a novel vocabulary. This, he claims, is a sign\nthat connectionist models merely associate instances, and are unable\nto truly master abstract rules. On the other hand, Phillips (2002)\nargues that classical architectures are no better off in this respect.\nThe purported inability of connectionist models to generalize\nperformance in this way has become an important theme in the\nsystematicity debate. (See Section 7 below.)\n\nA somewhat different concern about the adequacy of connectionist\nlanguage processing focuses on tasks that mimic infant learning of\nsimple artificial grammars. Data on reaction time confirms that\ninfants can learn to distinguish well-formed from ill-formed sentences\nin a novel language created by experimenters. Shultz and Bale (2001)\nreport success in training neural nets on the same task. Vilcu and\nHadley (2005) object that this work fails to demonstrate true\nacquisition of the grammar, but see Shultz and Bale (2006) for a\ndetailed reply.\n4. Strengths and Weaknesses of Neural Network Models\n\nPhilosophers are interested in neural networks because they may\nprovide a new framework for understanding the nature of the mind and\nits relation to the brain (Rumelhart & McClelland 1986: Chapter\n1). Connectionist models seem particularly well matched to what we\nknow about neurology. The brain is indeed a neural net, formed from\nmassively many units (neurons) and their connections (synapses).\nFurthermore, several properties of neural network models suggest that\nconnectionism may offer an especially faithful picture of the nature\nof cognitive processing. Neural networks exhibit robust flexibility in\nthe face of the challenges posed by the real world. Noisy input or\ndestruction of units causes graceful degradation of function. The\nnet\u2019s response is still appropriate, though somewhat less\naccurate. In contrast, noise and loss of circuitry in classical\ncomputers typically result in catastrophic failure. Neural networks\nare also particularly well adapted for problems that require the\nresolution of many conflicting constraints in parallel. There is ample\nevidence from research in artificial intelligence that cognitive tasks\nsuch as object recognition, planning, and even coordinated motion\npresent problems of this kind. Although classical systems are capable\nof multiple constraint satisfaction, connectionists argue that neural\nnetwork models provide much more natural mechanisms for dealing with\nsuch problems.\n\nOver the centuries, philosophers have struggled to understand how our\nconcepts are defined. It is now widely acknowledged that trying to\ncharacterize ordinary notions with necessary and sufficient conditions\nis doomed to failure. Exceptions to almost any proposed definition are\nalways waiting in the wings. For example, one might propose that a\ntiger is a large black and orange feline. But then what about albino\ntigers? Philosophers and cognitive psychologists have argued that\ncategories are delimited in more flexible ways, for example via a\nnotion of family resemblance or similarity to a prototype.\nConnectionist models seem especially well suited to accommodating\ngraded notions of category membership of this kind. Nets can learn to\nappreciate subtle statistical patterns that would be very hard to\nexpress as hard and fast rules. Connectionism promises to explain\nflexibility and insight found in human intelligence using methods that\ncannot be easily expressed in the form of exception free principles\n(Horgan & Tienson 1989, 1990), thus avoiding the brittleness that\narises from standard forms of symbolic representation.\n\nDespite these intriguing features, there are some weaknesses in\nconnectionist models that bear mentioning. First, most neural network\nresearch abstracts away from many interesting and possibly important\nfeatures of the brain. For example, connectionists usually do not\nattempt to explicitly model the variety of different kinds of brain\nneurons, nor the effects of neurotransmitters and hormones.\nFurthermore, it is far from clear that the brain contains the kind of\nreverse connections that would be needed if the brain were to learn by\na process like backpropagation, and the immense number of repetitions\nneeded for such training methods seems far from realistic. Attention\nto these matters will probably be necessary if convincing\nconnectionist models of human cognitive processing are to be\nconstructed. A more serious objection must also be met. It is widely\nfelt, especially among classicists, that neural networks are not\nparticularly good at the kind of rule based processing that is thought\nto undergird language, reasoning, and higher forms of thought. (For a\nwell known critique of this kind see Pinker and Prince 1988.) We will\ndiscuss the matter further when we turn to\n the systematicity debate.\n\nThere has been a cottage industry in developing more\nbiologically-plausible algorithms for error-driven training that can\nbe shown to approximate the results of backpropagation without its\nimplausible features. Prominent examples include\nO\u2019Reilly\u2019s Generalized Error Recirculation algorithm\n(O\u2019Reilly 1996), using randomized error signals rather than\nerror signals individually computed for each neuron (Lillicrap,\nCownden, Tweed, & Akerman 2016), and modifying weights using\nspike-timing dependent plasticity--the latter of which has been a\nfavorite of prominent figures in deep learning research (Bengio et al.\n2017). (For more on deep learning see\n section 11\n below.) \n5. The Shape of the Controversy between Connectionists and Classicists\n\nThe last forty years have been dominated by the classical view that\n(at least higher) human cognition is analogous to symbolic computation\nin digital computers. On the classical account, information is\nrepresented by strings of symbols, just as we represent data in\ncomputer memory or on pieces of paper. The connectionist claims, on\nthe other hand, that information is stored non-symbolically in the\nweights, or connection strengths, between the units of a neural net.\nThe classicist believes that cognition resembles digital processing,\nwhere strings are produced in sequence according to the instructions\nof a (symbolic) program. The connectionist views mental processing as\nthe dynamic and graded evolution of activity in a neural net, each\nunit\u2019s activation depending on the connection strengths and\nactivity of its neighbors.\n\nOn the face of it, these views seem very different. However many\nconnectionists do not view their work as a challenge to classicism and\nsome overtly support the classical picture. So-called implementational\nconnectionists seek an accommodation between the two paradigms. They\nhold that the brain\u2019s net implements a symbolic processor. True,\nthe mind is a neural net; but it is also a symbolic processor at a\nhigher and more abstract level of description. So the role for\nconnectionist research according to the implementationalist is to\ndiscover how the machinery needed for symbolic processing can be\nforged from neural network materials, so that classical processing can\nbe reduced to the neural network account.\n\nHowever, many connectionists resist the implementational point of\nview. Such radical connectionists claim that symbolic processing was a\nbad guess about how the mind works. They complain that classical\ntheory does a poor job of explaining graceful degradation of function,\nholistic representation of data, spontaneous generalization,\nappreciation of context, and many other features of human intelligence\nwhich are captured in their models. The failure of classical\nprogramming to match the flexibility and efficiency of human cognition\nis by their lights a symptom of the need for a new paradigm in\ncognitive science. So radical connectionists would eliminate symbolic\nprocessing from cognitive science forever.\n\nThe controversy between radical and implementational connectionists is\ncomplicated by the invention of what are called hybrid connectionist\narchitectures. Here elements of classical symbolic processing are\nincluded in neural nets (Wermter & Sun 2000). For example,\nMiikkulainen (1993) champions a complex collection of neural net\nmodules that share data coded in activation patterns. Since one of the\nmodules acts as a memory, the system taken as a whole resembles a\nclassical processor with separate mechanisms for storing and operating\non digital \u201cwords\u201d. Smolensky (1990) is famous for\ninventing so called tensor product methods for simulating the process\nof variable binding, where symbolic information is stored at and\nretrieved from known \u201clocations\u201d. More recently, Eliasmith\n(2013) has proposed complex and massive architectures that use what\nare called semantic pointers, which exhibit features of classical\nvariable binding. Once hybrid architectures such as these are on the\ntable, it becomes more difficult to classify a given connectionist\nmodel as radical or merely implementational. This opens the\ninteresting prospect that whether symbolic processing is actually\npresent in the human brain may turn out to be a matter of degree.\n\nThe disagreement concerning the degree to which human cognition\ninvolves symbolic processing is naturally embroiled with the\ninnateness debate\u2014whether higher level abilities such as\nlanguage and reasoning are part of the human genetic endowment, or\nwhether they are learned. The success of connectionist models at\nlearning tasks starting from randomly chosen weights gives heart to\nempiricists, who would think that the infant brain is able to\nconstruct intelligence from perceptual input using a simple learning\nmechanism (Elman et al. 1996). On the other hand, nativists in the\nrationalist tradition argue that at least for grammar-based language,\nthe poverty of perceptual stimulus (Chomsky 1965: 58) entails the\nexistence of a genetically determined mechanism tailored to learning\ngrammar. However, the alignment between connectionism and non-nativism\nis not so clear-cut. There is no reason that connectionist models\ncannot be interpreted from a nativist point of view, where the ongoing\n\u201clearning\u201d represents the process of evolutionary\nrefinement from generation to generation of a species. The idea that\nthe human brain has domain specific knowledge that is genetically\ndetermined can be accommodated in the connectionist paradigm by\nbiasing the initial weights of the models to make that knowledge easy\nor trivial to learn. Connectionist research makes best contact with\nthe innateness debate by providing a new strategy for disarming\npoverty of stimulus arguments. Nativists argue that association of\nideas, the mechanism for learning proposed by the traditional\nempiricist, is too slender a reed to support the development of higher\nlevel cognitive abilities. They suppose that innate mechanisms are\nessential for learning (for example) a grammar of English from a\nchild\u2019s linguistic input, because the statistical regularities\navailable to \u201cmere association\u201d massively underdetermine\nthat grammar. Connectionism could support an empiricism here by\nproviding a proof-of-concept that such structured knowledge can be\nlearned from inputs available to humans using only learning mechanisms\nfound in non-classical architectures. Of course it is too soon to tell\nwhether this promise can be realized. \n6. Connectionist Representation\n\nConnectionist models provide a new paradigm for understanding how\ninformation might be represented in the brain. A seductive but naive\nidea is that single neurons (or tiny neural bundles) might be devoted\nto the representation of each thing the brain needs to record. For\nexample, we may imagine that there is a grandmother neuron that fires\nwhen we think about our grandmother. However, such local\nrepresentation is not likely. There is good evidence that our\ngrandmother thought involves complex patterns of activity distributed\nacross relatively large parts of cortex.\n\nIt is interesting to note that distributed, rather than local\nrepresentations on the hidden units are the natural products of\nconnectionist training methods. The activation patterns that appear on\nthe hidden units while NETtalk processes text serve as an example.\nAnalysis reveals that the net learned to represent such categories as\nconsonants and vowels, not by creating one unit active for consonants\nand another for vowels, but rather in developing two different\ncharacteristic patterns of activity across all the hidden units.\n\nGiven the expectations formed from our experience with local\nrepresentation on the printed page, distributed representation seems\nboth novel and difficult to understand. But the technique exhibits\nimportant advantages. For example, distributed representations,\n(unlike symbols stored in separate fixed memory locations) remain\nrelatively well preserved when parts of the model are destroyed or\noverloaded. More importantly, since representations are coded in\npatterns rather than firings of individual units, relationships\nbetween representations are coded in the similarities and differences\nbetween these patterns. So the internal properties of the\nrepresentation carry information on what it is about (Clark 1993: 19).\nIn contrast, local representation is conventional. No intrinsic\nproperties of the representation (a unit\u2019s firing) determine its\nrelationships to the other symbols. This self-reporting feature of\ndistributed representations promises to resolve a philosophical\nconundrum about meaning. In a symbolic representational scheme, all\nrepresentations are composed out of symbolic atoms (like words in a\nlanguage). Meanings of complex symbol strings may be defined by the\nway they are built up out of their constituents, but what fixes the\nmeanings of the atoms?\n\nConnectionist representational schemes provide an end run around the\npuzzle by simply dispensing with atoms. Every distributed\nrepresentation is a pattern of activity across all the units, so there\nis no principled way to distinguish between simple and complex\nrepresentations. To be sure, representations are composed out of the\nactivities of the individual units. But none of these\n\u201catoms\u201d codes for any symbol. The representations are\nsub-symbolic in the sense that analysis into their components leaves\nthe symbolic level behind.\n\nThe sub-symbolic nature of distributed representation provides a novel\nway to conceive of information processing in the brain. If we model\nthe activity of each neuron with a number, then the activity of the\nwhole brain can be given by a giant vector (or list) of numbers, one\nfor each neuron. Both the brain\u2019s input from sensory systems and\nits output to individual muscle neurons can also be treated as vectors\nof the same kind. So the brain amounts to a vector processor, and the\nproblem of psychology is transformed into questions about which\noperations on vectors account for the different aspects of human\ncognition.\n\nSub-symbolic representation has interesting implications for the\nclassical hypothesis that the brain must contain symbolic\nrepresentations that are similar to sentences of a language. This\nidea, often referred to as the language of thought (or LOT) thesis may\nbe challenged by the nature of connectionist representations. It is\nnot easy to say exactly what the LOT thesis amounts to, but van Gelder\n(1990) offers an influential and widely accepted benchmark for\ndetermining when the brain should be said to contain sentence-like\nrepresentations. It is that when a representation is tokened one\nthereby tokens the constituents of that representation. For example,\nif I write \u201cJohn loves Mary\u201d I have thereby written the\nsentence\u2019s constituents: \u201cJohn\u201d \u201cloves\u201d\nand \u201cMary\u201d. Distributed representations for complex\nexpressions like \u201cJohn loves Mary\u201d can be constructed that\ndo not contain any explicit representation of their parts (Smolensky\n1990). The information about the constituents can be extracted from\nthe representations, but neural network models do not need to\nexplicitly extract this information themselves in order to process it\ncorrectly (Chalmers 1990). This suggests that neural network models\nserve as counterexamples to the idea that the language of thought is a\nprerequisite for human cognition. However, the matter is still a topic\nof lively debate (Fodor 1997).\n\nThe novelty of distributed and superimposed connectionist information\nstorage naturally causes one to wonder about the viability of\nclassical notions of symbolic computation in describing the brain.\nRamsey (1997) argues that though we may attribute symbolic\nrepresentations to neural nets, those attributions do not figure in\nlegitimate explanations of the model\u2019s behavior. This claim is\nimportant because the classical account of cognitive processing, (and\nfolk intuitions) presume that representations play an explanatory role\nin understanding the mind. It has been widely thought that cognitive\nscience requires, by its very nature, explanations that appeal to\nrepresentations (Von Eckardt 2003). If Ramsey is right, the point may\ncut in two different ways. Some may use it to argue for a new and\nnon-classical understanding of the mind, while others would use it to\nargue that connectionism is inadequate since it cannot explain what it\nmust. However, Haybron (2000) argues against Ramsey that there is\nample room for representations with explanatory role in radical\nconnectionist architectures. Roth (2005) makes the interesting point\nthat contrary to first impressions, it may also make perfect sense to\nexplain a net\u2019s behavior by reference to a computer program,\neven if there is no way to discriminate a sequence of steps of the\ncomputation through time.\n\nThe debate concerning the presence of classical representations and a\nlanguage of thought has been clouded by lack of clarity in defining\nwhat should count as the representational \u201cvehicles\u201d in\ndistributed neural models. Shea (2007) makes the point that the\nindividuation of distributed representations should be defined by the\nway activation patterns on the hidden units cluster together. It is\nthe relationships between clustering regions in the space of\npossible activation patterns that carry representational content, not\nthe activations themselves, nor the collection of units responsible\nfor the activation. On this understanding, prospects are improved for\nlocating representational content in neural nets that can be compared\nin nets of different architectures, that is causally involved in\nprocessing, and which overcomes some objections to holistic accounts\nof meaning.\n\nIn a series of papers Horgan and Tienson (1989, 1990) have championed\na view called representations without rules. According to this view\nclassicists are right to think that human brains (and good\nconnectionist models of them) contain explanatorily robust\nrepresentations; but they are wrong to think that those\nrepresentations enter in to hard and fast rules like the steps of a\ncomputer program. The idea that connectionist systems may follow\ngraded or approximate regularities (\u201csoft laws\u201d as Horgan\nand Tienson call them) is intuitive and appealing. However, Aizawa\n(1994) argues that given an arbitrary neural net with a representation\nlevel description, it is always possible to outfit it with hard and\nfast representation-level rules. Guarini (2001) responds that if we\npay attention to notions of rule following that are useful to\ncognitive modeling, Aizawa\u2019s constructions will seem beside the\npoint.\n7. The Systematicity Debate\n\nThe major points of controversy in the philosophical literature on\nconnectionism have to do with whether connectionists provide a viable\nand novel paradigm for understanding the mind. One complaint is that\nconnectionist models are only good at processing associations. But\nsuch tasks as language and reasoning cannot be accomplished by\nassociative methods alone and so connectionists are unlikely to match\nthe performance of classical models at explaining these higher-level\ncognitive abilities. However, it is a simple matter to prove that\nneural networks can do anything that symbolic processors can do, since\nnets can be constructed that mimic a computer\u2019s circuits. So the\nobjection can not be that connectionist models are unable to account\nfor higher cognition; it is rather that they can do so only if they\nimplement the classicist\u2019s symbolic processing tools.\nImplementational connectionism may succeed, but radical connectionists\nwill never be able to account for the mind.\n\nFodor and Pylyshyn\u2019s often cited paper (1988) launches a debate\nof this kind. They identify a feature of human intelligence called\nsystematicity which they feel connectionists cannot explain. The\nsystematicity of language refers to the fact that the ability to\nproduce/understand/think some sentences is intrinsically connected to\nthe ability to produce/understand/think others of related structure.\nFor example, no one with a command of English who understands\n\u201cJohn loves Mary\u201d can fail to understand \u201cMary loves\nJohn.\u201d From the classical point of view, the connection between\nthese two abilities can easily be explained by assuming that masters\nof English represent the constituents (\u201cJohn\u201d,\n\u201cloves\u201d and \u201cMary\u201d) of \u201cJohn loves\nMary\u201d and compute its meaning from the meanings of these\nconstituents. If this is so, then understanding a novel sentence like\n\u201cMary loves John\u201d can be accounted for as another instance\nof the same symbolic process. In a similar way, symbolic processing\nwould account for the systematicity of reasoning, learning and\nthought. It would explain why there are no people who are capable of\nconcluding P from P & (Q &\nR), but incapable of concluding P from P\n& Q, why there are no people capable of learning to\nprefer a red cube to green square who cannot learn to prefer a green\ncube to the red square, and why there isn\u2019t anyone who can think\nthat John loves Mary who can\u2019t also think that Mary loves\nJohn.\n\nFodor and McLaughlin (1990) argue in detail that connectionists do not\naccount for systematicity. Although connectionist models can be\ntrained to be systematic, they can also be trained, for example, to\nrecognize \u201cJohn loves Mary\u201d without being able to\nrecognize \u201cMary loves John.\u201d Since connectionism does not\nguarantee systematicity, it does not explain why systematicity is\nfound so pervasively in human cognition. Systematicity may exist in\nconnectionist architectures, but where it exists, it is no more than a\nlucky accident. The classical solution is much better, because in\nclassical models, pervasive systematicity comes for free.\n\nThe charge that connectionist nets are disadvantaged in explaining\nsystematicity has generated a lot of interest. Chalmers (1993) points\nout that Fodor and Pylyshyn\u2019s argument proves too much, for it\nentails that all neural nets, even those that implement a classical\narchitecture, do not exhibit systematicity. Given the uncontroversial\nconclusion that the brain is a neural net, it would follow that\nsystematicity is impossible in human thought. Another often mentioned\npoint of rebuttal (Aizawa 1997b; Matthews 1997; Hadley 1997b) is that\nclassical architectures do no better at explaining systematicity.\nThere are also classical models that can be programmed to recognize\n\u201cJohn loves Mary\u201d without being able to recognize\n\u201cMary loves John,\u201d for this depends on exactly which\nsymbolic rules govern the classical processing. The point is that\nneither the use of connectionist architecture alone nor the use of\nclassical architecture alone enforces a strong enough constraint to\nexplain pervasive systematicity. In both architectures, further\nassumptions about the nature of the processing must be made to ensure\nthat \u201cMary loves John\u201d and \u201cJohn loves Mary\u201d\nare treated alike.\n\nA discussion of this point should mention Fodor and McLaughlin\u2019s\nrequirement that systematicity be explained as a matter of nomic\nnecessity, that is, as a matter of natural law. The complaint against\nconnectionists is that while they may implement systems that exhibit\nsystematicity, they will not have explained it unless it follows from\ntheir models as a nomic necessity. However, the demand for nomic\nnecessity is a very strong one, and one that classical architectures\nclearly cannot meet either. So the only tactic for securing a telling\nobjection to connectionists along these lines would be to weaken the\nrequirement on the explanation of systematicity to one which classical\narchitectures can and connectionists cannot meet. A convincing case of\nthis kind has yet to be made.\n\nAs the systematicity debate has evolved, attention has been focused on\ndefining the benchmarks that would answer Fodor and Pylyshyn\u2019s\nchallenge. Hadley (1994a, 1994b) distinguishes three brands of\nsystematicity. Connectionists have clearly demonstrated the weakest of\nthese by showing that neural nets can learn to correctly recognize\nnovel sequences of words (e.g., \u201cMary loves John\u201d) that\nwere not in the training set. However, Hadley claims that a convincing\nrebuttal must demonstrate strong systematicity, or better, strong\nsemantical systematicity. Strong systematicity would require (at\nleast) that \u201cMary loves John\u201d be recognized even if\n\u201cMary\u201d never appears in the subject position in any\nsentence in the training set. Strong semantical systematicity would\nrequire as well that the net show abilities at correct semantical\nprocessing of the novel sentences rather than merely distinguishing\ngrammatical from ungrammatical forms. Niklasson and van Gelder (1994)\nhave claimed success at strong systematicity, though Hadley complains\nthat this is at best a borderline case. Hadley and Hayward (1997)\ntackle strong semantical systematicity, but by Hadley\u2019s own\nadmission it is not clear that they have avoided the use of a\nclassical architecture. Boden and Niklasson (2000) claim to have\nconstructed a model that meets at least the spirit of strong\nsemantical systematicity, but Hadley (2004) argues that even strong\nsystematicity has not been demonstrated there. Whether one takes a\npositive or a negative view of these attempts, it is safe to say that\nno one has met the challenge of providing a neural net capable of\nlearning complex semantical processing that generalizes to a full\nrange of truly novel inputs.\n\nResearch on nets that clearly demonstrate strong systematicity has\ncontinued. Jansen and Watter (2012) provide a good summary of more\nrecent efforts along these lines, and propose an interesting basis for\nsolving the problem. They use a more complex architecture that\ncombines unsupervised self-organizing maps with features of simple\nrecurrent nets. However, the main innovation is to allow codes for the\nwords being processed to represent sensory-motor features of what the\nwords represent. Once trained, their nets displayed very good accuracy\nin distinguishing the grammatical features of sentences whose words\nnever even appeared in the training set. This may appear to be\ncheating since the word codes might surreptitiously represent\ngrammatical categories, or at least they may unfairly facilitate\nlearning those categories. Jansen and Watter note however, that the\nsensory-motor features of what a word represents are apparent to a\nchild who has just acquired a new word, and so that information is not\noff-limits in a model of language learning. They make the interesting\nobservation that a solution to the systematicity problem may require\nincluding sources of environmental information that have so far been\nignored in theories of language learning. This work complicates the\nsystematicity debate, since it opens a new worry about what\ninformation resources are legitimate in responding to the challenge.\nHowever, this reminds us that architecture alone (whether classical or\nconnectionist) is not going to solve the systematicity problem in any\ncase, so the interesting questions concern what sources of\nsupplemental information are needed to make the learning of grammar\npossible.\n\nKent Johnson (2004) argues that the whole systematicity debate is\nmisguided. Attempts at carefully defining the systematicity of\nlanguage or thought leaves us with either trivialities or falsehoods.\nConnectionists surely have explaining to do, but Johnson recommends\nthat it is fruitless to view their burden under the rubric of\nsystematicity. Aizawa (2014) also suggests the debate is no longer\ngermane given the present climate in cognitive science. What is needed\ninstead is the development of neurally plausible connectionist models\ncapable of processing a language with a recursive syntax, which react\nimmediately to the introduction of new items in the lexicon without\nintroducing the features of classical architecture. The\n\u201csystematicity\u201d debate may have already gone as Johnson\nadvises, for Hadley\u2019s demand for strong semantical systematicity\nmay be thought of as the requirement that connectionists exhibit\nsuccess in that direction.\n\nRecent work (Loula, Baroni, & Lake 2018) sheds new light on the\ncontroversy. Here recurrent neural nets were trained to interpret\ncomplex commands in a simple language that includes primitives such as\n\u201cjump\u201d, \u201cwalk\u201d, \u201cleft\u201d,\n\u201cright\u201d, \u201copposite\u201d and \u201caround\u201d.\n\u201cOpposite\u201d is interpreted as a request to perform a\ncommand twice, and \u201caround\u201d to do so four times. So\n\u201cjump around left\u201d requests a left jump four times. The\nauthors report that their nets showed very accurate generalization at\ntasks that qualify for demonstrating strong semantic systematicity.\nThe nets correctly parsed commands in the test set containing\n\u201cjump around right\u201d even though this phrase never appeared\nin the training set. Nevertheless the net\u2019s failures at more\nchallenging tasks point to limitations in their abilities to\ngeneralize in ways that would demonstrate genuine systematicity. The\nnets exhibited very poor performance when commands in the test set\nwere longer (or even shorter), than those presented in the training\nset. So they appeared unable to spontaneously compose the meaning of\ncomplex expressions from the meanings of their parts. New research is\nneeded to understand the nature of these failures, whether they can be\novercome in non-classical architectures, and the extent to which\nhumans would exhibit similar mistakes under analogous\ncircumstances.\n\nIt has been almost thirty years since the systematicity debate first\nbegan, with over 3,000 citations to Fodor and Pylyshyn\u2019s\noriginal paper. So this brief account is necessarily incomplete.\nAizawa (2003) provides an\nexcellent view of the literature, and Calvo and Symons (2014) serves\nas another more recent resource. \n8. Connectionism and Semantic Similarity\n\nOne of the attractions of distributed representations in connectionist\nmodels is that they suggest a solution to the problem of providing a\ntheory of how brain states could have meaning. The idea is that the\nsimilarities and differences between activation patterns along\ndifferent dimensions of neural activity record semantical information.\nSo the similarity properties of neural activations provide intrinsic\nproperties that determine meaning. However, when it comes to\ncompositional linguistic representations, Fodor and Lepore (1992: Ch.\n6) challenge similarity based accounts, on two fronts. The first\nproblem is that human brains presumably vary significantly in the\nnumber of and connections between their neurons. Although it is\nstraightforward to define similarity measures on two nets that contain\nthe same number of units, it is harder to see how this can be done\nwhen the basic architectures of two nets differ. The second problem\nFodor and Lepore cite is that even if similarity measures for meanings\ncan be successfully crafted, they are inadequate to the task of\nmeeting the desiderata which a theory of meaning must satisfy.\n\nChurchland (1998) shows that the first of these two objections can be\nmet. Citing the work of Laakso and Cottrell (2000) he explains how\nsimilarity measures between activation patterns in nets with radically\ndifferent structures can be defined. Not only that, Laakso and\nCottrell show that nets of different structures trained on the same\ntask develop activation patterns which are strongly similar according\nto the measures they recommend. This offers hope that empirically well\ndefined measures of similarity of concepts and thoughts across\ndifferent individuals might be forged.\n\nOn the other hand, the development of a traditional theory of meaning\nbased on similarity faces severe obstacles (Fodor & Lepore 1999),\nfor such a theory would be required to assign sentences truth\nconditions based on an analysis of the meaning of their parts, and it\nis not clear that similarity alone is up to such tasks as fixing\ndenotation in the way a standard theory demands. However, most\nconnectionists who promote similarity based accounts of meaning reject\nmany of the presupposition of standard theories. They hope to craft a\nworking alternative which either rejects or modifies those\npresuppositions while still being faithful to the data on human\nlinguistic abilities.\n\nCalvo Garz\u00f3n (2003) complains that there are reasons to think\nthat connectionists must fail. Churchland\u2019s response has no\nanswer to the collateral information challenge. That problem is that\nthe measured similarities between activation patterns for a concept\n(say: grandmother) in two human brains are guaranteed to be very low\nbecause two people\u2019s (collateral) information on their\ngrandmothers (name, appearance, age, character) is going to be very\ndifferent. If concepts are defined by everything we know, then the\nmeasures for activation patterns of our concepts are bound to be far\napart. This is a truly deep problem in any theory that hopes to define\nmeaning by functional relationships between brain states. Philosophers\nof many stripes must struggle with this problem. Given the lack of a\nsuccessfully worked out theory of concepts in either traditional or\nconnectionist paradigms, it is only fair to leave the question for\nfuture research.\n9. Connectionism and the Elimination of Folk Psychology\n\nAnother important application of connectionist research to\nphilosophical debate about the mind concerns the status of folk\npsychology. Folk psychology is the conceptual structure that we\nspontaneously apply to understanding and predicting human behavior.\nFor example, knowing that John desires a beer and that he believes\nthat there is one in the refrigerator allows us to explain why John\njust went into the kitchen. Such knowledge depends crucially on our\nability to conceive of others as having desires and goals, plans for\nsatisfying them, and beliefs to guide those plans. The idea that\npeople have beliefs, plans and desires is a commonplace of ordinary\nlife; but does it provide a faithful description of what is actually\nto be found in the brain?\n\nIts defenders will argue that folk psychology is too good to be false\n(Fodor 1988: Ch. 1). What more can we ask for the truth of a theory\nthan that it provides an indispensable framework for successful\nnegotiations with others? On the other hand, eliminativists will\nrespond that the useful and widespread use of a conceptual scheme does\nnot argue for its truth (Churchland 1989: Ch. 1). Ancient astronomers\nfound the notion of celestial spheres useful (even essential) to the\nconduct of their discipline, but now we know that there are no\ncelestial spheres. From the eliminativists\u2019 point of view, an\nallegiance to folk psychology, like allegiance to folk (Aristotelian)\nphysics, stands in the way of scientific progress. A viable psychology\nmay require as radical a revolution in its conceptual foundations as\nis found in quantum mechanics.\n\nEliminativists are interested in connectionism because it promises to\nprovide a conceptual foundation that might replace folk psychology.\nFor example Ramsey, Stich, & Garon (1991) have argued that certain\nfeed-forward nets show that simple cognitive tasks can be performed\nwithout employing features that could correspond to beliefs, desires\nand plans. Presuming that such nets are faithful to how the brain\nworks, concepts of folk psychology fare no better than do celestial\nspheres. Whether connectionist models undermine folk psychology in\nthis way is still controversial. There are two main lines of response\nto the claim that connectionist models support eliminativist\nconclusions. One objection is that the models used by Ramsey et\nal. are feed forward nets, which are too weak to explain some of the\nmost basic features of cognition such as short term memory. Ramsey et\nal. have not shown that beliefs and desires must be absent in a class\nof nets adequate for human cognition. A second line of rebuttal\nchallenges the claim that features corresponding to beliefs and\ndesires are necessarily absent even in the feed forward nets at issue\n(Von Eckardt 2005).\n\nThe question is complicated further by disagreements about the nature\nof folk psychology. Many philosophers treat the beliefs and desires\npostulated by folk psychology as brain states with symbolic contents.\nFor example, the belief that there is a beer in the refrigerator is\nthought to be a brain state that contains symbols corresponding to\nbeer and a refrigerator. From this point of view, the fate of folk\npsychology is strongly tied to the symbolic processing hypothesis. So\nif connectionists can establish that brain processing is essentially\nnon-symbolic, eliminativist conclusions will follow. On the other\nhand, some philosophers do not think folk psychology is essentially\nsymbolic, and some would even challenge the idea that folk psychology\nis to be treated as a theory in the first place. Under this\nconception, it is much more difficult to forge links between results\nin connectionist research and the rejection of folk psychology.\n10. Predictive Coding Models of Cognition\n\nAs connectionist research has matured from its \u201cGolden\nAge\u201d in the 1980s, the main paradigm has radiated into a number\nof distinct approaches. Two important trends worth mention are\npredicative coding and deep learning (which will be covered in the\nfollowing section). Predictive coding is a well-established\ninformation processing tool with a wide range of applications. It is\nuseful, for example, in compressing the size of data sets. Suppose you\nwish to transmit a picture of a landscape with a blue sky. Since most\nof the pixels in the top half of your image are roughly the same\nshade, it is very inefficient to record the color value (say Red: 46\nGreen: 78 Blue: FF in hexadecimal) over and over again for each pixel\nin the top half of the image. Since the value of one pixel strongly\npredicts the value of its neighbor, the efficient thing to do is\nrecord at each pixel location, the difference between the predicted\nvalue (an average of its neighbors) and the actual value for that\npixel. (In the case of representing an even shaded sky, we would only\nneed to record the blue value once, followed by lots of zeros.) This\nway, major coding resources are only needed to keep track of points in\nthe image (such as edges) where there are large changes, that is\npoints of \u201csurprise\u201d or \u201cunexpected\u201d\nvariation.\n\nIt is well known that early visual processing in the brain involves\ntaking differences between nearby values, (for example, to identify\nvisual boundaries). It is only natural then to explore how the brain\nmight take advantage of predictive coding in perception, inference, or\neven action. (See Clark 2013 for an excellent summary and entry point\nto the literature.) There is wide variety in the models presented in\nthe predictive coding paradigm, and they tend to be specified at a\nhigher level of generality than are connectionist models so far\ndiscussed. Assume we have a neural net with input, hidden and output\nlevels that has been trained on a task (say face recognition) and so\npresumably has information about faces stored in the weights\nconnecting the hidden level nodes. Three features would classify this\nnet as a predictive coding (PC) model. First, the model will have\ndownward connections from the higher levels that are able to predict\nthe next input for that task. (The prediction might be a\nrepresentation of a generic face.) Second, the data sent to the higher\nlevels for a given input is not the value recorded at the input nodes,\nbut the difference between the predicted values and the values\nactually present. (So in the example, the data provided tracks the\ndifferences between the face to be recognized and the generic face.)\nIn this way the data being received by the net is already preprocessed\nfor coding efficiency. Third, the model is trained by adjusting the\nweights in such a way that the error is minimized at the inputs. In\nother words, the trained net reduces as much as possible the\n\u201csurprise\u201d registered in the difference between the raw\ninput and its prediction. In so doing it comes to be able to predict\nthe face of the individual to be recognized to eliminate the error.\nSome advocates of predictive coding models suggest that this scheme\nprovides a unified account of all cognitive phenomena, including\nperception, reasoning, planning and motor control. By minimizing\nprediction error in interacting with the environment, the net is\nforced to develop the conceptual resources to model the causal\nstructure of the external world, and so navigate that world more\neffectively. \n\nThe predictive coding (PC) paradigm has attracted a lot of attention.\nThere is ample evidence that PC models capture essential details of\nvisual function in the mammalian brain (Rao & Ballard 1999; Huang\n& Rao 2011). For example, when trained on typical visual input, PC\nmodels spontaneously develop functional areas for edge, orientation\nand motion detection known to exist in visual cortex. This work also\nraises the interesting point that the visual architecture may develop\nin response to the statistics of the scenes being encountered, so that\norganisms in different environments have visual systems specially\ntuned to their needs.\n\nIt must be admitted that there is still no convincing evidence that\nthe essential features of PC models are directly implemented as\nanatomical structures in the brain. Although it is conjectured that\nsuperficial pyramidal cells may transmit prediction error, and deep\npyramidal cells predictions, we do not know that that is how they\nactually function. On the other hand, PC models do appear more\nneurally plausible than backpropagation architectures, for there is no\nneed for a separate process of training on an externally provided set\nof training samples. Instead, predictions replace the role of the\ntraining set, so that learning and interacting with the environment\nare two sides of a unified unsupervised process. \n\nPC models also show promise for explaining higher-level cognitive\nphenomena. An often-cited example is binocular rivalry. When presented\nwith entirely different images in two eyes, humans report an\noscillation between the two images as each in turn comes into\n\u201cfocus\u201d. The PC explanation is that the system succeeds in\neliminating error by predicting the scene for one eye, but only to\nincrease the error for the other eye. So the system is unstable,\n\u201chunting\u201d from one prediction to the other. Predictive\ncoding also has a natural explanation for why we are unaware of our\nblind spot, for the lack of input in that area amounts to a report of\nno error, with the result that one perceives \u201cmore of the\nsame\u201d.\n\nPC accounts of attention have also been championed. For example, Hohwy\n(2012) notes that realistic PC models, which must tolerate noisy\ninputs, need to include parameters that track the desired precision to\nbe used in reporting error. So PC models need to make predictions of\nthe error precision relevant for a given situation. Hohwy explores the\nidea that mechanisms for optimizing precision expectations map onto\nthose that account for attention, and argues that attentional\nphenomena such as change blindness can be explained within the PC\nparadigm. \n\nPredictive coding has interesting implications for themes in the\nphilosophy of cognitive science. By integrating the processes of\ntop-down prediction with bottom-up error detection, the PC account of\nperception views it as intrinsically theory-laden. Deployment of the\nconceptual categorization of the world embodied in higher levels of\nthe net is essential to the very process of gathering data about the\nworld. This underscores, as well, tight linkages between belief,\nimaginative abilities, and perception (Grush 2004). The PC paradigm\nalso tends to support situated or embodied conceptions of cognition,\nfor it views action as a dynamic interaction between the\norganism\u2019s effects on the environment, its predictions\nconcerning those effects (its plans), and its continual monitoring of\nerror, which provides feedback to help ensure success.\n\nIt is too early to evaluate the importance and scope of PC models in\naccounting for the various aspects of cognition. Providing a unified\ntheory of brain function in general is, after all, an impossibly high\nstandard. Clark\u2019s target article (2013) provides a useful forum\nfor airing complaints against PC models and some possible responses.\nOne objection that is often heard is that an organism with a PC brain\ncan be expected to curl up in a dark room and die, for this is the\nbest way to minimize error at its sensory inputs. However, that view\nmay take too narrow a view of the sophistication of the predictions\navailable to the organism. If it is to survive at all, its genetic\nendowment coupled with what it can learn along the way may very well\nendow it with the expectation that it go out and seek needed resources\nin the environment. Minimizing error for that prediction of its\nbehavior will get it out of the dark room. However, it remains to be\nseen whether a theory of biological urges is usefully recast in PC\nterminology in this way, or whether PC theory is better characterized\nas only part of the explanation. Another complaint is that the\ntop-down influence on our perception coupled with the constraint that\nthe brain receives error signals rather than raw data would impose an\nunrealistic divide between a represented world of fantasy and the\nworld as it really is. It is hard to evaluate whether that qualifies\nas a serious objection. Were PC models actually to provide an account\nof our phenomenological experience, and characterize the relations\nbetween that experience and what we count as real, then skeptical\nconclusions to be drawn would count as features of the view rather\nthan objections to it. A number of responders to Clark\u2019s target\narticle also worry that PC-models count as overly general. In trying\nto explain everything they explain nothing. Without sufficient\nconstraints on the architecture, it is too easy to pretend to explain\ncognitive phenomena by merely redescribing them in a story written in\nthe vocabulary of prediction, comparison, error minimization, and\noptimized precision. The real proof of the pudding will come with the\ndevelopment of more complex and detailed computer models in the PC\nframework that are biologically plausible, and able to demonstrate the\ndefining features of cognition.\n",
    "bibliography": {
        "categories": [],
        "cat_ref_text": {
            "ref_list": [
                "Aizawa, Kenneth, 1994, \u201cRepresentations without Rules,\nConnectionism and the Syntactic Argument\u201d, <em>Synthese</em>,\n101(3): 465\u2013492. doi:10.1007/BF01063898",
                "\u2013\u2013\u2013, 1997a, \u201cExhibiting versus Explaining\nSystematicity: A Reply to Hadley and Hayward\u201d, <em>Minds and\nMachines</em>, 7(1): 39\u201355. doi:10.1023/A:1008203312152",
                "\u2013\u2013\u2013, 1997b, \u201cExplaining\nSystematicity\u201d, <em>Mind &amp; Language</em>, 12(2):\n115\u2013136. doi:10.1111/j.1468-0017.1997.tb00065.x",
                "\u2013\u2013\u2013, 2003, <em>The Systematicity Arguments</em>,\nDordrecht: Kluwer.",
                "\u2013\u2013\u2013, 2014, \u201cA Tough Time to be Talking\nSystematicity\u201d, in Calvo and Symons 2014: 77\u2013101.",
                "Bechtel, William, 1987, \u201cConnectionism and the Philosophy of\nMind: An Overview\u201d, <em>The Southern Journal of Philosophy</em>,\n26(S1): 17\u201341. doi:10.1111/j.2041-6962.1988.tb00461.x",
                "\u2013\u2013\u2013, 1988, \u201cConnectionism and Rules and\nRepresentation Systems: Are They Compatible?\u201d, <em>Philosophical\nPsychology</em>, 1(1): 5\u201316. doi:10.1080/09515088808572922",
                "Bechtel, William and Adele Abrahamsen, 1990, <em>Connectionism and\nthe Mind: An Introduction to Parallel Processing in Networks</em>,\nCambridge, MA: Blackwell.",
                "Bengio, Yoshua and Olivier Delalleau, 2011, \u201cOn the\nExpressive Power of Deep Architectures\u201d, in <em>International\nConference on Algorithmic Learning Theory (ALT 2011)</em>, Jyrki\nKivinen, Csaba Szepesv\u00e1ri, Esko Ukkonen, and Thomas Zeugmann\n(eds.) (Lecture Notes in Computer Science 6925), Berlin, Heidelberg:\nSpringer Berlin Heidelberg, 18\u201336.\ndoi:10.1007/978-3-642-24412-4_3",
                "Bengio, Yoshua, Thomas Mesnard, Asja Fischer, Saizheng Zhang, and\nYuhuai Wu, 2017, \u201cSTDP-Compatible Approximation of\nBackpropagation in an Energy-Based Model\u201d, <em>Neural\nComputation</em>, 29(3): 555\u2013577. doi:10.1162/NECO_a_00934",
                "Bod\u00e9n, Mikael and Lars Niklasson, 2000, \u201cSemantic\nSystematicity and Context in Connectionist Networks\u201d,\n<em>Connection Science</em>, 12(2): 111\u2013142.\ndoi:10.1080/09540090050129754",
                "Buckner, Cameron, 2018, \u201cEmpiricism without Magic:\nTransformational Abstraction in Deep Convolutional Neural\nNetworks\u201d, <em>Synthese</em>, 195(12): 5339\u20135372.\ndoi:10.1007/s11229-018-01949-1",
                "Butler, Keith, 1991, \u201cTowards a Connectionist Cognitive\nArchitecture\u201d, <em>Mind &amp; Language</em>, 6(3):\n252\u2013272. doi:10.1111/j.1468-0017.1991.tb00191.x",
                "Calvo Garz\u00f3n, Francisco, 2003, \u201cConnectionist\nSemantics and the Collateral Information Challenge\u201d, <em>Mind\n&amp; Language</em>, 18(1): 77\u201394.\ndoi:10.1111/1468-0017.00215",
                "Calvo, Paco and John Symons, 2014, <em>The Architecture of\nCognition: Rethinking Fodor and Pylyshyn\u2019s Systematicity\nChallenge</em>, Cambridge: MIT Press.",
                "Chalmers, David J., 1990, \u201cSyntactic Transformations on\nDistributed Representations\u201d, <em>Connection Science</em>,\n2(1\u20132): 53\u201362. doi:10.1080/09540099008915662",
                "\u2013\u2013\u2013, 1993, \u201cConnectionism and\nCompositionality: Why Fodor and Pylyshyn Were Wrong\u201d,\n<em>Philosophical Psychology</em>, 6(3): 305\u2013319.\ndoi:10.1080/09515089308573094",
                "Chomsky, Noam, 1965, <em>Aspects of the Theory of Syntax</em>,\nCambridge, MA: MIT Press. ",
                "Christiansen, Morten H. and Nick Chater, 1994,\n\u201cGeneralization and Connectionist Language Learning\u201d,\n<em>Mind &amp; Language</em>, 9(3): 273\u2013287.\ndoi:10.1111/j.1468-0017.1994.tb00226.x",
                "\u2013\u2013\u2013, 1999a, \u201cToward a Connectionist Model\nof Recursion in Human Linguistic Performance\u201d, <em>Cognitive\nScience</em>, 23(2): 157\u2013205.\ndoi:10.1207/s15516709cog2302_2",
                "\u2013\u2013\u2013, 1999b, \u201cConnectionist Natural\nLanguage Processing: The State of the Art\u201d, <em>Cognitive\nScience</em>, 23(4): 417\u2013437.\ndoi:10.1207/s15516709cog2304_2",
                "Churchland, Paul M., 1989, <em>A Neurocomputational Perspective:\nThe Nature of Mind and the Structure of Science</em>, Cambridge, MA:\nMIT Press.",
                "\u2013\u2013\u2013, 1995, <em>The Engine of Reason, the Seat of\nthe Soul: A Philosophical Journey into the Brain</em>, Cambridge, MA:\nMIT Press.",
                "\u2013\u2013\u2013, 1998, \u201cConceptual Similarity Across\nSensory and Neural Diversity: The Fodor/Lepore Challenge\nAnswered\u201d, <em>Journal of Philosophy</em>, 95(1): 5\u201332.\ndoi:10.5840/jphil19989514",
                "Clark, Andy, 1989, <em>Microcognition: Philosophy, Cognitive\nScience, and Parallel Distributed Processing</em>, (Explorations in\nCognitive Science), Cambridge, MA: MIT Press.",
                "\u2013\u2013\u2013, 1990 [1995], \u201cConnectionist\nMinds\u201d, <em>Proceedings of the Aristotelian Society</em>, 90:\n83\u2013102. Reprinted in MacDonald and MacDonald 1995:\n339\u2013356. doi:10.1093/aristotelian/90.1.83",
                "\u2013\u2013\u2013, 1993, <em>Associative Engines:\nConnectionism, Concepts, and Representational Change</em>, Cambridge,\nMA: MIT Press.",
                "\u2013\u2013\u2013, 2013, \u201cWhatever next? Predictive\nBrains, Situated Agents, and the Future of Cognitive Science\u201d,\n<em>Behavioral and Brain Sciences</em>, 36(3): 181\u2013204.\ndoi:10.1017/S0140525X12000477",
                "Clark, Andy and Rudi Lutz (eds.), 1992, <em>Connectionism in\nContext</em>, London: Springer London.\ndoi:10.1007/978-1-4471-1923-4",
                "Cotrell G.W. and S.L. Small, 1983, \u201cA Connectionist Scheme\nfor Modeling Word Sense Disambiguation\u201d, <em>Cognition and Brain\nTheory</em>, 6(1): 89\u2013120.",
                "Cummins, Robert, 1991, \u201cThe Role of Representation in\nConnectionist Explanations of Cognitive Capacities\u201d, in Ramsey,\nStich, and Rumelhart 1991: 91\u2013114.",
                "\u2013\u2013\u2013, 1996, \u201cSystematicity\u201d:,\n<em>Journal of Philosophy</em>, 93(12): 591\u2013614.\ndoi:10.2307/2941118",
                "Cummins, Robert and Georg Schwarz, 1991, \u201cConnectionism,\nComputation, and Cognition\u201d, in Horgan and Tienson 1991:\n60\u201373. doi:10.1007/978-94-011-3524-5_3",
                "Davies, Martin, 1989, \u201cConnectionism, Modularity, and Tacit\nKnowledge\u201d, <em>The British Journal for the Philosophy of\nScience</em>, 40(4): 541\u2013555. doi:10.1093/bjps/40.4.541",
                "\u2013\u2013\u2013, 1991, \u201cConcepts, Connectionism and\nthe Language of Thought\u201d, in Ramsey, Stich, and Rumelhart 1991:\n229\u2013257.",
                "Dinsmore, John (ed.), 1992, <em>The Symbolic and Connectionist\nParadigms: Closing the Gap</em>, Hillsdale, NJ: Erlbaum.",
                "Ehsan, Upol, Brent Harrison, Larry Chan, and Mark O. Riedl, 2018,\n\u201cRationalization: A Neural Machine Translation Approach to\nGenerating Natural Language Explanations\u201d, in <em>Proceedings of\nthe 2018 AAAI/ACM Conference on AI, Ethics, and Society (AIES\n\u201918)</em>, New Orleans, LA: ACM Press, 81\u201387.\ndoi:10.1145/3278721.3278736",
                "Eliasmith, Chris, 2007, \u201cHow to Build a Brain: From Function\nto Implementation\u201d, <em>Synthese</em>, 159(3): 373\u2013388.\ndoi:10.1007/s11229-007-9235-0",
                "\u2013\u2013\u2013, 2013, <em>How to Build a Brain: a Neural\nArchitecture for Biological Cognition</em>, New York: Oxford\nUniversity Press.",
                "Elman, Jeffrey L., 1991, \u201cDistributed Representations,\nSimple Recurrent Networks, and Grammatical Structure\u201d, in\nTouretzky 1991: 91\u2013122. doi:10.1007/978-1-4615-4008-3_5",
                "Elman, Jeffrey, Elizabeth Bates, Mark H. Johnson, Annette\nKarmiloff-Smith,Domenico Parisi, and Kim Plunkett, 1996,\n<em>Rethinking Innateness: A Connectionist Perspective on\nDevelopment</em>, Cambridge, MA: MIT Press. ",
                "Elsayed, Gamaleldin F., Shreya Shankar, Brian Cheung, Nicolas\nPapernot, Alexey Kurakin, Ian Goodfellow, and Jascha Sohl-Dickstein,\n2018, \u201cAdversarial Examples That Fool Both Computer Vision and\nTime-Limited Humans\u201d, in <em>Proceedings of the 32Nd\nInternational Conference on Neural Information Processing Systems,\n(NIPS\u201918)</em>, 31: 3914\u20133924.",
                "Fodor, Jerry A., 1988, <em>Psychosemantics: The Problem of Meaning\nin the Philosophy of Mind</em>, Cambridge, MA: MIT Press.",
                "\u2013\u2013\u2013, 1997, \u201cConnectionism and the Problem\nof Systematicity (Continued): Why Smolensky\u2019s Solution Still\nDoesn\u2019t Work\u201d, <em>Cognition</em>, 62(1): 109\u2013119.\ndoi:10.1016/S0010-0277(96)00780-9",
                "Fodor, Jerry and Ernest Lepore, 1992, <em>Holism: A\nShopper\u2019s Guide</em>, Cambridge: Blackwell.",
                "Fodor, Jerry and Ernie Lepore, 1999, \u201cAll at Sea in Semantic\nSpace: Churchland on Meaning Similarity\u201d, <em>Journal of\nPhilosophy</em>, 96(8): 381\u2013403. doi:10.5840/jphil199996818",
                "Fodor, Jerry and Brian P. McLaughlin, 1990, \u201cConnectionism\nand the Problem of Systematicity: Why Smolensky\u2019s Solution\nDoesn\u2019t Work\u201d, <em>Cognition</em>, 35(2): 183\u2013204.\ndoi:10.1016/0010-0277(90)90014-B",
                "Fodor, Jerry A. and Zenon W. Pylyshyn, 1988, \u201cConnectionism\nand Cognitive Architecture: A Critical Analysis\u201d,\n<em>Cognition</em>, 28(1\u20132): 3\u201371.\ndoi:10.1016/0010-0277(88)90031-5",
                "Friston, Karl, 2005, \u201cA Theory of Cortical Responses\u201d,\n<em>Philosophical Transactions of the Royal Society B: Biological\nSciences</em>, 360(1456): 815\u2013836.\ndoi:10.1098/rstb.2005.1622",
                "Friston, Karl J. and Klaas E. Stephan, 2007, \u201cFree-Energy\nand the Brain\u201d, <em>Synthese</em>, 159(3): 417\u2013458.\ndoi:10.1007/s11229-007-9237-y",
                "Fukushima, Kunihiko, 1980, \u201cNeocognitron: A Self-Organizing\nNeural Network Model for a Mechanism of Pattern Recognition Unaffected\nby Shift in Position\u201d, <em>Biological Cybernetics</em>, 36(4):\n193\u2013202. doi:10.1007/BF00344251",
                "Garfield, Jay L., 1997, \u201cMentalese Not Spoken Here:\nComputation, Cognition and Causation\u201d, <em>Philosophical\nPsychology</em>, 10(4): 413\u2013435.\ndoi:10.1080/09515089708573231",
                "Garson, James W., 1991, \u201cWhat Connectionists Cannot Do: The\nThreat to Classical AI\u201d, in Horgan and Tienson 1991:\n113\u2013142. doi:10.1007/978-94-011-3524-5_6",
                "\u2013\u2013\u2013, 1994, \u201cCognition without Classical\nArchitecture\u201d, <em>Synthese</em>, 100(2): 291\u2013305.\ndoi:10.1007/BF01063812",
                "\u2013\u2013\u2013, 1997, \u201cSyntax in a Dynamic\nBrain\u201d, <em>Synthese</em>, 110(3): 343\u2013355.",
                "Goodfellow, Ian, Yoshua Bengio, and Aaron Courville, 2016,\n<em>Deep Learning</em>, Cambridge, MA: MIT Press. ",
                "Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy, 2015,\n\u201cExplaining and Harnessing Adversarial Examples.\u201d, in\n<em>3rd International Conference on Learning Representations, ICLR\n2015</em>, San Diego, CA, May 7\u20139, 2015,\n <a href=\"http://arxiv.org/abs/1412.6572\" target=\"other\">available online</a>.",
                "Goodfellow, Ian J., Jean Pouget-Abadie, Mehdi Mirza, Bing Xu,\nDavid Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio,\n2014, \u201cGenerative Adversarial Nets\u201d, in <em>Proceedings of\nthe 27th International Conference on Neural Information Processing\nSystems, (NIPS\u201914)</em>, Cambridge, MA: MIT Press, 2:\n2672\u20132680.",
                "Goodman, Bryce and Seth Flaxman, 2017, \u201cEuropean Union\nRegulations on Algorithmic Decision-Making and a \u2018Right to\nExplanation\u2019\u201d, <em>AI Magazine</em>, 38(3): 50\u201357.\ndoi:10.1609/aimag.v38i3.2741",
                "Goodman, Nelson, 1955, <em>Fact, Fiction, and Forecast</em>,\nCambridge, MA: Harvard University Press.",
                "Grush, Rick, 2004, \u201cThe Emulation Theory of Representation:\nMotor Control, Imagery, and Perception\u201d, <em>Behavioral and\nBrain Sciences</em>, 27(3): 377\u2013396.\ndoi:10.1017/S0140525X04000093",
                "Guarini, Marcello, 2001, \u201cA Defence of Connectionism Against\nthe \u2018Syntactic\u2019 Argument\u201d, <em>Synthese</em>,\n128(3): 287\u2013317. doi:10.1023/A:1011905917986",
                "Hadley, Robert F., 1994a, \u201cSystematicity in Connectionist\nLanguage Learning\u201d, <em>Mind &amp; Language</em>, 9(3):\n247\u2013272. doi:10.1111/j.1468-0017.1994.tb00225.x",
                "\u2013\u2013\u2013, 1994b, \u201cSystematicity Revisited:\nReply to Christiansen and Chater and Niklasson and van Gelder\u201d,\n<em>Mind &amp; Language</em>, 9(4): 431\u2013444.\ndoi:10.1111/j.1468-0017.1994.tb00317.x",
                "\u2013\u2013\u2013, 1997a, \u201cExplaining Systematicity: A\nReply to Kenneth Aizawa\u201d, <em>Minds and Machines</em>, 7(4):\n571\u2013579. doi:10.1023/A:1008252322227",
                "\u2013\u2013\u2013, 1997b, \u201cCognition, Systematicity and\nNomic Necessity\u201d, <em>Mind &amp; Language</em>, 12(2):\n137\u2013153. doi:10.1111/j.1468-0017.1997.tb00066.x",
                "\u2013\u2013\u2013, 2004, \u201cOn The Proper Treatment of\nSemantic Systematicity\u201d, <em>Minds and Machines</em>, 14(2):\n145\u2013172. doi:10.1023/B:MIND.0000021693.67203.46",
                "Hadley, Robert F. and Michael B. Hayward, 1997, \u201cStrong\nSemantic Systematicity from Hebbian Connectionist Learning\u201d,\n<em>Minds and Machines</em>, 7(1): 1\u201337.\ndoi:10.1023/A:1008252408222",
                "Hanson, Stephen J. and Judy Kegl, 1987, \u201cPARSNIP: A\nConnectionist Network that Learns Natural Language Grammar from\nExposure to Natural Language Sentences\u201d, <em>Ninth Annual\nConference of the Cognitive Science Society</em>, Hillsdale, NJ:\nErlbaum, pp. 106\u2013119.",
                "Harman, Gilbert and Sanjeev Kulkarni, 2007, <em>Reliable\nReasoning: Induction and Statistical Learning Theory</em>, Cambridge\nMA: MIT Press. ",
                "Hatfield, Gary, 1991a, \u201cRepresentation in Perception and\nCognition: Connectionist Affordances\u201d, in Ramsey, Stich, and\nRumelhart 1991: 163\u2013195.",
                "\u2013\u2013\u2013, 1991b, \u201cRepresentation and\nRule-Instantiation in Connectionist Systems\u201d, in Horgan and\nTienson 1991: 90\u2013112. doi:10.1007/978-94-011-3524-5_5",
                "Hawthorne, John, 1989, \u201cOn the Compatibility of\nConnectionist and Classical Models\u201d, <em>Philosophical\nPsychology</em>, 2(1): 5\u201315. doi:10.1080/09515088908572956",
                "Haybron, Daniel M., 2000, \u201cThe Causal and Explanatory Role\nof Information Stored in Connectionist Networks\u201d, <em>Minds and\nMachines</em>, 10(3): 361\u2013380. doi:10.1023/A:1026545231550",
                "Hinton, Geoffrey E., 1990 [1991], \u201cMapping Part-Whole\nHierarchies into Connectionist Networks\u201d, <em>Artificial\nIntelligence</em>, 46(1\u20132): 47\u201375. Reprinted in Hinton\n1991: 47\u201376. doi:10.1016/0004-3702(90)90004-J",
                "\u2013\u2013\u2013 (ed.), 1991, <em>Connectionist Symbol\nProcessing</em>, Cambridge, MA: MIT Press.",
                "\u2013\u2013\u2013, 1992, \u201cHow Neural Networks Learn from\nExperience\u201d, <em>Scientific American</em>, 267(3):\n145\u2013151.",
                "\u2013\u2013\u2013, 2010, \u201cLearning to Represent Visual\nInput\u201d, <em>Philosophical Transactions of the Royal Society B:\nBiological Sciences</em>, 365(1537): 177\u2013184.\ndoi:10.1098/rstb.2009.0200",
                "Hinton, Geoffrey E., James L. McClelland, and David E. Rumelhart,\n1986, \u201cDistributed Representations\u201d, Rumelhart,\nMcClelland, and the PDP group 1986: chapter 3.",
                "Hohwy, Jakob, 2012, \u201cAttention and Conscious Perception in\nthe Hypothesis Testing Brain\u201d, <em>Frontiers in Psychology</em>,\n3(96): 1\u201314. doi:10.3389/fpsyg.2012.00096",
                "Hong, Ha, Daniel L K Yamins, Najib J Majaj, and James J DiCarlo,\n2016, \u201cExplicit Information for Category-Orthogonal Object\nProperties Increases along the Ventral Stream\u201d, <em>Nature\nNeuroscience</em>, 19(4): 613\u2013622. doi:10.1038/nn.4247",
                "Horgan, Terence E. and John Tienson, 1989, \u201cRepresentations\nwithout Rules\u201d, <em>Philosophical Topics</em>, 17(1):\n147\u2013174.",
                "\u2013\u2013\u2013, 1990, \u201cSoft Laws\u201d, <em>Midwest\nStudies In Philosophy</em>, 15: 256\u2013279.\ndoi:10.1111/j.1475-4975.1990.tb00217.x",
                "\u2013\u2013\u2013 (eds.), 1991, <em>Connectionism and the\nPhilosophy of Mind</em>, Dordrecht: Kluwer.\ndoi:10.1007/978-94-011-3524-5",
                "\u2013\u2013\u2013, 1996, <em>Connectionism and the Philosophy\nof Psychology</em>, Cambridge, MA: MIT Press.",
                "Hosoya, Toshihiko, Stephen A. Baccus, and Markus Meister, 2005,\n\u201cDynamic Predictive Coding by the Retina\u201d,\n\n<em>Nature</em>, 436(7047): 71\u201377. doi:10.1038/nature03689",
                "Huang, Yanping and Rajesh P. N. Rao, 2011, \u201cPredictive\nCoding\u201d, <em>Wiley Interdisciplinary Reviews: Cognitive\nScience</em>, 2(5): 580\u2013593. doi:10.1002/wcs.142",
                "Hubel, David H. and Torsten N. Wiesel, 1965, \u201cReceptive\nFields and Functional Architecture in Two Nonstriate Visual Areas (18\nand 19) of the Cat\u201d, <em>Journal of Neurophysiology</em>, 28(2):\n229\u2013289. doi:10.1152/jn.1965.28.2.229",
                "Jansen, Peter A. and Scott Watter, 2012, \u201cStrong\nSystematicity through Sensorimotor Conceptual Grounding: An\nUnsupervised, Developmental Approach to Connectionist Sentence\nProcessing\u201d, <em>Connection Science</em>, 24(1): 25\u201355.\ndoi:10.1080/09540091.2012.664121",
                "Johnson, Kent, 2004, \u201cOn the Systematicity of Language and\nThought\u201d:, <em>Journal of Philosophy</em>, 101(3):\n111\u2013139. doi:10.5840/jphil2004101321",
                "Jones, Matt and Bradley C. Love, 2011, \u201cBayesian\nFundamentalism or Enlightenment? On the Explanatory Status and\nTheoretical Contributions of Bayesian Models of Cognition\u201d,\n<em>Behavioral and Brain Sciences</em>, 34(4): 169\u2013188.\ndoi:10.1017/S0140525X10003134",
                "Khaligh-Razavi, Seyed-Mahdi and Nikolaus Kriegeskorte, 2014,\n\u201cDeep Supervised, but Not Unsupervised, Models May Explain IT\nCortical Representation\u201d, <em>PLoS Computational Biology</em>,\n10(11): e1003915. doi:10.1371/journal.pcbi.1003915",
                "Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton, 2012,\n\u201cImagenet Classification with Deep Convolutional Neural\nNetworks\u201d, <em>Advances in Neural Information Processing\nSystems</em>, 25: 1097\u20131105.",
                "Kubilius, Jonas, Stefania Bracci, and Hans P. Op de Beeck, 2016,\n\u201cDeep Neural Networks as a Computational Model for Human Shape\nSensitivity\u201d, <em>PLOS Computational Biology</em>, 12(4):\ne1004896. doi:10.1371/journal.pcbi.1004896",
                "Laakso, Aarre and Garrison Cottrell, 2000, \u201cContent and\nCluster Analysis: Assessing Representational Similarity in Neural\nSystems\u201d, <em>Philosophical Psychology</em>, 13(1): 47\u201376.\ndoi:10.1080/09515080050002726",
                "Lake, Brenden M., Ruslan Salakhutdinov, and Joshua B. Tenenbaum,\n2015, \u201cHuman-Level Concept Learning through Probabilistic\nProgram Induction\u201d, <em>Science</em>, 350(6266):\n1332\u20131338. doi:10.1126/science.aab3050",
                "Lake, Brenden M., Wojciech Zaremba, Rob Fergus, and Todd M.\nGureckis, 2015, \u201cDeep Neural Networks Predict Category\nTypicality Ratings for Images\u201d, <em>Proceedings of the 37th\nAnnual Cognitive Science Society</em>, Pasadena, CA, 22\u201325 July\n2015, <a href=\"https://mindmodeling.org/cogsci2015/papers/0219/\" target=\"other\">available online</a>.",
                "Lillicrap, Timothy P., Daniel Cownden, Douglas B. Tweed, and Colin\nJ. Akerman, 2016, \u201cRandom Synaptic Feedback Weights Support\nError Backpropagation for Deep Learning\u201d, <em>Nature\nCommunications</em>, 7(1): 13276. doi:10.1038/ncomms13276",
                "Loula, Jo\u00e3o, Marco Baroni, and Brenden Lake, 2018,\n\u201cRearranging the Familiar: Testing Compositional Generalization\nin Recurrent Networks\u201d, in <em>Proceedings of the 2018 EMNLP\nWorkshop BlackboxNLP: Analyzing and Interpreting Neural Networks for\nNLP</em>, Brussels, Belgium: Association for Computational\nLinguistics, 108\u2013114. doi:10.18653/v1/W18-5413",
                "MacDonald, Cynthia and Graham MacDonald (eds), 1995,\n<em>Connectionism</em>, (Debates on Psychological Explanation, 2),\nOxford: Blackwell.",
                "Matthews, Robert J., 1997, \u201cCan Connectionists Explain\nSystematicity?\u201d, <em>Mind &amp; Language</em>, 12(2):\n154\u2013177. doi:10.1111/j.1468-0017.1997.tb00067.x",
                "Marcus, Gary F., 1998, \u201cRethinking Eliminative\nConnectionism\u201d, <em>Cognitive Psychology</em>, 37(3):\n243\u2013282. doi:10.1006/cogp.1998.0694",
                "\u2013\u2013\u2013, 2001, <em>The Algebraic Mind: Integrating\nConnectionism and Cognitive Science</em>, Cambridge, MA: MIT\nPress.",
                "McClelland, James L and Jeffrey L Elman, 1986, \u201cThe TRACE\nModel of Speech Perception\u201d, <em>Cognitive Psychology</em>,\n18(1): 1\u201386. doi:10.1016/0010-0285(86)90015-0",
                "McClelland, James L., David E. Rumelhart, and the PDP Research\nGroup (ed.), 1986, <em>Parallel Distributed Processing, Volume II:\nExplorations in the Microstructure of Cognition: Psychological and\nBiological Models</em>, Cambridge, MA: MIT Press.",
                "McLaughlin, Brian P., 1993, \u201cThe Connectionism/Classicism\nBattle to Win Souls\u201d, <em>Philosophical Studies</em>, 71(2):\n163\u2013190. doi:10.1007/BF00989855",
                "Miikkulainen, Risto, 1993, <em>Subsymbolic Natural Language\nProcessing: An Integrated Model of Scripts, Lexicon, and Memory</em>,\nCambridge, MA: MIT Press.",
                "Miikkulainen, Risto and Michael G. Dyer, 1991, \u201cNatural\nLanguage Processing With Modular Pdp Networks and Distributed\nLexicon\u201d, <em>Cognitive Science</em>, 15(3): 343\u2013399.\ndoi:10.1207/s15516709cog1503_2",
                "Miracchi, Lisa, 2019, \u201cA Competence Framework for Artificial\nIntelligence Research\u201d, <em>Philosophical Psychology</em>,\n32(5): 588\u2013633. doi:10.1080/09515089.2019.1607692",
                "Montavon, Gr\u00e9goire, Wojciech Samek, and Klaus-Robert\nM\u00fcller, 2018, \u201cMethods for Interpreting and Understanding\nDeep Neural Networks\u201d, <em>Digital Signal Processing</em>, 73:\n1\u201315. doi:10.1016/j.dsp.2017.10.011",
                "Mont\u00fafar, Guido, Razvan Pascanu, Kyunghyun Cho, and Yoshua\nBengio, 2014, \u201cOn the Number of Linear Regions of Deep Neural\nNetworks\u201d, in <em>Proceedings of the 27th International\nConference on Neural Information Processing Systems\n(NIPS\u201914)</em>, Cambridge, MA: MIT Press, 2: 2924\u20132932.\n",
                "Morris, William C., Garrison W. Cottrell, and Jeffrey Elman, 2000,\n\u201cA Connectionist Simulation of the Empirical Acquisition of\nGrammatical Relations\u201d, in Wermter and Sun 2000:\n1778:175\u2013193. doi:10.1007/10719871_12",
                "Nguyen, Anh, Jason Yosinski, Jeff Clune, 2015, \u201cDeep Neural\nNetworks Are Easily Fooled: High Confidence Predictions for\nUnrecognizable Images\u201d, <em>Proceedings of the 28th IEEE\nConference on Computer Vision and Pattern Recognition (CVPR\n2015)</em>, 427\u2013436, \n <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Nguyen_Deep_Neural_Networks_2015_CVPR_paper.html\" target=\"other\">available online</a>.",
                "Niklasson, Lars F. and Tim van Gelder, 1994, \u201cOn Being\nSystematically Connectionist\u201d, <em>Mind &amp; Language</em>,\n9(3): 288\u2013302. doi:10.1111/j.1468-0017.1994.tb00227.x",
                "O\u2019Reilly, Randall C., 1996, \u201cBiologically Plausible\nError-Driven Learning Using Local Activation Differences: The\nGeneralized Recirculation Algorithm\u201d, <em>Neural\nComputation</em>, 8(5): 895\u2013938.\ndoi:10.1162/neco.1996.8.5.895",
                "Phillips, Steven, 2002, \u201cDoes Classicism Explain\nUniversality?\u201d, <em>Minds and Machines</em>, 12(3):\n423\u2013434. doi:10.1023/A:1016160512967",
                "Pinker, Steven and Jacques Mehler (eds.), 1988, <em>Connections\nand Symbols</em>, Cambridge, MA: MIT Press.",
                "Pinker, Steven and Alan Prince, 1988, \u201cOn Language and\nConnectionism: Analysis of a Parallel Distributed Processing Model of\nLanguage Acquisition\u201d, <em>Cognition</em>, 28(1\u20132):\n73\u2013193. doi:10.1016/0010-0277(88)90032-7",
                "Pollack, Jordan B., 1989, \u201cImplications of Recursive\nDistributed Representations\u201d, in Touretzky 1989: 527\u2013535,\n <a href=\"http://papers.nips.cc/paper/114-implications-of-recursive-distributed-representations\" target=\"other\">available online</a>.",
                "\u2013\u2013\u2013, 1991, \u201cInduction of Dynamical\nRecognizers\u201d, in Touretzky 1991: 123\u2013148.\ndoi:10.1007/978-1-4615-4008-3_6",
                "Pollack, Jordan B., 1990 [1991], \u201cRecursive Distributed\nRepresentations\u201d, <em>Artificial Intelligence</em>,\n46(1\u20132): 77\u2013105. Reprinted in Hinton 1991: 77\u2013106.\ndoi:10.1016/0004-3702(90)90005-K",
                "Port, Robert F., 1990, \u201cRepresentation and Recognition of\nTemporal Patterns\u201d, <em>Connection Science</em>, 2(1\u20132):\n151\u2013176. doi:10.1080/09540099008915667",
                "Port, Robert F. and Timothy van Gelder, 1991, \u201cRepresenting\nAspects of Language\u201d, <em>Proceedings of the Thirteenth Annual\nConference of the Cognitive Science Society</em>, Hillsdale, N.J.:\nErlbaum, 487\u2013492,\n <a href=\"http://mindmodeling.org/cogscihistorical/cogsci_13.pdf\" target=\"other\">available online</a>.",
                "Quine, W. V., 1969, \u201cNatural Kinds\u201d, in <em>Essays in\nHonor of Carl G. Hempel</em>, Nicholas Rescher (ed.), Dordrecht:\nSpringer Netherlands, 5\u201323. doi:10.1007/978-94-017-1466-2_2",
                "Raghu, Maithra, Ben Poole, Jon Kleinberg, Surya Ganguli, and\nJascha Sohl-Dickstein, 2017, \u201cOn the Expressive Power of Deep\nNeural Networks\u201d, in <em>Proceedings of the 34th International\nConference on Machine Learning</em>, 70: 2847\u20132854,\n <a href=\"http://proceedings.mlr.press/v70/raghu17a.html\" target=\"other\">available online</a>.",
                "Ramsey, William, 1997, \u201cDo Connectionist Representations\nEarn Their Explanatory Keep?\u201d, <em>Mind &amp; Language</em>,\n12(1): 34\u201366. doi:10.1111/j.1468-0017.1997.tb00061.x",
                "Ramsey, William, Stephen P. Stich, and Joseph Garon, 1991,\n\u201cConnectionism, Eliminativism, and the Future of Folk\nPsychology\u201d, in Ramsey, Stich, and Rumelhart 1991:\n199\u2013228.",
                "Ramsey, William, Stephen P. Stich, and David E. Rumelhart, 1991,\n<em>Philosophy and Connectionist Theory</em>, Hillsdale, N.J.:\nErlbaum.",
                "Rao, Rajesh P. N. and Dana H. Ballard, 1999, \u201cPredictive\nCoding in the Visual Cortex: A Functional Interpretation of Some\nExtra-Classical Receptive-Field Effects\u201d, <em>Nature\nNeuroscience</em>, 2(1): 79\u201387. doi:10.1038/4580",
                "Rohde, Douglas L. T. and David C. Plaut, 2003,\n\u201cConnectionist Models of Language Processing\u201d,\n<em>Cognitive Studies</em> (Japan), 10(1): 10\u201328.\ndoi:10.11225/jcss.10.10",
                "Roth, Martin, 2005, \u201cProgram Execution in Connectionist\nNetworks\u201d, <em>Mind &amp; Language</em>, 20(4): 448\u2013467.\ndoi:10.1111/j.0268-1064.2005.00295.x",
                "Rumelhart, David E. and James L. McClelland, 1986, \u201cOn\nLearning the Past Tenses of English Verbs\u201d, in McClelland,\nRumelhart, and the PDP group 1986: 216\u2013271.",
                "Rumelhart, David E., James L. McClelland, and the PDP Research\nGroup (eds), 1986, <em>Parallel Distributed Processing, Volume 1:\nExplorations in the Microstructure of Cognition: Foundations</em>,\nCambridge, MA: MIT Press.",
                "Sadler, Matthew and Natasha Regan, 2019, <em>Game Changer:\nAlphaZero\u2019s Groundbreaking Chess Strategies and the Promise of\nAI</em>, Alkmaar: New in Chess.",
                "Schmidhuber, J\u00fcrgen, 2015, \u201cDeep Learning in Neural\nNetworks: An Overview\u201d, <em>Neural Networks</em>, 61:\n85\u2013117. doi:10.1016/j.neunet.2014.09.003",
                "Schwarz, Georg, 1992, \u201cConnectionism, Processing,\nMemory\u201d, <em>Connection Science</em>, 4(3\u20134):\n207\u2013226. doi:10.1080/09540099208946616",
                "Sejnowski, Terrence J. and Charles R. Rosenberg, 1987,\n\u201cParallel Networks that Learn to Pronounce English Text\u201d,\n<em>Complex Systems</em>, 1(1): 145\u2013168,\n <a href=\"https://www.complex-systems.com/abstracts/v01_i01_a10/\" target=\"other\">available online</a>.",
                "Servan-Schreiber, David, Axel Cleeremans, and James L. McClelland,\n1991, \u201cGraded State Machines: The Representation of Temporal\nContingencies in Simple Recurrent Networks\u201d, in Touretzky 1991:\n57\u201389. doi:10.1007/978-1-4615-4008-3_4",
                "Shastri, Lokendra and Venkat Ajjanagadde, 1993, \u201cFrom Simple\nAssociations to Systematic Reasoning: A Connectionist Representation\nof Rules, Variables and Dynamic Bindings Using Temporal\nSynchrony\u201d, <em>Behavioral and Brain Sciences</em>, 16(3):\n417\u2013451. doi:10.1017/S0140525X00030910",
                "Shea, Nicholas, 2007, \u201cContent and Its Vehicles in\nConnectionist Systems\u201d, <em>Mind &amp; Language</em>, 22(3):\n246\u2013269. doi:10.1111/j.1468-0017.2007.00308.x",
                "Shevlin, Henry and Marta Halina, 2019, \u201cApply Rich\nPsychological Terms in AI with Care\u201d, <em>Nature Machine\nIntelligence</em>, 1(4): 165\u2013167.\ndoi:10.1038/s42256-019-0039-y",
                "Shultz, Thomas R. and Alan C. Bale, 2001, \u201cNeural Network\nSimulation of Infant Familiarization to Artificial Sentences\u201d,\n<em>Infancy</em>, 2(4): 501\u2013536.",
                "\u2013\u2013\u2013, 2006, \u201cNeural Networks Discover a\nNear-Identity Relation to Distinguish Simple Syntactic Forms\u201d,\n<em>Minds and Machines</em>, 16(2): 107\u2013139.\ndoi:10.1007/s11023-006-9029-z",
                "Silver, David, Thomas Hubert, Julian Schrittwieser, Ioannis\nAntonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, et al., 2018,\n\u201cA General Reinforcement Learning Algorithm That Masters Chess,\nShogi, and Go through Self-Play\u201d, <em>Science</em>, 362(6419):\n1140\u20131144. doi:10.1126/science.aar6404",
                "Smolensky, Paul, 1987, \u201cThe Constituent Structure of\nConnectionist Mental States: A Reply to Fodor and Pylyshyn\u201d,\n<em>The Southern Journal of Philosophy</em>, 26(S1): 137\u2013161.\ndoi:10.1111/j.2041-6962.1988.tb00470.x",
                "\u2013\u2013\u2013, 1988, \u201cOn the Proper Treatment of\nConnectionism\u201d, <em>Behavioral and Brain Sciences</em>, 11(1):\n1\u201323. doi:10.1017/S0140525X00052432",
                "\u2013\u2013\u2013, 1990 [1991], \u201cTensor Product Variable\nBinding and the Representation of Symbolic Structures in Connectionist\nSystems\u201d, <em>Artificial Intelligence</em>, 46(1\u20132):\n159\u2013216. Reprinted in Hinton 1991: 159\u2013216.\ndoi:10.1016/0004-3702(90)90007-M",
                "\u2013\u2013\u2013, 1995, \u201cConstituent Structure and\nExplanation in an Integrated Connectionist/Symbolic Cognitive\nArchitecture\u201d, in MacDonald and MacDonald 1995: .",
                "St. John, Mark F. and James L. McClelland, 1990 [1991],\n\u201cLearning and Applying Contextual Constraints in Sentence\nComprehension\u201d, <em>Artificial Intelligence</em>, 46(1\u20132):\n217\u2013257. Reprinted in Hinton 1991: 217\u2013257\ndoi:10.1016/0004-3702(90)90008-N",
                "Tomberlin, James E. (ed.), 1995, <em>Philosophical Perspectives 9:\nAI, Connectionism and Philosophical Psychology</em>, Atascadero:\nRidgeview Press.",
                "Touretzky, David S. (ed.), 1989, <em>Advances in Neural\nInformation Processing Systems I</em>, San Mateo, CA: Kaufmann,\n <a href=\"http://papers.nips.cc/book/advances-in-neural-information-processing-systems-1-1988\" target=\"other\">available online</a>.",
                "\u2013\u2013\u2013 (ed.), 1990, <em>Advances in Neural\nInformation Processing Systems II</em>, San Mateo, CA: Kaufmann.",
                "\u2013\u2013\u2013 (ed.), 1991, <em>Connectionist Approaches to\nLanguage Learning</em>, Boston, MA: Springer US.\ndoi:10.1007/978-1-4615-4008-3",
                "Touretzky, David S., Geoffrey E. Hinton, and Terrence Joseph\nSejnowski (eds), 1988, <em>Proceedings of the 1988 Connectionist\nModels Summer School</em>, San Mateo, CA: Kaufmann.",
                "Van Gelder, Tim, 1990, \u201cCompositionality: A Connectionist\nVariation on a Classical Theme\u201d, <em>Cognitive Science</em>,\n14(3): 355\u2013384. doi:10.1016/0364-0213(90)90017-Q",
                "\u2013\u2013\u2013, 1991, \u201cWhat is the \u2018D\u2019 in\nPDP?\u201d in Ramsey, Stich, and Rumelhart 1991: 33\u201359.",
                "Van Gelder, Timothy and Robert Port, 1993, \u201cBeyond Symbolic:\nProlegomena to a Kama-Sutra of Compositionality\u201d, in Vasant G\nHonavar, Leonard Uhr (eds.), <em>Symbol Processing and Connectionist\nModels in AI and Cognition: Steps Towards Integration</em>, Boston:\nAcademic Press.",
                "Vilcu, Marius and Robert F. Hadley, 2005, \u201cTwo Apparent\n\u2018Counterexamples\u2019 to Marcus: A Closer Look\u201d,\n<em>Minds and Machines</em>, 15(3\u20134): 359\u2013382.\ndoi:10.1007/s11023-005-9000-4",
                "Von Eckardt, Barbara, 2003, \u201cThe Explanatory Need for Mental\nRepresentations in Cognitive Science\u201d, <em>Mind &amp;\nLanguage</em>, 18(4): 427\u2013439. doi:10.1111/1468-0017.00235",
                "\u2013\u2013\u2013, 2005, \u201cConnectionism and the\nPropositional Attitudes\u201d, in Christina Erneling and David Martel\nJohnson (eds.), <em>The Mind as a Scientific Object: Between Brain and\nCulture</em>, New York: Oxford University Press.",
                "Waltz, David L. and Jordan B. Pollack, 1985, \u201cMassively\nParallel Parsing: A Strongly Interactive Model of Natural Language\nInterpretation*\u201d, <em>Cognitive Science</em>, 9(1): 51\u201374.\ndoi:10.1207/s15516709cog0901_4",
                "Wermter, Stefan and Ron Sun (eds.), 2000, <em>Hybrid Neural\nSystems</em>, (Lecture Notes in Computer Science 1778), Berlin,\nHeidelberg: Springer Berlin Heidelberg. doi:10.1007/10719871",
                "Yamins, Daniel L. K. and James J. DiCarlo, 2016, \u201cUsing\nGoal-Driven Deep Learning Models to Understand Sensory Cortex\u201d,\n<em>Nature Neuroscience</em>, 19(3): 356\u2013365.\ndoi:10.1038/nn.4244",
                "Yosinski, Jason, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod\nLipson, 2015, \u201cUnderstanding Neural Networks Through Deep\nVisualization\u201d, <em>Deep Learning Workshop, 31st International\nConference on Machine Learning</em>, Lille, France,\n <a href=\"http://yosinski.com/media/papers/Yosinski__2015__ICML_DL__Understanding_Neural_Networks_Through_Deep_Visualization__.pdf\" target=\"other\">available online</a>.",
                "Zhou, Zhenglong and Chaz Firestone, 2019, \u201cHumans Can\nDecipher Adversarial Images\u201d, <em>Nature Communications</em>,\n10(1): 1334. doi:10.1038/s41467-019-08931-6"
            ]
        },
        "raw_text": "<div id=\"bibliography\">\n<h2><a name=\"Bib\">Bibliography</a></h2>\n<ul class=\"hanging\">\n<li>Aizawa, Kenneth, 1994, \u201cRepresentations without Rules,\nConnectionism and the Syntactic Argument\u201d, <em>Synthese</em>,\n101(3): 465\u2013492. doi:10.1007/BF01063898</li>\n<li>\u2013\u2013\u2013, 1997a, \u201cExhibiting versus Explaining\nSystematicity: A Reply to Hadley and Hayward\u201d, <em>Minds and\nMachines</em>, 7(1): 39\u201355. doi:10.1023/A:1008203312152</li>\n<li>\u2013\u2013\u2013, 1997b, \u201cExplaining\nSystematicity\u201d, <em>Mind &amp; Language</em>, 12(2):\n115\u2013136. doi:10.1111/j.1468-0017.1997.tb00065.x</li>\n<li>\u2013\u2013\u2013, 2003, <em>The Systematicity Arguments</em>,\nDordrecht: Kluwer.</li>\n<li>\u2013\u2013\u2013, 2014, \u201cA Tough Time to be Talking\nSystematicity\u201d, in Calvo and Symons 2014: 77\u2013101.</li>\n<li>Bechtel, William, 1987, \u201cConnectionism and the Philosophy of\nMind: An Overview\u201d, <em>The Southern Journal of Philosophy</em>,\n26(S1): 17\u201341. doi:10.1111/j.2041-6962.1988.tb00461.x</li>\n<li>\u2013\u2013\u2013, 1988, \u201cConnectionism and Rules and\nRepresentation Systems: Are They Compatible?\u201d, <em>Philosophical\nPsychology</em>, 1(1): 5\u201316. doi:10.1080/09515088808572922</li>\n<li>Bechtel, William and Adele Abrahamsen, 1990, <em>Connectionism and\nthe Mind: An Introduction to Parallel Processing in Networks</em>,\nCambridge, MA: Blackwell.</li>\n<li>Bengio, Yoshua and Olivier Delalleau, 2011, \u201cOn the\nExpressive Power of Deep Architectures\u201d, in <em>International\nConference on Algorithmic Learning Theory (ALT 2011)</em>, Jyrki\nKivinen, Csaba Szepesv\u00e1ri, Esko Ukkonen, and Thomas Zeugmann\n(eds.) (Lecture Notes in Computer Science 6925), Berlin, Heidelberg:\nSpringer Berlin Heidelberg, 18\u201336.\ndoi:10.1007/978-3-642-24412-4_3</li>\n<li>Bengio, Yoshua, Thomas Mesnard, Asja Fischer, Saizheng Zhang, and\nYuhuai Wu, 2017, \u201cSTDP-Compatible Approximation of\nBackpropagation in an Energy-Based Model\u201d, <em>Neural\nComputation</em>, 29(3): 555\u2013577. doi:10.1162/NECO_a_00934</li>\n<li>Bod\u00e9n, Mikael and Lars Niklasson, 2000, \u201cSemantic\nSystematicity and Context in Connectionist Networks\u201d,\n<em>Connection Science</em>, 12(2): 111\u2013142.\ndoi:10.1080/09540090050129754</li>\n<li>Buckner, Cameron, 2018, \u201cEmpiricism without Magic:\nTransformational Abstraction in Deep Convolutional Neural\nNetworks\u201d, <em>Synthese</em>, 195(12): 5339\u20135372.\ndoi:10.1007/s11229-018-01949-1</li>\n<li>Butler, Keith, 1991, \u201cTowards a Connectionist Cognitive\nArchitecture\u201d, <em>Mind &amp; Language</em>, 6(3):\n252\u2013272. doi:10.1111/j.1468-0017.1991.tb00191.x</li>\n<li>Calvo Garz\u00f3n, Francisco, 2003, \u201cConnectionist\nSemantics and the Collateral Information Challenge\u201d, <em>Mind\n&amp; Language</em>, 18(1): 77\u201394.\ndoi:10.1111/1468-0017.00215</li>\n<li>Calvo, Paco and John Symons, 2014, <em>The Architecture of\nCognition: Rethinking Fodor and Pylyshyn\u2019s Systematicity\nChallenge</em>, Cambridge: MIT Press.</li>\n<li>Chalmers, David J., 1990, \u201cSyntactic Transformations on\nDistributed Representations\u201d, <em>Connection Science</em>,\n2(1\u20132): 53\u201362. doi:10.1080/09540099008915662</li>\n<li>\u2013\u2013\u2013, 1993, \u201cConnectionism and\nCompositionality: Why Fodor and Pylyshyn Were Wrong\u201d,\n<em>Philosophical Psychology</em>, 6(3): 305\u2013319.\ndoi:10.1080/09515089308573094</li>\n<li>Chomsky, Noam, 1965, <em>Aspects of the Theory of Syntax</em>,\nCambridge, MA: MIT Press. </li>\n<li>Christiansen, Morten H. and Nick Chater, 1994,\n\u201cGeneralization and Connectionist Language Learning\u201d,\n<em>Mind &amp; Language</em>, 9(3): 273\u2013287.\ndoi:10.1111/j.1468-0017.1994.tb00226.x</li>\n<li>\u2013\u2013\u2013, 1999a, \u201cToward a Connectionist Model\nof Recursion in Human Linguistic Performance\u201d, <em>Cognitive\nScience</em>, 23(2): 157\u2013205.\ndoi:10.1207/s15516709cog2302_2</li>\n<li>\u2013\u2013\u2013, 1999b, \u201cConnectionist Natural\nLanguage Processing: The State of the Art\u201d, <em>Cognitive\nScience</em>, 23(4): 417\u2013437.\ndoi:10.1207/s15516709cog2304_2</li>\n<li>Churchland, Paul M., 1989, <em>A Neurocomputational Perspective:\nThe Nature of Mind and the Structure of Science</em>, Cambridge, MA:\nMIT Press.</li>\n<li>\u2013\u2013\u2013, 1995, <em>The Engine of Reason, the Seat of\nthe Soul: A Philosophical Journey into the Brain</em>, Cambridge, MA:\nMIT Press.</li>\n<li>\u2013\u2013\u2013, 1998, \u201cConceptual Similarity Across\nSensory and Neural Diversity: The Fodor/Lepore Challenge\nAnswered\u201d, <em>Journal of Philosophy</em>, 95(1): 5\u201332.\ndoi:10.5840/jphil19989514</li>\n<li>Clark, Andy, 1989, <em>Microcognition: Philosophy, Cognitive\nScience, and Parallel Distributed Processing</em>, (Explorations in\nCognitive Science), Cambridge, MA: MIT Press.</li>\n<li>\u2013\u2013\u2013, 1990 [1995], \u201cConnectionist\nMinds\u201d, <em>Proceedings of the Aristotelian Society</em>, 90:\n83\u2013102. Reprinted in MacDonald and MacDonald 1995:\n339\u2013356. doi:10.1093/aristotelian/90.1.83</li>\n<li>\u2013\u2013\u2013, 1993, <em>Associative Engines:\nConnectionism, Concepts, and Representational Change</em>, Cambridge,\nMA: MIT Press.</li>\n<li>\u2013\u2013\u2013, 2013, \u201cWhatever next? Predictive\nBrains, Situated Agents, and the Future of Cognitive Science\u201d,\n<em>Behavioral and Brain Sciences</em>, 36(3): 181\u2013204.\ndoi:10.1017/S0140525X12000477</li>\n<li>Clark, Andy and Rudi Lutz (eds.), 1992, <em>Connectionism in\nContext</em>, London: Springer London.\ndoi:10.1007/978-1-4471-1923-4</li>\n<li>Cotrell G.W. and S.L. Small, 1983, \u201cA Connectionist Scheme\nfor Modeling Word Sense Disambiguation\u201d, <em>Cognition and Brain\nTheory</em>, 6(1): 89\u2013120.</li>\n<li>Cummins, Robert, 1991, \u201cThe Role of Representation in\nConnectionist Explanations of Cognitive Capacities\u201d, in Ramsey,\nStich, and Rumelhart 1991: 91\u2013114.</li>\n<li>\u2013\u2013\u2013, 1996, \u201cSystematicity\u201d:,\n<em>Journal of Philosophy</em>, 93(12): 591\u2013614.\ndoi:10.2307/2941118</li>\n<li>Cummins, Robert and Georg Schwarz, 1991, \u201cConnectionism,\nComputation, and Cognition\u201d, in Horgan and Tienson 1991:\n60\u201373. doi:10.1007/978-94-011-3524-5_3</li>\n<li>Davies, Martin, 1989, \u201cConnectionism, Modularity, and Tacit\nKnowledge\u201d, <em>The British Journal for the Philosophy of\nScience</em>, 40(4): 541\u2013555. doi:10.1093/bjps/40.4.541</li>\n<li>\u2013\u2013\u2013, 1991, \u201cConcepts, Connectionism and\nthe Language of Thought\u201d, in Ramsey, Stich, and Rumelhart 1991:\n229\u2013257.</li>\n<li>Dinsmore, John (ed.), 1992, <em>The Symbolic and Connectionist\nParadigms: Closing the Gap</em>, Hillsdale, NJ: Erlbaum.</li>\n<li>Ehsan, Upol, Brent Harrison, Larry Chan, and Mark O. Riedl, 2018,\n\u201cRationalization: A Neural Machine Translation Approach to\nGenerating Natural Language Explanations\u201d, in <em>Proceedings of\nthe 2018 AAAI/ACM Conference on AI, Ethics, and Society (AIES\n\u201918)</em>, New Orleans, LA: ACM Press, 81\u201387.\ndoi:10.1145/3278721.3278736</li>\n<li>Eliasmith, Chris, 2007, \u201cHow to Build a Brain: From Function\nto Implementation\u201d, <em>Synthese</em>, 159(3): 373\u2013388.\ndoi:10.1007/s11229-007-9235-0</li>\n<li>\u2013\u2013\u2013, 2013, <em>How to Build a Brain: a Neural\nArchitecture for Biological Cognition</em>, New York: Oxford\nUniversity Press.</li>\n<li>Elman, Jeffrey L., 1991, \u201cDistributed Representations,\nSimple Recurrent Networks, and Grammatical Structure\u201d, in\nTouretzky 1991: 91\u2013122. doi:10.1007/978-1-4615-4008-3_5</li>\n<li>Elman, Jeffrey, Elizabeth Bates, Mark H. Johnson, Annette\nKarmiloff-Smith,Domenico Parisi, and Kim Plunkett, 1996,\n<em>Rethinking Innateness: A Connectionist Perspective on\nDevelopment</em>, Cambridge, MA: MIT Press. </li>\n<li>Elsayed, Gamaleldin F., Shreya Shankar, Brian Cheung, Nicolas\nPapernot, Alexey Kurakin, Ian Goodfellow, and Jascha Sohl-Dickstein,\n2018, \u201cAdversarial Examples That Fool Both Computer Vision and\nTime-Limited Humans\u201d, in <em>Proceedings of the 32Nd\nInternational Conference on Neural Information Processing Systems,\n(NIPS\u201918)</em>, 31: 3914\u20133924.</li>\n<li>Fodor, Jerry A., 1988, <em>Psychosemantics: The Problem of Meaning\nin the Philosophy of Mind</em>, Cambridge, MA: MIT Press.</li>\n<li>\u2013\u2013\u2013, 1997, \u201cConnectionism and the Problem\nof Systematicity (Continued): Why Smolensky\u2019s Solution Still\nDoesn\u2019t Work\u201d, <em>Cognition</em>, 62(1): 109\u2013119.\ndoi:10.1016/S0010-0277(96)00780-9</li>\n<li>Fodor, Jerry and Ernest Lepore, 1992, <em>Holism: A\nShopper\u2019s Guide</em>, Cambridge: Blackwell.</li>\n<li>Fodor, Jerry and Ernie Lepore, 1999, \u201cAll at Sea in Semantic\nSpace: Churchland on Meaning Similarity\u201d, <em>Journal of\nPhilosophy</em>, 96(8): 381\u2013403. doi:10.5840/jphil199996818</li>\n<li>Fodor, Jerry and Brian P. McLaughlin, 1990, \u201cConnectionism\nand the Problem of Systematicity: Why Smolensky\u2019s Solution\nDoesn\u2019t Work\u201d, <em>Cognition</em>, 35(2): 183\u2013204.\ndoi:10.1016/0010-0277(90)90014-B</li>\n<li>Fodor, Jerry A. and Zenon W. Pylyshyn, 1988, \u201cConnectionism\nand Cognitive Architecture: A Critical Analysis\u201d,\n<em>Cognition</em>, 28(1\u20132): 3\u201371.\ndoi:10.1016/0010-0277(88)90031-5</li>\n<li>Friston, Karl, 2005, \u201cA Theory of Cortical Responses\u201d,\n<em>Philosophical Transactions of the Royal Society B: Biological\nSciences</em>, 360(1456): 815\u2013836.\ndoi:10.1098/rstb.2005.1622</li>\n<li>Friston, Karl J. and Klaas E. Stephan, 2007, \u201cFree-Energy\nand the Brain\u201d, <em>Synthese</em>, 159(3): 417\u2013458.\ndoi:10.1007/s11229-007-9237-y</li>\n<li>Fukushima, Kunihiko, 1980, \u201cNeocognitron: A Self-Organizing\nNeural Network Model for a Mechanism of Pattern Recognition Unaffected\nby Shift in Position\u201d, <em>Biological Cybernetics</em>, 36(4):\n193\u2013202. doi:10.1007/BF00344251</li>\n<li>Garfield, Jay L., 1997, \u201cMentalese Not Spoken Here:\nComputation, Cognition and Causation\u201d, <em>Philosophical\nPsychology</em>, 10(4): 413\u2013435.\ndoi:10.1080/09515089708573231</li>\n<li>Garson, James W., 1991, \u201cWhat Connectionists Cannot Do: The\nThreat to Classical AI\u201d, in Horgan and Tienson 1991:\n113\u2013142. doi:10.1007/978-94-011-3524-5_6</li>\n<li>\u2013\u2013\u2013, 1994, \u201cCognition without Classical\nArchitecture\u201d, <em>Synthese</em>, 100(2): 291\u2013305.\ndoi:10.1007/BF01063812</li>\n<li>\u2013\u2013\u2013, 1997, \u201cSyntax in a Dynamic\nBrain\u201d, <em>Synthese</em>, 110(3): 343\u2013355.</li>\n<li>Goodfellow, Ian, Yoshua Bengio, and Aaron Courville, 2016,\n<em>Deep Learning</em>, Cambridge, MA: MIT Press. </li>\n<li>Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy, 2015,\n\u201cExplaining and Harnessing Adversarial Examples.\u201d, in\n<em>3rd International Conference on Learning Representations, ICLR\n2015</em>, San Diego, CA, May 7\u20139, 2015,\n <a href=\"http://arxiv.org/abs/1412.6572\" target=\"other\">available online</a>.</li>\n<!-- https://dblp.org/db/conf/iclr/iclr2015 -->\n<li>Goodfellow, Ian J., Jean Pouget-Abadie, Mehdi Mirza, Bing Xu,\nDavid Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio,\n2014, \u201cGenerative Adversarial Nets\u201d, in <em>Proceedings of\nthe 27th International Conference on Neural Information Processing\nSystems, (NIPS\u201914)</em>, Cambridge, MA: MIT Press, 2:\n2672\u20132680.</li>\n<li>Goodman, Bryce and Seth Flaxman, 2017, \u201cEuropean Union\nRegulations on Algorithmic Decision-Making and a \u2018Right to\nExplanation\u2019\u201d, <em>AI Magazine</em>, 38(3): 50\u201357.\ndoi:10.1609/aimag.v38i3.2741</li>\n<li>Goodman, Nelson, 1955, <em>Fact, Fiction, and Forecast</em>,\nCambridge, MA: Harvard University Press.</li>\n<li>Grush, Rick, 2004, \u201cThe Emulation Theory of Representation:\nMotor Control, Imagery, and Perception\u201d, <em>Behavioral and\nBrain Sciences</em>, 27(3): 377\u2013396.\ndoi:10.1017/S0140525X04000093</li>\n<li>Guarini, Marcello, 2001, \u201cA Defence of Connectionism Against\nthe \u2018Syntactic\u2019 Argument\u201d, <em>Synthese</em>,\n128(3): 287\u2013317. doi:10.1023/A:1011905917986</li>\n<li>Hadley, Robert F., 1994a, \u201cSystematicity in Connectionist\nLanguage Learning\u201d, <em>Mind &amp; Language</em>, 9(3):\n247\u2013272. doi:10.1111/j.1468-0017.1994.tb00225.x</li>\n<li>\u2013\u2013\u2013, 1994b, \u201cSystematicity Revisited:\nReply to Christiansen and Chater and Niklasson and van Gelder\u201d,\n<em>Mind &amp; Language</em>, 9(4): 431\u2013444.\ndoi:10.1111/j.1468-0017.1994.tb00317.x</li>\n<li>\u2013\u2013\u2013, 1997a, \u201cExplaining Systematicity: A\nReply to Kenneth Aizawa\u201d, <em>Minds and Machines</em>, 7(4):\n571\u2013579. doi:10.1023/A:1008252322227</li>\n<li>\u2013\u2013\u2013, 1997b, \u201cCognition, Systematicity and\nNomic Necessity\u201d, <em>Mind &amp; Language</em>, 12(2):\n137\u2013153. doi:10.1111/j.1468-0017.1997.tb00066.x</li>\n<li>\u2013\u2013\u2013, 2004, \u201cOn The Proper Treatment of\nSemantic Systematicity\u201d, <em>Minds and Machines</em>, 14(2):\n145\u2013172. doi:10.1023/B:MIND.0000021693.67203.46</li>\n<li>Hadley, Robert F. and Michael B. Hayward, 1997, \u201cStrong\nSemantic Systematicity from Hebbian Connectionist Learning\u201d,\n<em>Minds and Machines</em>, 7(1): 1\u201337.\ndoi:10.1023/A:1008252408222</li>\n<li>Hanson, Stephen J. and Judy Kegl, 1987, \u201cPARSNIP: A\nConnectionist Network that Learns Natural Language Grammar from\nExposure to Natural Language Sentences\u201d, <em>Ninth Annual\nConference of the Cognitive Science Society</em>, Hillsdale, NJ:\nErlbaum, pp. 106\u2013119.</li>\n<li>Harman, Gilbert and Sanjeev Kulkarni, 2007, <em>Reliable\nReasoning: Induction and Statistical Learning Theory</em>, Cambridge\nMA: MIT Press. </li>\n<li>Hatfield, Gary, 1991a, \u201cRepresentation in Perception and\nCognition: Connectionist Affordances\u201d, in Ramsey, Stich, and\nRumelhart 1991: 163\u2013195.</li>\n<li>\u2013\u2013\u2013, 1991b, \u201cRepresentation and\nRule-Instantiation in Connectionist Systems\u201d, in Horgan and\nTienson 1991: 90\u2013112. doi:10.1007/978-94-011-3524-5_5</li>\n<li>Hawthorne, John, 1989, \u201cOn the Compatibility of\nConnectionist and Classical Models\u201d, <em>Philosophical\nPsychology</em>, 2(1): 5\u201315. doi:10.1080/09515088908572956</li>\n<li>Haybron, Daniel M., 2000, \u201cThe Causal and Explanatory Role\nof Information Stored in Connectionist Networks\u201d, <em>Minds and\nMachines</em>, 10(3): 361\u2013380. doi:10.1023/A:1026545231550</li>\n<li>Hinton, Geoffrey E., 1990 [1991], \u201cMapping Part-Whole\nHierarchies into Connectionist Networks\u201d, <em>Artificial\nIntelligence</em>, 46(1\u20132): 47\u201375. Reprinted in Hinton\n1991: 47\u201376. doi:10.1016/0004-3702(90)90004-J</li>\n<li>\u2013\u2013\u2013 (ed.), 1991, <em>Connectionist Symbol\nProcessing</em>, Cambridge, MA: MIT Press.</li>\n<li>\u2013\u2013\u2013, 1992, \u201cHow Neural Networks Learn from\nExperience\u201d, <em>Scientific American</em>, 267(3):\n145\u2013151.</li>\n<li>\u2013\u2013\u2013, 2010, \u201cLearning to Represent Visual\nInput\u201d, <em>Philosophical Transactions of the Royal Society B:\nBiological Sciences</em>, 365(1537): 177\u2013184.\ndoi:10.1098/rstb.2009.0200</li>\n<li>Hinton, Geoffrey E., James L. McClelland, and David E. Rumelhart,\n1986, \u201cDistributed Representations\u201d, Rumelhart,\nMcClelland, and the PDP group 1986: chapter 3.</li>\n<li>Hohwy, Jakob, 2012, \u201cAttention and Conscious Perception in\nthe Hypothesis Testing Brain\u201d, <em>Frontiers in Psychology</em>,\n3(96): 1\u201314. doi:10.3389/fpsyg.2012.00096</li>\n<li>Hong, Ha, Daniel L K Yamins, Najib J Majaj, and James J DiCarlo,\n2016, \u201cExplicit Information for Category-Orthogonal Object\nProperties Increases along the Ventral Stream\u201d, <em>Nature\nNeuroscience</em>, 19(4): 613\u2013622. doi:10.1038/nn.4247</li>\n<li>Horgan, Terence E. and John Tienson, 1989, \u201cRepresentations\nwithout Rules\u201d, <em>Philosophical Topics</em>, 17(1):\n147\u2013174.</li>\n<li>\u2013\u2013\u2013, 1990, \u201cSoft Laws\u201d, <em>Midwest\nStudies In Philosophy</em>, 15: 256\u2013279.\ndoi:10.1111/j.1475-4975.1990.tb00217.x</li>\n<li>\u2013\u2013\u2013 (eds.), 1991, <em>Connectionism and the\nPhilosophy of Mind</em>, Dordrecht: Kluwer.\ndoi:10.1007/978-94-011-3524-5</li>\n<li>\u2013\u2013\u2013, 1996, <em>Connectionism and the Philosophy\nof Psychology</em>, Cambridge, MA: MIT Press.</li>\n<li>Hosoya, Toshihiko, Stephen A. Baccus, and Markus Meister, 2005,\n\u201cDynamic Predictive Coding by the Retina\u201d,\n\n<em>Nature</em>, 436(7047): 71\u201377. doi:10.1038/nature03689</li>\n<li>Huang, Yanping and Rajesh P. N. Rao, 2011, \u201cPredictive\nCoding\u201d, <em>Wiley Interdisciplinary Reviews: Cognitive\nScience</em>, 2(5): 580\u2013593. doi:10.1002/wcs.142</li>\n<li>Hubel, David H. and Torsten N. Wiesel, 1965, \u201cReceptive\nFields and Functional Architecture in Two Nonstriate Visual Areas (18\nand 19) of the Cat\u201d, <em>Journal of Neurophysiology</em>, 28(2):\n229\u2013289. doi:10.1152/jn.1965.28.2.229</li>\n<li>Jansen, Peter A. and Scott Watter, 2012, \u201cStrong\nSystematicity through Sensorimotor Conceptual Grounding: An\nUnsupervised, Developmental Approach to Connectionist Sentence\nProcessing\u201d, <em>Connection Science</em>, 24(1): 25\u201355.\ndoi:10.1080/09540091.2012.664121</li>\n<li>Johnson, Kent, 2004, \u201cOn the Systematicity of Language and\nThought\u201d:, <em>Journal of Philosophy</em>, 101(3):\n111\u2013139. doi:10.5840/jphil2004101321</li>\n<li>Jones, Matt and Bradley C. Love, 2011, \u201cBayesian\nFundamentalism or Enlightenment? On the Explanatory Status and\nTheoretical Contributions of Bayesian Models of Cognition\u201d,\n<em>Behavioral and Brain Sciences</em>, 34(4): 169\u2013188.\ndoi:10.1017/S0140525X10003134</li>\n<li>Khaligh-Razavi, Seyed-Mahdi and Nikolaus Kriegeskorte, 2014,\n\u201cDeep Supervised, but Not Unsupervised, Models May Explain IT\nCortical Representation\u201d, <em>PLoS Computational Biology</em>,\n10(11): e1003915. doi:10.1371/journal.pcbi.1003915</li>\n<li>Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton, 2012,\n\u201cImagenet Classification with Deep Convolutional Neural\nNetworks\u201d, <em>Advances in Neural Information Processing\nSystems</em>, 25: 1097\u20131105.</li>\n<li>Kubilius, Jonas, Stefania Bracci, and Hans P. Op de Beeck, 2016,\n\u201cDeep Neural Networks as a Computational Model for Human Shape\nSensitivity\u201d, <em>PLOS Computational Biology</em>, 12(4):\ne1004896. doi:10.1371/journal.pcbi.1004896</li>\n<li>Laakso, Aarre and Garrison Cottrell, 2000, \u201cContent and\nCluster Analysis: Assessing Representational Similarity in Neural\nSystems\u201d, <em>Philosophical Psychology</em>, 13(1): 47\u201376.\ndoi:10.1080/09515080050002726</li>\n<li>Lake, Brenden M., Ruslan Salakhutdinov, and Joshua B. Tenenbaum,\n2015, \u201cHuman-Level Concept Learning through Probabilistic\nProgram Induction\u201d, <em>Science</em>, 350(6266):\n1332\u20131338. doi:10.1126/science.aab3050</li>\n<li>Lake, Brenden M., Wojciech Zaremba, Rob Fergus, and Todd M.\nGureckis, 2015, \u201cDeep Neural Networks Predict Category\nTypicality Ratings for Images\u201d, <em>Proceedings of the 37th\nAnnual Cognitive Science Society</em>, Pasadena, CA, 22\u201325 July\n2015, <a href=\"https://mindmodeling.org/cogsci2015/papers/0219/\" target=\"other\">available online</a>.</li>\n<li>Lillicrap, Timothy P., Daniel Cownden, Douglas B. Tweed, and Colin\nJ. Akerman, 2016, \u201cRandom Synaptic Feedback Weights Support\nError Backpropagation for Deep Learning\u201d, <em>Nature\nCommunications</em>, 7(1): 13276. doi:10.1038/ncomms13276</li>\n<li>Loula, Jo\u00e3o, Marco Baroni, and Brenden Lake, 2018,\n\u201cRearranging the Familiar: Testing Compositional Generalization\nin Recurrent Networks\u201d, in <em>Proceedings of the 2018 EMNLP\nWorkshop BlackboxNLP: Analyzing and Interpreting Neural Networks for\nNLP</em>, Brussels, Belgium: Association for Computational\nLinguistics, 108\u2013114. doi:10.18653/v1/W18-5413</li>\n<li>MacDonald, Cynthia and Graham MacDonald (eds), 1995,\n<em>Connectionism</em>, (Debates on Psychological Explanation, 2),\nOxford: Blackwell.</li>\n<li>Matthews, Robert J., 1997, \u201cCan Connectionists Explain\nSystematicity?\u201d, <em>Mind &amp; Language</em>, 12(2):\n154\u2013177. doi:10.1111/j.1468-0017.1997.tb00067.x</li>\n<li>Marcus, Gary F., 1998, \u201cRethinking Eliminative\nConnectionism\u201d, <em>Cognitive Psychology</em>, 37(3):\n243\u2013282. doi:10.1006/cogp.1998.0694</li>\n<li>\u2013\u2013\u2013, 2001, <em>The Algebraic Mind: Integrating\nConnectionism and Cognitive Science</em>, Cambridge, MA: MIT\nPress.</li>\n<li>McClelland, James L and Jeffrey L Elman, 1986, \u201cThe TRACE\nModel of Speech Perception\u201d, <em>Cognitive Psychology</em>,\n18(1): 1\u201386. doi:10.1016/0010-0285(86)90015-0</li>\n<li>McClelland, James L., David E. Rumelhart, and the PDP Research\nGroup (ed.), 1986, <em>Parallel Distributed Processing, Volume II:\nExplorations in the Microstructure of Cognition: Psychological and\nBiological Models</em>, Cambridge, MA: MIT Press.</li>\n<li>McLaughlin, Brian P., 1993, \u201cThe Connectionism/Classicism\nBattle to Win Souls\u201d, <em>Philosophical Studies</em>, 71(2):\n163\u2013190. doi:10.1007/BF00989855</li>\n<li>Miikkulainen, Risto, 1993, <em>Subsymbolic Natural Language\nProcessing: An Integrated Model of Scripts, Lexicon, and Memory</em>,\nCambridge, MA: MIT Press.</li>\n<li>Miikkulainen, Risto and Michael G. Dyer, 1991, \u201cNatural\nLanguage Processing With Modular Pdp Networks and Distributed\nLexicon\u201d, <em>Cognitive Science</em>, 15(3): 343\u2013399.\ndoi:10.1207/s15516709cog1503_2</li>\n<li>Miracchi, Lisa, 2019, \u201cA Competence Framework for Artificial\nIntelligence Research\u201d, <em>Philosophical Psychology</em>,\n32(5): 588\u2013633. doi:10.1080/09515089.2019.1607692</li>\n<li>Montavon, Gr\u00e9goire, Wojciech Samek, and Klaus-Robert\nM\u00fcller, 2018, \u201cMethods for Interpreting and Understanding\nDeep Neural Networks\u201d, <em>Digital Signal Processing</em>, 73:\n1\u201315. doi:10.1016/j.dsp.2017.10.011</li>\n<li>Mont\u00fafar, Guido, Razvan Pascanu, Kyunghyun Cho, and Yoshua\nBengio, 2014, \u201cOn the Number of Linear Regions of Deep Neural\nNetworks\u201d, in <em>Proceedings of the 27th International\nConference on Neural Information Processing Systems\n(NIPS\u201914)</em>, Cambridge, MA: MIT Press, 2: 2924\u20132932.\n</li>\n<li>Morris, William C., Garrison W. Cottrell, and Jeffrey Elman, 2000,\n\u201cA Connectionist Simulation of the Empirical Acquisition of\nGrammatical Relations\u201d, in Wermter and Sun 2000:\n1778:175\u2013193. doi:10.1007/10719871_12</li>\n<li>Nguyen, Anh, Jason Yosinski, Jeff Clune, 2015, \u201cDeep Neural\nNetworks Are Easily Fooled: High Confidence Predictions for\nUnrecognizable Images\u201d, <em>Proceedings of the 28th IEEE\nConference on Computer Vision and Pattern Recognition (CVPR\n2015)</em>, 427\u2013436, \n <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Nguyen_Deep_Neural_Networks_2015_CVPR_paper.html\" target=\"other\">available online</a>.</li>\n<li>Niklasson, Lars F. and Tim van Gelder, 1994, \u201cOn Being\nSystematically Connectionist\u201d, <em>Mind &amp; Language</em>,\n9(3): 288\u2013302. doi:10.1111/j.1468-0017.1994.tb00227.x</li>\n<li>O\u2019Reilly, Randall C., 1996, \u201cBiologically Plausible\nError-Driven Learning Using Local Activation Differences: The\nGeneralized Recirculation Algorithm\u201d, <em>Neural\nComputation</em>, 8(5): 895\u2013938.\ndoi:10.1162/neco.1996.8.5.895</li>\n<li>Phillips, Steven, 2002, \u201cDoes Classicism Explain\nUniversality?\u201d, <em>Minds and Machines</em>, 12(3):\n423\u2013434. doi:10.1023/A:1016160512967</li>\n<li>Pinker, Steven and Jacques Mehler (eds.), 1988, <em>Connections\nand Symbols</em>, Cambridge, MA: MIT Press.</li>\n<li>Pinker, Steven and Alan Prince, 1988, \u201cOn Language and\nConnectionism: Analysis of a Parallel Distributed Processing Model of\nLanguage Acquisition\u201d, <em>Cognition</em>, 28(1\u20132):\n73\u2013193. doi:10.1016/0010-0277(88)90032-7</li>\n<li>Pollack, Jordan B., 1989, \u201cImplications of Recursive\nDistributed Representations\u201d, in Touretzky 1989: 527\u2013535,\n <a href=\"http://papers.nips.cc/paper/114-implications-of-recursive-distributed-representations\" target=\"other\">available online</a>.</li>\n<li>\u2013\u2013\u2013, 1991, \u201cInduction of Dynamical\nRecognizers\u201d, in Touretzky 1991: 123\u2013148.\ndoi:10.1007/978-1-4615-4008-3_6</li>\n<li>Pollack, Jordan B., 1990 [1991], \u201cRecursive Distributed\nRepresentations\u201d, <em>Artificial Intelligence</em>,\n46(1\u20132): 77\u2013105. Reprinted in Hinton 1991: 77\u2013106.\ndoi:10.1016/0004-3702(90)90005-K</li>\n<li>Port, Robert F., 1990, \u201cRepresentation and Recognition of\nTemporal Patterns\u201d, <em>Connection Science</em>, 2(1\u20132):\n151\u2013176. doi:10.1080/09540099008915667</li>\n<li>Port, Robert F. and Timothy van Gelder, 1991, \u201cRepresenting\nAspects of Language\u201d, <em>Proceedings of the Thirteenth Annual\nConference of the Cognitive Science Society</em>, Hillsdale, N.J.:\nErlbaum, 487\u2013492,\n <a href=\"http://mindmodeling.org/cogscihistorical/cogsci_13.pdf\" target=\"other\">available online</a>.</li>\n<li>Quine, W. V., 1969, \u201cNatural Kinds\u201d, in <em>Essays in\nHonor of Carl G. Hempel</em>, Nicholas Rescher (ed.), Dordrecht:\nSpringer Netherlands, 5\u201323. doi:10.1007/978-94-017-1466-2_2</li>\n<li>Raghu, Maithra, Ben Poole, Jon Kleinberg, Surya Ganguli, and\nJascha Sohl-Dickstein, 2017, \u201cOn the Expressive Power of Deep\nNeural Networks\u201d, in <em>Proceedings of the 34th International\nConference on Machine Learning</em>, 70: 2847\u20132854,\n <a href=\"http://proceedings.mlr.press/v70/raghu17a.html\" target=\"other\">available online</a>.</li>\n<li>Ramsey, William, 1997, \u201cDo Connectionist Representations\nEarn Their Explanatory Keep?\u201d, <em>Mind &amp; Language</em>,\n12(1): 34\u201366. doi:10.1111/j.1468-0017.1997.tb00061.x</li>\n<li>Ramsey, William, Stephen P. Stich, and Joseph Garon, 1991,\n\u201cConnectionism, Eliminativism, and the Future of Folk\nPsychology\u201d, in Ramsey, Stich, and Rumelhart 1991:\n199\u2013228.</li>\n<li>Ramsey, William, Stephen P. Stich, and David E. Rumelhart, 1991,\n<em>Philosophy and Connectionist Theory</em>, Hillsdale, N.J.:\nErlbaum.</li>\n<li>Rao, Rajesh P. N. and Dana H. Ballard, 1999, \u201cPredictive\nCoding in the Visual Cortex: A Functional Interpretation of Some\nExtra-Classical Receptive-Field Effects\u201d, <em>Nature\nNeuroscience</em>, 2(1): 79\u201387. doi:10.1038/4580</li>\n<li>Rohde, Douglas L. T. and David C. Plaut, 2003,\n\u201cConnectionist Models of Language Processing\u201d,\n<em>Cognitive Studies</em> (Japan), 10(1): 10\u201328.\ndoi:10.11225/jcss.10.10</li>\n<li>Roth, Martin, 2005, \u201cProgram Execution in Connectionist\nNetworks\u201d, <em>Mind &amp; Language</em>, 20(4): 448\u2013467.\ndoi:10.1111/j.0268-1064.2005.00295.x</li>\n<li>Rumelhart, David E. and James L. McClelland, 1986, \u201cOn\nLearning the Past Tenses of English Verbs\u201d, in McClelland,\nRumelhart, and the PDP group 1986: 216\u2013271.</li>\n<li>Rumelhart, David E., James L. McClelland, and the PDP Research\nGroup (eds), 1986, <em>Parallel Distributed Processing, Volume 1:\nExplorations in the Microstructure of Cognition: Foundations</em>,\nCambridge, MA: MIT Press.</li>\n<li>Sadler, Matthew and Natasha Regan, 2019, <em>Game Changer:\nAlphaZero\u2019s Groundbreaking Chess Strategies and the Promise of\nAI</em>, Alkmaar: New in Chess.</li>\n<li>Schmidhuber, J\u00fcrgen, 2015, \u201cDeep Learning in Neural\nNetworks: An Overview\u201d, <em>Neural Networks</em>, 61:\n85\u2013117. doi:10.1016/j.neunet.2014.09.003</li>\n<li>Schwarz, Georg, 1992, \u201cConnectionism, Processing,\nMemory\u201d, <em>Connection Science</em>, 4(3\u20134):\n207\u2013226. doi:10.1080/09540099208946616</li>\n<li>Sejnowski, Terrence J. and Charles R. Rosenberg, 1987,\n\u201cParallel Networks that Learn to Pronounce English Text\u201d,\n<em>Complex Systems</em>, 1(1): 145\u2013168,\n <a href=\"https://www.complex-systems.com/abstracts/v01_i01_a10/\" target=\"other\">available online</a>.</li>\n<li>Servan-Schreiber, David, Axel Cleeremans, and James L. McClelland,\n1991, \u201cGraded State Machines: The Representation of Temporal\nContingencies in Simple Recurrent Networks\u201d, in Touretzky 1991:\n57\u201389. doi:10.1007/978-1-4615-4008-3_4</li>\n<li>Shastri, Lokendra and Venkat Ajjanagadde, 1993, \u201cFrom Simple\nAssociations to Systematic Reasoning: A Connectionist Representation\nof Rules, Variables and Dynamic Bindings Using Temporal\nSynchrony\u201d, <em>Behavioral and Brain Sciences</em>, 16(3):\n417\u2013451. doi:10.1017/S0140525X00030910</li>\n<li>Shea, Nicholas, 2007, \u201cContent and Its Vehicles in\nConnectionist Systems\u201d, <em>Mind &amp; Language</em>, 22(3):\n246\u2013269. doi:10.1111/j.1468-0017.2007.00308.x</li>\n<li>Shevlin, Henry and Marta Halina, 2019, \u201cApply Rich\nPsychological Terms in AI with Care\u201d, <em>Nature Machine\nIntelligence</em>, 1(4): 165\u2013167.\ndoi:10.1038/s42256-019-0039-y</li>\n<li>Shultz, Thomas R. and Alan C. Bale, 2001, \u201cNeural Network\nSimulation of Infant Familiarization to Artificial Sentences\u201d,\n<em>Infancy</em>, 2(4): 501\u2013536.</li>\n<li>\u2013\u2013\u2013, 2006, \u201cNeural Networks Discover a\nNear-Identity Relation to Distinguish Simple Syntactic Forms\u201d,\n<em>Minds and Machines</em>, 16(2): 107\u2013139.\ndoi:10.1007/s11023-006-9029-z</li>\n<li>Silver, David, Thomas Hubert, Julian Schrittwieser, Ioannis\nAntonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, et al., 2018,\n\u201cA General Reinforcement Learning Algorithm That Masters Chess,\nShogi, and Go through Self-Play\u201d, <em>Science</em>, 362(6419):\n1140\u20131144. doi:10.1126/science.aar6404</li>\n<li>Smolensky, Paul, 1987, \u201cThe Constituent Structure of\nConnectionist Mental States: A Reply to Fodor and Pylyshyn\u201d,\n<em>The Southern Journal of Philosophy</em>, 26(S1): 137\u2013161.\ndoi:10.1111/j.2041-6962.1988.tb00470.x</li>\n<li>\u2013\u2013\u2013, 1988, \u201cOn the Proper Treatment of\nConnectionism\u201d, <em>Behavioral and Brain Sciences</em>, 11(1):\n1\u201323. doi:10.1017/S0140525X00052432</li>\n<li>\u2013\u2013\u2013, 1990 [1991], \u201cTensor Product Variable\nBinding and the Representation of Symbolic Structures in Connectionist\nSystems\u201d, <em>Artificial Intelligence</em>, 46(1\u20132):\n159\u2013216. Reprinted in Hinton 1991: 159\u2013216.\ndoi:10.1016/0004-3702(90)90007-M</li>\n<li>\u2013\u2013\u2013, 1995, \u201cConstituent Structure and\nExplanation in an Integrated Connectionist/Symbolic Cognitive\nArchitecture\u201d, in MacDonald and MacDonald 1995: .</li>\n<li>St. John, Mark F. and James L. McClelland, 1990 [1991],\n\u201cLearning and Applying Contextual Constraints in Sentence\nComprehension\u201d, <em>Artificial Intelligence</em>, 46(1\u20132):\n217\u2013257. Reprinted in Hinton 1991: 217\u2013257\ndoi:10.1016/0004-3702(90)90008-N</li>\n<li>Tomberlin, James E. (ed.), 1995, <em>Philosophical Perspectives 9:\nAI, Connectionism and Philosophical Psychology</em>, Atascadero:\nRidgeview Press.</li>\n<li>Touretzky, David S. (ed.), 1989, <em>Advances in Neural\nInformation Processing Systems I</em>, San Mateo, CA: Kaufmann,\n <a href=\"http://papers.nips.cc/book/advances-in-neural-information-processing-systems-1-1988\" target=\"other\">available online</a>.</li>\n<li>\u2013\u2013\u2013 (ed.), 1990, <em>Advances in Neural\nInformation Processing Systems II</em>, San Mateo, CA: Kaufmann.</li>\n<li>\u2013\u2013\u2013 (ed.), 1991, <em>Connectionist Approaches to\nLanguage Learning</em>, Boston, MA: Springer US.\ndoi:10.1007/978-1-4615-4008-3</li>\n<li>Touretzky, David S., Geoffrey E. Hinton, and Terrence Joseph\nSejnowski (eds), 1988, <em>Proceedings of the 1988 Connectionist\nModels Summer School</em>, San Mateo, CA: Kaufmann.</li>\n<li>Van Gelder, Tim, 1990, \u201cCompositionality: A Connectionist\nVariation on a Classical Theme\u201d, <em>Cognitive Science</em>,\n14(3): 355\u2013384. doi:10.1016/0364-0213(90)90017-Q</li>\n<li>\u2013\u2013\u2013, 1991, \u201cWhat is the \u2018D\u2019 in\nPDP?\u201d in Ramsey, Stich, and Rumelhart 1991: 33\u201359.</li>\n<li>Van Gelder, Timothy and Robert Port, 1993, \u201cBeyond Symbolic:\nProlegomena to a Kama-Sutra of Compositionality\u201d, in Vasant G\nHonavar, Leonard Uhr (eds.), <em>Symbol Processing and Connectionist\nModels in AI and Cognition: Steps Towards Integration</em>, Boston:\nAcademic Press.</li>\n<li>Vilcu, Marius and Robert F. Hadley, 2005, \u201cTwo Apparent\n\u2018Counterexamples\u2019 to Marcus: A Closer Look\u201d,\n<em>Minds and Machines</em>, 15(3\u20134): 359\u2013382.\ndoi:10.1007/s11023-005-9000-4</li>\n<li>Von Eckardt, Barbara, 2003, \u201cThe Explanatory Need for Mental\nRepresentations in Cognitive Science\u201d, <em>Mind &amp;\nLanguage</em>, 18(4): 427\u2013439. doi:10.1111/1468-0017.00235</li>\n<li>\u2013\u2013\u2013, 2005, \u201cConnectionism and the\nPropositional Attitudes\u201d, in Christina Erneling and David Martel\nJohnson (eds.), <em>The Mind as a Scientific Object: Between Brain and\nCulture</em>, New York: Oxford University Press.</li>\n<li>Waltz, David L. and Jordan B. Pollack, 1985, \u201cMassively\nParallel Parsing: A Strongly Interactive Model of Natural Language\nInterpretation*\u201d, <em>Cognitive Science</em>, 9(1): 51\u201374.\ndoi:10.1207/s15516709cog0901_4</li>\n<li>Wermter, Stefan and Ron Sun (eds.), 2000, <em>Hybrid Neural\nSystems</em>, (Lecture Notes in Computer Science 1778), Berlin,\nHeidelberg: Springer Berlin Heidelberg. doi:10.1007/10719871</li>\n<li>Yamins, Daniel L. K. and James J. DiCarlo, 2016, \u201cUsing\nGoal-Driven Deep Learning Models to Understand Sensory Cortex\u201d,\n<em>Nature Neuroscience</em>, 19(3): 356\u2013365.\ndoi:10.1038/nn.4244</li>\n<li>Yosinski, Jason, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod\nLipson, 2015, \u201cUnderstanding Neural Networks Through Deep\nVisualization\u201d, <em>Deep Learning Workshop, 31st International\nConference on Machine Learning</em>, Lille, France,\n <a href=\"http://yosinski.com/media/papers/Yosinski__2015__ICML_DL__Understanding_Neural_Networks_Through_Deep_Visualization__.pdf\" target=\"other\">available online</a>.</li>\n<li>Zhou, Zhenglong and Chaz Firestone, 2019, \u201cHumans Can\nDecipher Adversarial Images\u201d, <em>Nature Communications</em>,\n10(1): 1334. doi:10.1038/s41467-019-08931-6</li>\n</ul>\n</div>"
    },
    "related_entries": {
        "entry_list": [
            "artificial intelligence",
            "language of thought hypothesis",
            "mental representation"
        ],
        "entry_link": [
            {
                "../artificial-intelligence/": "artificial intelligence"
            },
            {
                "../language-thought/": "language of thought hypothesis"
            },
            {
                "../mental-representation/": "mental representation"
            }
        ]
    },
    "academic_tools": {
        "listed_text": [
            "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=connectionism\" target=\"other\">How to cite this entry</a>.",
            "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://leibniz.stanford.edu/friends/preview/connectionism/\" target=\"other\">Preview the PDF version of this entry</a> at the\n <a href=\"https://leibniz.stanford.edu/friends/\" target=\"other\">Friends of the SEP Society</a>.",
            "<img alt=\"inpho icon\" src=\"../../symbols/inpho.png\"/>",
            "<a href=\"https://www.inphoproject.org/entity?sep=connectionism&amp;redirect=True\" target=\"other\">Look up topics and thinkers related to this entry</a>\n at the Internet Philosophy Ontology Project (InPhO).",
            "<img alt=\"phil papers icon\" src=\"../../symbols/pp.gif\"/>",
            "<a href=\"http://philpapers.org/sep/connectionism/\" target=\"other\">Enhanced bibliography for this entry</a>\nat <a href=\"http://philpapers.org/\" target=\"other\">PhilPapers</a>, with links to its database."
        ],
        "listed_links": [
            {
                "https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=connectionism": "How to cite this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/preview/connectionism/": "Preview the PDF version of this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/": "Friends of the SEP Society"
            },
            {
                "https://www.inphoproject.org/entity?sep=connectionism&redirect=True": "Look up topics and thinkers related to this entry"
            },
            {
                "http://philpapers.org/sep/connectionism/": "Enhanced bibliography for this entry"
            },
            {
                "http://philpapers.org/": "PhilPapers"
            }
        ]
    },
    "other_internet_resources": {
        "listed_text": [
            "Bengio, Yoshua, Aaron Courville, and Pascal Vincent, 2014,\n\u201c<a href=\"http://arxiv.org/abs/1206.5538\" target=\"other\">Representation Learning: A Review and New Perspectives</a>\u201d,\nmanuscript at arXiv.org, original submission 2012.",
            "Buckner, C., 2019, \n\u201c<a href=\"https://philsci-archive.pitt.edu/id/eprint/16326/contents\" target=\"other\">Deep Learning: A Philosophical Introduction</a>\u201d,\n preprint at PhilSci Archives.",
            "<a href=\"https://www.darpa.mil/program/explainable-artificial-intelligence\" target=\"other\">Explainable Artificial Intelligence</a> (XAI),\n DARPA /I2O program.",
            "Guest, Olivia and Bradley C. Love, 2019, \n\u201c<a href=\"https://www.biorxiv.org/content/10.1101/626374v1\" target=\"other\">Levels of Representation in a Deep Learning Model of Categorization</a>\u201d,\n manuscript at bioRxiv.org.",
            "Hendricks, Lisa Anne, Zeynep Akata, Marcus Rohrbach, Jeff Donahue,\nBernt Schiele, and Trevor Darrell, 2016, \n\u201c<a href=\"http://arxiv.org/abs/1603.08507\" target=\"other\">Generating Visual Explanations</a>\u201d,\n manuscript at arXiv.org, 28 March 2016.",
            "Ilyas, Andrew, Shibani Santurkar, Dimitris Tsipras, Logan\nEngstrom, Brandon Tran, and Aleksander Madry, 2019, \n\u201c<a href=\"http://arxiv.org/abs/1905.02175\" target=\"other\">Adversarial Examples Are Not Bugs, They Are Features</a>\u201d,\n manuscript at arXiv.org, 19 June 2019.",
            "Lipton, Zachary C., 2016, \n\u201c<a href=\"https://arxiv.org/abs/1606.03490\" target=\"other\">The Mythos of Model Interpretability</a>\u201d,\n manuscript at arXiv.org.",
            "Zednik, Carlos, 2019, \n \u201c<a href=\"https://arxiv.org/abs/1903.04361\" target=\"other\">Solving the Black Box Problem: A General-Purpose Recipe for Explainable Artificial Intelligence</a>\u201d,\n manuscript at arXiv.org.",
            "Zhang, Chiyuan, Samy Bengio, Moritz Hardt, Benjamin Recht, and\nOriol Vinyals, 2016, \n\u201c<a href=\"https://arxiv.org/abs/1611.03530\" target=\"other\">Understanding Deep Learning Requires Rethinking Generalization</a>\u201d,\n manuscript at arXiv.org.",
            "<a href=\"https://philpapers.org/s/connectionism\" target=\"other\">Papers on Connectionism</a>,\n search result fot the topic \u201cconnectionism\u201d at\nphilpapers.org."
        ],
        "listed_links": [
            {
                "http://arxiv.org/abs/1206.5538": "Representation Learning: A Review and New Perspectives"
            },
            {
                "https://philsci-archive.pitt.edu/id/eprint/16326/contents": "Deep Learning: A Philosophical Introduction"
            },
            {
                "https://www.darpa.mil/program/explainable-artificial-intelligence": "Explainable Artificial Intelligence"
            },
            {
                "https://www.biorxiv.org/content/10.1101/626374v1": "Levels of Representation in a Deep Learning Model of Categorization"
            },
            {
                "http://arxiv.org/abs/1603.08507": "Generating Visual Explanations"
            },
            {
                "http://arxiv.org/abs/1905.02175": "Adversarial Examples Are Not Bugs, They Are Features"
            },
            {
                "https://arxiv.org/abs/1606.03490": "The Mythos of Model Interpretability"
            },
            {
                "https://arxiv.org/abs/1903.04361": "Solving the Black Box Problem: A General-Purpose Recipe for Explainable Artificial Intelligence"
            },
            {
                "https://arxiv.org/abs/1611.03530": "Understanding Deep Learning Requires Rethinking Generalization"
            },
            {
                "https://philpapers.org/s/connectionism": "Papers on Connectionism"
            }
        ]
    }
}