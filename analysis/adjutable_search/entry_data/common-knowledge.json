{
    "url": "common-knowledge",
    "title": "Common Knowledge",
    "authorship": {
        "year": "Copyright \u00a9 2022",
        "author_text": "Peter Vanderschraaf\n\nGiacomo Sillari\n<gsillari@luiss.it>",
        "author_links": [
            {
                "https://moralscience.arizona.edu/person/peter-vanderschraaf": "Peter Vanderschraaf"
            },
            {
                "https://scienzepolitiche.luiss.it/en/docenti/cv/020229": "Giacomo Sillari"
            },
            {
                "mailto:gsillari%40luiss%2eit": "gsillari@luiss.it"
            }
        ],
        "raw_html": "<div id=\"article-copyright\">\n<p>\n<a href=\"../../info.html#c\">Copyright \u00a9 2022</a> by\n\n<br/>\n<a href=\"https://moralscience.arizona.edu/person/peter-vanderschraaf\" target=\"other\">Peter Vanderschraaf</a>\n<br/>\n<a href=\"https://scienzepolitiche.luiss.it/en/docenti/cv/020229\" target=\"other\">Giacomo Sillari</a>\n&lt;<a href=\"mailto:gsillari%40luiss%2eit\"><em>gsillari<abbr title=\" at \">@</abbr>luiss<abbr title=\" dot \">.</abbr>it</em></a>&gt;\n    </p>\n</div>"
    },
    "pubinfo": [
        "First published Tue Aug 28, 2001",
        "substantive revision Fri Aug 5, 2022"
    ],
    "preamble": "\n\nA proposition \\(A\\) is mutual knowledge among a set of agents\nif each agent knows that \\(A\\). Mutual knowledge by itself implies\nnothing about what, if any, knowledge anyone attributes to anyone\nelse. Suppose each student arrives for a class meeting knowing that\nthe instructor will be late. That the instructor will be late is\nmutual knowledge, but each student might think only she knows the\ninstructor will be late. However, if one of the students says openly\n\u201cPeter told me he will be late again,\u201d then each student\nknows that each student knows that the instructor will be late, each\nstudent knows that each student knows that each student knows that the\ninstructor will be late, and so on, ad infinitum. The\nannouncement made the mutually known fact common knowledge\namong the students.\n\nCommon knowledge is a phenomenon which underwrites much of social\nlife. In order to communicate or otherwise coordinate their behavior\nsuccessfully, individuals typically require mutual or common\nunderstandings or background knowledge. Indeed, if a particular\ninteraction results in \u201cfailure\u201d, the usual explanation\nfor this is that the agents involved did not have the common knowledge\nthat would have resulted in success. If a married couple are separated\nin a department store, they stand a good chance of finding one another\nbecause their common knowledge of each others\u2019 tastes and\nexperiences leads them each to look for the other in a part of the\nstore both know that both would tend to frequent. Since the spouses\nboth love cappuccino, each expects the other to go to the coffee bar,\nand they find one another. But in a less happy case, if a pedestrian\ncauses a minor traffic jam by crossing against a red light, she\nexplains her mistake as the result of her not noticing, and therefore\nnot knowing, the status of the traffic signal that all the motorists\nknew. The spouses coordinate successfully given their common\nknowledge, while the pedestrian and the motorists miscoordinate as the\nresult of a breakdown in common knowledge.\n\nGiven the importance of common knowledge in social interactions, it is\nremarkable that only quite recently have philosophers and social\nscientists attempted to analyze the concept. David Hume (1740) was\nperhaps the first to make explicit reference to the role of mutual\nknowledge in coordination. In his account of convention in A\nTreatise of Human Nature, Hume argued that a necessary condition\nfor coordinated activity was that agents all know what behavior to\nexpect from one another. Without the requisite mutual knowledge, Hume\nmaintained, mutually beneficial social conventions would disappear.\nMuch later, J. E. Littlewood (1953) presented some examples of\ncommon-knowledge-type reasoning, and Thomas Schelling (1960) and John\nHarsanyi (1967\u20131968) argued that something like common knowledge\nis needed to explain certain inferences people make about each other.\nThe philosopher Robert Nozick describes, but does not develop, the\nnotion in his doctoral dissertation (Nozick 1963), while the first\nmathematical analysis and application of the notion of common\nknowledge is found in the technical report by Friedell (1967), then\npublished as (Friedell\n 1969).[1]\n The first full-fledged philosophical analysis of common knowledge was\noffered by David Lewis (1969) in the monograph Convention.\nStephen Schiffer (1972), Robert Aumann (1976), and Gilbert Harman\n(1977) independently gave alternate definitions of common knowledge.\nJon Barwise (1988, 1989) gave a precise formulation of Harman\u2019s\nintuitive account. Throughout the 1980s a number of epistemic\nlogicians, both from philosophy and from computer science, studied the\nlogical structure of common knowledge, and the interested reader\nshould consult the relevant portions of the two important monographs\n(Fagin et al. 1995) and (Meyer and Van der Hoek 1995). Margaret\nGilbert (1989) proposed a somewhat different account of common\nknowledge which she argues is preferable to the standard account.\nOthers have developed accounts of mutual knowledge, approximate\ncommon knowledge, and common belief which require less\nstringent assumptions than the standard account, and which serve as\nmore plausible models of what agents know in cases where strict common\nknowledge seems impossible (Brandenburger and Dekel 1987,\n\u00a0Monderer and Samet 1989, Rubinstein 1992). The analysis and\napplications of common knowledge and related multi-agent knowledge\nconcepts has become a lively field of research.\n\nThe purpose of this essay is to overview of some of the most important\nresults stemming from this contemporary research. The topics reviewed\nin each section of this essay are as follows: Section 1 gives\nmotivating examples which illustrate a variety of ways in which the\nactions of agents depend crucially upon their having, or lacking,\ncertain common knowledge. Section 2 discusses alternative analyses of\ncommon knowledge. Section 3 reviews applications of multi-agent\nknowledge concepts, particularly to game theory (von Neumann\nand Morgenstern 1944), in which common knowledge assumptions have been\nfound to have great importance in justifying solution\nconcepts for mathematical games. Section 4 discusses skeptical\ndoubts about the attainability of common knowledge. Finally, Section 5\ndiscusses the common belief concept which result from\nweakening the assumptions of Lewis\u2019 account of common\nknowledge.\n",
    "toc": [
        {
            "#1": "1. Motivating Examples"
        },
        {
            "#1.1": "1.1 The Clumsy Waiter"
        },
        {
            "#1.2": "1.2 The Barbeque Problem"
        },
        {
            "#1.3": "1.3 The Farmers\u2019 Dilemma"
        },
        {
            "#1.4": "1.4 The Centipede"
        },
        {
            "#1.5": "1.5 The Department Store"
        },
        {
            "#2": "2. Alternative Accounts of Common Knowledge"
        },
        {
            "#2.1": "2.1 The Hierarchical Account"
        },
        {
            "#2.2": "2.2 Lewis\u2019 Account"
        },
        {
            "#2.3": "2.3 Aumann\u2019s Account"
        },
        {
            "#2.4": "2.4 Barwise\u2019s Account"
        },
        {
            "#2.5": "2.5 Gilbert\u2019s Account"
        },
        {
            "#3": "3. Applications of Mutual and Common Knowledge"
        },
        {
            "#3.1": "3.1 The \u201cNo Disagreement\u201d Theorem"
        },
        {
            "#3.2": "3.2 Convention"
        },
        {
            "#3.3": "3.3 Strategic Form Games"
        },
        {
            "#3.4": "3.4 Games of Perfect Information"
        },
        {
            "#3.5": "3.5 Communication Networks"
        },
        {
            "#4": "4. Is Common Knowledge Attainable?"
        },
        {
            "#5": "5. Coordination and Common \\(p\\)-Belief"
        },
        {
            "#5.1": "5.1 The Email Coordination Example"
        },
        {
            "#5.2": "5.2 Common \\(p\\)-Belief"
        },
        {
            "#Bib": "Bibliography"
        },
        {
            "#Aca": "Academic Tools"
        },
        {
            "#Oth": "Other Internet Resources"
        },
        {
            "#Rel": "Related Entries"
        }
    ],
    "main_text": "\n1. Motivating Examples\n\nMost of the examples in this section are familiar in the common\nknowledge literature, although some of the details and interpretations\npresented here are new. Readers may want to ask themselves what, if\nany, distinctive aspects of mutual and common knowledge reasoning each\nexample illustrates.\n1.1. The Clumsy Waiter\n\nA waiter serving dinner slips, and spills gravy on a guest\u2019s\nwhite silk evening gown. The guest glares at the waiter, and the\nwaiter declares \u201cI\u2019m sorry. It was my fault.\u201d Why\ndid the waiter say that he was at fault? He knew that he was at fault,\nand he knew from the guest\u2019s angry expression that she knew he\nwas at fault. However, the sorry waiter wanted assurance that the\nguest knew that he knew he was at fault. By saying openly\nthat he was at fault, the waiter knew that the guest knew what he\nwanted her to know, namely, that he knew he was at fault. Note that\nthe waiter\u2019s declaration established at least three levels of\nnested\n knowledge.[2]\n\n\nCertain assumptions are implicit in the preceding story. In\nparticular, the waiter must know that the guest knows he has spoken\nthe truth, and that she can draw the desired conclusion from what he\nsays in this context. More fundamentally, the waiter must know that if\nhe announces \u201cIt was my fault\u201d to the guest, she will\ninterpret his intended meaning correctly and will infer what his\nmaking this announcement ordinarily implies in this context. This in\nturn implies that the guest must know that if the waiter announces\n\u201cIt was my fault\u201d in this context, then the waiter indeed\nknows he is at fault. Then on account of his announcement, the waiter\nknows that the guest knows that he knows he was at fault. The\nwaiter\u2019s announcement was meant to generate\nhigher-order levels of knowledge of a fact each already\nknew.\n\nJust a slight strengthening of the stated assumptions results in even\nhigher levels of nested knowledge. Suppose the waiter and the guest\neach know that the other can infer what he infers from the\nwaiter\u2019s announcement. Can the guest now believe that the waiter\ndoes not know that she knows that he knows he is at fault? If the\nguest considers this question, she reasons that if the waiter falsely\nbelieves it is possible that she does not know that he knows he is at\nfault, then the waiter must believe it to be possible that she cannot\ninfer that he knows he is at fault from his own declaration. Since she\nknows she can infer that the waiter knows he is at fault from\nhis declaration, she knows that the waiter knows she can infer this,\nas well. Hence the waiter\u2019s announcement establishes the\nfourth-order knowledge claim: The guest knows that the waiter knows\nthat she knows that he knows he is at fault. By similar, albeit\nlengthier, arguments, the agents can verify that corresponding\nknowledge claims of even higher-order must also obtain under these\nassumptions.\n1.2 The Barbecue Problem\n\nThis is a variation of an example first published by Littlewood\n(1953), although he notes that his version of the example was already\nwell-known at the\n time.[3]\n \\(N\\) individuals enjoy a picnic supper together which includes\nbarbecued spareribs. At the end of the meal, \\(k \\ge 1\\) of these\ndiners have barbecue sauce on their faces. Since no one can see her\nown face, none of the messy diners knows whether he or she is messy.\nThen the cook who served the spareribs returns with a carton of ice\ncream. Amused by what he sees, the cook rings the dinner bell and\nmakes the following announcement: \u201cAt least one of you has\nbarbecue sauce on her face. I will ring the dinner bell over and over,\nuntil anyone who is messy has wiped her face. Then I will serve\ndessert.\u201d For the first \\(k - 1\\) rings, no one does anything.\nThen, at the \\(k\\)th ring, each of the messy individuals\nsuddenly reaches for a napkin, and soon afterwards, the diners are all\nenjoying their ice cream.\n\nHow did the messy diners finally realize that their faces needed\ncleaning? The \\(k = 1\\) case is easy, since in this case, the lone\nmessy individual will realize he is messy immediately, since he sees\nthat everyone else is clean. Consider the \\(k = 2\\) case next. At the\nfirst ring, messy individual \\(i_1\\) knows that one other person,\n\\(i_2\\), is messy, but does not yet know about himself. At the second\nring, \\(i_1\\) realizes that he must be messy, since had \\(i_2\\) been\nthe only messy one, \\(i_2\\) would have known this after the first ring\nwhen the cook made his announcement, and would have cleaned her face\nthen. By a symmetric argument, messy diner \\(i_2\\) also concludes that\nshe is messy at the second ring, and both pick up a napkin at that\ntime.\n\nThe general case follows by induction. Suppose that if \\(k = j\\), then\neach of the \\(j\\) messy diners can determine that he is messy after\n\\(j\\) rings. Then if \\(k = j + 1\\), then at the \\(j + 1\\)st\nring, each of the \\(j + 1\\) individuals will realize that he is messy.\nFor if he were not messy, then the other \\(j\\) messy ones would have\nall realized their messiness at the \\(j\\)th ring and\ncleaned themselves then. Since no one cleaned herself after the\n\\(j\\)th ring, at the \\(j + 1\\)st ring each messy\nperson will conclude that someone besides the other \\(j\\) messy people\nmust also be messy, namely, himself.\n\nThe \u201cparadox\u201d of this argument is that for \\(k \\gt 1\\),\nlike the case of the clumsy waiter of Example 1.1, the cook\u2019s\nannouncement told the diners something that each already knew. Yet\napparently the cook\u2019s announcement also gave the diners useful\ninformation. How could this be? By announcing a fact already known to\nevery diner, the cook made this fact common knowledge among\nthem, enabling each of them to eventually deduce the condition of his\nown face after sufficiently many rings of the\n bell.[4]\n1.3 The Farmer\u2019s Dilemma\n\nDoes meeting one\u2019s obligations to others serve one\u2019s\nself-interest? Plato and his successors recognized that in certain\ncases, the answer seems to be \u201cNo.\u201d Hobbes (1651, pp.\n101\u2013102) considers the challenge of a \u201cFoole\u201d, who\nclaims that it is irrational to honor an agreement made with another\nwho has already fulfilled his part of the agreement. Noting that in\nthis situation one has gained all the benefit of the other\u2019s\ncompliance, the Foole contends that it would now be best for him to\nbreak the agreement, thereby saving himself the costs of compliance.\nOf course, if the Foole\u2019s analysis of the situation is correct,\nthen would the other party to the agreement not anticipate the\nFoole\u2019s response to agreements honored, and act accordingly?\n\nHume (1740, pp. 520\u2013521) takes up this question, using an\nexample: Two neighboring farmers each expect a bumper crop of corn.\nEach will require his neighbor\u2019s help in harvesting his corn\nwhen it ripens, or else a substantial portion will rot in the field.\nSince their corn will ripen at different times, the two farmers can\nensure full harvests for themselves by helping each other when their\ncrops ripen, and both know this. Yet the farmers do not help each\nother. For the farmer whose corn ripens later reasons that if she were\nto help the other farmer, then when her corn ripens he would be in the\nposition of Hobbes\u2019 Foole, having already benefited from her\nhelp. He would no longer have anything to gain from her, so he would\nnot help her, sparing himself the hard labor of a second harvest.\nSince she cannot expect the other farmer to return her aid when the\ntime comes, she will not help when his corn ripens first, and of\ncourse the other farmer does not help her when her corn ripens\nlater.\n\nThe structure of Hume\u2019s Farmers\u2019 Dilemma problem\ncan be summarized using the following tree diagram:\n\n\n\nFigure 1.1a\n\n\nThis tree is an example of a game in extensive form. At each\nstage \\(i\\), the agent who moves can either choose \\(C^i\\), which\ncorresponds to helping or cooperating, or \\(D^i\\), which\ncorresponds to not helping or defecting. The relative\npreferences of the two agents over the various outcomes are reflected\nby the ordered pairs of payoffs each receives at any\nparticular outcome. If, for instance, Fiona chooses \\(C^i\\) and Alan\nchooses \\(D^i\\), then Fiona\u2019s payoff is 0, her worst payoff, and\nAlan\u2019s is 4, his best payoff. In a game such as the Figure 1.1.a\ngame, agents are (Bayesian) rational if each chooses an act\nthat maximizes her expected payoff, given what she knows.\n\nIn the Farmers\u2019 Dilemma game, following the \\(C^1,C^2\\)-path is\nstrictly better for both farmers than following the \\(D^1,D^2\\)-path.\nHowever, Fiona chooses \\(D^1\\), as the result of the following simple\nargument: \u201cIf I were to choose \\(C^1\\), then Alan, who is\nrational and who knows the payoff structure of the game, would choose\n\\(D^2\\). I am also rational and know the payoff structure of the game.\nSo I should choose \\(D^1\\).\u201d Since Fiona knows that Alan is\nrational and knows the game\u2019s payoffs, she concludes that she\nneed only analyze the reduced game in the following\nfigure:\n\n\n\nFigure 1.1b\n\n\nIn this reduced game, Fiona is certain to gain a strictly higher\npayoff by choosing \\(D^1\\) than if she chooses \\(C^1\\), so \\(D^1\\) is\nher unique best choice. Of course, when Fiona chooses \\(D^1\\), Alan,\nbeing rational, responds by choosing \\(D^2\\). If Fiona and Alan know:\n(i) that they are both rational, (ii) that they both know the payoff\nstructure of the game, and (iii) that they both know (i) and (ii),\nthen they both can predict what the other will do at every node of the\nFigure 1.1.a game, and conclude that they can rule out the\n\\(D^1,C^2\\)-branch of the Figure 1.1.b game and analyze just the\nreduced game of the following figure:\n\n\n\nFigure 1.1c\n\n\nOn account of this mutual knowledge, both know that Fiona\nwill choose \\(D^1\\), and that Alan will respond with \\(D^2\\). Hence,\nthe \\(D^1,D^2\\)-outcome results if the Farmers\u2019 Dilemma game is\nplayed by agents having this mutual knowledge, though it is suboptimal\nsince both agents would fare better at the\n \\(C^1,C^2\\)-branch.[5]\n This argument, which in its essentials is Hume\u2019s argument, is\nan example of a standard technique for solving sequential games known\nas backwards\n induction.[6]\n The basic idea behind backwards induction is that the agents engaged\nin a sequential game deduce how each will act throughout the entire\ngame by ruling out the acts that are not payoff-maximizing for the\nagents who would move last, then ruling out the acts that are not\npayoff-maximizing for the agents who would move next-to-last, and so\non. Clearly, backwards induction arguments rely crucially upon what,\nif any, mutual knowledge the agents have regarding their situation,\nand they typically require the agents to evaluate the truth values of\ncertain subjunctive conditionals, such as \u201cIf I (Fiona) were to\nchoose \\(C^1\\), then Alan would choose \\(D^2\\)\u201d.\n1.4 The Centipede\n\nThe mutual knowledge assumptions required to construct a backwards\ninduction solution to a game become more complex as the number of\nstages in the game increases. To see this, consider the sequential\nCentipede game depicted in the following figure:\n\n\n\nFigure 1.2\n\n\nAt each stage i\\), the agent who moves can either choose \\(R^i\\),\nwhich in the first three stages gives the other agent an opportunity\nto move, or \\(L^i\\), which ends the game.\n\nLike the Farmers\u2019 Dilemma, this game is a commitment problem for\nthe agents. If each agent could trust the other to choose \\(R^i\\) at\neach stage, then they would each expect to receive a payoff of 3.\nHowever, Alan chooses \\(L^1\\), leaving each with a payoff of only 1,\nas the result of the following backwards induction argument: \u201cIf\nnode \\(n_4\\) were to be reached, then Fiona, (being rational) would\nchoose \\(L^4\\). I, knowing this, would (being rational) choose \\(L^3\\)\nif node \\(n_3\\) were to be reached. Fiona, knowing this,\nwould (being rational) choose \\(L^2\\) if node \\(n_2\\) were to be\nreached. Hence, I (being rational) should choose \\(L^1\\).\u201d To\ncarry out this backwards induction argument, Alan implicitly assumes\nthat: (i) he knows that Fiona knows he is rational, and (ii) he knows\nthat Fiona knows that he knows she is rational. Put another way, for\nAlan to carry out the backwards induction argument, at node \\(n_1\\) he\nmust know what Fiona must know at node \\(n_2\\) to make \\(L^2\\) her\nbest response should \\(n_2\\) be reached. While in the Farmer\u2019s\nDilemma Fiona needed only first-order knowledge of\nAlan\u2019s rationality and second-order knowledge of\nAlan\u2019s knowledge of the game to derive the backwards induction\nsolution, in the Figure 1.2 game, for Alan to be able to derive the\nbackwards induction solution, the agents must have third-order\nmutual knowledge of the game and second-order mutual\nknowledge of rationality, and Alan must have\nfourth-order knowledge of this mutual knowledge of the game\nand third-order knowledge of their mutual knowledge of\nrationality. This argument also involves several counterfactuals,\nsince to construct it the agents must be able to evaluate conditionals\nof the form, \u201cIf node \\(n_i\\) were to be reached, Alan (Fiona)\nwould choose \\(L^i (R^i)\\)\u201d, which for \\(i \\gt 1\\) are\ncounterfactual, since third-order mutual knowledge of rationality\nimplies that nodes \\(n_2,\\) \\(n_3\\), and \\(n_4\\) are never\nreached.\n\nThe method of backwards induction can be applied to any sequential\ngame of perfect information, in which the agents can observe\neach others\u2019 moves in turn and can recall the entire history of\nplay. However, as the number of potential stages of play increases,\nthe backwards induction argument evidently becomes harder to\nconstruct. This raises certain questions: (1) What precisely are the\nmutual or common knowledge assumptions that are required to justify\nthe backwards induction argument for a particular sequential game? (2)\nAs a sequential game increases in complexity, would we expect the\nmutual knowledge that is required for backwards induction to start to\nfail?\n1.5 The Department Store\n\nWhen a man loses his wife in a department store without any prior\nunderstanding on where to meet if they get separated, the chances are\ngood that they will find each other. It is likely that each will think\nof some obvious place to meet, so obvious that each will be sure that\nit is \u201cobvious\u201d to both of them. One does not simply\npredict where the other will go, which is wherever the first predicts\nthe second to predict the first to go, and so ad infinitum.\nNot \u201cWhat would I do if I were she?\u201d but \u201cWhat would\nI do if I were she wondering what she would do if she were wondering\nwhat I would do if I were she \u2026 ?\u201d\n\u2014Thomas Schelling, The Strategy of\nConflict\n\n\nSchelling\u2019s department store problem is an example of a pure\ncoordination problem, that is, an interaction problem in which\nthe interests of the agents coincide perfectly. Schelling (1960) and\nLewis (1969), who were the first to make explicit the role common\nknowledge plays in social coordination, were also among the first to\nargue that coordination problems can be modeled using the analytic\nvocabulary of game theory. A very simple example of such a\ncoordination problem is given in the next figure:\n\n\n\n\n\u00a0\n\u00a0\nRobert \n\n\u00a0\n\u00a0\n\\(s_1\\)\n\\(s_2\\)\n\\(s_3\\)\n\\(s_4\\)  \n\n\nLiz\n\\(s_1\\)\n(4,3)\n(1,2)\n(1,2)\n(3,4) \n\n\\(s_3\\)\n(3,4)\n(1,3)\n(1,3)\n(4,3) \n\n\\(s_3\\)\n(3,4)\n(1,3)\n(1,3)\n(4,3) \n\n\\(s_3\\)\n(3,4)\n(1,3)\n(1,3)\n(4,3)  \n\n\n\\(s_i =\\) search on floor \\(i\\), \\(1 \\leq i \\leq 4\\)\n\nFigure 1.3\n\n\nThe matrix of Figure 1.3 is an example of a game in strategic\nform. At each outcome of the game, which corresponds to a cell in\nthe matrix, the row (column) agent receives as payoff the first\n(second) element of the ordered pair in the corresponding cell.\nHowever, in strategic form games, each agent chooses without first\nbeing able to observe the choices of any other agent, so that all must\nchoose as if they were choosing simultaneously. The Figure 1.3 game is\na game of pure coordination (Lewis 1969), that is, a game in\nwhich at each outcome, each agent receives exactly the same payoff.\nOne interpretation of this game is that Schelling\u2019s spouses, Liz\nand Robert, are searching for each other in the department store with\nfour floors, and they find each other if they go to the same floor.\nFour outcomes at which the spouses coordinate correspond to the\nstrategy profiles \\((s_j, s_j), 1 \\le j \\le 4\\), of the Figure 1.3\ngame. These four profiles are strict Nash equilibria (Nash\n1950, 1951) of the game, that is, each agent has a decisive reason to\nfollow her end of one of these strategy profiles provided that the\nother also follows this\n profile.[7]\n\nThe difficulty the agents face is trying to select an equilibrium to\nfollow. For suppose that Robert hopes to coordinate with Liz on a\nparticular equilibrium of the game, say \\((s_2, s_2)\\). Robert reasons\nas follows: \u201cSince there are several strict equilibria we might\nfollow, I should follow my end of \\((s_2, s_2)\\) if, and only if, I\nhave sufficiently high expectations that Liz will follow her end of\n\\((s_2, s_2)\\). But I can only have sufficiently high expectations\nthat Liz will follow \\((s_2, s_2 )\\) if she has sufficiently high\nexpectations that I will follow \\((s_2, s_2)\\). For her to have such\nexpectations, Liz must have sufficiently high (second-order)\nexpectations that I have sufficiently high expectations that she will\nfollow \\((s_2, s_2)\\), for if Liz doesn\u2019t have these\n(second-order) expectations, then she will believe I don\u2019t have\nsufficient reason to follow \\((s_2, s_2)\\) and may therefore deviate\nfrom \\((s_2, s_2)\\) herself. So I need to have sufficiently high\n(third-order) expectations that Liz has sufficiently high\n(second-order) expectations that I have sufficiently high expectations\nthat she will follow \\((s_2, s_2 )\\), which involves her in\nfourth-order expectations regarding me, which involves me in\nfifth-order expectations regarding Liz, and so on.\u201d What would\nsuffice for Robert, and Liz, to have decisive reason to follow \\((s_2,\ns_2)\\) is that they each know that the other knows\nthat \u2026 that the other will follow \\((s_2, s_2)\\) for any number\nof levels of knowledge, which is to say that between Liz and Robert it\nis common knowledge that they will follow \\((s_2, s_2)\\). If agents\nfollow a strict equilibrium in a pure coordination game as a\nconsequence of their having common knowledge of the game, their\nrationality and their intentions to follow this equilibrium, and no\nother, then the agents are said to be following a\nLewis-convention (Lewis 1969).\n\nLewis\u2019 theory of convention applies to a more general class of\ngames than pure coordination games, but pure coordination games\nalready model a variety of important social interactions. In\nparticular, Lewis models conventions of language as equilibrium points\nof a pure coordination game. The role common knowledge plays in games\nof pure coordination sketched above of course raises further\nquestions: (1) Can people ever attain the common knowledge which\ncharacterizes a Lewis-convention? (2) Would less stringent epistemic\nassumptions suffice to justify Nash equilibrium behavior in a\ncoordination problem?\n2. Alternative Accounts of Common Knowledge\n\n2.1 The Hierarchical Account\n2.2 Lewis\u2019 Account\n2.3 Aumann\u2019s Account\n2.4 Barwise\u2019s Account\n2.5 Gilbert\u2019s Account\n\n\nInformally, a proposition \\(A\\) is mutually known among a set\nof agents if each agent knows that \\(A\\). Mutual knowledge by itself\nimplies nothing about what, if any, knowledge anyone attributes to\nanyone else. Suppose each student arrives for a class meeting knowing\nthat the instructor will be late. That the instructor will be late is\nmutual knowledge, but each student might think only she knows the\ninstructor will be late. However, if one of the students says openly\n\u201cPeter told me he will be late again,\u201d then the mutually\nknown fact is now commonly known. Each student now knows that\nthe instructor will be late, and so on, ad infinitum. The\nagents have common knowledge in the sense articulated informally by\nSchelling (1960), and more precisely by Lewis (1969) and Schiffer\n(1972). Schiffer uses the formal vocabulary of\n epistemic logic\n (Hintikka 1962) to state his definition of common knowledge.\nSchiffer\u2019s general approach was to augment a system of\nsentential logic with a set of knowledge operators corresponding to a\nset of agents, and then to define common knowledge as a hierarchy of\npropositions in the augmented system. Bacharach (1992) and Bicchieri\n(1993) adopt this approach, and develop logical theories of common\nknowledge which include soundness and completeness theorems in the\nstyle of (Fagin et al. 1995). One can also develop formal accounts of\ncommon knowledge in set-theoretic terms, as it was done in the early\nFriedell (1969) and in the economic literature after Aumann (1976).\nSuch an approach, easily proven to be equivalent to the ones cast in\nepistemic logic, is taken also in this\n article.[8]\n2.1 The Hierarchical Account\n\nMonderer and Samet (1988) and Binmore and Brandenburger (1989) give a\nparticularly elegant set-theoretic definition of common knowledge. I\nwill review this definition here, and then show that it is logically\nequivalent to the \u2018\\(i\\) knows that \\(j\\) knows that \\(\\ldots\nk\\) knows that A\u2019 hierarchy that Lewis (1969) and Schiffer\n(1972) argue characterizes common\n knowledge.[9]\n\nSome preliminary notions must be stated first. Following C. I. Lewis\n(1943\u20131944) and Carnap (1947), propositions are formally subsets\nof a set \\(\\Omega\\) of state descriptions or possible\nworlds. One can think of the elements of \\(\\Omega\\) as\nrepresenting Leibniz\u2019s possible worlds or Wittgenstein\u2019s\npossible states of affairs. Some results in the common knowledge\nliterature presuppose that \\(\\Omega\\) is of finite cardinality. If\nthis admittedly unrealistic assumption is needed in any context, this\nwill be explicitly stated in this essay, and otherwise one may assume\nthat \\(\\Omega\\) may be either a finite or an infinite set. A\ndistinguished actual world \\(\\omega_{\\alpha}\\) is an element of\n\\(\\Omega\\). A proposition \\(A \\subseteq \\Omega\\) obtains (or is true)\nif the actual world \\(\\omega_{\\alpha} \\in A\\). In general, we say that\n\\(A\\) obtains at a world \\(\\omega \\in \\Omega\\) if \\(\\omega\n\\in A\\). What an agent \\(i\\) knows about the possible worlds is stated\nformally in terms of a knowledge operator \\(\\mathbf{K}_i\\).\nGiven a proposition \\(A \\subseteq \\Omega , \\mathbf{K}_i (A)\\) denotes\na new proposition, corresponding to the set of possible worlds at\nwhich agent \\(i\\) knows that A obtains. \\(\\mathbf{K}_i (A)\\) is read\nas \u2018\\(i\\) knows (that) \\(A\\) (is the case)\u2019. The knowledge\noperator \\(\\mathbf{K}_i\\) satisfies certain axioms, including:\n\n\\[\\begin{align}\n\\tag{K1} \\mathbf{K}_i (A) &\\subseteq A \\\\\n\\tag{K2} \\Omega &\\subseteq \\mathbf{K}_i (\\Omega) \\\\\n\\tag{K3} \\mathbf{K}_i(\\bigcap_k A_k) &= \\bigcap_k \\mathbf{K}_i (A_k) \\\\\n\\tag{K4} \\mathbf{K}_i (A) &\\subseteq \\mathbf{K}_i\\mathbf{K}_i(A) \\\\\n\\tag{K5} -\\mathbf{K}_i (A) &\\subseteq \\mathbf{K}_i -\\mathbf{K}_{i}(A)\n\\end{align}\\]\n\n\nIn words, K1 says that if \\(i\\) knows \\(A\\), then \\(A\\) must be the\ncase. K2 says that \\(i\\) knows that some possible world in \\(\\Omega\\)\noccurs no matter which possible world \\(\\omega\\) occurs.\n K3[10]\n says that \\(i\\) knows a conjunction if, and only if, \\(i\\) knows each\nconjunct. K4 is a reflection axiom, sometimes also presented\nas the axiom of transparency (or of positive\nintrospection), which says that if \\(i\\) knows \\(A\\), then \\(i\\)\nknows that she knows \\(A\\). Finally, K5 says that if the agent does\nnot know an event, then she knows that she does not know.\nThis axiom is presented as the axiom of negative\nintrospection, or as the axiom of wisdom (since the\nagents possess Socratic wisdom, knowing that they do not know.) Note\nthat by K3, if \\(A \\subseteq B\\) then \\(\\mathbf{K}_i (A) \\subseteq\n\\mathbf{K}_i (B)\\), by K1 and K2, \\(\\mathbf{K}_i (\\Omega) = \\Omega\\),\nand by K1 and K4, \\(\\mathbf{K}_i (A) = \\mathbf{K}_i \\mathbf{K}_i\n(A)\\). Any system of knowledge satisfying K1\u2013K5 corresponds to\nthe modal system S5, while any system satisfying K1\u2013K4\ncorresponds to S4 (Kripke 1963). If one drops the K1 axiom and retains\nthe others, the resulting system would give a formal account of what\nan agent believes, but does not necessarily\nknow.\n\nA useful notion in the formal analysis of knowledge is that of a\npossibility set. An agent i\u2019s possibility set at a\nstate of the world \\(\\Omega\\) is the smallest set of possible worlds\nthat \\(i\\) thinks could be the case if \\(\\omega\\) is the actual world.\nMore precisely,\n\n\nDefinition 2.1\n\nAgent \\(i\\)\u2019s possibility set \\(\\mathcal{H}_i\n(\\omega)\\) at \\(\\omega \\in \\Omega\\) is defined as \n\n\\[\\mathcal{H}_i (\\omega) \\equiv \\bigcap \\{ E \\mid \\omega \\in \\mathbf{K}_i (E) \\}\\]\n\n\nThe collection of sets \n\n\\[\\mathcal{H}_i = \\bigcup_{\\omega \\in \\Omega} \\mathcal{H}_{i}(\\omega)\\]\n\n\nis \\(i\\)\u2019s private information system.\n\n\nSince in words, \\(\\mathcal{H}_i (\\omega)\\) is the intersection of all\npropositions which \\(i\\) knows at \\(\\omega , \\mathcal{H}_i (\\omega)\\)\nis the smallest proposition in \\(\\Omega\\) that \\(i\\) knows at\n\\(\\omega\\). Put another way, \\(\\mathcal{H}_i (\\omega)\\) is the most\nspecific information that \\(i\\) has about the possible world\n\\(\\omega\\). The intuition behind assigning agents private information\nsystems is that while an agent \\(i\\) may not be able to perceive or\ncomprehend every last detail of the world in which \\(i\\) lives, \\(i\\)\ndoes know certain facts about that world. The elements of\n\\(i\\)\u2019s information system represent what \\(i\\) knows\nimmediately at a possible world. We also have the following:\n\nProposition 2.2\n\n\\(\\mathbf{K}_i (A) = \\{ \\omega \\mid \\mathcal{H}_i (\\omega) \\subseteq A\n\\}\\) \n\nIn many formal analyses of knowledge in the literature, possibility\nsets are taken as primitive and Proposition 2.2 is given as the\ndefinition of knowledge. If one adopts this viewpoint, then the axioms\nK1\u2013K5 follow as consequences of the definition of knowledge. In\nmany applications, the agents\u2019 possibility sets are assumed to\n partition[11]\n the set, in which case \\(\\mathcal{H}_i\\) is called i\u2019s\nprivate information partition. Notice that if axioms\nK1\u2013K5 hold, then the possibility sets of each agent always\npartition the state set, and vice versa.\n\nTo illustrate the idea of possibility sets, let us return to the\nBarbecue Problem described in Example 1.2. Suppose there are three\ndiners: Cathy, Jennifer and Mark. Then there are 8 relevant states of\nthe world, summarized by Table 2.1:\n\nTable 2.1\n\n\n\\(\\omega_1\\)\n\\(\\omega_2\\)\n\\(\\omega_3\\)\n\\(\\omega_4\\)\n\\(\\omega_5\\)\n\\(\\omega_6\\)\n\\(\\omega_7\\)\n\\(\\omega_8\\) \n\nCathy\nclean\nmessy\nclean\nclean\nmessy\nmessy\nclean\nmessy \n\nJennifer\nclean\nclean\nmessy\nclean\nmessy\nclean\nmessy\nmessy \n\nMark\nclean\nclean\nclean\nmessy\nclean\nmessy\nmessy\nmessy \n\n\nEach diner knows the condition of the other diners\u2019 faces, but\nnot her own. Suppose the cook makes no announcement, after all. Then\nnone of the diners knows the true state of the world whatever \\(\\omega\n\\in \\Omega\\) the actual world turns out to be, but they do know a\npriori that certain propositions are true at various states of\nthe world. For instance, Cathy\u2019s information system before any\nannouncement is made is depicted in Figure 2.1a:\n\n\n\nFigure 2.1a\n\n\nIn this case, Cathy\u2019s information system is a partition\n\\(\\mathcal{H}_1\\) of \\(\\Omega\\) defined by \n\n\\[\n\\mathcal{H}_1 = \\{H_{CC}, H_{CM}, H_{MC}, H_{MM}\\}\n\\]\n\n\nwhere \n\n\\[\\begin{align}\nH_{CC} &= \\{\\omega_1, \\omega_2\\} \\text{ (i.e., Jennifer and Mark are both clean)} \\\\\nH_{CM} &= \\{\\omega_4, \\omega_6\\} \\text{ (i.e., Jennifer is clean and Mark is messy)} \\\\\nH_{MC} &= \\{\\omega_3, \\omega_5\\} \\text{ (i.e., Jennifer is messy and Mark is clean)} \\\\\nH_{MM} &= \\{\\omega_7, \\omega_8\\} \\text{ (i.e., Jennifer and Mark are both messy)}\n\\end{align}\\]\n\n\nCathy knows immediately which cell \\(\\mathcal{H}_1 (\\omega)\\) in her\npartition is the case at any state of the world, but does not know\nwhich is the true state at any \\(\\omega \\in \\Omega\\).\n\nIf we add in the assumption stated in Example 1.2 that if there is at\nleast one messy diner, then the cook announces the fact, then\nCathy\u2019s information partition is depicted by Figure 2.1b:\n\n\n\nFigure 2.1b\n\n\nIn this case, Cathy\u2019s information system is a partition\n\\(\\mathcal{H}_1\\) of \\(\\Omega\\) defined by \n\n\\[\n\\mathcal{H}_1 = \\{H_{CCC}, H_{MCC}, H_{CM}, H_{MC}, H_{MM}\\}\n\\]\n\n\nwhere \n\n\\[\\begin{align}\nH_{CCC} &= \\{\\omega_1\\} &&\\text{ (i.e., Jennifer, Mark, and I are all clean)} \\\\\nH_{MCC} &= \\{\\omega_2\\} &&\\text{ (i.e., Jennifer and Mark are clean and I am messy)} \\\\\nH_{CM} &=  \\{\\omega_4, \\omega_6\\} && \\text{ (i.e., Jennifer is clean and Mark is messy)} \\\\\nH_{MC} &= \\{\\omega_3, \\omega_5\\} &&\\text{ (i.e., Jennifer is messy and Mark is clean)} \\\\\nH_{MM} &= \\{\\omega_7, \\omega_8\\} &&\\text{ (i.e., Jennifer and Mark are both messy)} \n\\end{align}\\]\n\n\nIn this case, Cathy\u2019s information partition is a\nrefinement of the partition she has when there is no\nannouncement, for in this case, then Cathy knows a priori\nthat if \\(\\omega_1\\) is the case there will be no announcement and\nwill know immediately that she is clean, and Cathy knows a\npriori that if \\(\\omega_2\\) is the case, then she will know\nimmediately from the cook\u2019s announcement that she is messy.\n\nSimilarly, if the cook makes an announcement only if he sees at least\ntwo messy diners, Cathy\u2019s possibility set is the one represented\nin fig. 2.1c:\n\n\n\nFigure 2.1c\n\n\nCathy\u2019s information partition is now defined by \n\n\\[\n\\mathcal{H}_1 = \\{H_{CC}, H_{CMC}, H_{CCM}, H_{MMC}, H_{MCM}, H_{MM}\\}\n\\]\n\n\nwhere \n\n\\[\\begin{align}\nH_{CC} &= \\{\\omega_1, \\omega_2\\} &&\\text{ (i.e., Jennifer and Mark are both clean)} \\\\\nH_{CMC} &= \\{\\omega_3\\} &&\\text{ (i.e., Mark and I are clean, Jennifer is messy)} \\\\\nH_{CCM} &= \\{\\omega_4\\} &&\\text{ (i.e., Jennifer and I are clean, Mark is messy)} \\\\\nH_{CCM} &= \\{\\omega_5\\} &&\\text{ (i.e., Jennifer and I are messy, Mark is clean)} \\\\\nH_{CCM} &= \\{\\omega_6\\} &&\\text{ (i.e., Mark and I are messy, Jennifer is clean)} \\\\\nH_{MM} &= \\{\\omega_7, \\omega_8\\} &&\\text{ (i.e., Jennifer and Mark are both messy)}\n\\end{align}\\]\n\n\nIn this case, Cathy knows a priori that if \\(\\omega_3\\)\nobtains there will be no announcement, and similarly for \\(\\omega_4\\).\nThus, she will be able to distinguish these states from \\(\\omega_5\\)\nand \\(\\omega_6\\), respectively.\n\nAs mentioned earlier in this subsection, the assumption that\nagents\u2019 possibility sets partition the state space depends on\nthe modeler\u2019s choice of specific axioms for the knowledge\noperators. For example, if we drop axiom K5 (preserving the validity\nof K1\u2013K4) the agent\u2019s possibility sets need not partition\nthe space set (follow the link for an\n example.\n For more details and applications, cf. Samet 1990.) It was\nconjectured (cf. Geanakoplos 1989) that lack of negative introspection\n(i.e. systems without K5) would allow to incorporate unforeseen\ncontingencies in the epistemic model, by representing the\nagents\u2019 unawareness of certain events (i.e. the case in\nwhich the agent does not know that an event occurs and also does not\nknow that she does not know that.) It was later shown by Dekel et al.\n(1998) that standard models are not suitable to represent\nagents\u2019 unawareness. An original non-standard model to represent\nunawareness is provided in Heifetz et al. (2006). For a\ncomprehensive bibliography on modeling unawareness and applications of\nthe notion, cf. the external links at the end on this entry.\n\nWe can now define mutual and common knowledge as follows:\n\n\nDefinition 2.3\n\nLet a set \\(\\Omega\\) of possible worlds together with a set of agents\n\\(N\\) be given.\n\n1. The proposition that \\(A\\) is (first level or first\norder) mutual knowledge for the agents of N,\n\\(\\mathbf{K}^{1}_N (A)\\), is the set defined by \n\n\\[\n\\mathbf{K}^1_N (A) \\equiv \\bigcap_{i\\in N} \\mathbf{K}_i(A).\n\\]\n\n\n2. The proposition that \\(A\\) is \\(m\\)th level (or\n\\(m\\)th order) mutual knowledge among the\nagents of N, \\(\\mathbf{K}^m_N(A),\\) is defined recursively as the\nset \n\n\\[\n\\mathbf{K}^m_N(A) \\equiv \\bigcap_{i\\in N} \\mathbf{K}_i (\\mathbf{K}^{m-1}_N(A)).\n\\]\n\n\n3. The proposition that \\(A\\) is common knowledge among the\nagents of \\(N, \\mathbf{K}^{*}_N (A),\\) is defined as the\n set[12]\n \n\n\\[\n\\mathbf{K}^*_N (A) \\equiv \\bigcap_{m=1}^{\\infty} \\mathbf{K}^m_N(A).\n\\]\n\n\n\nCommon knowledge of a proposition \\(E\\) implies common knowledge of\nall that \\(E\\) implies, as is shown in the following:\n\n\nProposition 2.4\n\nIf \\(\\omega \\in \\mathbf{K}^{*}_N (E)\\) and \\(E \\subseteq F\\), then\n\\(\\omega \\in \\mathbf{K}^{*}_N (F)\\).\n\nProof.\n\n\nNote that \\((\\mathbf{K}^m_N(E))_{m\\ge 1}\\) is a decreasing sequence of\nevents, in the sense that \\(\\mathbf{K}^{m+1}_N (E) \\subseteq\n\\mathbf{K}^m_N(E)\\), for all \\(m \\ge 1\\). It is also easy to check\nthat if everyone knows \\(E\\), then \\(E\\) must be true, that is,\n\\(\\mathbf{K}^1_N (E) \\subseteq E\\). If \\(\\Omega\\) is assumed to be\nfinite, then if \\(E\\) is common knowledge at \\(\\omega\\), this implies\nthat there must be a finite \\(m\\) such that \n\n\\[\n\\mathbf{K}^m_N(E) = \\bigcap_{n=1}^{\\infty} \\mathbf{K}^n_N(E).\n\\]\n\n\nThe following result relates the set-theoretic definition of common\nknowledge to the hierarchy of \u2018\\(i\\) knows that \\(j\\) knows that\n\u2026 knows \\(A\\)\u2019 statements.\n\n\nProposition 2.5\n\n\\(\\omega \\in \\mathbf{K}^m_N(A)\\) iff\n\n(1) For all agents \\(i_1, i_2 , \\ldots ,i_m \\in N, \\omega \\in\n\\mathbf{K}_{i_1}\\mathbf{K}_{i_2} \\ldots \\mathbf{K}_{i_m}(A)\\)\n\nHence, \\(\\omega \\in \\mathbf{K}^*_N (A)\\) iff (1) is the case for each\n\\(m \\ge 1\\).\n\nProof.\n\n\nThe condition that \\(\\omega \\in \\mathbf{K}_{i_1}\\mathbf{K}_{i_2}\n\\ldots \\mathbf{K}_{i_m}(A)\\) for all \\(m \\ge 1\\) and all \\(i_1, i_2 ,\n\\ldots ,i_m \\in N\\) is Schiffer\u2019s definition of common\nknowledge, and is often used as the definition of common knowledge in\nthe literature.\n2.2 Lewis\u2019 Account\n\nLewis is credited with the idea of characterizing common knowledge as\na hierarchy of \u2018\\(i\\) knows that \\(j\\) knows that \u2026 knows\nthat \\(A\\)\u2019 propositions. However, Lewis is aware of the\ndifficulties that such an infinitary definition raises. A first\nproblem is whether it is possible to reduce the infinity inherent in\nthe hierarchical account into a workable finite definition. A second\nproblem is the issue that finite agents cannot entertain the infinite\namount of epistemic states which is necessary for common knowledge to\nobtain. Lewis tackles both problems, but his presentation is informal.\nAumann is often credited with presenting the first finitary method of\ngenerating the common knowledge hierarchy (Aumann 1976), even though\n(Friedell 1969) in fact predates both Aumann\u2019s and Lewis\u2019s\nwork. Recently, Cubitt and Sugden (2003) have argued that\nAumann\u2019s and Lewis\u2019 accounts of common knowledge are\nradically different and irreconcilable.\n\nAlthough Lewis introduced the technical term \u2018common\nknowledge,\u2019 his analysis is about belief, rather than knowledge.\nIndeed, Lewis offers his solution to the second problem mentioned\nabove by introducing a distinction between actual belief and\nreason to believe. Reasons to believe are interpreted as\npotential beliefs of agents, so that the infinite hierarchy of\nepistemic states becomes harmless, consisting in an infinite number of\nstates of potential belief. The solution to the first problem is given\nby providing a finite set of conditions that, if met, generate the\ninfinite series of reasons to believe. Such conditions taken together\nrepresent Lewis\u2019 official definition of common knowledge. Notice\nthat it would be more appropriate to speak of \u2018common reason to\nbelieve,\u2019 or, at least, of \u2018common belief.\u2019 Lewis\nhimself later acknowledges that \u201c[t]hat term [common knowledge]\nwas unfortunate, since there is no assurance that it will be\nknowledge, or even that it will be true.\u201d Cf. (Lewis 1978, p.\n44, n.13) Disregarding the distinction between reasons to believe and\nactual belief, we follow (Vanderschraaf 1998) to give the details of a\nformal account of Lewis\u2019s definition here, and show that\nLewis\u2019 analysis does result in the common knowledge hierarchy\nfollowing from a finite set of axioms. It is however debatable whether\na possible worlds approach can properly render the subtleties of\nLewis\u2019 characterization. Cubitt and Sugden (2003), for example,\nabandon the possible worlds framework altogether and propose a\ndifferent formal interpretation of Lewis in which, among other\nelements, the distinction between reasons to believe and actual belief\nis taken into account. An attempt to reconcile the two positions can\nbe found in (Sillari 2005), where Lewis\u2019 characterization is\nformalized in a richer possible worlds semantic framework where the\ndistinction between reasons to believe and actual believe is\nrepresented.\n\nLewis presents his account of common knowledge on pp. 52\u201357 of\nConvention. Lewis does not specify what account of knowledge\nis needed for common knowledge. As it turns out, Lewis\u2019 account\nis satisfactory for any formal account of knowledge in which the\nknowledge operators \\(\\mathbf{K}_i, i \\in N\\), satisfy K1, K2, and K3.\nA crucial assumption in Lewis\u2019 analysis of common knowledge is\nthat agents know they share the same \u201crationality, inductive\nstandards and background information\u201d (Lewis 1969, p. 53) with\nrespect to a state of affairs \\(A'\\), that is, if an agent can draw\nany conclusion from \\(A'\\), she knows that all can do likewise. This\nidea is made precise in the following:\n\nDefinition 2.6\n\nGiven a set of agents \\(N\\) and a proposition \\(A' \\subseteq \\Omega\\),\nthe agents of \\(N\\) are symmetric reasoners with respect to\n\\(A' (or A'\\)-symmetric reasoners) iff, for each \\(i, j \\in\nN\\) and for any proposition \\(E \\subseteq \\Omega\\), if \\(\\mathbf{K}_i\n(A') \\subseteq \\mathbf{K}_i (E)\\) and \\(\\mathbf{K}_i (A') \\subseteq\n\\mathbf{K}_i\\mathbf{K}_j(A')\\), then \\(\\mathbf{K}_i (A') \\subseteq\n \\mathbf{K}_i\\mathbf{K}_j(E)\\).[13]\n\n\nThe definiens says that for each agent \\(i\\), if \\(i\\) can infer from\n\\(A'\\) that \\(E\\) is the case and that everyone knows that \\(A'\\) is\nthe case, then \\(i\\) can also infer that everyone knows that \\(E\\) is\nthe case.\n\n\nDefinition 2.7\n\nA proposition \\(E\\) is Lewis-common knowledge at \\(\\omega \\in\n\\Omega\\) among the agents of a set \\(N = \\{1, \\ldots ,n\\}\\) iff there\nis a proposition \\(A\\)* such that \\(\\omega \\in A\\)*, the agents of\n\\(N\\) are \\(A\\)*-symmetric reasoners, and for every \\(i \\in N\\),\n\n\\[\\begin{align}\n\\tag{L1} &\\omega \\in \\mathbf{K}_i (A^*) \\\\\n\\tag{L2} &\\mathbf{K}_i(A*) \\subseteq \\mathbf{K}_i(\\bigcap_{j\\in N} \\mathbf{K}_j(A^*)) \\\\\n\\tag{L3} &\\mathbf{K}_i (A*) \\subseteq \\mathbf{K}_i (E)\n\\end{align}\\]\n\n\n\\(A\\)* is a basis for the agents\u2019 common knowledge.\n\\(\\mathbf{L}*_N (E)\\) denotes the proposition defined by L1\u2013L3\nfor a set \\(N\\) of \\(A\\)*-symmetric reasoners, so we can say that\n\\(E\\) is Lewis-common knowledge for the agents of \\(N\\) iff \\(\\omega\n\\in \\mathbf{L}*_N (E)\\).\n\n\nIn words, L1 says that \\(i\\) knows \\(A\\)* at \\(\\omega\\). L2 says that\nif \\(i\\) knows that \\(A\\)* obtains, then \\(i\\) knows that everyone\nknows that \\(A\\)* obtains. This axiom is meant to capture the idea\nthat common knowledge is based upon a proposition \\(A\\)* that is\npublicly known, as is the case when agents hear a public\nannouncement. If the agents\u2019 knowledge is represented by\npartitions, then a typical basis for the agents\u2019 common\nknowledge would be an element \\(\\mathcal{M}(\\omega)\\) in the\n meet[14]\n of their partitions. L3 says that \\(i\\) can infer from \\(A\\)* that\n\\(E\\). Lewis\u2019 definition implies the entire common knowledge\nhierarchy, as is shown in the following result.\n\n\nProposition 2.8\n\n\\(\\mathbf{L}*_N (E) \\subseteq \\mathbf{K}*_N (E)\\), that is,\nLewis-common knowledge of \\(E\\) implies common knowledge of \\(E\\).\n\nProof.\n\n\nAs mentioned above, it has recently come into question whether a\nformal rendition of Lewis\u2019 definition as the one given above\nadequately represents all facets of Lewis\u2019 approach. Cubitt and\nSugden (2003) argue that it does not, their critique hinging on a\nfeature of Lewis\u2019 analysis that is lost in the possible worlds\nframework, namely the 3-place relation of indication used by\nLewis. The definition of indication can be found at pp. 52\u201353 of\nConvention:\n\nDefinition 2.9\n\nA state of affairs \\(A\\) indicates \\(E\\) to agent \\(i\\) \\((A\n\\indi E)\\) if and only if, if \\(i\\) had reason to believe that \\(A\\)\nheld, \\(i\\) would thereby have reason to believe that \\(E\\)\n\nThe wording of Lewis\u2019 definition and the use he makes of the\nindication relation in the definitory clauses for common knowledge,\nsuggest that Lewis is careful to distinguish indication and material\nimplication. Cubitt and Sugden (2003) incorporate such distinction in\ntheir formal reconstruction. Paired with their interpretation of\n\u201c\\(i\\) has reason to believe \\(x\\)\u201d as \u201c\\(x\\) is\nyielded by some logic of reasoning that \\(i\\) endorses,\u201d we have\nthat, if \\(A \\indi x,\\) then \\(i\\)\u2019s reason to believe \\(A\\)\nprovides \\(i\\) with reason to believe \\(x\\) as well. Given that Lewis\ndoes want to endow agents with deductive reasoning, (Cubitt and Sugden\n2003) list the following axioms, claiming that they capture the\ndesired properties of indication. For all agents \\(i, j,\\) with\n\\(\\mathbf{R}_i A\\) standing for \u201cagent \\(i\\) has reason to\nbelieve A\u201d, we have \n\n\\[\\begin{align}\n\\tag{CS1} (\\mathbf{R}_i A \\wedge A \\indi x) &\\to \\mathbf{R}_i x \\\\\n\\tag{CS2} (A \\text{ entails } B) &\\to A \\indi B \\\\\n\\tag{CS3} (A \\indi x \\wedge A \\indi y) &\\to A \\indi (x \\wedge y) \\\\\n\\tag{CS4} (A \\indi B \\wedge B \\indi x) &\\to A \\indi x \\\\\n\\tag{CS5} ((A \\indi \\mathbf{R}_j B) \\wedge \\mathbf{R}_i(B \\indj x)) &\\to A \\indi \\mathbf{R}_j x\n\\end{align}\\]\n\n\nThe first axioms captures the intuition behind indication. It says\nthat if an agent has reason to believe that \\(A\\) holds, then, if\n\\(A\\) indicates \\(x\\) to her, she has reason to believe \\(x\\) as well.\nCS2 says that indication extends material implication. CS3 says that\nif two propositions \\(x\\) and \\(y\\) are indicated to an agent by a\nproposition \\(A\\), then \\(A\\) indicates to her also the conjunction of\n\\(x\\) and \\(y\\). The next axiom states that indication is transitive.\nCS5 says that if a proposition \\(A\\) indicates to \\(i\\) that agent\n\\(j\\) has reason to believe \\(B\\), and \\(i\\) has reason to believe\nthat \\(B\\) indicates \\(x\\) to \\(j\\), then \\(A\\) indicates to \\(i\\)\nalso that \\(j\\) has reason to believe \\(x\\).\n\nArmed with these axioms, it is possible to give the following\ndefinition.\n\n\nDefinition 2.10\n\nIn any given population \\(P\\) a proposition \\(A\\) is a reflexive\ncommon indicator that x if and only if, for all \\(i, j \\in P\\)\nand all propositions \\(x, y,\\) the following four conditions hold:\n\n\\[\\begin{align}\n\\tag{RCI1} &A \\to \\mathbf{R}_i  A \\\\\n\\tag{RCI2} &A \\indi \\mathbf{R}_j A \\\\\n\\tag{RCI3} &A \\indi x \\\\\n\\tag{RCI4} &A \\indj y \\to \\mathbf{R}_i(A \\indj y)\n\\end{align}\\]\n\n\n\nClauses RCI1\u2013RCI3 above render L1\u2013L3 of definition 2.7\nabove in the formal language that underlies axioms CS1\u2013CS5;\nwhile RCI4 affirms (cf. definition 2.6 above) that agents are\nsymmetric reasoners, i.e. that if a proposition indicates another\nproposition to a certain agent, then it does so to all agents in the\npopulation.\n\nThe following proposition shows that RCI1\u2013RCI4 are sufficient\nconditions for \u2018common reason to believe\u2019 to arise:\n\n\nProposition 2.11\n\nIf \\(A\\) holds, and if \\(A\\) is a common reflexive indicator in the\npopulation \\(P\\) that \\(x\\), then there is common reason to believe in\n\\(P\\) that \\(x\\).\n\nProof.\n\n\nA group of (ideal) faultless reasoners who have common reason\nto believe that \\(p\\), will achieve common belief in \\(p\\).\n\nIs it possible to take formally in account the insights of\nLewis\u2019 definition of common knowledge without abandoning the\npossible world framework? (Sillari 2005) puts forth an attempt to give\na positive answer to that question by articulating in a possible world\nsemantics the distinction between actual belief and reason to believe.\nAs in (Cubitt and Sugden 2003), the basic epistemic operator\nrepresents reasons to believe. The idea is then to impose an\nawareness structure over possible worlds, adopting the\nframework first introduced by Fagin and Halpern (1988). Simply put, an\nawareness structure associates to each agent, for every possible\nworld, a set of events of which the agent is said to be aware. An\nagent entertains an actual belief that a certain event occurs if and\nonly if she has reason to believe that the event occurs and\nsuch event is in her awareness set at the world under consideration. A\ndifferent avenue to the formalization of Lewis\u2019s account of\ncommon knowledge is offered by Paternotte (2011), where the central\nnotion is probabilistic common belief (see section 5.2 below).\n2.3 Aumann\u2019s Account\n\nAumann (1976) gives a different characterization of common knowledge\nwhich gives another simple algorithm for determining what information\nis commonly known. Aumann\u2019s original account assumes that the\neach agent\u2019s possibility set forms a private information\npartition of the space \\(\\Omega\\) of possible worlds. Aumann shows\nthat a proposition C is common knowledge if, and only if, C contains a\ncell of the meet of the agents\u2019 partitions. One way to compute\nthe meet \\(\\mathcal{M}\\) of the partitions \\(\\mathcal{H}_i, i \\in N\\)\nis to use the idea of \u201creachability\u201d.\n\n\nDefinition 2.13\n\nA state \\(\\omega ' \\in \\Omega\\) is reachable from \\(\\omega\n\\in \\Omega\\) iff there exists a sequence \n\n\\[\\omega =\\omega_0, \\omega_1, \\omega_2 , \\ldots ,\\omega_m =\\omega'\\]\n\n\nsuch that for each \\(k \\in \\{0,1, \\ldots ,m-1\\}\\), there exists an\nagent \\(i_k \\in N\\) such that \\(\\mathcal{H}_{i_{ k} }(\\omega_k) =\n\\mathcal{H}_{i_{ k} }(\\omega_{k+1})\\).\n\n\nIn words, \\(\\omega '\\) is reachable from \\(\\omega\\) if there exists a\nsequence or \u201cchain\u201d of states from \\(\\omega\\) to \\(\\omega\n'\\) such that two consecutive states are in the same cell of some\nagent\u2019s information partition. To illustrate the idea of\nreachability, let us return to the modified Barbecue Problem in which\nCathy, Jennifer and Mark receive no announcement. Their information\npartitions are all depicted in Figure 2.1d:\n\n\n\nFigure 2.1d\n\n\nOne can understand the importance of the notion of reachability in the\nfollowing way: If \\(\\omega '\\) is reachable from \\(\\omega\\), then if\n\\(\\omega\\) obtains then some agent can reason that some other agent\nthinks that \\(\\omega '\\) is possible. Looking at Figure 2.1d, if\n\\(\\omega = \\omega_1\\) occurs, then Cathy (who knows only that\n\\(\\{\\omega_1, \\omega_2\\}\\) has occurred) knows that Jennifer thinks\nthat \\(\\omega_5\\) might have occurred (even though Cathy knows that\n\\(\\omega_5\\) did not occur). So Cathy cannot rule out the possibility\nthat Jennifer thinks that Mark thinks that that \\(\\omega_8\\) might\nhave occurred. And Cathy cannot rule out the possibility that Jennifer\nthinks that Mark thinks that Cathy believes that \\(\\omega_7\\) is\npossible. In this sense, \\(\\omega_7\\) is reachable from \\(\\omega_1\\).\nThe chain of states which establishes this is \\(\\omega_1, \\omega_2,\n\\omega_5, \\omega_8, \\omega_7\\), since \\(\\mathcal{H}_1 (\\omega_1) =\n\\mathcal{H}_1 (\\omega_2),\\) \\(\\mathcal{H}_2 (\\omega_2) = \\mathcal{H}_2\n(\\omega_5),\\) \\(\\mathcal{H}_3 (\\omega_5) = \\mathcal{H}_3 (\\omega_8),\\)\nand \\(\\mathcal{H}_1 (\\omega_8) = \\mathcal{H}_1 (\\omega_7)\\). Note that\none can show similarly that in this example any state is reachable\nfrom any other state. This example also illustrates the following\nimmediate result:\n\n\nProposition 2.14\n\n\\(\\omega'\\) is reachable from \\(\\omega\\) iff there is a sequence\n\\(i_1, i_2 , \\ldots ,i_m \\in N\\) such that  \n\n\\[\n \\omega' \\in \\mathcal{H}_{i_m}(\\cdots(\\mathcal{H}_{i_2}(\\mathcal{H}_{i_1}(\\omega))))\n\\]\n\n\n\nOne can read (1) as: \u2018At \\(\\omega , i_1\\) thinks that \\(i_2\\)\nthinks that \\(\\ldots ,i_m\\) thinks that \\(\\omega'\\) is\npossible.\u2019\n\nWe now have:\n\n\nLemma 2.15\n\n\\(\\omega' \\in \\mathcal{M}(\\omega)\\) iff \\(\\omega '\\) is reachable from\n\\(\\omega\\).\n\nProof.\n\n\nand\n\n\nLemma 2.16\n\n\\(\\mathcal{M}(\\omega)\\) is common knowledge for the agents of \\(N\\) at\n\\(\\omega\\).\n\nProof.\n\n\nand\n\n\nProposition 2.17 (Aumann 1976)\n\nLet \\(\\mathcal{M}\\) be the meet of the agents\u2019 partitions\n\\(\\mathcal{H}_i\\) for each \\(i \\in N\\). A proposition \\(E \\subseteq\n\\Omega\\) is common knowledge for the agents of \\(N\\) at \\(\\omega\\) iff\n\\(\\mathcal{M}(\\omega) \\subseteq E\\). (In Aumann (1976), \\(E\\) is\ndefined to be common knowledge at \\(\\omega\\) iff\n\\(\\mathcal{M}(\\omega) \\subseteq E\\).)\n\nProof.\n\n\nIf \\(E = \\mathbf{K}^1_N (E)\\), then \\(E\\) is a public event\n(Milgrom 1981) or a common truism (Binmore and Brandenburger\n1989). Clearly, a common truism is common knowledge whenever it\noccurs, since in this case \\(E = \\mathbf{K}^1_N (E) = \\mathbf{K}^2_N\n(E) =\\ldots\\) , so \\(E = \\mathbf{K}^*_N (E)\\). The proof of\nProposition 2.17 shows that the common truisms are precisely the\nelements of \\(\\mathcal{M}\\) and unions of elements of \\(\\mathcal{M}\\),\nso any commonly known event is the consequence of a common truism.\n2.4 Barwise\u2019s Account\n\nBarwise (1988) proposes another definition of common knowledge that\navoids explicit reference to the hierarchy of \u2018\\(i\\) knows that\n\\(j\\) knows that \u2026 knows that \\(A\\)\u2019 propositions.\nBarwise\u2019s analysis builds upon an informal proposal by Harman\n(1977). Consider the situation of the guest and clumsy waiter in\nExample 1 when he announces that he was at fault. They are now in a\nsetting where they have heard the waiter\u2019s announcement and know\nthat they are in the setting. Harman adopts the circularity in this\ncharacterization of the setting as fundamental, and propses a\ndefinition of common knowledge in terms of this circularity.\nBarwise\u2019s formal analysis gives a precise formulation of\nHarman\u2019s intuitive analysis of common knowledge as a fixed\npoint. Given a function \\(f, A\\) is a fixed point of \\(f\\) if\n\\(f(A)=A.\\) Now note that \n\n\\[\\begin{align}\n\\mathbf{K}^1_N (E \\cap \\bigcap_{m=1}^{\\infty} \\mathbf{K}^m_N(E))\n   &= \\mathbf{K}^1_N (E) \\cap \\mathbf{K}^1_N( \\bigcap_{m=1}^{\\infty} \\mathbf{K}^m_N(E)) \\\\\n   &= \\mathbf{K}^1_N (E) \\cap (\\bigcap_{m=1}^{\\infty} \\mathbf{K}^1_N (\\mathbf{K}^m_N (E))) \\\\\n   &= \\mathbf{K}^1_N (E) \\cap (\\bigcap_{m=1}^{\\infty} \\mathbf{K}^m_N (E)) \\\\\n   &= \\bigcap_{m=1}^{\\infty} \\mathbf{K}^m_N (E)\n\\end{align}\\]\n\n\nSo we have established that \\(\\mathbf{K}^{*}_N (E)\\) is a fixed point\nof the function \\(f_E\\) defined by \\(f_E (X) = \\mathbf{K}^{1}_N (E\n\\cap X). f_E\\) has other fixed points. For instance, any contradiction\n\\(B \\cap B^c = \\varnothing\\) is a fixed point of\n \\(f_E\\).[15]\n Note also that if \\(A \\subseteq B\\), then \\(E \\cap A \\subseteq E \\cap\nB\\) and so \n\n\\[\nf_E (A) = \\mathbf{K}^1_N (E \\cap A) \\subseteq \\mathbf{K}^1_N (E \\cap B) = f_E(B)\n\\]\n\n\nthat is, \\(f_E\\) is monotone. (We saw that \\(\\mathbf{K}^1_N\\)\nis also monotone in the proof of Proposition 2.4.) Barwise\u2019s\nanalysis of common knowledge can be developed using the following\nresult from set theory:\n\n\nProposition\n\nA monotone function \\(f\\) has a unique fixed point \\(C\\) such that if\n\\(B\\) is a fixed point of \\(f\\), then \\(B\\subseteq C.\\) \\(C\\) is the\ngreatest fixed point of \\(f.\\)\n\n\nThis proposition establishes that \\(f_E\\) has a greatest fixed point,\nwhich characterizes common knowledge in Barwise\u2019s account. As\nBarwise himself observes, the fixed point analysis of common knowledge\nis closely related to Aumann\u2019s partition account. This is easy\nto see when one compares the fixed point analysis to the notion of\ncommon truisms that Aumann\u2019s account generates. Some authors\nregard the fixed point analysis as an alternate formulation of\nAumann\u2019s analysis. Barwise\u2019s fixed point analysis of\ncommon knowledge is favored by those who are especially interested in\nthe applications of common knowledge to problems in logic, while the\nhierarchical and the partition accounts are favored by those who wish\nto apply common knowledge in social philosophy and social science.\nWhen knowledge operators satisfy the axioms (K1)-(K5), the Barwise\naccount of common knowledge is equivalent to the hierarchical\naccount.\n\n\nProposition 2.18\n\nLet \\(C^*_N\\) be the greatest fixed point of \\(f_E.\\) Then \\(C^{*}_N\n(E) = K^*_N (E).\\) (In Barwise (1988, 1989), \\(E\\) is defined\nto be common knowledge at \\(\\omega\\) iff \\(\\omega \\in C^*_N(E).)\\)\n\nProof.\n\n\nBarwise argues that in fact the fixed point analysis is more flexible\nand consequently more general than the hierarchical account. This may\nsurprise readers in light of Proposition 2.18, which shows that\nBarwise\u2019s fixed point definition is equivalent to the\nhierarchical account. Indeed, while Barwise (1988, 1989) proves a\nresult showing that the fixed point account implies the hierarchical\naccount and gives examples that satisfy the common knowledge hierarchy\nbut fail to be fixed points, a number of authors who have written\nafter Barwise have given various proofs of the equivalence of the two\ndefinitions, as was shown in Proposition 2.18. In fact, as (Heifetz\n1999) shows, the hierarchical and fixed-point accounts are equivalent\nfor all finite levels of iteration, while fixed-point common knowledge\nimplies the conjunction of mutual knowledge up to any transfinite\norder, but it is never implied by any such conjunction.\n2.5 Gilbert\u2019s Account\n\nGilbert (1989, Chapter 3) presents an alternative account of common\nknowledge, which is meant to be more intuitively plausible than\nLewis\u2019 and Aumann\u2019s accounts. Gilbert gives a highly\ndetailed description of the circumstances under which agents have\ncommon knowledge.\n\n\nDefinition 2.19\n\nA set of agents \\(N\\) are in a common knowledge situation\n\\(\\mathcal{S}(A)\\) with respect to a proposition \\(A\\) if, and only\nif, \\(\\omega \\in A\\) and for each \\(i \\in N\\),\n\n(\\(G_1\\))\n\\(i\\) is epistemically normal, in the sense that \\(i\\)\nhas normal perceptual organs which are functioning normally and has\nnormal reasoning\n capacity.[16]\n(\\(G_2\\))\n\\(i\\) has the concepts needed to fulfill the other\nconditions.\n(\\(G_3\\))\n\\(i\\) perceives the other agents of \\(N\\).\n(\\(G_4\\))\n\\(i\\) perceives that G\\(_1\\) and G\\(_2\\) are the case.\n(\\(G_5\\))\n\\(i\\) perceives that the state of affairs described by \\(A\\) is\nthe case.\n(\\(G_6\\))\n\\(i\\) perceives that all the agents of \\(N\\) perceive that \\(A\\)\nis the case.\n\n\n\nGilbert\u2019s definition appears to contain some redundancy, since\npresumably an agent would not perceive A unless A is the case. Gilbert\nis evidently trying to give a more explicit account of single agent\nknowledge than Lewis and Aumann give. For Gilbert, agent \\(i\\) knows\nthat a proposition \\(E\\) is the case if, and only if, \\(\\omega \\in\nE\\), that is, \\(E\\) is true, and either \\(i\\) perceives that the state\nof affairs \\(E\\) describes obtains or \\(i\\) can infer \\(E\\) as a\nconsequence of other propositions \\(i\\) knows, given sufficient\ninferential capacity.\n\nLike Lewis, Gilbert recognizes that human agents do not in fact have\nunlimited inferential capacity. To generate the infinite hierarchy of\nmutual knowledge, Gilbert introduces the device of an agent\u2019s\nsmooth-reasoner counterpart. The smooth-reasoner counterpart\n\\(i'\\) of an agent \\(i\\) is an agent that draws every logical\nconclusion from every fact that \\(i\\) knows. Gilbert stipulates that\n\\(i'\\) does not have any of the constraints on time, memory, or\nreasoning ability that \\(i\\) might have, so \\(i'\\) can literally think\nthrough the infinitely many levels of a common knowledge\nhierarchy.\n\n\nDefinition 2.20\n\nIf a set of agents \\(N\\) are in a common knowledge situation\n\\(\\mathcal{S}_N (A)\\) with respect to \\(A\\), then the corresponding\nset \\(N'\\) of their smooth-reasoner counterparts is in a parallel\nsituation \\(\\mathcal{S}'_{N'}(A)\\) if, and only if, for each \\(i'\n\\in N\\),\n\n\n(\\(G'_1\\))\n\\(i'\\) can perceive anything that the counterpart \\(i\\) can\nperceive.\n\n(\\(G_2'\\))\n\\(G_2\\)\u2013\\(G_6\\) obtain for \\(i'\\) with respect to \\(A\\)\nand \\(N'\\), same as for the counterpart \\(i\\) with respect to \\(A\\)\nand \\(N\\).\n\n(\\(G_3'\\))\n\\(i'\\) perceives that all the agents of \\(N'\\) are\nsmooth-reasoners.\n\n\n\nFrom this definition we get the following immediate consequence:\n\n\nProposition 2.21\n\nIf a set of smooth-reasoner counterparts to a set \\(N\\) of agents are\nin a situation \\(\\mathcal{S}'_{N'}(A)\\) parallel to a common knowledge\nsituation \\(\\mathcal{S}_N (A)\\) of \\(N\\), then\n\nfor all \\(m \\in \\mathbb{N}\\) and for any \\(i_1', \\ldots ,i_m',\n\\mathbf{K}_{i_1'}\\mathbf{K}_{i_2'} \\ldots \\mathbf{K}_{i_m'}(A)\\).\n\nConsequently, \\(\\mathbf{K}^{m}_{N'}(A)\\) for any \\(m \\in\n\\mathbb{N}.\\)\n\n\nGilbert argues that, given \\(\\mathcal{S}'_{N'}(A)\\), the\nsmooth-reasoner counterparts of the agents of \\(N\\) actually satisfy a\nmuch stronger condition, namely mutual knowledge\n\\(\\mathbf{K}^{\\alpha}_{N'}(A)\\) to the level of any ordinal number\n\\(\\alpha\\), finite or infinite. When this stronger condition is\nsatisfied, the proposition \\(A\\) is said to be open* to the agents\nof \\(N\\). With the concept of open*-ness, Gilbert gives her\ndefinition of common knowledge.\n\n\nDefinition 2.22\n\nA proposition \\(E \\subseteq \\Omega\\) is Gilbert-common\nknowledge among the agents of a set \\(N = \\{1, \\ldots,n\\},\\) if\nand only if,\n\n\n(\\(G_1^*\\))\n\\(E\\) is open* to the agents of \\(N\\). \n\n(\\(G_2^*\\))\nFor every \\(i \\in N,\\) \\(\\mathbf{K}_i(G_1^*).\\) \n\n\n\\(\\mathbf{G}_N^*(E)\\) denotes the proposition defined by \\(G_1^*\\) and\n\\(G_2^*\\) for a set \\(N\\) of \\(A^*\\)-symmetric reasoners, so we can\nsay that \\(E\\) is Lewis-common knowledge for the agents of \\(N\\) iff\n\\(\\omega \\in \\mathbf{G}_N^*(E)\\).\n\n\nOne might think that an immediate corollary to Gilbert\u2019s\ndefinition is that Gilbert-common knowledge implies the hierarchical\ncommon knowledge of Proposition 2.5. However, this claim follows only\non the assumption that an agent knows all of the propositions that her\nsmooth-reasoner counterpart reasons through. Gilbert does not\nexplicitly endorse this position, although she correctly observes that\nLewis and Aumann are committed to something like\n it.[17]\n Gilbert maintains that her account of common knowledge expresses our\nintuitions with respect to common knowledge better than Lewis\u2019\nand Aumann\u2019s accounts, since the notion of open*-ness presumably\nmakes explicit that when a proposition is common knowledge, it is\n\u201cout in the open\u201d, so to speak.\n3. Applications of Mutual and Common Knowledge\n\nReaders primarily interested in philosophical applications of common\nknowledge may want to focus on the No Disagreement Theorem and\nConvention subsections. Readers interested in applications of common\nknowledge in game theory may continue with the Strategic Form Games,\nand Games of Perfect Information subsections.\n\n3.1 The \u201cNo Disagreement\u201d Theorem\n3.2 Convention\n3.3 Strategic Form Games\n3.4 Games of Perfect Information\n3.5 Communication Networks\n\n3.1 The \u201cNo Disagreement\u201d Theorem\n\nAumann (1976) originally used his definition of common knowledge to\nprove a celebrated result that says that in a certain sense, agents\ncannot \u201cagree to disagree\u201d about their beliefs, formalized\nas probability distributions, if they start with common prior beliefs.\nSince agents in a community often hold different opinions and know\nthey do so, one might attribute such differences to the agents\u2019\nhaving different private information. Aumann\u2019s surprising result\nis that even if agents condition their beliefs on private information,\nmere common knowledge of their conditioned beliefs and a common prior\nprobability distribution implies that their beliefs cannot be\ndifferent, after all!\n\n\nProposition 3.1\n\nLet \\(\\Omega\\) be a finite set of states of the world. Suppose\nthat\n\nAgents \\(i\\) and \\(j\\) have a common prior probability\ndistribution \\(\\mu(\\cdot)\\) over the events of \\(\\Omega\\) such that\n\\(\\mu(\\omega) \\gt 0\\), for each \\(\\omega \\in \\Omega\\), and\nIt is common knowledge at \\(\\omega\\) that \\(i\\)\u2019s posterior\nprobability of event \\(E\\) is \\(q_i(E)\\) and that \\(j\\)\u2019s\nposterior probability of \\(E\\) is \\(q_j(E)\\).\n\n\nThen \\(q_i(E) = q_j(E)\\).\n\n\nProof.\n \n[Note that in the proof of this proposition, and in the sequel,\n\\(\\mu(\\cdot\\mid B)\\) denotes conditional probability; that is, given\n\\(\\mu(B)\\gt 0, \\mu(A\\mid B) = \\mu(A\\cap B)/\\mu(B)\\).]\n\n\nIn a later article, Aumann (1987) argues that the assumptions that\n\\(\\Omega\\) is finite and that \\(\\mu(\\omega) \\gt 0\\) for each \\(\\omega\n\\in \\Omega\\) reflect the idea that agents only regard as\n\u201creally\u201d possible a finite collection of salient worlds to\nwhich they assign positive probability, so that one can drop the\nstates with probability 0 from the description of the state space.\nAumann also notes that this result implicitly assumes that the agents\nhave common knowledge of their partitions, since a description of each\npossible world includes a description of the agents\u2019 possibility\nsets. And of course, this result depends crucially upon (i), which is\nknown as the common prior assumption (CPA).\n\nAumann\u2019s \u201cno disagreement\u201d theorem has been\ngeneralized in a number of ways in the literature. Cave 1983\ngeneralizes the argument to 3 agents. Bacharach 1985 extends it to\ncases in which agents observe each other\u2019s decisions rather than\nposteriors. Milgrom and Stokey, 1982 use it crucially for their\nno-trade theorem, applying no disagreement to show that speculative\ntrade is impossible. Geanakoplos and Polemarchakis 1982 generalize the\nargument to a dynamic setting in which two agents communicate their\nposterior probabilities back and forth until they reach an agreement\n\u2013 this particular take on the agreement theorem has been\ncharacterized in terms of dynamic epistemic logic by D\u00e9gremont\nand Roy, 2009 and applied to cases of epistemic peer disagreement by\nSillari 2019. McKelvey and Page 1986 further extend the results of\nGeanakoplos and Polemarchakis to the case of \\(n\\) individuals. (See\nalso Monderer and Samet 1989 and, for a survey, Geanakoplos 1994.)\n\nHowever, all of these \u201cno disagreement\u201d results raise the\nsame philosophical puzzle raised by Aumann\u2019s original result:\nHow are we to explain differences in belief? Aumann\u2019s result\nleaves us with two options: (1) admit that at some level, common\nknowledge of the agents\u2019 beliefs or how they form their beliefs\nfails, or (2) deny the CPA. Thus, even if agents do assign precise\nposterior probabilities to an event, Aumann shows that if they have\nmerely first-order mutual knowledge of the posteriors, they can\n\u201cagree to\n disagree\u201d.[18]\n Another way Aumann\u2019s result might fail is if agents do not have\ncommon knowledge that they update their beliefs by Bayesian\nconditionalization. Then clearly, agents can explain divergent\nopinions as the result of others having modified their beliefs in the\n\u201cwrong\u201d way. However, there are cases in which neither\nexplanation will seem convincing and denying the requisite common\nknowledge seems a rather ad hoc move. Why should one think\nthat such failures of common knowledge provide a general explanation\nfor divergent beliefs?\n\nWhat of the second option, that is, denying the\n CPA?[19]\n The main argument put forward in favor of the CPA is that any\ndifferences in agents\u2019 probabilities should be the result of\ntheir having different information only, that is, there is no reason\nto think that the different beliefs that agents have regarding the\nsame event are the result of anything other than their having\ndifferent information. However, one can reply that this argument\namounts simply to a restatement of the Harsanyi\n Doctrine.[20]\n3.2 Convention\n\nSchelling\u2019s Department Store problem of Example 1.5 is a very\nsimple example in which the agents \u201csolve\u201d their\ncoordination problem appropriately by establishing a\nconvention. (see also the entry on\n convention\n in this encyclopedia.) Using the vocabulary of game theory, Lewis\n(1969) defines a convention as a strict coordination\nequilibrium of a game which agents follow on account of their\ncommon knowledge that they all prefer to follow this coordination\nequilibrium in a recurrent coordination problem. A coordination\nequilibrium of a game is a strategy combination such that no agent is\nbetter off if any agent unilaterally deviates from this combination.\nAs with equilibria in general, a coordination equilibrium is\nstrict if any agent who deviates unilaterally from the\nequilibrium is strictly worse off. The strategic form game of Figure\n1.3 summarizes Liz\u2019s and Robert\u2019s situation. The\nDepartment Store game has four Nash equilibrium outcomes in pure\nstrategies: \\((s_1, s_1),\\) \\((s_2, s_2),\\) \\((s_3, s_3)\\), and\n\\((s_4,\n s_4)\\).[21]\n These four equilibria are all strict coordination equilibria. If the\nagents follow either of these equilibria, then they coordinate\nsuccessfully. For agents to be following a Lewis-convention in this\nsituation, they must follow one of the game\u2019s coordination\nequilibria. However, for Lewis to follow a coordination equilibrium is\nnot a sufficient condition for agents to be following a convention.\nFor suppose that Liz and Robert fail to analyze their predicament\nproperly at all, but Liz chooses \\(s_2\\) and Robert chooses \\(s_2\\),\nso that they coordinate at \\((s_2, s_2)\\) by sheer luck. Lewis does\nnot count accidental coordination of this sort as a convention.\n\nSuppose next that both agents are Bayesian rational, and that part of\nwhat each agent knows is the payoff structure of the Intersection\ngame. If the agents expect each other to follow \\((s_2, s_2)\\) and\nthey consequently coordinate successfully, are they then following a\nconvention? Not necessarily, contends Lewis in a subtle argument on p.\n59 of Convention. For while each agent knows the game and\nthat she is rational, still she might not attribute the same knowledge\nto the other agent. If each agent believes that the other agent will\nfollow her end of the \\((s_2, s_2)\\) equilibrium mindlessly, then her\nbest response is to follow her end of \\((s_2, s_2)\\). But in this case\nthe agents coordinated as the result of their each falsely believing\nthat the other acts like an automaton, and Lewis thinks that any\nproper account of convention must require that agents have\ncorrect beliefs about one another. In particular, Lewis\nrequires that each agent involved in a convention must have mutual\nexpectations that each is acting with the aim of coordinating with the\nother. The argument can be carried further on. What if both agents\nbelieve that they will follow \\((s_2, s_2)\\), and believe that each\nother will do so thinking that the other will choose \\(s_2\\)\nrationally and not mindlessly? Then, say, Liz would coordinate as the\nresult of her false second-order belief that Robert believes that Liz\nacts mindlessly. Similarly for third-order beliefs and so on for any\nhigher order of knowledge.\n\nLewis concludes that a necessary condition for agents to be following\na convention is that their preferences to follow the corresponding\ncoordination equilibrium be common knowledge (the issue whether\nconventions need to be common knowledge has been debated recently, cf.\nCubitt and Sugden 2003, Binmore 2008, Sillari 2008, and, for an\nexperimental approach, see Devetag et al. 2013, for a connection to\nthe topic of rule-following, see Sillari 2013). So on Lewis\u2019\naccount, a convention for a set of agents is a coordination\nequilibrium which the agents follow on account of their common\nknowledge of their rationality, the payoff structure of the relevant\ngame and that each agent follows her part of the equilibrium.\n\nA regularity \\(R\\) in the behavior of members of a population \\(P\\)\nwhen they are agents in a recurrent situation \\(S\\) is a\nconvention if and only if it is true that, and it is common\nknowledge in \\(P\\) that, in any instance of \\(S\\) among the members of\n\\(P\\),\n\n\neveryone conforms to \\(R\\);\neveryone expects everyone else to conform to \\(R\\);\neveryone has approximately the same preferences regarding all\npossible combinations of actions;\neveryone prefers that everyone conform to \\(R\\), on condition that\nat least all but one conform to R;\neveryone would prefer that everyone conform to \\(R'\\), on\ncondition that at least all but one conform to \\(R'\\),\n\n\nwhere \\(R'\\) is some possible regularity in the behavior of members of\n\\(P\\) in \\(S\\), such that no one in any instance of \\(S\\) among\nmembers of \\(P\\) could conform both to \\(R'\\) and to \\(R\\).\n\n(Lewis 1969, p.\n 76)[22]\n\n\nLewis includes the requirement that there be an alternate coordination\nequilibrium \\(R'\\) besides the equilibrium \\(R\\) that all follow in\norder to capture the fundamental intuition that how the agents who\nfollow a convention behave depends crucially upon how they expect the\nothers to behave.\n\nSugden (1986) and Vanderschraaf (1998) argue that it is not crucial to\nthe notion of convention that the corresponding equilibrium be a\ncoordination equilibrium. Lewis\u2019 key insight is that a\nconvention is a pattern of mutually beneficial behavior which depends\non the agents\u2019 common knowledge that all follow this\npattern, and no other. Vanderschraaf gives a more general definition\nof convention as a strict equilibrium together with common\nknowledge that all follow this equilibrium and that all would have\nfollowed a different equilibrium had their beliefs about each other\nbeen different. An example of this more general kind of convention is\ngiven below in the discussion of the Figure 3.1 example.\n3.3 Strategic Form Games\n\nLewis formulated the notion of common knowledge as part of his general\naccount of conventions. In the years following the publication of\nConvention, game theorists have recognized that any\nexplanation of a particular pattern of play in a game depends\ncrucially on mutual and common knowledge assumptions. More\nspecifically, solution concepts in game theory are both\nmotivated and justified in large part by the mutual or common\nknowledge the agents in the game have regarding their situation.\n\nTo establish the notation that will be used in the discussion that\nfollows, the usual definitions of a game in strategic form, expected\nutility and agents\u2019 distributions over their opponents\u2019\nstrategies, are given here:\n\n\nDefinition 3.2\n\nA game \\(\\Gamma\\) is an ordered triple \\((N, S,\n\\boldsymbol{u})\\) consisting of the following elements:\n\n\nA finite set \\(N = \\{1,2, \\ldots ,n\\}\\), called the set of\nagents or players.\nFor each agent \\(k \\in N\\), there is a finite set \\(S_k =\n\\{s_{k1},s_{k2}, \\ldots ,s_{kn_k}\\}\\), called the alternative pure\nstrategies for agent \\(k\\). The Cartesian product \\(S = S_1\n\\times \\ldots \\times S_n\\) is called the pure strategy set\nfor the game \\(\\Gamma\\).\nA map \\(\\boldsymbol{u} : S \\rightarrow \\Re^n,\\) called the\nutility or payoff function on the pure strategy set.\nAt each strategy combination \\(\\boldsymbol{s} = (s_{1j_1}, \\ldots\n,s_{nj_n}) \\in S\\), agent \\(k\\)\u2019s particular payoff or utility\nis given by the \\(k\\)th component of the value of\n\\(\\boldsymbol{u}\\), that is, agent \\(k\\)\u2019s utility \\(u_k\\) at\n\\(\\boldsymbol{s}\\) is determined by \n\n\\[\nu_k (\\boldsymbol{s}) = I_k (\\boldsymbol{u}(s_{1j_1}, \\ldots ,s_{nj_n}))\n\\]\n\n\nwhere \\(I_k (\\boldsymbol{x})\\) projects \\(\\boldsymbol{x} \\in \\Re^n\\)\nonto its \\(k\\)th component. \n\n\n\nThe subscript \u2018\\(-k\\)\u2019 indicates the result of removing\nthe \\(k\\)th component of an \\(n\\)-tuple or an \\(n\\)-fold\nCartesian product. For instance, \n\n\\[\nS_{-k} = S_1 \\times \\ldots \\times S_{k-1} \\times S_{k+1} \\times \\ldots \\times S_n\n\\]\n\n\ndenotes the pure strategy combinations that agent \\(k\\)\u2019s\nopponents may play.\n\nNow let us formally introduce a system of the agents\u2019 beliefs\ninto this framework. \\(\\Delta_k (S_{-k})\\) denotes the set of\nprobability distributions over the measurable space \\((S_{-k},\n\\mathfrak{F}_k)\\), where \\(\\mathfrak{F}_k\\) denotes the Boolean\nalgebra generated by the strategy combinations \\(S_{-k}\\). Each agent\n\\(k\\) has a probability distribution \\(\\mu_k \\in \\Delta_k(S_{-k})\\),\nand this distribution determines the (Savage) expected\nutilities for each of \\(k\\)\u2019s possible acts:\n\n\\[\nE(u_k (s_{k j})) = \\sum_{A_{-k} \\in S_{-k}} u_k (s_{kj}, \\boldsymbol{s}_{-k}) \\mu_k (\\boldsymbol{s}_{-k}),\\ j = 1, 2, \\ldots ,n_k\n\\]\n\n\nIf \\(i\\) is an opponent of \\(k\\), then \\(i\\)\u2019s individual\nstrategy \\(s_{i j}\\) may be characterized as a union of strategy\ncombinations \\(\\bigcup\\{\\boldsymbol{s}_{-k}\\mid s_{ij}\\in\n\\boldsymbol{s}_{-k}\\} \\in \\mathfrak{F}_k\\), and so \\(k\\)\u2019s\nmarginal probability for \\(i\\)\u2019s strategy \\(s_{i j}\\) may be\ncalculated as follows: \n\n\\[\n\\mu_k (s_{ij}) = \\sum_{\\{s_{-k}\\mid s_{ij}\\in s_{-k}\\}} \\mu_{k}(s_{-k})\n\\]\n\n\n\\(\\mu_k(\\cdot \\mid A)\\) denotes \\(k\\)\u2019s conditional probability\ndistribution given a set \\(A\\), and \\(E(\\cdot \\mid A)\\) denotes\n\\(k\\)\u2019s conditional expectation given \\(\\mu_k(\\cdot\\mid\nA).\\)\n\nSuppose first that the agents have common knowledge of the full payoff\nstructure of the game they are engaged in and that they are all\nrational, and that no other information is common knowledge. In other\nwords, each agent knows that her opponents are expected utility\nmaximizers, but does not in general know exactly which strategies they\nwill choose or what their probabilities for her acts are. These common\nknowledge assumptions are the motivational basis for the solution\nconcept for noncooperative games known as rationalizability,\nintroduced independently by Bernheim (1984) and Pearce (1984). Roughly\nspeaking, a rationalizable strategy is any strategy an agent\nmay choose without violating common knowledge of Bayesian rationality.\nBernheim and Pearce argue that when only the structure of the game and\nthe agents\u2019 Bayesian rationality are common knowledge, the game\nshould be considered \u201csolved\u201d if every agent plays a\nrationalizable strategy. For instance, in the \u201cChicken\u201d\ngame with payoff structure defined by Figure 3.1,\n\n\n\n\n\u00a0\n\u00a0\nJoanna \n\n\u00a0\n\u00a0\n\\(s_1\\)\n\\(s_2\\)  \n\n\nLizzi\n\\(s_1\\)\n(3,3)\n(2,4) \n\n\\(s_2\\)\n(4,2)\n(0,0)  \n\n\nFigure 3.1\n\n\nif Joanna and Lizzi have common knowledge of all of the payoffs at\nevery strategy combination, and they have common knowledge that both\nare Bayesian rational, then any of the four pure strategy profiles is\nrationalizable. For if their beliefs about each other are defined by\nthe probabilities \n\n\\[\\begin{align}\n\\alpha_1 &= \\mu_1 \\text{ (Joanna plays } s_1), \\text{ and} \\\\\n\\alpha_2 &= \\mu_2 \\text{ (Lizzi plays } s_1)\n\\end{align}\\]\n\n\nthen \n\n\\[\n E(u_i (s_1)) = 3\\alpha_i + 2(1 - \\alpha_i) = \\alpha_i + 2\n\\]\n\n\nand \n\n\\[\n E(u_i (s_2)) = 4\\alpha_i + 0(1 - \\alpha_i) = 4\\alpha_i, \\ i = 1, 2\n\\]\n\n\nso each agent maximizes her expected utility by playing \\(s_1\\) if\n\\(\\alpha_i + 2 \\ge 4\\alpha_i\\) or \\(\\alpha_i \\le 2/3\\) and maximizes\nher expected utility by playing \\(s_2\\) if \\(\\alpha_i \\ge 2/3\\). If it\nso happens that \\(\\alpha_i \\gt 2/3\\) for both agents, then both\nconform with Bayesian rationality by playing their respective ends of\nthe strategy combination \\((s_2,s_2)\\) given their beliefs,\neven though each would want to defect from this strategy combination\nwere she to discover that the other is in fact going to play \\(s_2\\).\nNote that the game\u2019s pure strategy Nash equilibria, \\((s_1,\ns_2)\\) and \\((s_2, s_1)\\), are rationalizable, since it is rational\nfor Lizzi and Joanna to conform with either equilibrium given\nappropriate distributions. In general, the set of a game\u2019s\nrationalizable strategy combinations contains the set of the\ngame\u2019s pure strategy Nash\n equilibria.[23]\n\nRationalizability can be defined formally in several ways. A variation\nof Bernheim\u2019s original (1984) definition is given here.\n\n\nDefinition 3.3\n\nGiven that each agent \\(k \\in N\\) has a probability distribution\n\\(\\mu_k \\in \\Delta_k(s_{-k})\\), the system of beliefs \n\n\\[\n  \\boldsymbol{\\mu} = (\\mu_1 , \\ldots ,\\mu_n) \\in \\Delta_1 (S_{-1}) \\times \\cdots \\times \\Delta_n (S_{-n})\n\\]\n\n\nis Bayes concordant if and only if,\n\n(3.i)\nFor \\(i \\ne k, \\mu_i (s_{kj}) \\gt 0 \\Rightarrow s_{kj}\\) maximizes\n\\(k\\)\u2019s expected utility for some \\(\\sigma_k \\in\n\\Delta_k(s_{-k}),\\)\n\n\nand (3.i) is common knowledge. A pure strategy combination\n\\(\\boldsymbol{s} = (s_{1j_1}, \\ldots ,s_{nj_n}) \\in S\\) is\nrationalizable if and only if the agents have a Bayes\nconcordant system \\(\\mu\\) of beliefs and, for each\nagent \\(k \\in N\\),\n\n(3.ii)\n\\(E(u_k (s_{kj_k})) \\ge E(u_k (s_{ki_k})),\\) for \\(i_k \\ne\n j_k\\).[24]\n\n\n\n\nThe following result shows that the common knowledge restriction on\nthe distributions in Definition 3.1 formalizes the assumption that the\nagents have common knowledge of Bayesian rationality.\n\n\nProposition 3.4\n\nIn a game \\(\\Gamma\\), common knowledge of Bayesian rationality is\nsatisfied if, and only if, (3.i) is common knowledge.\n\nProof.\n\n\nWhen agents have common knowledge of the game and their Bayesian\nrationality only, one can predict that they will follow a\nrationalizable strategy profile. However, rationalizability becomes an\nunstable solution concept if the agents come to know more about one\nanother. For instance, in the Chicken example above with \\(\\alpha_i\n\\gt 2/3, i = 1, 2\\), if either agent were to discover the other\nagent\u2019s beliefs about her, she would have good reason not to\nfollow the \\((s_2,s_2)\\) profile and to revise her own beliefs\nregarding the other agent. If, in the other hand, it so happens that\n\\(\\alpha_1 = 1\\) and \\(\\alpha_2 = 0\\), so that the agents maximize\nexpected payoff by following the \\((s_2, s_1)\\) profile, then should\nthe agents discover their beliefs about each other, they will still\nfollow \\((s_2, s_1)\\). Indeed, if their beliefs are common knowledge,\nthen one can predict with certainty that they will follow\n\\((s_2,s_1)\\). The Nash equilibrium \\((s_2,s_1)\\) is characterized by\nthe belief distributions defined by \\(\\alpha_1 = 1\\) and \\(\\alpha_2 =\n0\\).\n\nThe Nash equilibrium is a special case of correlated equilibrium\nconcepts, which are defined in terms of the belief distributions\nof the agents in a game. In general, a correlated\nequilibrium-in-beliefs is a system of agents\u2019 probability\ndistributions which remains stable given common knowledge of the game,\nrationality and the beliefs themselves. We will review two\nalternative correlated equilibrium concepts (Aumann 1974, 1987;\nVanderschraaf 1995, 2001), and show how each generalizes the Nash\nequilibrium concept.\n\n\nDefinition 3.5\n\nGiven that each agent \\(k \\in N\\) has a probability distribution\n\\(\\mu_k \\in \\Delta_k (s_{-k})\\), the system of beliefs\n\n\\[\n \\boldsymbol{\\mu}^* = (\\mu_1^*, \\ldots ,\\mu_n^* ) \\in \\Delta_1 (s_{-1}) \\times \\ldots \\times \\Delta_n (s_{-n})\n\\]\n\n\nis an endogenous correlated equilibrium if, and only if,\n\n(3.iii)\n For \\(i \\ne k, \\mu_i^*(s_{kj}) \\gt 0 \\Rightarrow s_{kj}\\)\nmaximizes \\(k\\)\u2019s expected utility given \\(\\mu_k^*.\\)\n\n\nIf \\(\\boldsymbol{\\mu}^*\\) is an endogenous correlated equilibrium a\npure strategy combination \\(\\boldsymbol{s}^* = (s_1^*, \\ldots ,s_n^*)\n\\in S\\) is an endogenous correlated equilibrium strategy\ncombination given \\(\\boldsymbol{\\mu}^*\\) if, and only if, for\neach agent \\(k \\in N,\\)\n\n(3.iv)\n\\(E(u_k (s_k^*)) \\ge E(u_k (s_{ki}))\\) for \\(s_{ki} \\ne\ns_k^*.\\)\n\n\n\nHence, the endogenous correlated equilibrium \\(\\boldsymbol{\\mu}^*\\)\nrestricts the set of strategies that the agents might follow, as do\nthe Bayes concordant beliefs of rationalizability. However, the\nendogenous correlated equilibrium concept is a proper refinement of\nrationalizability, because the latter does not presuppose that\ncondition (3.iii) holds with respect to the beliefs one\u2019s\nopponents actually have. If exactly one pure strategy combination\n\\(\\boldsymbol{s}^*\\) satisfies (3.iv) given \\(\\boldsymbol{\\mu}^*\\),\nthen \\(\\boldsymbol{\\mu}^*\\) is a strict equilibrium, and in\nthis case one can predict with certainty what the agents will do given\ncommon knowledge of the game, rationality and their beliefs. Note that\nDefinition 3.5 says nothing about whether or not the agents regard\ntheir opponents\u2019 strategy combinations as probabilistically\nindependent. Also, this definition does not require that the\nagents\u2019 probabilities are consistent, in the sense that\nagents\u2019 probabilities for a mutual opponent\u2019s acts agree.\nA simple refinement of the endogenous correlated equilibrium concept\ncharacterizes the Nash equilibrium concept.\n\n\nDefinition 3.6\n\nA system of agents\u2019 beliefs \\(\\boldsymbol{\\mu}^*\\) is a Nash\nequilibrium if, and only if,\n\n\ncondition (3.iii) is satisfied,\nFor each \\(k \\in N, \\mu_k^*\\) satisfies probabilistic\nindependence, and\nFor each \\(s_{kj} \\in s_k\\), if \\(i, l \\ne k\\) then\n\\(\\mu_i^*(s_{kj}) = \\mu_l^*(s_{kj})\\).\n\n\n\nIn other words, an endogenous correlated equilibrium is a Nash\nequilibrium-in-beliefs when each agent regards the moves of his\nopponents as probabilistically independent and the agents\u2019\nprobabilities are consistent. Note that in the 2-agent case,\nconditions (b) and (c) of the Definition 3.6 are always satisfied, so\nfor 2-agent games the endogenous correlated equilibrium concept\nreduces to the Nash equilibrium concept. Conditions (b) and (c) are\ntraditionally assumed in game theory, but Skyrms (1991) and\nVanderschraaf (1995, 2001) argue that there may be good reasons to\nrelax these assumptions in games with 3 or more agents.\n\nBrandenburger and Dekel (1988) show that in 2-agent games, if the\nbeliefs of the agents are common knowledge, condition (3.iii)\ncharacterizes a Nash equilibrium-in-beliefs. As they note, condition\n(3.iii) characterizes a Nash equilibrium in beliefs for the\n\\(n\\)-agent case if the probability distributions are consistent and\nsatisfy probabilistic independence. Proposition 3.7 extends\nBrandenburger and Dekel\u2019s result to the endogenous correlated\nequilibrium concept by relaxing the consistency and probabilistic\nindependence assumptions.\n\n\nProposition 3.7\n\nAssume that the probabilities \n\n\\[\n \\mu = (\\mu_1 ,\\ldots ,\\mu_n) \\in \\Delta_1 (s_{-1}) \\times \\ldots \\times \\Delta_n (s_{-n})\n\\]\n\n\nare common knowledge. Then common knowledge of Bayesian rationality is\nsatisfied if, and only if, \\(\\boldsymbol{\\mu}\\) is an endogenous\ncorrelated equilibrium.\n\nProof.\n\n\nIn addition, we have:\n\n\nCorollary 3.8 (Brandenburger and Dekel, 1988)\n\nAssume in a 2-agent game that the probabilities \n\n\\[\n \\boldsymbol{\\mu} = (\\mu_1,\\mu_2) \\in \\Delta_1 (s_{-1}) \\times \\Delta_2 (s_{-2})\n\\]\n\n\nare common knowledge. Then common knowledge of Bayesian rationality is\nsatisfied if, and only if, \\(\\boldsymbol{\\mu}\\) is a Nash\nequilibrium.\n\nProof.\n\nThe endogenous correlated equilibrium concept reduces to the Nash\nequilibrium concept in the 2-agent case, so the corollary follows by\nProposition 3.7.\n\n\nIf \\(\\boldsymbol{\\mu}^*\\) is a strict equilibrium, then one can\npredict which pure strategy profile the agents in a game will follow\ngiven common knowledge of the game, rationality and\n\\(\\boldsymbol{\\mu}^*.\\) But if \\(\\boldsymbol{\\mu}^*\\) is such that\nseveral distinct pure strategy profiles satisfy (3.iv) with respect to\n\\(\\boldsymbol{\\mu}^*\\), then one can no longer predict with certainty\nwhat the agents will do. For instance, in the Chicken game of Figure\n3.1, the belief distributions defined by \\(\\alpha_1 = \\alpha_2 = 2/3\\)\ntogether are a Nash equilibrium-in-beliefs. Given common knowledge of\nthis equilibrium, either pure strategy is a best reply for each agent,\nin the sense that either pure strategy maximizes expected utility.\nIndeed, if agents can also adopt randomized or mixed\nstrategies at which they follow one of several pure strategies\naccording to the outcome of a chance experiment, then any of the\ninfinitely mixed strategies an agent might adopt in Chicken is a best\nreply given\n \\(\\boldsymbol{\\mu}^*\\).[25]\n So the endogenous correlated equilibrium concept does not determine\nthe exact outcome of a game in all cases, even if one assumes\nprobabilistic consistency and independence so that the equilibrium is\na Nash equilibrium.\n\nAnother correlated equilibrium concept formalized by Aumann (1974,\n1987) does give a determinate prediction of what agents will do in a\ngame given appropriate common knowledge. To illustrate Aumann\u2019s\ncorrelated equilibrium concept, let us consider the Figure 3.1 game\nonce more. If Joanna and Lizzi can tie their strategies to their\nknowledge of the possible worlds in a certain way, they can follow a\nsystem of correlated strategies which will yield a payoff vector they\nboth prefer to that of the mixed Nash equilibrium and which is itself\nan equilibrium. One way they can achieve this is to have their friend\nRon play a variation of the familiar shell game by hiding a pea under\none of three walnut shells, numbered 1, 2 and 3. Joanna and Lizzi both\nthink that each of the three relevant possible worlds corresponding to\n\\(\\omega_k = \\{\\)the pea lies under shell \\(k\\}\\) is equally likely.\nRon then gives Lizzi and Joanna each a private recommendation, based\nupon the outcome of the game, which defines a system of strategy\ncombinations f as follows \n\n\\[\\tag{\\(\\star\\)}\nf(\\omega) =\n\\begin{cases}\n (s_1, s_1) \\text{ if } \\omega_k = \\omega_1 \\\\\n (s_1, s_2) \\text{ if } \\omega_k = \\omega_2 \\\\\n (s_2, s_1) \\text{ if } \\omega_k = \\omega_3\n\\end{cases}\n\\]\n\n\n\\(f\\) is a correlated strategy system because the agents tie\ntheir strategies, by following their recommendations, to the same set\nof states of the world \\(\\Omega\\). \\(f\\) is also a strict Aumann\ncorrelated equilibrium, for if each agent knows how Ron makes his\nrecommendations, but knows only the recommendation he gives her,\neither would do strictly worse were she to deviate from her\n recommendation.[26]\n Since there are several strict equilibria of Chicken, \\(f\\)\ncorresponds to a convention as defined in Vanderschraaf (1998). The\noverall expected payoff vector of \\(f\\) is (3,3), which lies outside\nthe convex hull of the payoffs for the game\u2019s Nash equilibria\nand which Pareto-dominates the expected payoff vector (4/3, 4/3), of\nthe mixed Nash equilibrium defined by \\(\\alpha_1 = 2/3\\), \\(i = 1,\n 2.\\)[27]\n The correlated equilibrium f is characterized by the probability\ndistribution of the agents\u2019 play over the strategy profiles,\ngiven in Figure 3.3:\n\n\n\n\n\u00a0\n\u00a0\nJoanna \n\n\u00a0\n\u00a0\n\\(s_1\\)\n\\(s_2\\)  \n\n\nLizzi\n\\(s_1\\)\n\u2153\n\u2153 \n\n\\(s_2\\)\n\u2153\n0  \n\n\nFigure 3.3\n\n\nAumann (1987) proves a result relating his correlated equilibrium\nconcept to common knowledge. To review this result, we must give the\nformal definition of Aumann correlated equilibrium.\n\n\nDefinition 3.9\n\nGiven a game \\(\\Gamma = (N, S, \\boldsymbol{u})\\) together with a\nfinite set of possible worlds \\(\\Omega\\), the vector valued function\n\\(f: \\Omega \\rightarrow S\\) is a correlated n-tuple. If\n\\(f(\\omega) = (f_1 (\\omega), \\ldots ,f_n (\\omega))\\) denotes the\ncomponents of \\(f\\) for the agents of \\(N\\), then agent \\(k\\)\u2019s\nrecommended strategy at \\(\\omega\\) is \\(f_k (\\omega).\\) \\(f\\)\nis an Aumann correlated equilibrium iff \n\n\\[\nE(u_k \\circ f) \\ge E(u_k (f_{-k}, g_k)),\n\\]\n\n\nfor each \\(k \\in N\\) and for any function \\(g_k\\) that is a function\nof \\(f_i\\).\n\n\nThe agents are at Aumann correlated equilibrium if at each possible\nworld \\(\\omega \\in \\Omega\\), no agent will want to deviate from his\nrecommended strategy, given that the others follow their recommended\nstrategies. Hence, Aumann correlated equilibrium uniquely specifies\nthe strategy of each agent, by explicitly introducing a space of\npossible worlds to which agents can correlate their acts. The\ndeviations \\(g_i\\) are required to be functions of \\(f_i\\), that is,\ncompositions of some other function with \\(f_i\\), because \\(i\\) is\ninformed of \\(f_i (\\omega)\\) only, and so can only distinguish between\nthe possible worlds of \\(\\Omega\\) that are distinguished by \\(f_i\\).\nAs noted already, the primary difference between Aumann\u2019s notion\nof correlated equilibrium and the endogenous correlated equilibrium is\nthat in Aumann\u2019s correlated equilibrium, the agents correlate\ntheir strategies to some event \\(\\omega \\in \\Omega\\) that is external\nto the game. One way to view this difference is that agents who\ncorrelate their strategies exogenously can calculate their expected\nutilities conditional on their own strategies.\n\nIn Aumann\u2019s model, a description of each possible world\n\\(\\omega\\) includes descriptions of the following: the game\n\\(\\Gamma\\), the agent\u2019s private information partitions, and the\nactions chosen by each agent at \\(\\omega\\), and each agent\u2019s\nprior probability distribution \\(\\mu_k(\\cdot)\\) over \\(\\Omega\\). The\nbasic idea is that conditional on \\(\\omega\\), everyone knows\neverything that can be the object of uncertainty on the part of any\nagent, but in general, no agent necessarily knows which world\n\\(\\omega\\) is the actual world. The agents can use their priors to\ncalculate the probabilities that the various act combinations\n\\(\\boldsymbol{s} \\in S\\) are played. If the agents\u2019 priors are\nsuch that for all \\(i, j \\in N,\\) \\(\\mu_i(\\omega) = 0\\) iff \\(\\mu_j\n(\\omega) = 0,\\) then the agents\u2019 priors are mutually\nabsolutely continuous. If the agents\u2019 priors all agree,\nthat is, \\(\\mu_1 (\\omega) = \\ldots = \\mu_n (\\omega) = \\mu(\\omega)\\)\nfor each \\(\\omega \\in \\Omega\\), then it is said that the common\nprior assumption, or CPA, is satisfied. If agents are following\nan Aumann correlated equilibrium \\(f\\) and the CPA is satisfied, then\n\\(f\\) is an objective Aumann correlated equilibrium. An\nAumann correlated equilibrium is a Nash equilibrium if the CPA is\nsatisfied and the agents\u2019 distributions satisfy probabilistic\n independence.[28]\n\nLet \\(s_i (\\omega)\\) denote the strategy chosen by agent \\(i\\) at\npossible world \\(\\omega\\). Then \\(s: \\Omega \\rightarrow S\\) defined by\n\\(s(\\omega) = ( s_1 (\\omega),\\ldots ,s_n (\\omega) )\\) is a correlated\n\\(n\\)-tuple. Given that \\(\\mathcal{H}_i\\) is a partition of\n \\(\\Omega\\),[29]\n the function \\(s_i : \\Omega \\rightarrow s_i\\) defined by \\(s\\) is\n\\(\\mathcal{H}_i\\)-measurable if for each \\(\\mathcal{H}_{ij}\n\\in \\mathcal{H}_{i}, s_i (\\omega')\\) is constant for each \\(\\omega'\n\\in \\mathcal{H}_{ij}.\\) \\(\\mathcal{H}_i\\)-measurability is a formal\nway of saying that \\(i\\) knows what she will do at each possible\nworld, given her information.\n\n\nDefinition 3.10\n\nAgent \\(i\\) is Bayes rational with respect to \\(\\omega \\in\n\\Omega\\) (alternatively, \\(\\omega\\)-Bayes rational) iff\n\\(s_i\\) is \\(\\mathcal{H}_i\\)-measurable and \n\n\\[\nE(u_i \\circ s \\mid \\mathcal{H}_i)(\\omega) \\ge E(u_i (v_i, s_{-i}) \\mid \\mathcal{H}_i)(\\omega)\n\\]\n\n\nfor any \\(\\mathcal{H}_i\\)-measurable function \\(v_i : \\Omega\n\\rightarrow s_i\\).\n\n\nNote that Aumann\u2019s definition of \\(\\omega\\)-Bayesian rationality\nimplies that \\(\\mu_i (\\mathcal{H}_i (\\omega)) \\gt 0\\), so that the\nconditional expectations are defined. Aumann\u2019s main result,\ngiven next, implicitly assumes that \\(\\mu_i (\\mathcal{H}_i (\\omega))\n\\gt 0\\) for every agent \\(i \\in N\\) and every possible world \\(\\omega\n\\in \\Omega\\). This poses no technical difficulties if the CPA is\nsatisfied, or even if the priors are only mutually absolutely\ncontinuous, since if this is the case then one can simply drop any\n\\(\\omega\\) with zero prior from consideration.\n\n\nProposition 3.11 (Aumann 1987)\n\nIf each agent \\(i \\in N\\) is \\(\\omega\\)-Bayes rational at each\npossible world \\(\\omega \\in \\Omega\\), then the agents are following an\nAumann correlated equilibrium. If the CPA is satisfied, then the\ncorrelated equilibrium is objective.\n\nProof.\n\n\nPart of the uncertainty the agents might have about their situation is\nwhether or not all agents are rational. But if it is assumed that all\nagents are \\(\\omega\\)-Bayesian rational at each \\(\\omega \\in \\Omega\\),\nthen a description of this fact forms part of the description of each\npossible \\(\\omega\\) and thus lies in the meet of the agents\u2019\npartitions. As noted already, descriptions of the agents\u2019\npriors, their partitions and the game also form part of the\ndescription of each possible world, so propositions corresponding to\nthese facts also lie in the meet of the agents\u2019 partitions. So\nanother way of stating Aumann\u2019s main result is as follows:\nCommon knowledge of \\(\\omega\\)-Bayesian rationality at\neach possible world implies that the agents follow an Aumann\ncorrelated equilibrium.\n\nPropositions 3.7 and 3.11 are powerful results. They say that common\nknowledge of rationality and of agents beliefs about each other,\nquantified as their probability distributions over the strategy\nprofiles they might follow, implies that the agents\u2019 beliefs\ncharacterize an equilibrium of the game. Then if the agents\u2019\nbeliefs are unconditional, Proposition 3.7 says that the agents are\nrational to follow a strategy profile consistent with the\ncorresponding endogenous correlated equilibrium. If their beliefs are\nconditional on their private information partitions, then Proposition\n3.11 says they are rational to follow the strategies the corresponding\nAumann correlated equilibrium recommends. However, we must not\noverestimate the importance of these results, for they say nothing\nabout the origins of the common knowledge of rationality and\nbeliefs. For instance, in the Chicken game of Figure 3.1, we\nconsidered an example of a correlated equilibrium in which it was\nassumed that Lizzi and Joanna had common knowledge of the\nsystem of recommended strategies defined by \\((\\star).\\) Given this\ncommon knowledge, Joanna and Lizzi indeed have decisive reason to\nfollow the Aumann correlated equilibrium f. But where did this common\nknowledge come from? How, in general, do agents come to have the\ncommon knowledge which justifies their conforming to an equilibrium?\nPhilosophers and social scientists have made only limited progress in\naddressing this question.\n3.4 Games of Perfect Information\n\nIn extensive form games, the agents move in sequence. At each stage,\nthe agent who is to move must base her decisions upon what she knows\nabout the preceding moves. This part of the agent\u2019s knowledge is\ncharacterized by an information set, which is the set of\nalternative moves that an agent knows her predecessor might have\nchosen. For instance, consider the extensive form game of Figure\n3.4:\n\n\n\nFigure 3.4\n\n\nWhen Joanna moves she is at her information set \\(I^{22} = \\{C^1,\nD^1\\},\\) that is, she moves knowing that Lizzi might have chosen\neither \\(C^1\\) or \\(D^1\\), so this game is an extensive form\nrepresentation of the Chicken game of Figure 3.1.\n\nIn a game of perfect information, each information set consists of a\nsingle node in the game tree, since by definition at each state the\nagent who is to move knows exactly how her predecessors have moved. In\nExample 1.4 it was noted that the method of backwards induction can be\napplied to any game of perfect\n information.[30]\n The backwards induction solution is the unique Nash equilibrium of a\ngame of perfect information. The following result gives sufficient\nconditions to justify backwards induction play in a game of perfect\ninformation:\n\n\nProposition 3.12 (Bicchieri 1993)\n\nIn an extensive form game of perfect information, the agents follow\nthe backwards induction solution if the following conditions are\nsatisfied for each agent \\(i\\) at each information set \\(I^{ik}\\):\n\n\\(i\\) is rational, \\(i\\) knows this and \\(i\\) knows the game,\nand\nAt any information set \\(I^{jk + 1}\\) that immediately follows\n\\(I^{ik}, i\\) knows at \\(I^{ik}\\) what \\(j\\) knows at \\(I^{jk +\n1}\\).\n\nProof.\n \n\nProposition 3.12 says that far less than common knowledge of the game\nand of rationality suffices for the backwards induction solution to\nobtain in a game of perfect information. All that is needed is for\neach agent at each of her information sets to be rational, to know the\ngame and to know what the next agent to move knows! For instance, in\nthe Figure 1.2 game, if \\(R_1 (R_2)\\) stands for \u201cAlan (Fiona)\nis rational\u201d and \\(\\mathbf{K}_i (\\Gamma)\\) stands for\n\u201c\\(i\\) knows the game \\(\\Gamma\\)\u201d, then the backwards\ninduction solution is implied by the following:\n\nAt \\(I^{24}, R_2\\) and \\(\\mathbf{K}_2 (\\Gamma)\\).\nAt \\(I^{13}, R_1, \\mathbf{K}_1 (\\Gamma), \\mathbf{K}_1 (R_2)\\), and\n\\(\\mathbf{K}_1\\mathbf{K}_2 (\\Gamma)\\).\nAt \\(I^{22}, \\mathbf{K}_2 (R_1), \\mathbf{K}_2\\mathbf{K}_1 (R_2)\\),\nand \\(\\mathbf{K}_2\\mathbf{K}_1\\mathbf{K}_2 (\\Gamma)\\).\nAt \\(I ^{11}, \\mathbf{K}_1\\mathbf{K}_2 (R_1),\n\\mathbf{K}_1\\mathbf{K}_2\\mathbf{K}_1 (R_2)\\), and\n\\(\\mathbf{K}_1\\mathbf{K}_2\\mathbf{K}_1\\mathbf{K}_2\n (\\Gamma)\\).[31]\n\n\nOne might think that a corollary to Proposition 3.11 is that in a game\nof perfect information, common knowledge of the game and of\nrationality implies the backwards induction solution. This is the\nclassical argument for the backwards induction solution. Many\ngame theorists continue to accept the classical argument, but in\nrecent years, the argument has come under strong challenge, led by the\nwork of Reny (1988, 1992), Binmore (1987) and Bicchieri (1989, 1993).\nThe basic idea underlying their criticisms of backwards induction can\nbe illustrated with the Figure 1.2 game. According to the classical\nargument, if Alan and Fiona have common knowledge of rationality and\nthe game, then each will predict that the other will follow her end of\nthe backwards induction solution, to which his end of the backwards\ninduction solution is his unique best response. However, what if Fiona\nreconsiders what to do if she finds herself at the information set\n\\(I^{22}\\)? If the information set \\(I^{22}\\) is reached, then Alan\nhas of course not followed the backwards induction solution. If we\nassume that at \\(I^{22}\\), Fiona knows only what is stated in (iii),\nthen she can explain her being at \\(I^{22}\\) as a failure of either\n\\(\\mathbf{K}_1\\mathbf{K}_2\\mathbf{K}_1 (R_2)\\) or\n\\(\\mathbf{K}_1\\mathbf{K}_2\\mathbf{K}_1\\mathbf{K}_2 (\\Gamma)\\) at\n\\(I^{11}\\). In this case, Fiona\u2019s thinking that either\n\\(\\neg\\mathbf{K}_1\\mathbf{K}_2\\mathbf{K}_1 (R_2)\\) or\n\\(\\neg\\mathbf{K}_1\\mathbf{K}_2\\mathbf{K}_1\\mathbf{K}_2(\\Gamma)\\) at\n\\(I^{11}\\) is compatible with what Alan in fact does know at\n\\(I^{11}\\), so Fiona should not necessarily be surprised to find\nherself at \\(I^{22}\\), and given that what she knows there is\ncharacterized by (iii), following the backwards induction solution is\nher best strategy. But if rationality and the game are common\nknowledge, or even if Fiona and Alan both have just have mutual\nknowledge of the statements characterized by (iii) and (iv), then at\n\\(I^{22}\\), Fiona knows that \\(\\mathbf{K}_1\\mathbf{K}_2\\mathbf{K}_1\n(R_2)\\) or \\(\\mathbf{K}_1\\mathbf{K}_2\\mathbf{K}_1\\mathbf{K}_2\n(\\Gamma)\\) at \\(I^{11}\\). Hence given this much mutual knowledge,\nFiona no longer can explain why Alan has deviated from the backwards\ninduction solution, since this deviation contradicts part of what is\ntheir mutual knowledge. So if she finds herself at \\(I^{22}\\), Fiona\ndoes not necessarily have good reason to think that Alan will follow\nthe backwards induction solution of the subgame beginning at\n\\(I^{22}\\), and hence she might not have good reason to follow the\nbackwards induction solution, either. Bicchieri (1993), who along with\nBinmore (1987) and Reny (1988, 1992) extends this argument to games of\nperfect information with arbitrary length, draws a startling\nconclusion: If agents have strictly too few or strictly too\nmany levels of mutual knowledge of rationality and the game\nrelative to the number of potential moves, one cannot predict that\nthey will follow the backwards induction solution. This would\nundermine the central role backwards induction has played in the\nanalysis of extensive form games. For why should the number of levels\nof mutual knowledge the agents have depend upon the length of the\ngame?\n\nThe classical argument for backwards induction implicitly assumes that\nat each stage of the game, the agents discount the preceding moves as\nstrategically irrelevant. Defenders of the classical argument can\nargue that this assumption makes sense, since by definition at any\nagents\u2019 decision node, the previous moves that led to this node\nare now fixed. Critics of the classical argument question this\nassumption, contending that when reasoning about how to move at any of\nhis information sets, including those not on the backwards\ninduction equilibrium path, part of what an agent must consider\nis what conditions might have led to his being at that information\nset. In other words, agents should incorporate reasoning about the\nreasoning of the previous movers, or forward induction\nreasoning, into their deliberations over how to move at a given\ninformation set. Binmore (1987) and Bicchieri (1993) contend that a\nbackwards induction solution to a game should be consistent with the\nsolution a corresponding forward induction argument recommends. As we\nhave seen, given common knowledge of the game and of rationality,\nforward induction reasoning can lead the agents to an apparent\ncontradiction: The classical argument for backwards induction is\npredicated on what agents predict they would do at nodes in the tree\nthat are never reached. They make these predictions based upon their\ncommon knowledge of the game and of rationality. But forward induction\nreasoning seems to imply that if any off-equilibrium node had been\nreached, common knowledge of rationality and the game must have\nfailed, so how could the agents have predicted what would happen at\nthese nodes?\n3.5 Communication Networks\n\nSituations in which a member of a population \\(P\\) is willing to\nengage in a certain course of action provided that a large enough\nportion of \\(P\\) engages in some appropriate behavior are typical\nproblems of collective action. Consider the case of an agent\nwho is debating whether to join a revolt. Her decision to join or not\nto join will depend on the number of other agents whom she expects to\njoin the revolt. If such a number is too low, she will prefer not to\nrevolt, while if the number is sufficiently large, she will prefer to\nrevolt. Michael Chwe proposes a model where such a situation is\nmodeled game-theoretically. Players\u2019 knowledge about other\nplayers\u2019 intentions depends on a social network in\nwhich players are located. The individual \u2018thresholds\u2019 for\neach player (the number of other agents that are needed for that\nspecific player to revolt) are only known by the immediate neighbors\nin the network. Besides the intrinsic value of the results obtained by\nChwe\u2019s analysis regarding the subject of collective action, his\nmodel also provides insights about both the relation between social\nnetworks and common knowledge and about the role of common knowledge\nin collective action. For example, in some situations, first-order\nknowledge of other agents\u2019 personal thresholds is not sufficient\nto motivate an agent to take action, whereas higher-order knowledge\nor, in the limit, common knowledge is.\n\nWe present Chwe\u2019s model following (Chwe 1999) and (Chwe 2000).\nSuppose there is a group \\(P\\) of \\(n\\) people, and each agent has two\nstrategies: \\(r\\) (revolt, that is participating in the collective\naction) and \\(s\\) (stay home and not participate). Each agent has her\nown individual threshold \\(\\theta \\in \\{1, 2,\\ldots, n+1\\}\\)\nand she prefers \\(r\\) over \\(s\\) if and only if the total number of\nplayers who revolt is greater than or equal to her threshold. An agent\nwith threshold 1 always revolts; an agent with threshold 2 revolts\nonly if another agent does; an agent with threshold \\(n\\) revolts only\nif all agents do; an agent with threshold \\(n+1\\) never revolts, etc.\nThe agents are located in a social network, represented by a binary\nrelation \\(\\rightarrow\\) over \\(P\\). The intended meaning of \\(i\n\\rightarrow j\\) is that agent \\(i\\) \u2018talks\u2019 to agent\n\\(j\\), that is to say, agent \\(i\\) knows the threshold of\nagent \\(j.\\) If we define \\(B(i)\\) to be the set \\(\\{j \\in P : j\n\\rightarrow i\\},\\) we can interpret \\(B(i)\\) as \\(i\\)\u2019s\n\u2018neighborhood\u2019 and say that, in general, \\(i\\) knows the\nthresholds of all agents in her neighborhood. A further assumption is\nthat, for all \\(j,k \\in B(i),\\) \\(i\\) knows whether \\(j \\rightarrow\nk\\) or not, that is, every agent knows whether her neighbors are\ncommunicating with each other. The relation \\(\\rightarrow\\) is taken\nto be reflexive (one knows her own threshold).\n\nPlayers\u2019 knowledge is represented as usual in a possible worlds\nframework. Consider for example the case in which there are two\nagents, both with one of thresholds 1, 2 or 3. There are nine possible\nworlds represented by ordered pairs of numbers, representing the first\nand second player\u2019s individual thresholds respectively: 11, 12,\n13,\u2026, 32, 33. If the players do not communicate, each knows her\nown threshold only. Player 1\u2019s information partition reflects\nher ignorance about player\u2019s 2 threshold and it consists of the\nsets \\(\\{11, 12, 13\\},\\) \\(\\{21, 22, 23\\},\\) \\(\\{31, 32, 33\\};\\)\nwhereas, similarly, player 2\u2019s partition consists of the sets\n\\(\\{11, 21, 31\\},\\) \\(\\{12, 22, 32\\},\\) \\(\\{13, 23, 33\\}.\\) If player\n1\u2019s threshold is 1, she revolts no matter what player 2\u2019s\nthreshold is. Hence, player 1 revolts in \\(\\{11, 12, 13\\}\\). If player\n1\u2019s threshold is 3, she never revolts. Hence, she plays \\(s\\) in\n\\(\\{31, 32, 33\\}\\). If her threshold is 2, she revolts only if the\nother player revolts as well. Since in this example we are assuming\nthat there is no communication between the agents, player 1 cannot be\nsure of player\u2019s 2 action, and chooses the non-risky \\(s\\) in\n\\(\\{21, 22, 23\\}\\) as well. Similarly, player 2 plays \\(r\\) in \\(\\{11,\n21, 31\\}\\) and \\(s\\) otherwise. Consider now the case in which \\(1\n\\rightarrow 2\\) and \\(2 \\rightarrow 1\\). Both players have now the\nfinest information partitions. Thresholds of 1 and 3 yield \\(r\\) and\n\\(s\\), respectively, for both players again. However, in player\n1\u2019s cells \\(\\{21\\}\\) and \\(\\{22\\}\\), she knows that player 2\nwill revolt, and, having threshold 2, she revolts as well. Similarly\nfor player 2 in his cells \\(\\{12\\}\\) and \\(\\{22\\}\\). Note, that the\ncase in which both players have threshold 2, yields both the\nequilibrium in which both players revolt and the equilibrium in which\neach player stays home. It is assumed that in the case of multiple\nequilibria, the one which results in the most revolt will obtain.\n\n\n\nFigure 3.5\n\n\nThe analysis of the example above applies to general networks with\n\\(n\\) agents. Consider for example the three person network \\(1\n\\rightarrow 2, 2 \\rightarrow 1, 2 \\rightarrow 3\\), represented in\nfigure 3.5a (notice that symmetric links are represented by a line\nwithout arrowheads) and assume that each player has threshold 2. The\nnetwork between players 1 and 2 is the same as the one above, hence if\nthey have threshold 2, they both revolt regardless of the threshold of\nplayer 3. Player 3, on the other hand, knows her own threshold and\nplayer 2\u2019s. Hence, if they all have threshold 2, she cannot\ndistinguish between the possibilities in the set \\(\\{122, 222, 322,\n422\\}\\). At 422, in particular, neither player 1 nor player 2 revolt,\nhence player 3 cannot take the risk and does not revolt, even\nif, in fact, she has a neighbor who revolts. Adding the link \\(1\n\\rightarrow 3\\) to the network (cf. figure 3.5b) we provide player 3\nwith knowledge about player 1\u2019s action, hence in this case, if\nthey all have threshold 2, they all revolt. Notice that if we break\nthe link between players 1 and 2 (so that the network is \\(1\n\\rightarrow 3\\) and \\(2 \\rightarrow 3)\\), player 3 knows that 1 and 2\ncannot communicate and hence do not revolt at 222, therefore she\nchooses \\(s\\) as well. Knowledge of what other players know about\nother players is crucial.\n\n\n\nFigure 3.6\n\n\nThe next example reveals that in some cases not even first-order\nknowledge is sufficient to trigger action, and higher levels of\nknowledge are necessary. Consider four players, each with threshold 3,\nin the two different networks represented in figure 3.6\n(\u2018square\u2019, in figure 3.6a, and \u2018kite\u2019, in\nfigure 3.6b.) In the square network, player 1 knows that both\n2 and 4 have threshold 3. However, she does not know about player\n3\u2019s threshold. If player 3 has threshold 5, then player 2 will\nnever revolt, since he does not know about player 4\u2019s threshold\nand it is then possible for him that player 4 has threshold 5 as well.\nPlayer 1\u2019s uncertainty about player 3 together with player\n1\u2019s knowledge of player 2\u2019s uncertainty about player 4\nforce her not to revolt, although she has threshold 3 and two\nneighbors with threshold 3 as well. Similar reasoning applies to all\nother players, hence in the square no one revolts. Consider now the\nkite network. Player 4 ignores player 1\u2019s and player\n2\u2019s thresholds, hence he does not revolt. However, player 1\nknows that players 2 and 3 have threshold 3, that they know that they\ndo, and that they know that player 1 knows that they do. This is\nenough to trigger action \\(r\\) for the three of them, and indeed if\nplayers 1, 2 and 3 all revolt in all states in \\(\\{3331, 3332, 3333,\n3334, 3335\\}\\), this is an equilibrium since in all states at least\nthree people revolt each with threshold three.\n\nThe difference between the square and the kite networks is that,\nalthough in the square enough agents are willing to revolt for a\nrevolt to actually take place, and they all individually know this, no\nagent knows that others know it. In the kite, on the other hand,\nagents in the triangle not only know that there are three agents with\nthreshold 3, but they also know that they all know it, know that they\nall know that they all know it, and so on. There is common knowledge\nof such fact among them. It is interesting to notice that in\nChwe\u2019s model, common knowledge obtains without there been a\npublicly known fact (cf. section 2.2). The proposition\n\u201cplayers 1, 2 and 3 all have threshold 3\u201d (semantically:\nthe event \\(\\{3331, 3332, 3333, 3334, 3335\\})\\) is known by players 1,\n2 and 3 because of the network structure, and becomes common knowledge\nbecause the network structure is known by the players. To be sure, the\nnetwork structure is not just simply known, but it is actually\ncommonly known by the players. Player 1, for example, does not only\nknow that players 2 and 3 communicate with each other. She also knows\nthat players 2 and 3 know that she knows that they communicate with\neach other, and so on.\n\nIn complete networks (networks in which all players\ncommunicate with everyone else, as within the triangle in the kite\nnetwork) the information partitions of the players coincide, and they\nare the finest partitions of the set of possible worlds. Hence, if\nplayers have sufficiently low thresholds, such fact is commonly known\nand there is an equilibrium in which all players revolt.\n\n\nDefinition 3.13\n\nWe say that \\(\\rightarrow\\) is a sufficient network if there\nis an equilibrium such that all players choose to revolt.\n\n\nFor a game in which all players have sufficiently low thresholds, the\ncomplete network is clearly sufficient. Is the complete network\nnecessary to obtain an equilibrium in which all players revolt? It\nturns out that it is not. A crucial role is played by structures of\nthe same kind as the \u2018triangle\u2019 group in the kite network,\ncalled cliques. In such structures, \u2018local\u2019\ncommon knowledge (that is, limited to the players part of the\nstructure) arises naturally. In a complete network (that is, a network\nin which there is sufficient but not superfluous communication for it\nto fully revolt) in which cliques cover the entire population, if one\nclique speaks to another then every member of that clique speaks to\nevery member of the other clique. Moreover, for every two cliques such\nthat one is talking to the other, there exists a \u2018chain\u2019\nof cliques with a starting element. In other words, every pair of\ncliques in the relation are part of a chain (of length at least 2)\nwith a starting element (a leading clique.) Revolt propagates\nin the network moving from \u2018leading adopters\u2019 to\n\u2018followers\u2019, according to the social role\nhierarchy defined by the cliques and their relation. Consider the\nfollowing example, in which cliques are represented by circles and\nnumbers represent the thresholds of individual players:\n\n\n\nFigure 3.7\n\n\nHere the threshold 3 clique is the leading clique, igniting revolt in\nthe threshold 5 follower clique. In turn, the clique of a single\nthreshold 3 element follows. Notice that although she does not need to\nknow that the leading clique actually revolts to be willing to revolt,\nthat information is needed to ensure that the threshold 5 clique does\nrevolt, and hence that it is safe for her to join the revolt. While in\neach clique information about thresholds and hence willingness to\nrevolt is common knowledge, in a chain of cliques information is\n\u2018linear\u2019; each clique knows about the clique of which it\nis a follower, but does not know about earlier cliques.\n\nAnalyzing Chwe\u2019s models for collective action under the respect\nof weak versus strong links (cf. both Chwe 1999 and Chwe 2000)\nprovides further insights about the interaction between communication\nnetworks and common knowledge. A strong link, roughly speaking, joins\nclose friends, whereas a weak link joins acquaintances. Strong links\ntend to increase more slowly than weak ones, since people have common\nclose friends more often than they share acquaintances. In terms of\nspreading information and connecting society, then, weak links do a\nbetter job than strong links, since they traverse society more quickly\nand have therefore larger reach. What role do strong and weak links\nplay in collective action? In Chwe\u2019s dynamic analysis, strong\nlinks fare better when thresholds are low, whereas weak links are\nbetter when players\u2019 thresholds are higher. Intuitively, one\nsees that strong links tend to form small cliques right away (because\nof the symmetry intrinsic in them: my friends\u2019 friends tend to\nbe my friends as well); common knowledge arises quickly at the local\nlevel and, if thresholds are low, there is a better chance that a\ngroup tied by a strong link becomes a leading clique initiating\nrevolt. If, on the other hand, thresholds are high, local common\nknowledge in small cliques is fruitless, and weak links, reaching\nfurther distances more quickly, speed up communication and building of\nthe large cliques needed to sparkle collective action. Such\nconsiderations shed some light on the relation between social networks\nand common knowledge. While it is true that knowledge spreads faster\nin networks in which weak links predominate, higher-order knowledge\n(and, hence, common knowledge) tends to arise more slowly in this kind\nof networks. Networks with a larger number of strong links, on the\nother hand, facilitate the formation of common knowledge at the local\nlevel.\n4. Is Common Knowledge Attainable?\n\nLewis formulated an account of common knowledge which generates the\nhierarchy of \u2018\\(i\\) knows that \\(j\\) knows that \u2026 \\(k\\)\nknows that \\(A\\)\u2019 propositions in order to ensure that in his\naccount of convention, agents have correct beliefs about each other.\nBut since human agents obviously cannot reason their way through such\nan infinite hierarchy, it is natural to wonder whether any group of\npeople can have full common knowledge of any proposition. More\nbroadly, the analyses of common knowledge reviewed in \u00a73 would be\nof little worth to social scientists and philosophers if this common\nknowledge lies beyond the reach of human agents.\n\nFortunately for Lewis\u2019 program, there are strong arguments that\ncommon knowledge is indeed attainable. Lewis (1969) argues that the\ncommon knowledge hierarchy should be viewed as a chain of\nimplications, and not as steps in anyone\u2019s actual reasoning. He\ngives informal arguments that the common knowledge hierarchy is\ngenerated from a finite set of axioms. We saw in \u00a72 that it is\npossible to formulate Lewis\u2019 axioms precisely and to derive the\ncommon knowledge hierarchy from these axioms and a public\nevent functioning as a basis for common knowledge. Again, the\nbasic idea behind Lewis\u2019 argument is that for a set of agents,\nif a proposition \\(A\\) is publicly known among them and each agent\nknows that everyone can draw the same conclusion \\(p\\) from \\(A\\) that\nshe can, then \\(p\\) is common knowledge. These conditions are\nobviously context dependent, just as an individual\u2019s knowing or\nnot knowing a proposition is context dependent. Yet there are many\ncases where it is natural to assume that a public event generates\ncommon knowledge, because it is properly broadcast, agents in the\ngroup are in ideal conditions to perceive it, the inference from the\npublic event to the object of common knowledge is immediate, etc.\nHowever, common knowledge could fail if some of the people failed to\nperceive the public event, or if some of them believed that some of\nthe others could not understand the announcement, or hear it, or could\nnot draw the necessary inferences, and so on.\n\nIn fact, skeptical doubt about the attainability of common knowledge\nis certainly possible. A strong skeptical argument has been recently\nput forth by Lederman (2018b). Lederman builds an argument meant to\nundermine the possibility of deriving the common knowledge hierarchy,\nas done in \u00a72, on the basis of a public event or, as Lederman\ncalls it, public information. The principle that Lederman\ntargets is what he calls ideal common knowledge (or belief),\nthat is: If \\(p\\) is public information in a group \\(G\\) then \\(p\\) is\ncommon knowledge in \\(G\\), provided the agents in \\(G\\) are ideal\nreasoners. The argument rests on the privacy and interpersonal\nincomparability of mental states among agents, and although it is\noffered in terms of perceptual knowledge, its scope goes beyond\nperception to question the possibility of common knowledge tout\ncourt.\n\nLederman (2018b) uses the following scenario: Two contestants, Alice\nand Bob, observe the height of the mast of a toy sailboat (100 cm)\nthat is subsequently replaced with a randomly selected sailboat whose\nmast may be more or less tall than 100 cm. As a matter of fact, the\nmast of the selected boat is 300 cm tall. It is therefore public\ninformation that the mast is taller than 100 cm. The ideal common\nknowledge principle above, along with assumptions about Alice and\nBob\u2019s visual systems and their publicity, would entail that\nAlice and Bob have common knowledge that the mast is taller than 100\ncm, and yet Lederman\u2019s argument shows that they do not. The main\nidea is that there is some degree of approximation in how humans\nperceive, among other things, heights. Thus, for Alice it is\nepistemically compatible with the mast looking 300 cm tall to her,\nthat the mast looks somewhat shorter than 300 cm to Bob, say 299 cm.\nAlso, Alice knows that if the mast looks 299 cm tall to Bob, then it\nis epistemically compatible for him that the mast looks 298 cm tall to\nAlice. Also, Bob knows that Alice knows that if the mast looks 298 cm\ntall to Alice, then it is epistemically compatible for her that the\nmast looks 297 cm tall to Bob. The reasoning can be repeated until\nthere it is epistemically compatible for Alice and Bob that the mast\nis not taller than 100 cm, against the intuition that they have common\nknowledge that the mast is over 100 cm tall!\n\nLederman (2018b) generalizes the argument to arbitrary cases and\nsources of public information, to conclude that people never achieve\ncommon knowledge or belief. In his view, the unattainability of common\nknowledge is not a concern in terms of a possible loss of explanatory\npower for social behavior. While common knowledge and public\ninformation from which it proceeds have long been considered crucial\nfor coordinating behavior, Lederman claims that in fact coordination\nrequires neither (see the discussion of Lederman 2018a in the next\nsection.) Against Lederman, Immerman (2021) argues that the skeptical\nargument sketched here fails in a large set of circumstances, and\nhence fails to prove the unattainability of common knowledge. The key\nidea in Immerman\u2019s attempt to refute Lederman\u2019s argument\nis that there are many perceptual values that agents will not\nentertain to begin with, as if, in the original sailboat example, they\nknew that all masts within 100 and 300 cm tall had been stolen.\nAccording to Immerman, cases of such \u201cknowledge of gaps\u201d\nare not at all uncommon and their availability prevents\nLederman\u2019s argument to go through.\n\nEven if one were to reject Lederman\u2019s skeptical argument (be it\nby agreeing with Immerman\u2019s argument above, or with the argument\nby Thomason (2021) addressed in the next section, or otherwise), care\nmust be taken in ascribing common knowledge to a group of human\nagents. Common knowledge is a phenomenon highly sensitive to the\nagents\u2019 circumstances. The following section gives an example\nthat shows that in order for \\(A\\) to be a common truism for a set of\nagents, they ordinarily must perceive an event which implies \\(A\\)\nsimultaneously and publicly.\n5. Coordination and Common \\(p\\)-Belief\n\nIn certain contexts, agents might not be able to achieve common\nknowledge. The skeptical argument put forth by Lederman (2018b),\nindeed, rests on and generalizes related arguments about the\nattainability of common knowledge that were made in theoretical\ncomputer science in relation to the coordinated attack\nproblem (see Lederman 2018a, Halpern and Moses, 1990 and Fagin et al.\n1995, esp. chapters 6 and 11). In the context of distributed systems,\nusing the formal systems of epistemic logic that, as mentioned above,\nare equivalent to the semantic approach privileged by economists, it\ncan be proven formally that (i) common knowledge is necessary for\ncoordination and that (ii) the attainability of common knowledge\ndepends on assumptions made about the system. In particular,\nasynchronous systems do not allow for common knowledge of a\ncommunicated message to arise, making coordination impossible. Might\nthe agents achieve something \u201cclose\u201d to common knowledge?\nThere are various weakenings of the notion of common knowledge that\ncan be of use: \\(\\varepsilon\\)-common knowledge (agents will achieve\ncommon knowledge within time \\(\\varepsilon\\), hence they will\ncoordinate within time \\(\\varepsilon)\\), eventual common knowledge\n(agents will achieve common knowledge and therefore coordinate\neventually), probabilistic common knowledge (agents will achieve\nprobability \\(p\\) common belief, and hence with probability \\(p\\)\nsuccessfully coordinate), etc. Such weakenings of the notion of common\nknowledge might prove useful depending on the intended\napplication.\n\nAnother weakening of common knowledge to consider is of course\n\\(m\\)th level mutual knowledge. For a high value of \\(m,\n\\mathbf{K}^m_N(A)\\) might seem a good approximation of\n\\(\\mathbf{K}^{*}_N(A)\\). However, point (i) above maintains that no\narbitrary high value of \\(m\\) will help for instance with the\npractical task of achieving coordination, so that the full force of\ncommon knowledge is needed. We illustrate the point through the\nfollowing example, due to Rubinstein (1989, 1992), showing that simply\ntruncating the common knowledge hierarchy at any finite level can lead\nagents to behave as if they had no mutual knowledge at\n all.[32]\n5.1 The E-mail Coordination Example\n\nLizzi and Joanna are faced with the coordination problem summarized in\nthe following figure:\n\n\n\n\n\u00a0\n\u00a0\nJoanna \n\n\u00a0\n\u00a0\n\\(A\\)\n\\(B\\)  \n\n\nLizzi\n\\(A\\)\n(2,2)\n(0,\u22124) \n\n\\(B\\)\n(\u22124,0)\n(0,0)  \n\n\nFigure 5.1a \\(\\quad\\omega_1,\n\\mu(\\omega_1)=0.51\\)\n\n\n\n\n\n\u00a0\n\u00a0\nJoanna \n\n\u00a0\n\u00a0\n\\(A\\)\n\\(B\\)  \n\n\nLizzi\n\\(A\\)\n(2,2)\n(0,\u22124) \n\n\\(B\\)\n(\u22124,0)\n(0,0)  \n\n\nFigure 5.1b \\(\\quad\\omega_2,\n\\mu(\\omega_2)=0.49\\)\n\n\nIn Figure 5.1, the payoffs are dependent upon a pair of possible\nworlds. World \\(\\omega_1\\) occurs with probability \\(\\mu(\\omega_1) =\n.51,\\) while \\(\\omega_2\\) occurs with probability \\(\\mu(\\omega_2) =\n.49.\\) Hence, they coordinate with complete success by both choosing\n\\(A(B)\\) only if the state of the world is \\(\\omega_1(\\omega_2)\\).\n\nSuppose that Lizzi can observe the state of the world, but Joanna\ncannot. We can interpret this game as follows: Joanna and Lizzi would\nlike to have a dinner together prepared by Aldo, their favorite chef.\nAldo alternates between \\(A\\) and \\(B\\), the two branches of Sorriso,\ntheir favorite restaurant. State \\(\\omega_i\\) is Aldo\u2019s location\nthat day. At state \\(\\omega_1 (\\omega_2)\\), Aldo is at \\(A (B)\\).\nLizzi, who is on Sorriso\u2019s special mailing list, receives notice\nof \\(\\omega_i\\). Lizzi\u2019s and Joanna\u2019s best outcome occurs\nwhen they meet where Aldo is working, so they can have their planned\ndinner. If they meet but miss Aldo, they are disappointed and do not\nhave dinner after all. If either goes to \\(A\\) and finds herself\nalone, then she is again disappointed and does not have dinner. But\nwhat each really wants to avoid is going to \\(B\\) if the other goes to\n\\(A\\). If either of them arrives at \\(B\\) alone, she not only misses\ndinner but must pay the exorbitant parking fee of the hotel which\nhouses \\(B\\), since the headwaiter of \\(B\\) refuses to validate the\nparking ticket of anyone who asks for a table for two and then sits\nalone. This is what Harsanyi (1967) terms a game of incomplete\ninformation, since the game\u2019s payoffs depend upon states\nwhich not all the agents know.\n\n\\(A\\) is a \u201cplay-it-safe\u201d strategy for both Joanna and\n Lizzi.[33]\n By choosing \\(A\\) whatever the state of the world happens to be, the\nagents run the risk that they will fail to get the positive payoff of\nmeeting where Aldo is, but each is also sure to avoid the really bad\nconsequence of choosing \\(B\\) if the other chooses \\(A\\). And since\nonly Lizzi knows the state of the world, neither can use information\nregarding the state of the world to improve their prospects for\ncoordination. For Joanna has no such information, and since Lizzi\nknows this, she knows that Joanna has to choose accordingly, so Lizzi\nmust choose her best response to the move she anticipates Joanna to\nmake regardless of the state of the world Lizzi observes. Apparently\nLizzi and Joanna cannot achieve expected payoffs greater than 1.02 for\neach, their expected payoffs if they choose \\((A, A)\\) at either state\nof the world.\n\nIf the state \\(\\omega\\) were common knowledge, then the conditional\nstrategy profile \\((A, A)\\) if \\(\\omega = \\omega_1\\) and \\((B, B)\\),\nif \\(\\omega = \\omega_2\\) would be a strict Nash equilibrium at which\neach would achieve a payoff of 2. So the obvious remedy to their\npredicament would be for Lizzi to tell Joanna Aldo\u2019s location in\na face-to-face or telephone conversation and for them to agree to go\nwhere Aldo is, which would make the state \\(\\omega\\) and their\nintentions to coordinate on the best outcome given \\(\\omega\\) common\nknowledge between them. Suppose for some reason they cannot talk to\neach other, but they prearrange that Lizzi will send Joanna an e-mail\nmessage if, and only if, \\(\\omega_2\\) occurs. Suppose further that\nJoanna\u2019s and Lizzi\u2019s e-mail systems are set up to send a\nreply message automatically to the sender of any message received and\nviewed, and that due to technical problems there is a small\nprobability, \\(\\varepsilon \\gt 0\\), that any message can fail to\narrive at its destination. Then if Lizzi sends Joanna a message, and\nreceives an automatic confirmation, then Lizzi knows that Joanna knows\nthat \\(\\omega_2\\) has occurred. If Joanna receives an automatic\nconfirmation of Lizzi\u2019s automatic confirmation, then Joanna\nknows that Lizzi knows that Joanna knows that \\(\\omega_2\\) occurred,\nand so on. That \\(\\omega_2\\) has occurred would become common\nknowledge if each agent received infinitely many automatic\nconfirmations, assuming that all the confirmations could be sent and\nreceived in a finite amount of\n time.[34]\n However, because of the probability \\(\\varepsilon\\) of transmission\nfailure at every stage of communication, the sequence of confirmations\nstops after finitely many stages with probability one. With\nprobability one, therefore, the agents fail to achieve full common\nknowledge. But they do at least achieve something \u201cclose\u201d\nto common knowledge. Does this imply that they have good prospects of\nsettling upon \\((B, B)\\)?\n\nRubinstein shows by induction that if the number of automatically\nexchanged confirmation messages is finite, then \\(A\\) is the only\nchoice that maximizes expected utility for each agent, given what she\nknows about what they both know.\n\nRubinstein\u2019s Proof\n\n\nSo even if agents have \u201calmost\u201d common knowledge, in the\nsense that the number of levels of knowledge in \u201cJoanna knows\nthat Lizzi knows that \u2026 that Joanna knows that \\(\\omega_2\\)\noccurred\u201d is very large, their behavior is quite different from\ntheir behavior given common knowledge that \\(\\omega_2\\) has occurred.\nIndeed, as Rubinstein points out, given merely \u201calmost\u201d\ncommon knowledge, the agents choose as if no communication had\noccurred at all! Rubinstein also notes that this result violates our\nintuitions about what we would expect the agents to do in this case.\n(See Rubinstein 1992, p. 324.) If \\(T_i = 17\\), wouldn\u2019t we\nexpect agent \\(i\\) to choose \\(B\\)? Indeed, in many actual situations\nwe might think it plausible that the agents would each expect the\nother to choose \\(B\\) even if \\(T_1 = T_2 = 2\\), which is all that is\nneeded for Lizzi to know that Joanna has received her original message\nand for Joanna to know that Lizzi knows this! Binmore and Samelson\n(2001) in fact show that if Joanna and Lizzi incur a cost when paying\nattention to the messages they exchange, or if sending a message is\ncostly, then longer streams of messages are not paid attention to or\ndo not occur, respectively.\n\nLederman (2018a) proposes a radical solution to the paradoxes. In the\ncase of the coordinated attack, he argues that rational\ngenerals who commonly know that they are rational will attack if (and\nonly if) they have common knowledge that they will attack; since\ncommon knowledge is not attainable by exchanging messages, they will\nnot attack. However, admitting that the generals do not commonly\nbelieve that they are rational, a simple model can be built showing\nthat such generals do attack without common knowledge that they will.\nSimilarly, in the case of the e-mail game, he shows that if players\ncan be of an irrational type (so that she chooses game \\(B\\) even if\nher expected payoff is lower than for choosing game \\(A\\),) and one\nplayer believes with sufficiently high probability that the other\nplayer is of the irrational type, then players can coordinate on game\n\\(B\\) after a finite number of messages have been exchanged. Thus,\nLederman (2018a) argues that we should take common knowledge of\nrationality to be a simplifying assumption, useful to produce\ntractable mathematical models and yet generally false \u201cin the\nwild,\u201d where a commonsense notion of rationality does let\ngenerals and laymen easily coordinate after a small number of message\nexchanges. Thomason (2021) takes issue with Lederman\u2019s use of\nthe notion of commonsense rationality, and argues about the importance\nof considering instead the cognitive and deliberative processes that\nlead to the emergence of both individual and commonly held attitudes.\nDespite their disagreement, both Lederman (2018a, 2018b) and Thomason\n(2021) emphasize the importance of the relation between (commonly)\nheld beliefs or knowledge and practical reasoning. An interesting\napplication of practical issues pertaining to the attainability of\ncommon knowledge is offered in Halpern and Pass (2017), where a\nblockchain protocol (and consensus and hence coordination therein) is\nanalyzed in terms of suitable weakenings of the notion of common\n knowledge.[35]\n5.2 Common \\(p\\)-Belief\n\nThe example in Section 5.1 hints that mutual knowledge is not the only\nweakening of common knowledge that is relevant to coordination.\nBrandenburger and Dekel (1987) and\u00a0Monderer and Samet (1989)\nexplore another option, which is to weaken the properties of the\n\\(\\mathbf{K}^{*}_N\\) operator. Monderer and Samet motivate this\napproach by noting that even if a mutual knowledge hierarchy stops at\na certain level, agents might still have higher level mutual\nbeliefs about the proposition in question. So they replace\nthe knowledge operator \\(\\mathbf{K}_i\\) with a belief\noperator \\(\\mathbf{B}^p_i\\):\n\n\nDefinition 5.1\n\nIf \\(\\mu_i(\\cdot)\\) is agent \\(i\\)\u2019s probability distribution\nover \\(\\Omega\\), then \n\n\\[\n\\mathbf{B}^p_i(A) = \\{\\omega \\mid \\mu_i (A \\mid \\mathcal{H}_i (\\omega)) \\ge p \\}\n\\]\n\n\n\n\\(\\mathbf{B}^p_i(A)\\) is to be read \u2018\\(i\\) believes \\(A\\) (given\n\\(i\\)\u2019s private information) with probability at least \\(p\\) at\n\\(\\omega\\)\u2019, or \u2018\\(i\\) \\(p\\)-believes \\(A\\)\u2019. The\nbelief operator \\(\\mathbf{B}^p_i\\) satisfies axioms K2, K3, and K4 of\nthe knowledge operator. \\(\\mathbf{B}^p_i\\) does not satisfy K1, but\ndoes satisfy the weaker property \n\n\\[\n\\mu_i (A \\mid \\mathbf{B}^p_i(A)) \\ge p\n\\]\n\n\nthat is, if one believes \\(A\\) with probability at least \\(p\\), then\nthe probability of \\(A\\) is indeed at least \\(p\\).\n\nOne can define mutual and common p-beliefs\nrecursively in a manner similar to the definition of mutual and common\nknowledge:\n\n\nDefinition 5.2\n\nLet a set \\(\\Omega\\) of possible worlds together with a set of agents\n\\(N\\) be given.\n\n\n(1) The proposition that \\(A\\) is (first level or first order)\nmutual p-belief for the agents of \\(N, \\mathbf{B}^p_{N^1}(A),\\)\nis the set defined by \n\n\\[\n\\mathbf{B}^p_{N^1}(A) \\equiv \\bigcap_{i\\in N} \\mathbf{B}^p_i(A).\n\\]\n\n\n(2) The proposition that \\(A\\) is \\(m\\)th level\n(or \\(m\\)th order) mutual\\(p\\)-belief\namong the agents of \\(N, \\mathbf{B}^p_{N^m}(A),\\) is defined\nrecursively as the set \n\n\\[\n\\mathbf{B}^p_{N^m}(A) \\equiv \\bigcap_{i\\in N} \\mathbf{B}^p_i (\\mathbf{B}^p_{N^{m-1}}(A))\n\\]\n\n\n(3) The proposition that \\(A\\) is common p-belief among the\nagents of \\(N, \\mathbf{B}^p_{N^*}(A),\\) is defined as the set\n\n\\[\n\\mathbf{B}^p_{N^*}(A) \\equiv \\bigcap_{m=1}^{\\infty} \\mathbf{B}^p_{N^m}(A).\n\\]\n\n\n\nIf \\(A\\) is common (or \\(m\\)th level mutual) knowledge at\nworld \\(\\omega\\), then \\(A\\) is common \\((m\\)th level)\n\\(p\\)-belief at \\(\\omega\\) for every value of \\(p\\). So mutual and\ncommon \\(p\\)-beliefs formally generalize the mutual and common\nknowledge concepts. However, note that \\(\\mathbf{B}^1_{N^*}(A)\\) is\nnot necessarily the same proposition as \\(\\mathbf{K}^{*}_N (A)\\), that\nis, even if \\(A\\) is common 1-belief, \\(A\\) can fail to be common\nknowledge.\n\nCommon \\(p\\)-belief forms a hierarchy similar to a common knowledge\nhierarchy:\n\n\nProposition 5.3\n\n\\(\\omega \\in \\mathbf{B}^{p}_{N^m}(A)\\) iff\n\n(\u2217) For all agents \\(i_1, i_2 , \\ldots ,i_m \\in N,\\) \\(\\omega\n\\in \\mathbf{B}^{p}_{i_1}\\mathbf{B}^p_{i_2} \\ldots\n\\mathbf{B}^p_{i_m}(A)\\)\n\nHence, \\(\\omega \\in \\mathbf{B}^{p}_{N^*}(A)\\) iff (\u2217) is the\ncase for each \\(m \\ge 1.\\)\n\nProof. Similar to the\n Proof of Proposition 2.5.\n\n\nOne can draw several morals from the e-mail game of Example 5.1.\nRubinstein (1987) argues that his conclusion seems paradoxical for the\nsame reason the backwards induction solution of Alan\u2019s and\nFiona\u2019s perfect information game might seem paradoxical:\nMathematical induction does not appear to be part of our\n\u201ceveryday\u201d reasoning. This game also shows that in order\nfor A to be a common truism for a set of agents, they ordinarily must\nperceive an event which implies A simultaneously in each\nothers\u2019 presence. A third moral is that in some cases, it may\nmake sense for the agents to employ some solution concept weaker than\nNash or correlated equilibrium. In their analysis of the e-mail game,\nMonderer and Samet (1989) introduce the notions of ex ante\nand ex post \\(\\varepsilon\\)-equilibrium. An ex ante\nequilibrium \\(h\\) is a system of strategy profiles such that no agent\n\\(i\\) expects to gain more than \\(\\varepsilon\\)-utiles if \\(i\\)\ndeviates from \\(h\\). An ex post equilibrium \\(h'\\) is a\nsystem of strategy profiles such that no agent \\(i\\) expects to gain\nmore than \\(\\varepsilon\\)-utiles by deviating from \\(h'\\) given\n\\(i\\)\u2019s private information. When \\(\\varepsilon = 0\\), these\nconcepts coincide, and \\(h\\) is a Nash equilibrium. Monderer and Samet\nshow that, while the agents in the e-mail game can never achieve\ncommon knowledge of the world \\(\\omega\\), if they have common\n\\(p\\)-belief of \\(\\omega\\) for sufficiently high \\(p\\), then there is\nan ex ante equilibrium at which they follow \\((A,A)\\) if\n\\(\\omega = \\omega_1\\) and \\((B,B)\\), if \\(\\omega = \\omega_2\\). This\nequilibrium turns out not to be ex post. However, if the\nsituation is changed so that there are no replies, then Lizzi and\nJoanna could have at most first order mutual knowledge that \\(\\omega =\n\\omega_2\\). Monderer and Samet show that in this situation, given\nsufficiently high common \\(p\\)-belief that \\(\\omega = \\omega_2\\),\nthere is an ex post equilibrium at which Joanna and Lizzi\nchoose \\((B,B)\\) if \\(\\omega = \\omega_2\\)! So another way one might\nview this third moral of the e-mail game is that agents\u2019\nprospects for coordination can sometimes improve dramatically if they\nrely on their common beliefs as well as their mutual knowledge. More\nrecently, the notion of \\(p\\)-belief and \\(p\\)-common belief proved\nuseful (Paternotte, 2011) to analyze and formalize Lewis\u2019s\naccount of common knowledge, while Paternotte (2017), establishing a\nlink between \u201cordinary\u201d common knowledge and \\(p\\)-common\nbelief, uses the latter to show that only a limited number of\nexchanges in the e-mail game or coordinated attack paradox would be\nsufficient to determine coordination. The result, building on\nfoundations provided by Leitgeb (2014), is used to show that our\n\u201cordinary\u201d understanding of common knowledge is captured\nby probabilistic common belief, although at the price of decreased\nrobustness relative to the number of individuals sharing common belief\nand their awareness.\n",
    "bibliography": {
        "categories": [
            "Annotations",
            "References"
        ],
        "cat_ref_text": {
            "Annotations": "</h3>\n<p>\nLewis (1969) is the classic pioneering study of common knowledge and\nits potential applications to conventions and game theory. As Lewis\nacknowledges, parts of his work are foreshadowed in Hume (1740) and\nSchelling (1960).</p>\n<p>\nAumann (1976) gives the first mathematically rigorous formulation of\ncommon knowledge using set theory. Schiffer (1972) uses the formal\nvocabulary of <em>epistemic logic</em> (Hintikka 1962) to state his\ndefinition of common knowledge. Schiffer\u2019s general approach is\nto augment a system of sentential logic with a set of knowledge\noperators corresponding to a set of agents, and then to define common\nknowledge as a hierarchy of propositions in the augmented system.\nBacharach (1992), Bicchieri (1993) and Fagin, <em>et al</em>. (1995)\nadopt this approach, and develop logical theories of common knowledge\nwhich include soundness and completeness theorems. Fagin, et al. show\nthat the syntactic and set-theoretic approaches to developing common\nknowledge are logically equivalent.</p>\n<p>\nAumann (1995) gives a recent defense of the classical view of\nbackwards induction in games of imperfect information. For criticisms\nof the classical view, see Binmore (1987), Reny (1992), Bicchieri\n(1989) and especially Bicchieri (1993). Brandenburger (1992) surveys\nthe known results connecting mutual and common knowledge to solution\nconcepts in game theory. For more in-depth survey articles on common\nknowledge and its applications to game theory, see Binmore and\nBrandenburger (1989), Geanakoplos (1994) and Dekel and Gul (1997). For\nher alternate account of common knowledge along with an account of\nconventions which opposes Lewis\u2019 account, see Gilbert\n(1989).</p>\n<p>\nMonderer and Samet (1989) remains one of the best resources for the\nstudy of common p-belief.</p>\n<h3>",
            "References": [
                "</h3>\n<ul class=\"hanging\">",
                "Alberucci, Luca and Jaeger, Gerhard, 2005, \u201cAbout cut\nelimination for logics of common knowledge\u201d, <em>Annals of Pure\nand Applied Logic</em>, 133(1\u20133): 73\u201399.",
                "Aumann, Robert, 1974, \u201cSubjectivity and Correlation in\nRandomized Strategies\u201d, <em>Journal of Mathematical\nEconomics</em>, 1: 67\u201396.",
                "\u2013\u2013\u2013, 1976, \u201cAgreeing to Disagree\u201d,\n<em>Annals of Statistics</em>, 4: 1236\u20139.",
                "\u2013\u2013\u2013, 1987, \u201cCorrelated Equilibrium as an\nExpression of Bayesian Rationality\u201d, <em>Econometrica</em>, 55:\n1\u201318.",
                "\u2013\u2013\u2013, 1995, \u201cBackward Induction and Common\nKnowledge of Rationality\u201d, <em>Games and Economic Behavior</em>\n8: 6\u201319.",
                "Bacharach, Michael, 1985 \u201cSome Extensions of a Claim of\nAumann in an Axiomatic Model of Knowledge\u201d, <em>Journal of\nEconomic Theory</em>, 37(1): 167\u2013190.",
                "\u2013\u2013\u2013, 1992.\u201cBackward Induction and Beliefs\nAbout Oneself\u201d, <em>Synthese</em>, 91: 247\u2013284.",
                "Barwise, Jon, 1988, \u201cThree Views of Common Knowledge\u201d,\nin <em>Proceedings of the Second Conference on Theoretical Aspects of\nReasoning About Knowledge</em>, M.Y. Vardi (ed.), San Francisco:\nMorgan Kaufman, pp. 365\u2013379.",
                "\u2013\u2013\u2013, 1989, <em>The Situation in Logic</em>,\nStanford: Center for the Study of Language and Information.",
                "Bernheim, B. Douglas, 1984, \u201cRationalizable Strategic\nBehavior\u201d, <em>Econometrica</em>, 52: 1007\u20131028.",
                "Bicchieri, Cristina, 1989, \u201cSelf Refuting Theories of\nStrategic Interaction: A Paradox of Common Knowledge\u201d,\n<em>Erkenntnis</em>, 30: 69\u201385.",
                "\u2013\u2013\u2013, 1993, <em>Rationality and\nCoordination</em>, Cambridge: Cambridge University Press.",
                "\u2013\u2013\u2013, 2006, <em>The Grammar of Society</em>,\nCambridge: Cambridge University Press.",
                "Binmore, Ken, 1987, \u201cModelling Rational Players I\u201d,\n<em>Economics and Philosophy</em>, 3: 179\u2013241.",
                "\u2013\u2013\u2013, 1992, <em>Fun and Games</em>, Lexington,\nMA: D. C. Heath.",
                "\u2013\u2013\u2013, 2008, \u201cDo Conventions Need to be\nCommon Knowledge?\u201d, <em>Topoi</em>, 27: 17\u201327.",
                "Binmore, Ken and Brandenburger, Adam, 1988, \u201cCommon\nknowledge and Game theory\u201d ST/ICERD Discussion Paper 88/167,\nLondon School of Economics.",
                "Binmore, Ken and Samuelson, Larry, 2001, \u201cCoordinated Action\nin the Electronic Mail Game\u201d <em>Games and Economic\nBehavior</em>, 35(1): 6\u201330.",
                "Bonanno, Giacomo and Battigalli, Pierpaolo, 1999, \u201cRecent\nResults on Belief, Knowledge and the Epistemic Foundations of Game\nTheory\u201d, <em>Research in Economics</em>, 53(2):\n149\u2013225.",
                "Bonnay, D. and Egr\u00e9, Paul, 2009, \u201cInexact Knowledge\nwith Introspection\u201d, <em>Journal of Philosophical Logic</em>,\n38: 179\u2013227.",
                "Brandenburger, Adam, 1992, \u201cKnowledge and Equilibrium in\nGames\u201d, <em>Journal of Economic Perspectives</em>, 6:\n83\u2013101.",
                "Brandenburger, Adam, and Dekel, Eddie, 1987, \u201cCommon\nKnowledge with Probability 1\u201d, <em>Journal of Mathematical\nEconomics</em>, 16: 237\u2013245.",
                "\u2013\u2013\u2013, 1988, \u201cThe Role of Common Knowledge\nAssumptions in Game Theory\u201d, in <em>The Economics of Missing\nMarkets, Information and Games</em>, Frank Hahn (ed.), Oxford:\nClarendon Press, 46\u201361.",
                "Bruni, Riccardo and Giacomo Sillari, 2018, \u201cA Rational Way\nof Playing: Revision Theory for Strategic Interaction\u201d,\n<em>Journal of Philosophical Logic</em>, 47(3), 419\u2013448.",
                "Carnap, Rudolf, 1947, <em>Meaning and Necessity: A Study in\nSemantics and Modal Logic</em>, Chicago, University of Chicago\nPress.",
                "Cave, Jonathan AK, 1983, \u201cLearning to Agree\u201d,\n<em>Economics Letters</em>, 12(2): 147\u2013152.",
                "Chwe, Michael, 1999, \u201cStructure and Strategy in Collective\nAction\u201d, <em>American Journal of Sociology</em> 105:\n128\u201356.",
                "\u2013\u2013\u2013, 2000, \u201cCommuncation and Coordination\nin Social Networks\u201d, <em>Review of Economic Studies</em>, 67:\n1\u201316.",
                "\u2013\u2013\u2013, 2001, <em>Rational Ritual</em>, Princeton,\nNJ: Princeton University Press",
                "Cubitt, Robin and Sugden, Robert, 2003, \u201cCommon Knowledge,\nSalience and Convention: A Reconstruction of David Lewis\u2019 Game\nTheory\u201d, <em>Economics and Philosophy</em>, 19:\n175\u2013210.",
                "D\u00e9gremont, C\u00e9dric, and Oliver Roy, 2012,\n\u201cAgreement Theorems in Dynamic-Epistemic Logic\u201d,\n<em>Journal of Philosophical Logic</em>, 41(4): 735-764.",
                "Dekel, Eddie and Gul, Faruk, 1997, \u201cRationality and\nKnowledge in Game Theory\u201d, in <em>Advances in Economic Theory:\nSeventh World Congress of the Econometric Society</em>, D. Kreps and\nK. Wallace eds., Cambridge: Cambridge University Press.",
                "Dekel, Eddie, Lipman, Bart and Rustichini, Aldo, 1998,\n\u201cStandard State-Space Models Preclude Unawareness,\u201d\n<em>Econometrica</em>, 66: 159\u2013173.",
                "Devetag, Giovanna, Hosni, Hykel and Sillari, Giacomo, 2013,\n\u201cPlay 7: Mutual Versus Common Knowledge of Advice in a Weak-Link\nGame,\u201d <em>Synthese</em>, 190(8): 1351\u20131381",
                "Fagin, Ronald and Halpern, Joseph Y., 1988, \u201cAwareness and\nLimited Reasoning,\u201d <em>Artificial Intelligence</em>, 34:\n39\u201376.",
                "Fagin, Ronald, Halpern, Joseph Y., Moses, Yoram and Vardi, Moshe\nY., 1995, <em>Reasoning About Knowledge</em>, Cambridge, MA: MIT\nPress.",
                "Friedell, Morris, 1967, \u201cOn the Structure of Shared\nAwareness,\u201d <em>Working papers of the Center for Research on\nSocial Organizations</em> (Paper #27), Ann Arbor: University of\nMichigan.",
                "\u2013\u2013\u2013, 1969, \u201cOn the Structure of Shared\nAwareness,\u201d <em>Behavioral Science</em>, 14(1):\n28\u201339.",
                "Geanakoplos, John, 1989, \u201cGames Theory without Partitions,\nand Applications to Speculation and Consensus,\u201d Cowles\nFoundation Discussion Paper, No. 914.",
                "\u2013\u2013\u2013, 1994, \u201cCommon Knowledge\u201d, in\n<em>Handbook of Game Theory</em> (Volume 2), Robert Aumann and Sergiu\nHart (eds.), Amsterdam: Elsevier Science B.V., 1438\u20131496.",
                "Geanakoplos, John and Heraklis M. Polemarchakis, 1982, \u201cWe\nCan\u2019t Disagree Forever\u201d <em>Journal of Economic\ntheory</em> 28(1): 192\u2013200.",
                "Gilbert, Margaret, 1989, <em>On Social Facts</em>, Princeton:\nPrinceton University Press.",
                "Halpern, Jospeh, 2001, \u201cAlternative Semantics for\nUnawareness\u201d, <em>Games and Economic Behavior</em>, 37(2):\n321\u2013339",
                "Halpern, J. Y., &amp; Moses, Y. , 1990, \u201cKnowledge and\ncommon Knowledge in a Distributed Environment\u201d. <em>Journal of\nthe Association for Computing Machinery</em>, 37(3):\n549\u2013587.",
                "Halpern, J. Y., &amp; Pass, R., 2017, \u201cA Knowledge-Based\nAnalysis of the Blockchain Protocol\u201d. <em>arXiv preprint</em>\narXiv:1707.08751.",
                "Harman, Gilbert, 1977, \u201cReview of <em>Linguistic\nBehavior</em> by Jonathan Bennett\u201d, <em>Language</em>, 53:\n417\u2013424.",
                "Harsanyi, J., 1967, \u201cGames with Incomplete Information\nPlayed by \u201dBayesian\u201c Players, I: The basic model\u201d,\n<em>Management Science</em>, 14: 159\u201382.",
                "\u2013\u2013\u2013, 1968a, \u201cGames with Incomplete\nInformation Played by \u201dBayesian\u201c Players, II: Bayesian\nEquilibrium Points\u201d, <em>Management Science</em>, 14:\n320\u2013324.",
                "\u2013\u2013\u2013, 1968b, \u201cGames with Incomplete\nInformation Played by \u201dBayesian\u201c Players, III: The basic\nprobability distribution of the game\u201d, <em>Management\nScience</em>, 14: 486\u2013502.",
                "Heifetz, Aviad, 1999, \u201cIterative and Fixed Point Common\nBelief\u201d, <em>Journal of Philosophical Logic</em>, 28(1):\n61\u201379.",
                "Heifetz, Aviad, Meier, Martin and Schipper, Burkhard, 2006,\n\u201cInteractive Unawareness\u201d, <em>Journal of Economic\nTheory</em>, 130: 78\u201394.",
                "Hintikka, Jaakko, 1962, <em>Knowledge and Belief</em>, Ithaca, NY:\nCornell University Press.",
                "Hume, David, 1740 [1888, 1976], <em>A Treatise of Human\nNature</em>, L. A. Selby-Bigge (ed.), rev. 2nd. edition P. H. Nidditch\n(ed.), Oxford: Clarendon Press.",
                "Immerman, D., 2021, \u201cHow Common Knowledge Is\nPossible\u201d. <em>Mind</em>, first online 17 January 2021.\ndoi:10.1093/mind/fzaa090",
                "J\u00e4ger, Gerhard and Michel Marti, 2016, \u201cIntuitionistic\nCommon Knowledge or Belief\u201d, <em>Journal of Applied Logic</em>,\n18: 150\u2013163",
                "Lederman, Harvey, 2018a, \u201cTwo Paradoxes of Common Knowledge:\nCoordinated Attack and Electronic Mail\u201d, <em>No\u00fbs</em>,\n52: 921\u2013945.",
                "\u2013\u2013\u2013, 2018b, \u201cUncommon Knowledge\u201d\n<em>Mind</em> 127, 1069\u20131105.",
                "Leitgeb, Hannes, 2014, \u201cThe Stability Theory of\nBelief\u201d, <em>The Philosophical Review</em>, 123(2):\n131\u2013171.",
                "Lewis, C. I., 1943, \u201cThe Modes of Meaning\u201d,\n<em>Philosophy and Phenomenological Research</em>, 4:\n236\u2013250.",
                "Lewis, David, 1969, <em>Convention: A Philosophical Study</em>,\nCambridge, MA: Harvard University Press.",
                "\u2013\u2013\u2013, 1978, \u201cTruth in Fiction\u201d,\n<em>American Philosophical Quarterly</em>, 15: 37\u201346.",
                "Littlewood, J. E., 1953, <em>A Mathematical Miscellany</em>,\nLondon: Methuen; reprinted as <em>Littlewood\u2019s Miscellany</em>,\nB. Bollobas (ed.), Cambridge: Cambridge University Press, 1986.",
                "McKelvey, Richard and Page, Talbot, 1986, \u201cCommon Knowledge,\nConsensus and Aggregate Information\u201d, <em>Econometrica</em>, 54:\n109\u2013127.",
                "Meyer, J.-J.Ch. and van der Hoek, Wiebe, 1995, <em>Epistemic Logic\nfor Computer Science and Artificial Intelligence</em> (Cambridge\nTracts in Theoretical Computer Science 41), Cambridge: Cambridge\nUniversity Press.",
                "Milgrom, Paul, 1981, \u201cAn Axiomatic Characterization of\nCommon Knowledge\u201d, <em>Econometrica</em>, 49:\n219\u2013222.",
                "Milgrom, Paul, and Nancy Stokey, 1982, \u201cInformation, Trade\nand Common Knowledge\u201d, <em>Journal of Economic Theory</em>,\n26(1): 17\u201327.",
                "Monderer, Dov and Samet, Dov, 1989, \u201cApproximating Common\nKnowledge with Common Beliefs\u201d, <em>Games and Economic\nBehavior</em>, 1: 170\u2013190.",
                "Nash, John, 1950, \u201cEquilibrium Points in N-person\nGames\u201d. <em>Proceedings of the National Academy of Sciences of\nthe United States</em>, 36: 48\u201349.",
                "\u2013\u2013\u2013, 1951, \u201cNon-Cooperative Games\u201d.\n<em>Annals of Mathematics</em>, 54: 286\u2013295.",
                "Nozick, Robert, 1963, <em>The Normative Theory of Individual\nChoice</em>, Ph.D. dissertation, Princeton University",
                "Paternotte, C\u00e9dric, 2011, \u201cBeing Realistic about\nCommon Knowledge: a Lewisian Approach\u201d, <em>Synthese</em>,\n183(2): 249\u2013276.",
                "\u2013\u2013\u2013, 2017, \u201cThe Fragility of Common\nKnowledge\u201d, <em>Erkenntnis</em>, 82(3): 451\u2013472.",
                "Pearce, David, 1984, \u201cRationalizable Strategic Behavior and\nthe Problem of Perfection\u201d, <em>Econometrica</em>, 52:\n1029\u20131050.",
                "Reny, Philip J, 1988, \u201cCommon Knowledge and Games with\nPerfect Information.\u201d In <em>PSA: Proceedings of the Biennial\nMeeting of the Philosophy of Science Association</em>, vol. 1988, no.\n2, pp. 363\u2013369. East Lansing: Philosophy of Science\nAssociation.",
                "\u2013\u2013\u2013, 1992, \u201cRationality in Extensive Form\nGames\u201d, <em>Journal of Economic Perspectives</em>, 6:\n103\u2013118.",
                "Rubinstein, Ariel, 1987, \u201cA Game with \u201dAlmost Common\nKnowledge\u201c: An Example\u201d, in <em>Theoretical\nEconomics</em>, D. P. 87/165. London School of Economics.",
                "Samet, Dov, 1990, \u201cIgnoring Ignorance and Agreeing to\nDisagree\u201d, <em>Journal of Economic Theory</em>, 52:\n190\u2013207.",
                "Schelling, Thomas, 1960, <em>The Strategy of Conflict</em>,\nCambridge, MA: Harvard University Press.",
                "Schiffer, Stephen, 1972, <em>Meaning</em>, Oxford: Oxford\nUniversity Press.",
                "Sillari, Giacomo, 2005, \u201cA Logical Framework for\nConvention\u201d, <em>Synthese</em>, 147(2): 379\u2013400.",
                "\u2013\u2013\u2013, 2008, \u201cCommon Knowledge and\nConvention\u201d, <em>Topoi</em>, 27(1): 29\u201339.",
                "\u2013\u2013\u2013, 2013, \u201cRule-Following as\nCoordination: a Game-Theoretic Approach\u201d, <em>Synthese</em>,\n190(5): 871\u2013890.",
                "\u2013\u2013\u2013, 2019, \u201cLogics of Belief\u201d,\n<em>Rivista di Filosofia</em>, 110(2): 243\u2013262.",
                "Skyrms, Brian, 1984, <em>Pragmatics and Empiricism</em>, New\nHaven: Yale University Press.",
                "\u2013\u2013\u2013, 1990, <em>The Dynamics of Rational\nDeliberation</em>, Cambridge, MA: Harvard University Press",
                "\u2013\u2013\u2013, 1991, \u201cInductive Deliberation,\nAdmissible Acts, and Perfect Equilibrium\u201d, in <em>Foundations of\nDecision Theory</em>, Michael Bacharach and Susan Hurley eds.,\nCambridge, MA: Blackwell, pp. 220\u2013241.",
                "\u2013\u2013\u2013, 1998, \u201cThe Shadow of the\nFuture\u201d, in <em>Rational Commitment and Social Justice: Essays\nfor Gregory Kavka</em>, Jules Coleman and Christopher Morris eds.,\nCambridge: Cambridge University Press, pp. 12\u201322.",
                "Sugden, Robert, 1986, <em>The Economics of Rights, Cooperation and\nWelfare</em>, New York: Basil Blackwell.",
                "Thomason, R. H., 2021, \u201cCommon Knowledge, Common Attitudes\nand Social Reasoning\u201d, <em>Bulletin of the Section of\nLogic</em>, 50(2): 229\u2013247.",
                "Vanderschraaf, Peter, 1995, \u201cEndogenous Correlated\nEquilibria in Noncooperative Games\u201d, <em>Theory and\nDecision</em>, 38: 61\u201384.",
                "Vanderschraaf, Peter, 1998, \u201cKnowledge, Equilibrium and\nConvention\u201d, <em>Erkenntnis</em>, 49: 337\u2013369.",
                "\u2013\u2013\u2013, 2001. <em>A Study in Inductive\nDeliberation</em>, New York: Routledge.",
                "von Neumann, John and Morgenstern, Oskar, 1944, <em>Theory of\nGames and Economic Behavior</em>, Princeton: Princeton University\nPress.\n</ul>\n</div>"
            ]
        },
        "raw_text": "<div id=\"bibliography\">\n<h2><a name=\"Bib\">Bibliography</a></h2>\n<h3>Annotations</h3>\n<p>\nLewis (1969) is the classic pioneering study of common knowledge and\nits potential applications to conventions and game theory. As Lewis\nacknowledges, parts of his work are foreshadowed in Hume (1740) and\nSchelling (1960).</p>\n<p>\nAumann (1976) gives the first mathematically rigorous formulation of\ncommon knowledge using set theory. Schiffer (1972) uses the formal\nvocabulary of <em>epistemic logic</em> (Hintikka 1962) to state his\ndefinition of common knowledge. Schiffer\u2019s general approach is\nto augment a system of sentential logic with a set of knowledge\noperators corresponding to a set of agents, and then to define common\nknowledge as a hierarchy of propositions in the augmented system.\nBacharach (1992), Bicchieri (1993) and Fagin, <em>et al</em>. (1995)\nadopt this approach, and develop logical theories of common knowledge\nwhich include soundness and completeness theorems. Fagin, et al. show\nthat the syntactic and set-theoretic approaches to developing common\nknowledge are logically equivalent.</p>\n<p>\nAumann (1995) gives a recent defense of the classical view of\nbackwards induction in games of imperfect information. For criticisms\nof the classical view, see Binmore (1987), Reny (1992), Bicchieri\n(1989) and especially Bicchieri (1993). Brandenburger (1992) surveys\nthe known results connecting mutual and common knowledge to solution\nconcepts in game theory. For more in-depth survey articles on common\nknowledge and its applications to game theory, see Binmore and\nBrandenburger (1989), Geanakoplos (1994) and Dekel and Gul (1997). For\nher alternate account of common knowledge along with an account of\nconventions which opposes Lewis\u2019 account, see Gilbert\n(1989).</p>\n<p>\nMonderer and Samet (1989) remains one of the best resources for the\nstudy of common p-belief.</p>\n<h3>References</h3>\n<ul class=\"hanging\">\n<li>Alberucci, Luca and Jaeger, Gerhard, 2005, \u201cAbout cut\nelimination for logics of common knowledge\u201d, <em>Annals of Pure\nand Applied Logic</em>, 133(1\u20133): 73\u201399.</li>\n<li>Aumann, Robert, 1974, \u201cSubjectivity and Correlation in\nRandomized Strategies\u201d, <em>Journal of Mathematical\nEconomics</em>, 1: 67\u201396.</li>\n<li>\u2013\u2013\u2013, 1976, \u201cAgreeing to Disagree\u201d,\n<em>Annals of Statistics</em>, 4: 1236\u20139.</li>\n<li>\u2013\u2013\u2013, 1987, \u201cCorrelated Equilibrium as an\nExpression of Bayesian Rationality\u201d, <em>Econometrica</em>, 55:\n1\u201318.</li>\n<li>\u2013\u2013\u2013, 1995, \u201cBackward Induction and Common\nKnowledge of Rationality\u201d, <em>Games and Economic Behavior</em>\n8: 6\u201319.</li>\n<li>Bacharach, Michael, 1985 \u201cSome Extensions of a Claim of\nAumann in an Axiomatic Model of Knowledge\u201d, <em>Journal of\nEconomic Theory</em>, 37(1): 167\u2013190.</li>\n<li>\u2013\u2013\u2013, 1992.\u201cBackward Induction and Beliefs\nAbout Oneself\u201d, <em>Synthese</em>, 91: 247\u2013284.</li>\n<li>Barwise, Jon, 1988, \u201cThree Views of Common Knowledge\u201d,\nin <em>Proceedings of the Second Conference on Theoretical Aspects of\nReasoning About Knowledge</em>, M.Y. Vardi (ed.), San Francisco:\nMorgan Kaufman, pp. 365\u2013379.</li>\n<li>\u2013\u2013\u2013, 1989, <em>The Situation in Logic</em>,\nStanford: Center for the Study of Language and Information.</li>\n<li>Bernheim, B. Douglas, 1984, \u201cRationalizable Strategic\nBehavior\u201d, <em>Econometrica</em>, 52: 1007\u20131028.</li>\n<li>Bicchieri, Cristina, 1989, \u201cSelf Refuting Theories of\nStrategic Interaction: A Paradox of Common Knowledge\u201d,\n<em>Erkenntnis</em>, 30: 69\u201385.</li>\n<li>\u2013\u2013\u2013, 1993, <em>Rationality and\nCoordination</em>, Cambridge: Cambridge University Press.</li>\n<li>\u2013\u2013\u2013, 2006, <em>The Grammar of Society</em>,\nCambridge: Cambridge University Press.</li>\n<li>Binmore, Ken, 1987, \u201cModelling Rational Players I\u201d,\n<em>Economics and Philosophy</em>, 3: 179\u2013241.</li>\n<li>\u2013\u2013\u2013, 1992, <em>Fun and Games</em>, Lexington,\nMA: D. C. Heath.</li>\n<li>\u2013\u2013\u2013, 2008, \u201cDo Conventions Need to be\nCommon Knowledge?\u201d, <em>Topoi</em>, 27: 17\u201327.</li>\n<li>Binmore, Ken and Brandenburger, Adam, 1988, \u201cCommon\nknowledge and Game theory\u201d ST/ICERD Discussion Paper 88/167,\nLondon School of Economics.</li>\n<li>Binmore, Ken and Samuelson, Larry, 2001, \u201cCoordinated Action\nin the Electronic Mail Game\u201d <em>Games and Economic\nBehavior</em>, 35(1): 6\u201330.</li>\n<li>Bonanno, Giacomo and Battigalli, Pierpaolo, 1999, \u201cRecent\nResults on Belief, Knowledge and the Epistemic Foundations of Game\nTheory\u201d, <em>Research in Economics</em>, 53(2):\n149\u2013225.</li>\n<li>Bonnay, D. and Egr\u00e9, Paul, 2009, \u201cInexact Knowledge\nwith Introspection\u201d, <em>Journal of Philosophical Logic</em>,\n38: 179\u2013227.</li>\n<li>Brandenburger, Adam, 1992, \u201cKnowledge and Equilibrium in\nGames\u201d, <em>Journal of Economic Perspectives</em>, 6:\n83\u2013101.</li>\n<li>Brandenburger, Adam, and Dekel, Eddie, 1987, \u201cCommon\nKnowledge with Probability 1\u201d, <em>Journal of Mathematical\nEconomics</em>, 16: 237\u2013245.</li>\n<li>\u2013\u2013\u2013, 1988, \u201cThe Role of Common Knowledge\nAssumptions in Game Theory\u201d, in <em>The Economics of Missing\nMarkets, Information and Games</em>, Frank Hahn (ed.), Oxford:\nClarendon Press, 46\u201361.</li>\n<li>Bruni, Riccardo and Giacomo Sillari, 2018, \u201cA Rational Way\nof Playing: Revision Theory for Strategic Interaction\u201d,\n<em>Journal of Philosophical Logic</em>, 47(3), 419\u2013448.</li>\n<li>Carnap, Rudolf, 1947, <em>Meaning and Necessity: A Study in\nSemantics and Modal Logic</em>, Chicago, University of Chicago\nPress.</li>\n<li>Cave, Jonathan AK, 1983, \u201cLearning to Agree\u201d,\n<em>Economics Letters</em>, 12(2): 147\u2013152.</li>\n<li>Chwe, Michael, 1999, \u201cStructure and Strategy in Collective\nAction\u201d, <em>American Journal of Sociology</em> 105:\n128\u201356.</li>\n<li>\u2013\u2013\u2013, 2000, \u201cCommuncation and Coordination\nin Social Networks\u201d, <em>Review of Economic Studies</em>, 67:\n1\u201316.</li>\n<li>\u2013\u2013\u2013, 2001, <em>Rational Ritual</em>, Princeton,\nNJ: Princeton University Press</li>\n<li>Cubitt, Robin and Sugden, Robert, 2003, \u201cCommon Knowledge,\nSalience and Convention: A Reconstruction of David Lewis\u2019 Game\nTheory\u201d, <em>Economics and Philosophy</em>, 19:\n175\u2013210.</li>\n<li>D\u00e9gremont, C\u00e9dric, and Oliver Roy, 2012,\n\u201cAgreement Theorems in Dynamic-Epistemic Logic\u201d,\n<em>Journal of Philosophical Logic</em>, 41(4): 735-764.</li>\n<li>Dekel, Eddie and Gul, Faruk, 1997, \u201cRationality and\nKnowledge in Game Theory\u201d, in <em>Advances in Economic Theory:\nSeventh World Congress of the Econometric Society</em>, D. Kreps and\nK. Wallace eds., Cambridge: Cambridge University Press.</li>\n<li>Dekel, Eddie, Lipman, Bart and Rustichini, Aldo, 1998,\n\u201cStandard State-Space Models Preclude Unawareness,\u201d\n<em>Econometrica</em>, 66: 159\u2013173.</li>\n<li>Devetag, Giovanna, Hosni, Hykel and Sillari, Giacomo, 2013,\n\u201cPlay 7: Mutual Versus Common Knowledge of Advice in a Weak-Link\nGame,\u201d <em>Synthese</em>, 190(8): 1351\u20131381</li>\n<li>Fagin, Ronald and Halpern, Joseph Y., 1988, \u201cAwareness and\nLimited Reasoning,\u201d <em>Artificial Intelligence</em>, 34:\n39\u201376.</li>\n<li>Fagin, Ronald, Halpern, Joseph Y., Moses, Yoram and Vardi, Moshe\nY., 1995, <em>Reasoning About Knowledge</em>, Cambridge, MA: MIT\nPress.</li>\n<li>Friedell, Morris, 1967, \u201cOn the Structure of Shared\nAwareness,\u201d <em>Working papers of the Center for Research on\nSocial Organizations</em> (Paper #27), Ann Arbor: University of\nMichigan.</li>\n<li>\u2013\u2013\u2013, 1969, \u201cOn the Structure of Shared\nAwareness,\u201d <em>Behavioral Science</em>, 14(1):\n28\u201339.</li>\n<li>Geanakoplos, John, 1989, \u201cGames Theory without Partitions,\nand Applications to Speculation and Consensus,\u201d Cowles\nFoundation Discussion Paper, No. 914.</li>\n<li>\u2013\u2013\u2013, 1994, \u201cCommon Knowledge\u201d, in\n<em>Handbook of Game Theory</em> (Volume 2), Robert Aumann and Sergiu\nHart (eds.), Amsterdam: Elsevier Science B.V., 1438\u20131496.</li>\n<li>Geanakoplos, John and Heraklis M. Polemarchakis, 1982, \u201cWe\nCan\u2019t Disagree Forever\u201d <em>Journal of Economic\ntheory</em> 28(1): 192\u2013200.</li>\n<li>Gilbert, Margaret, 1989, <em>On Social Facts</em>, Princeton:\nPrinceton University Press.</li>\n<li>Halpern, Jospeh, 2001, \u201cAlternative Semantics for\nUnawareness\u201d, <em>Games and Economic Behavior</em>, 37(2):\n321\u2013339</li>\n<li>Halpern, J. Y., &amp; Moses, Y. , 1990, \u201cKnowledge and\ncommon Knowledge in a Distributed Environment\u201d. <em>Journal of\nthe Association for Computing Machinery</em>, 37(3):\n549\u2013587.</li>\n<li>Halpern, J. Y., &amp; Pass, R., 2017, \u201cA Knowledge-Based\nAnalysis of the Blockchain Protocol\u201d. <em>arXiv preprint</em>\narXiv:1707.08751.</li>\n<li>Harman, Gilbert, 1977, \u201cReview of <em>Linguistic\nBehavior</em> by Jonathan Bennett\u201d, <em>Language</em>, 53:\n417\u2013424.</li>\n<li>Harsanyi, J., 1967, \u201cGames with Incomplete Information\nPlayed by \u201dBayesian\u201c Players, I: The basic model\u201d,\n<em>Management Science</em>, 14: 159\u201382.</li>\n<li>\u2013\u2013\u2013, 1968a, \u201cGames with Incomplete\nInformation Played by \u201dBayesian\u201c Players, II: Bayesian\nEquilibrium Points\u201d, <em>Management Science</em>, 14:\n320\u2013324.</li>\n<li>\u2013\u2013\u2013, 1968b, \u201cGames with Incomplete\nInformation Played by \u201dBayesian\u201c Players, III: The basic\nprobability distribution of the game\u201d, <em>Management\nScience</em>, 14: 486\u2013502.</li>\n<li>Heifetz, Aviad, 1999, \u201cIterative and Fixed Point Common\nBelief\u201d, <em>Journal of Philosophical Logic</em>, 28(1):\n61\u201379.</li>\n<li>Heifetz, Aviad, Meier, Martin and Schipper, Burkhard, 2006,\n\u201cInteractive Unawareness\u201d, <em>Journal of Economic\nTheory</em>, 130: 78\u201394.</li>\n<li>Hintikka, Jaakko, 1962, <em>Knowledge and Belief</em>, Ithaca, NY:\nCornell University Press.</li>\n<li>Hume, David, 1740 [1888, 1976], <em>A Treatise of Human\nNature</em>, L. A. Selby-Bigge (ed.), rev. 2nd. edition P. H. Nidditch\n(ed.), Oxford: Clarendon Press.</li>\n<li>Immerman, D., 2021, \u201cHow Common Knowledge Is\nPossible\u201d. <em>Mind</em>, first online 17 January 2021.\ndoi:10.1093/mind/fzaa090</li>\n<li>J\u00e4ger, Gerhard and Michel Marti, 2016, \u201cIntuitionistic\nCommon Knowledge or Belief\u201d, <em>Journal of Applied Logic</em>,\n18: 150\u2013163</li>\n<li>Lederman, Harvey, 2018a, \u201cTwo Paradoxes of Common Knowledge:\nCoordinated Attack and Electronic Mail\u201d, <em>No\u00fbs</em>,\n52: 921\u2013945.</li>\n<li>\u2013\u2013\u2013, 2018b, \u201cUncommon Knowledge\u201d\n<em>Mind</em> 127, 1069\u20131105.</li>\n<li>Leitgeb, Hannes, 2014, \u201cThe Stability Theory of\nBelief\u201d, <em>The Philosophical Review</em>, 123(2):\n131\u2013171.</li>\n<li>Lewis, C. I., 1943, \u201cThe Modes of Meaning\u201d,\n<em>Philosophy and Phenomenological Research</em>, 4:\n236\u2013250.</li>\n<li>Lewis, David, 1969, <em>Convention: A Philosophical Study</em>,\nCambridge, MA: Harvard University Press.</li>\n<li>\u2013\u2013\u2013, 1978, \u201cTruth in Fiction\u201d,\n<em>American Philosophical Quarterly</em>, 15: 37\u201346.</li>\n<li>Littlewood, J. E., 1953, <em>A Mathematical Miscellany</em>,\nLondon: Methuen; reprinted as <em>Littlewood\u2019s Miscellany</em>,\nB. Bollobas (ed.), Cambridge: Cambridge University Press, 1986.</li>\n<li>McKelvey, Richard and Page, Talbot, 1986, \u201cCommon Knowledge,\nConsensus and Aggregate Information\u201d, <em>Econometrica</em>, 54:\n109\u2013127.</li>\n<li>Meyer, J.-J.Ch. and van der Hoek, Wiebe, 1995, <em>Epistemic Logic\nfor Computer Science and Artificial Intelligence</em> (Cambridge\nTracts in Theoretical Computer Science 41), Cambridge: Cambridge\nUniversity Press.</li>\n<li>Milgrom, Paul, 1981, \u201cAn Axiomatic Characterization of\nCommon Knowledge\u201d, <em>Econometrica</em>, 49:\n219\u2013222.</li>\n<li>Milgrom, Paul, and Nancy Stokey, 1982, \u201cInformation, Trade\nand Common Knowledge\u201d, <em>Journal of Economic Theory</em>,\n26(1): 17\u201327.</li>\n<li>Monderer, Dov and Samet, Dov, 1989, \u201cApproximating Common\nKnowledge with Common Beliefs\u201d, <em>Games and Economic\nBehavior</em>, 1: 170\u2013190.</li>\n<li>Nash, John, 1950, \u201cEquilibrium Points in N-person\nGames\u201d. <em>Proceedings of the National Academy of Sciences of\nthe United States</em>, 36: 48\u201349.</li>\n<li>\u2013\u2013\u2013, 1951, \u201cNon-Cooperative Games\u201d.\n<em>Annals of Mathematics</em>, 54: 286\u2013295.</li>\n<li>Nozick, Robert, 1963, <em>The Normative Theory of Individual\nChoice</em>, Ph.D. dissertation, Princeton University</li>\n<li>Paternotte, C\u00e9dric, 2011, \u201cBeing Realistic about\nCommon Knowledge: a Lewisian Approach\u201d, <em>Synthese</em>,\n183(2): 249\u2013276.</li>\n<li>\u2013\u2013\u2013, 2017, \u201cThe Fragility of Common\nKnowledge\u201d, <em>Erkenntnis</em>, 82(3): 451\u2013472.</li>\n<li>Pearce, David, 1984, \u201cRationalizable Strategic Behavior and\nthe Problem of Perfection\u201d, <em>Econometrica</em>, 52:\n1029\u20131050.</li>\n<li>Reny, Philip J, 1988, \u201cCommon Knowledge and Games with\nPerfect Information.\u201d In <em>PSA: Proceedings of the Biennial\nMeeting of the Philosophy of Science Association</em>, vol. 1988, no.\n2, pp. 363\u2013369. East Lansing: Philosophy of Science\nAssociation.</li>\n<li>\u2013\u2013\u2013, 1992, \u201cRationality in Extensive Form\nGames\u201d, <em>Journal of Economic Perspectives</em>, 6:\n103\u2013118.</li>\n<li>Rubinstein, Ariel, 1987, \u201cA Game with \u201dAlmost Common\nKnowledge\u201c: An Example\u201d, in <em>Theoretical\nEconomics</em>, D. P. 87/165. London School of Economics.</li>\n<li>Samet, Dov, 1990, \u201cIgnoring Ignorance and Agreeing to\nDisagree\u201d, <em>Journal of Economic Theory</em>, 52:\n190\u2013207.</li>\n<li>Schelling, Thomas, 1960, <em>The Strategy of Conflict</em>,\nCambridge, MA: Harvard University Press.</li>\n<li>Schiffer, Stephen, 1972, <em>Meaning</em>, Oxford: Oxford\nUniversity Press.</li>\n<li>Sillari, Giacomo, 2005, \u201cA Logical Framework for\nConvention\u201d, <em>Synthese</em>, 147(2): 379\u2013400.</li>\n<li>\u2013\u2013\u2013, 2008, \u201cCommon Knowledge and\nConvention\u201d, <em>Topoi</em>, 27(1): 29\u201339.</li>\n<li>\u2013\u2013\u2013, 2013, \u201cRule-Following as\nCoordination: a Game-Theoretic Approach\u201d, <em>Synthese</em>,\n190(5): 871\u2013890.</li>\n<li>\u2013\u2013\u2013, 2019, \u201cLogics of Belief\u201d,\n<em>Rivista di Filosofia</em>, 110(2): 243\u2013262.</li>\n<li>Skyrms, Brian, 1984, <em>Pragmatics and Empiricism</em>, New\nHaven: Yale University Press.</li>\n<li>\u2013\u2013\u2013, 1990, <em>The Dynamics of Rational\nDeliberation</em>, Cambridge, MA: Harvard University Press</li>\n<li>\u2013\u2013\u2013, 1991, \u201cInductive Deliberation,\nAdmissible Acts, and Perfect Equilibrium\u201d, in <em>Foundations of\nDecision Theory</em>, Michael Bacharach and Susan Hurley eds.,\nCambridge, MA: Blackwell, pp. 220\u2013241.</li>\n<li>\u2013\u2013\u2013, 1998, \u201cThe Shadow of the\nFuture\u201d, in <em>Rational Commitment and Social Justice: Essays\nfor Gregory Kavka</em>, Jules Coleman and Christopher Morris eds.,\nCambridge: Cambridge University Press, pp. 12\u201322.</li>\n<li>Sugden, Robert, 1986, <em>The Economics of Rights, Cooperation and\nWelfare</em>, New York: Basil Blackwell.</li>\n<li>Thomason, R. H., 2021, \u201cCommon Knowledge, Common Attitudes\nand Social Reasoning\u201d, <em>Bulletin of the Section of\nLogic</em>, 50(2): 229\u2013247.</li>\n<li>Vanderschraaf, Peter, 1995, \u201cEndogenous Correlated\nEquilibria in Noncooperative Games\u201d, <em>Theory and\nDecision</em>, 38: 61\u201384.</li>\n<li>Vanderschraaf, Peter, 1998, \u201cKnowledge, Equilibrium and\nConvention\u201d, <em>Erkenntnis</em>, 49: 337\u2013369.</li>\n<li>\u2013\u2013\u2013, 2001. <em>A Study in Inductive\nDeliberation</em>, New York: Routledge.</li>\n<li>von Neumann, John and Morgenstern, Oskar, 1944, <em>Theory of\nGames and Economic Behavior</em>, Princeton: Princeton University\nPress.</li>\n</ul>\n</div>"
    },
    "related_entries": {
        "entry_list": [
            "convention",
            "game theory",
            "logic: epistemic",
            "prisoner\u2019s dilemma",
            "social norms"
        ],
        "entry_link": [
            {
                "../convention/": "convention"
            },
            {
                "../game-theory/": "game theory"
            },
            {
                "../logic-epistemic/": "logic: epistemic"
            },
            {
                "../prisoner-dilemma/": "prisoner\u2019s dilemma"
            },
            {
                "../social-norms/": "social norms"
            }
        ]
    },
    "academic_tools": {
        "listed_text": [
            "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=common-knowledge\" target=\"other\">How to cite this entry</a>.",
            "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://leibniz.stanford.edu/friends/preview/common-knowledge/\" target=\"other\">Preview the PDF version of this entry</a> at the\n <a href=\"https://leibniz.stanford.edu/friends/\" target=\"other\">Friends of the SEP Society</a>.",
            "<img alt=\"inpho icon\" src=\"../../symbols/inpho.png\"/>",
            "<a href=\"https://www.inphoproject.org/entity?sep=common-knowledge&amp;redirect=True\" target=\"other\">Look up topics and thinkers related to this entry</a>\n at the Internet Philosophy Ontology Project (InPhO).",
            "<img alt=\"phil papers icon\" src=\"../../symbols/pp.gif\"/>",
            "<a href=\"https://philpapers.org/sep/common-knowledge/\" target=\"other\">Enhanced bibliography for this entry</a>\nat <a href=\"https://philpapers.org/\" target=\"other\">PhilPapers</a>, with links to its database."
        ],
        "listed_links": [
            {
                "https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=common-knowledge": "How to cite this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/preview/common-knowledge/": "Preview the PDF version of this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/": "Friends of the SEP Society"
            },
            {
                "https://www.inphoproject.org/entity?sep=common-knowledge&redirect=True": "Look up topics and thinkers related to this entry"
            },
            {
                "https://philpapers.org/sep/common-knowledge/": "Enhanced bibliography for this entry"
            },
            {
                "https://philpapers.org/": "PhilPapers"
            }
        ]
    },
    "other_internet_resources": {
        "listed_text": [
            "<a href=\"http://www-formal.stanford.edu/jmc/applications/applications.html\" target=\"other\">Applications of Circumscription to Formalizing Common Sense Knowledge</a>",
            "<a href=\"http://www.econ.ucdavis.edu/faculty/schipper/unaw.htm\" target=\"other\">Burkhard C. Schipper\u2019s Unawareness Bibliography</a>"
        ],
        "listed_links": [
            {
                "http://www-formal.stanford.edu/jmc/applications/applications.html": "Applications of Circumscription to Formalizing Common Sense Knowledge"
            },
            {
                "http://www.econ.ucdavis.edu/faculty/schipper/unaw.htm": "Burkhard C. Schipper\u2019s Unawareness Bibliography"
            }
        ]
    }
}