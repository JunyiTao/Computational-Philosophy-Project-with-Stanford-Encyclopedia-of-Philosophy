{
    "url": "information",
    "title": "Information",
    "authorship": {
        "year": "Copyright \u00a9 2023",
        "author_text": "Pieter Adriaans\n<pieter@pieter-adriaans.com>",
        "author_links": [
            {
                "http://www.uva.nl/en/profile/a/d/p.w.adriaans/p.w.adriaans.html": "Pieter Adriaans"
            },
            {
                "mailto:pieter%40pieter-adriaans%2ecom": "pieter@pieter-adriaans.com"
            }
        ],
        "raw_html": "<div id=\"article-copyright\">\n<p>\n<a href=\"../../info.html#c\">Copyright \u00a9 2023</a> by\n\n<br/>\n<a href=\"http://www.uva.nl/en/profile/a/d/p.w.adriaans/p.w.adriaans.html\" target=\"other\">Pieter Adriaans</a>\n&lt;<a href=\"mailto:pieter%40pieter-adriaans%2ecom\"><em>pieter<abbr title=\" at \">@</abbr>pieter-adriaans<abbr title=\" dot \">.</abbr>com</em></a>&gt;\n    </p>\n</div>"
    },
    "pubinfo": [
        "First published Fri Oct 26, 2012",
        "substantive revision Wed Nov 1, 2023"
    ],
    "preamble": "\n\nPhilosophy of Information deals with the philosophical analysis of the\nnotion of information both from a historical and a systematic\nperspective. With the emergence of the empiricist theory of knowledge\nin early modern philosophy, the development of various mathematical\ntheories of information in the twentieth century and the rise of\ninformation technology, the concept of \u201cinformation\u201d has\nconquered a central place in the sciences and in society. This\ninterest also led to the emergence of a separate branch of philosophy\nthat analyzes information in all its guises (Adriaans & van\nBenthem 2008a,b; Lenski 2010; Floridi 2002, 2011, 2019). Information\nhas become a central category in both the sciences and the humanities\nand the reflection on information influences a broad range of\nphilosophical disciplines varying from logic (Dretske 1981; van\nBenthem & van Rooij 2003; van Benthem 2006, see the entry on\n logic and information),\n epistemology (Simondon 1989) to ethics (Floridi 1999) and esthetics\n(Schmidhuber 1997a; Adriaans 2008) to ontology (Zuse 1969; Wheeler\n1990; Schmidhuber 1997b; Wolfram 2002; Hutter 2010). \n\nPhilosophy of information is a sub-discipline of\nphilosophy, intricately related to the philosophy of logic\nand mathematics. Philosophy of semantic information (Floridi\n2011, D\u2019Alfonso 2012, Adams & de Moraes, 2016) again is a\nsub-discipline of philosophy of information (see the\ninformational map in the entry on\n semantic conceptions of information).\n From this perspective philosophy of information is interested in the\ninvestigation of the subject at the most general level: data,\nwell-formed data, environmental data etc. Philosophy of semantic\ninformation adds the dimensions of meaning and\ntruthfulness, Long (2014), Lundgren (2019). It is possible to\ninterpret quantitative theories of information in the framework of a\nphilosophy of semantic information (see\n section 6.5\n for an in-depth discussion). \n\nSeveral authors have proposed a more or less coherent philosophy of\ninformation as an attempt to rethink philosophy from a new\nperspective: e.g., quantum physics (Mugur-Sch\u00e4chter 2002), logic\n(Brenner 2008), communication and message systems (Capurro &\nHolgate 2011) and meta-philosophy (Wu 2010, 2016). The work of Luciano\nFloridi on semantic information (Floridi 2011, 2013, 2014, 2019;\nD\u2019Alfonso 2012; Adams & de Moraes 2016, see the entry on\n semantic conceptions of information)\n deserves special mention. In a number of papers and books Floridi has\ndeveloped a systematic coherent transcendental philosophy of\ninformation, which defines him as one of the rare modern system\nbuilders in the continental tradition. The corner stone of his project\nis the inclusion of truthfulness in the definition of information.\nThis choice works as a demarcation criterion: the more technical\nquantitative concepts of information and computation do not deal with\ntruthfulness and consequently lie outside of the core of philosophy of\nsemantic information. The resulting concept of information is also\ncloser to the naive notion we use in everyday life. In contrast with\nthis is the approach of Adriaans & van Benthem 2008a,b. Under the\nslogan information is what information does, they take a more\npragmatic, less essentialistic, approach to the subject. The analysis\nof the philosophical consequences of technical developments in the\ntheory of information and computation is at the core of their research\nprogram. From this perspective, philosophy of information emerges as a\ntechnical discipline with deep roots in the history of philosophy and\nconsequences for various disciplines like methodology, epistemology\nand ethics. One might distinguish a school of thinking about\ninformation rooted in the research traditions of logic (Van Benthem)\nor complexity theory (Vitanyi) from an alternative approach\nrepresented by researchers like Bostrom and Floridi. \n\nWhatever one\u2019s interpretation of the nature of philosophy of\ninformation is, it seems to imply an ambitious research program\nconsisting of many sub-projects varying from the reinterpretation of\nthe history of philosophy in the context of modern theories of\ninformation, to an in depth analysis of the role of information in\nscience, the humanities and society as a whole. \n",
    "toc": [
        {
            "#ConcInf": "1. Concepts of information"
        },
        {
            "#InfoCollSpee": "1.1 Information in Colloquial Speech"
        },
        {
            "#TechDef": "1.2 Technical Definitions of the Concept of Information"
        },
        {
            "#HistTermConcInfo": "2. History of the Term and the Concept of Information"
        },
        {
            "#ClasPhil": "2.1 Classical Philosophy"
        },
        {
            "#MediPhil": "2.2 Medieval Philosophy"
        },
        {
            "#ModePhil": "2.3 Modern Philosophy"
        },
        {
            "#HistDeveMeanTermInfo": "2.4 Historical Development of the Meaning of the Term \u201cInformation\u201d"
        },
        {
            "#BuilBlocModeTheoInfo": "3. Building Blocks of Modern Theories of Information"
        },
        {
            "#Lang": "3.1 Languages"
        },
        {
            "#OptiCode": "3.2 Optimal Codes"
        },
        {
            "#Numb": "3.3 Numbers"
        },
        {
            "#Phys": "3.4 Physics"
        },
        {
            "#DevePhilInfo": "4. Developments in Philosophy of Information"
        },
        {
            "#PoppInfoDegrFals": "4.1 Popper: Information as Degree of Falsifiability"
        },
        {
            "#ShanInfoDefiTermProb": "4.2 Shannon: Information Defined in Terms of Probability"
        },
        {
            "#SoloKolmChaiInfoLengProg": "4.3 Solomonoff, Kolmogorov, Chaitin: Information as the Length of a Program"
        },
        {
            "#SystCons": "5. Systematic Considerations"
        },
        {
            "#PhilInfoExtePhilMath": "5.1 Philosophy of Information as An Extension of Philosophy of Mathematics"
        },
        {
            "#InfoNatuPhen": "5.1.1 Information as a natural phenomenon"
        },
        {
            "#SymbManiExteSetsMultStri": "5.1.2 Symbol manipulation and extensiveness: sets, multisets and strings"
        },
        {
            "#SetsNumb": "5.1.3 Sets and numbers"
        },
        {
            "#MeasInfoNumb": "5.1.4 Measuring information in numbers"
        },
        {
            "#MeasInfoProbSetsNumb": "5.1.5 Measuring information and probabilities in sets of numbers"
        },
        {
            "#PersForUnif": "5.1.6 Perspectives for unification"
        },
        {
            "#InfoProcFlowInfo": "5.1.7 Information processing and the flow of information"
        },
        {
            "#InfoPrimFact": "5.1.8 Information, primes, and factors"
        },
        {
            "#IncoArit": "5.1.9 Incompleteness of arithmetic"
        },
        {
            "#InfoSymbComp": "5.2 Information and Symbolic Computation"
        },
        {
            "#TuriMach": "5.2.1 Turing machines"
        },
        {
            "#UnivInva": "5.2.2 Universality and invariance"
        },
        {
            "#QuanInfoBeyo": "5.3 Quantum Information and Beyond"
        },
        {
            "#AnomParaProb": "6. Anomalies, Paradoxes, and Problems"
        },
        {
            "#ParaSystSear": "6.1 The Paradox of Systematic Search"
        },
        {
            "#EffeSearFiniSets": "6.2 Effective Search in Finite Sets"
        },
        {
            "#PVersNPProbDescCompVersTimeComp": "6.3 The P versus NP Problem, Descriptive Complexity Versus Time Complexity"
        },
        {
            "#ModeSeleDataComp": "6.4 Model Selection and Data Compression"
        },
        {
            "#DeteTher": "6.5 Determinism and Thermodynamics"
        },
        {
            "#LogiSemaInfo": "6.6 Logic and Semantic Information"
        },
        {
            "#MeanComp": "6.7 Meaning and Computation"
        },
        {
            "#Conc": "7. Conclusion"
        },
        {
            "#Bib": "Bibliography"
        },
        {
            "#Aca": "Academic Tools"
        },
        {
            "#Oth": "Other Internet Resources"
        },
        {
            "#Rel": "Related Entries"
        }
    ],
    "main_text": "\n1. Concepts of Information\n1.1 Information in Colloquial Speech\n\nThe term \u201cinformation\u201d in colloquial speech is currently\npredominantly used as an abstract mass-noun used to denote any amount\nof data, code or text that is stored, sent, received or manipulated in\nany medium. The lack of preciseness and the universal usefulness of\nthe term \u201cinformation\u201d go hand in hand. In our society, in\nwhich we explore reality by means of instruments and installations of\never increasing complexity (telescopes, cyclotrons) and communicate\nvia more advanced media (newspapers, radio, television, SMS, the\nInternet), it is useful to have an abstract mass-noun for the\n\u201cstuff\u201d that is created by the instruments and that\n\u201cflows\u201d through these media. Historically this general\nmeaning emerged rather late and seems to be associated with the rise\nof mass media and intelligence agencies (Devlin & Rosenberg 2008;\nAdriaans & van Benthem 2008b).\n\nIn present colloquial speech the term information is used in various\nloosely defined and often even conflicting ways. Most people, for\ninstance, would consider the following inference prima facie\nto be valid:\n\nIf I get the information that p then I know that p.\n\nThe same people would probably have no problems with the statement\nthat \u201cSecret services sometimes distribute false\ninformation\u201d, or with the sentence \u201cThe information\nprovided by the witnesses of the accident was vague and\nconflicting\u201d. The first statement implies that information\nnecessarily is true, while the other statements allow for the\npossibility that information is false, conflicting and vague . In\neveryday communication these inconsistencies do not seem to create\ngreat trouble and in general it is clear from the pragmatic context\nwhat type of information is designated. These examples suffice to\nargue that references to our intuitions as speakers of the English\nlanguage are of little help in the development of a rigorous\nphilosophical theory of information. There seems to be no pragmatic\npressure in everyday communication to converge to a more exact\ndefinition of the notion of information.\n1.2 Technical Definitions of the Concept of Information\n\nIn the twentieth century various proposals for formalisation of\nconcepts of information were made. The proposed concepts cluster\naround two central properties: \n\n\nInformation is extensive. Central is the concept of\nadditivity: the combination of two independent datasets with\nthe same amount of information contains twice as much\ninformation as the separate individual datasets. The mathematical\noperation of taking the logarithm captures this notion of\nextensiveness exactly as it reduces multiplication to addition: \\(\\log\na \\times b = \\log a + \\log b\\).\n\nThe notion of extensiveness emerges naturally in our interactions with\nthe world around us when we count and measure objects and structures.\nBasic conceptions of more abstract mathematical entities, like sets,\nmultisets and sequences, were developed early in history on the basis\nof structural rules for the manipulation of symbols (Schmandt-Besserat\n1992). The mathematical formalisation of extensiveness in terms of the\nlog function took place in the context of research in to\nthermodynamics in the nineteenth and early twentieth century. The\ndifferent notions of entropy defined in physics are mirrored in\nvarious proposals for concepts of information. We mention\nBoltzmann Entropy (Boltzmann, 1866) closely related to the\nHartley Function (Hartley 1928), Gibbs Entropy (Gibbs 1906)\nformally equivalent to Shannon entropy and various generalizations\nlike Tsallis Entropy (Tsallis 1988) and R\u00e9nyi\nEntropy (R\u00e9nyi 1961). When coded in terms of more advanced\nmulti-dimensional numbers systems (complex numbers, quaternions,\noctonions) the concept of extensiveness generalizes in to more subtle\nnotions of additivity that do not meet our everyday intuitions. Yet\nthey play an important role in recent developments of information\ntheory based on quantum physics (Von Neumann 1932; Redei &\nSt\u00f6ltzner 2001, see entry on\n quantum entanglement and information).\n \n\nInformation reduces uncertainty. The amount of\ninformation we get grows linearly with the amount by which it reduces\nour uncertainty until the moment that we have received all possible\ninformation and the amount of uncertainty is zero. The relation\nbetween uncertainty and information was probably first formulated by\nthe empiricists (Locke 1689; Hume 1748). Hume explicitly observes that\na choice from a larger selection of possibilities gives more\ninformation. This observation reached its canonical mathematical\nformulation in the function proposed by Hartley (1928) that defines\nthe amount of information we get when we select an element from a\nfinite set. The only mathematical function that unifies these two\nintuitions about extensiveness and probability is the one that defines\nthe information in terms of the negative log of the probability:\n\\(I(A)= -\\log P(A)\\) (Shannon 1948; Shannon & Weaver 1949,\nR\u00e9nyi 1961). \n\n\nWe give a concise overview of some relevant definitions: \n\nQuantitative Theories of Information\n\nNyquist\u2019s function: Nyquist (1924) was\nprobably the first to express the amount of \u201cintelligence\u201d\nthat could be transmitted given a certain line speed of a telegraph\nsystems in terms of a log function: \\(W= k \\log m\\), where W is\nthe speed of transmission, K is a constant, and m are\nthe different voltage levels one can choose from. The fact that\nNyquist used the term intelligence for his measure illustrates\nthe fluidity of terminology at the start of the twentieth century.\n\nFisher information: the amount of information\nthat an observable random variable X carries about an unknown\nparameter \\(\\theta\\) upon which the probability of X depends\n(Fisher 1925).\nThe Hartley function: (Hartley 1928, R\u00e9nyi\n1961, Vigo 2012). The amount of information we get when we select an\nelement \\(e\\) from a finite set S under uniform distribution is\nthe logarithm of the cardinality of that set: \\(I(e \\mid S) = \\log_a\n|S| \\). \nShannon information: the entropy, H, of a\ndiscrete random variable X is a measure of the amount of\nuncertainty associated with the value of X: \\(I(A)= -\\log\nP(A)\\) (Shannon 1948; Shannon & Weaver 1949). Shannon information\nis the best known quantitative definition of information but it is a\nrather weak concept that does not capture the the notion of\ndisorder that intuitively is essential for the thermodynamic\nconcept of entropy: the string \\(0000011111\\) contains just as much\nShannon information as the string \\(1001011100\\) because it has the\nsame number of ones and zeros. \nAlgorithmic complexity (also know as Kolmogorov\ncomplexity): the information in a binary string x is the length\nof the shortest program p that produces x on a reference\nuniversal Turing machine U (Turing 1937; Solomonoff 1960,\n1964a,b, 1997; 1965; Chaitin 1969, 1987). Algorithmic complexity is\nconceptually more powerful than Shannon information: it does recognise\nthat the string \\(1100100100001111110110101010001000100001\\) contains\nlittle information (because it gives the first 40 bits of the\nnumber \u03c0), whereas Shannon\u2019s theory would consider\nthis string to have almost maximal information. This power comes at a\nprice. Kolmogorov complexity quantifies over all possible computer\nprograms shorter than the data set. We cannot run all these programs\nin finite time since a lot of them will never terminate. This implies\nthat Kolmogorov complexity is uncomputable. The measurements\nwe make are all dependent on our choice of reference universal Turing\nmachine. The nature of algorithmic complexity as a measure of\ninformation is guaranteed by the universality of Turing\nmachines as a model of computation and by the so-called invariance\ntheorem: in the limit the the complexity assigned to a dataset by\ntwo different universal Turing machines only differs by a constant.\nAlgorithmic complexity is consequently an asymptotic measure\nthat does not tell us much about small finite datasets. Its practical\nvalue for everyday research is limited, although it has relevance from\na philosophical perspective and as a mathematical tool. \n\nInformation in Physics\n\nLandaur\u2019s Principle: the minimum energy\nneeded to erase one bit of information is proportional to the\ntemperature at which the system is operating (Landauer 1961, 1991).\n\nQuantum Information: The qubit is a\ngeneralization of the classical bit and is described by a quantum\nstate in a two-state quantum-mechanical system, which is formally\nequivalent to a two-dimensional vector space over the complex numbers\n(Von Neumann 1932; Redei & St\u00f6ltzner 2001).\n\nQualitative Theories of Information\n\nSemantic Information: Bar-Hillel and Carnap\ndeveloped a theory of semantic Information (1953). Floridi (2002,\n2003, 2011) defines semantic information as well-formed, meaningful\nand truthful data (Long 2014; Lundgren 2019). Formal entropy based\ndefinitions of information (Fisher, Shannon, Quantum, Kolmogorov) work\non a more general level and do not necessarily measure information in\nmeaningful truthful datasets, although one might defend the view that\nin order to be measurable the data must be well-formed (for a\ndiscussion see\n section 6.6 on Logic and Semantic Information).\n Semantic information is close to our everyday naive notion of\ninformation as something that is conveyed by true statements about the\nworld. \nInformation as a state of an agent: the formal\nlogical treatment of notions like knowledge and belief was initiated\nby Hintikka (1962, 1973). Dretske (1981) and van Benthem & van\nRooij (2003) studied these notions in the context of information\ntheory, cf. van Rooij (2003) on questions and answers, or Parikh &\nRamanujam (2003) on general messaging. Also Dunn seems to have this\nnotion in mind when he defines information as \u201cwhat is left of\nknowledge when one takes away belief, justification and truth\u201d\n(Dunn 2001: 423; 2008). Vigo proposed a Structure-Sensitive Theory of\nInformation based on the complexity of concept acquisition by agents\n(Vigo 2011, 2012). \n\n\n\nThe overview shows a domain of research in development in which the\ncontext of justification is not yet fully separated from the context\nof discovery. Many proposals have an engineering flavour and rely on\nnarratives (sending messages, selecting elements from a set, Turing\nmachines as abstract models human computers) that do not do justice to\nthe fundamental nature of the underlying concepts. Other proposals\nhave deeper roots in philosphy but are formulated in such a way that\nembedding in scientific research is problematic. Take three\ninfluential proposals and their definiens for\ninformation (Shannon-probability; Kolmogorov-computation;\nFloridi-truth) and observe that they have next to nothing in common.\nSome are even conflicting (truth vs. probability, deterministic\ncomputing vs. probability). A similar situation exists in the context\nof thermodynamics and information theory: they use the same formulas\nto describe fundamentally different phenomena (distribution velocities\nof particles in a gas vs. distribution of probabilities over sets of\nmessages).\n\nUntil recently the possibility of a unification of these theories was\ngenerally doubted (Adriaans & van Benthem 2008a), but after two\ndecades of research, perspectives for unification seem better. Various\nquantitative concepts of information are associated with different\nnarratives (counting, receiving messages, gathering information,\ncomputing) rooted in the same basic mathematical framework. Many\nproblems in philosophy of information center around related problems\nin philosophy of mathematics. Conversions and reductions between\nvarious formal models have been studied (Cover & Thomas 2006;\nGr\u00fcnwald & Vit\u00e1nyi 2008; Bais & Farmer 2008). The\nsituation that seems to emerge is not unlike the concept of energy:\nthere are various formal sub-theories about energy (kinetic,\npotential, electrical, chemical, nuclear) with well-defined\ntransformations between them. Apart from that, the term\n\u201cenergy\u201d is used loosely in colloquial speech. The\nemergence of a coherent theory to measure information quantitatively\nin the twentieth century is closely related to the development of the\ntheory of computing. Central in this context are the notions of\nUniversality, Turing equivalence and\nInvariance: because the concept of a Turing system\ndefines the notion of a universal programmable computer, all universal\nmodels of computation seem to have the same power. This implies that\nall possible measures of information definable for universal models of\ncomputation (Recursive Functions, Turing Machine, Lambda Calculus\netc.) are invariant modulo an additive constant. \n\nAdriaans (2020, 2021) proposed a unifying research program implied by\nthis insight under the name of Differential Information\nTheory (DIT): a purely mathematical non-algorithmic\ndescriptive theory of information, based on 1) measuring\ninformation in natural numbers using the log function (see\n section 5.1.7\n for an in-depth discussion) and 2) the concept of the information\nefficiency of recursive functions. Other quantitative proposals\nsuch a Shannon information and Kolmogorov complexity can be placed in\nthis purely descriptive framework as forms of Applied Information\nTheory involving semi-physical systems existing in domains where\na concept of time exists. A big advantage of DIT is the fact that\nrecursive functions are defined axiomatically. This allow for the\ndevelopment of a theory of information as a rigid discipline in line\nwith central concepts of mathematics and physics. Using differential\ninformation theory the creation and destruction of information of\ncomputational, stochastic (and mixed processes like game playing, or\ncreative processes) can be studied. \n2. History of the Term and the Concept of Information\n\nThe detailed history of both the term \u201cinformation\u201d and\nthe various concepts that come with it is complex and for the larger\npart still has to be written (Seiffert 1968; Schnelle 1976; Capurro\n1978, 2009; Capurro & Hj\u00f8rland 2003). The exact meaning of\nthe term \u201cinformation\u201d varies in different philosophical\ntraditions and its colloquial use varies geographically and over\ndifferent pragmatic contexts. Although an analysis of the notion of\ninformation has been a theme in Western philosophy from its early\ninception, the explicit analysis of information as a philosophical\nconcept is recent, and dates back to the second half of the twentieth\ncentury. At this moment it is clear that information is a pivotal\nconcept in the sciences and humanities and in our every day life.\nEverything we know about the world is based on information we received\nor gathered and every science in principle deals with information.\nThere is a network of related concepts of information, with roots in\nvarious disciplines like physics, mathematics, logic, biology, economy\nand epistemology. \n\nUntil the second half of the twentieth century almost no modern\nphilosopher considered \u201cinformation\u201d to be an important\nphilosophical concept. The term has no lemma in the well-known\nencyclopedia of Edwards (1967) and is not mentioned in Windelband\n(1903). In this context the interest in \u201cPhilosophy of\nInformation\u201d is a recent development. Yet, with hindsight from\nthe perspective of a history of ideas, reflection on the notion of\n\u201cinformation\u201d has been a predominant theme in the history\nof philosophy. The reconstruction of this history is relevant for the\nstudy of information.\n\nA problem with any \u201chistory of ideas\u201d approach is the\nvalidation of the underlying assumption that the concept one is\nstudying has indeed continuity over the history of philosophy. In the\ncase of the historical analysis of information one might ask whether\nthe concept of \u201cinformatio\u201d discussed by\nAugustine has any connection to Shannon information, other than a\nresemblance of the terms. At the same time one might ask whether\nLocke\u2019s \u201chistorical, plain method\u201d is an important\ncontribution to the emergence of the modern concept of information\nalthough in his writings Locke hardly uses the term\n\u201cinformation\u201d in a technical sense. As is shown below,\nthere is a conglomerate of ideas involving a notion of information\nthat has developed from antiquity till recent times, but further study\nof the history of the concept of information is necessary. \n\nAn important recurring theme in the early philosophical analysis of\nknowledge is the paradigm of manipulating a piece of wax: either by\nsimply deforming it, by imprinting a signet ring in it or by writing\ncharacters on it. The fact that wax can take different shapes and\nsecondary qualities (temperature, smell, touch) while the volume\n(extension) stays the same, make it a rich source of analogies,\nnatural to Greek, Roman and medieval culture, where wax was used both\nfor sculpture, writing (wax tablets) and encaustic painting. One finds\nthis topic in writings of such diverse authors as Democritus, Plato,\nAristotle, Theophrastus, Cicero, Augustine, Avicenna, Duns Scotus,\nAquinas, Descartes and Locke.\n2.1 Classical Philosophy\n\nIn classical philosophy \u201cinformation\u201d was a technical\nnotion associated with a theory of knowledge and ontology that\noriginated in Plato\u2019s (427\u2013347 BCE) theory of forms,\ndeveloped in a number of his dialogues (Phaedo, Phaedrus,\nSymposium, Timaeus, Republic). Various imperfect individual\nhorses in the physical world could be identified as horses, because\nthey participated in the static atemporal and aspatial idea of\n\u201chorseness\u201d in the world of ideas or forms. When later\nauthors like Cicero (106\u201343 BCE) and Augustine (354\u2013430\nCE) discussed Platonic concepts in Latin they used the terms\ninformare and informatio as a translation for\ntechnical Greek terms like eidos (essence), idea\n(idea), typos (type), morphe (form) and\nprolepsis (representation). The root \u201cform\u201d still\nis recognizable in the word in-form-ation (Capurro &\nHj\u00f8rland 2003). Plato\u2019s theory of forms was an attempt to\nformulate a solution for various philosophical problems: the theory of\nforms mediates between a static (Parmenides, ca. 450 BCE) and a\ndynamic (Herakleitos, ca. 535\u2013475 BCE) ontological conception of\nreality and it offers a model to the study of the theory of human\nknowledge. According to Theophrastus (371\u2013287 BCE) the analogy\nof the wax tablet goes back to Democritos (ca. 460\u2013380/370 BCE)\n(De Sensibus 50). In the Theaetetus (191c,d) Plato\ncompares the function of our memory with a wax tablet in which our\nperceptions and thoughts are imprinted like a signet ring stamps\nimpressions in wax. Note that the metaphor of imprinting symbols in\nwax is essentially spatial (extensive) and can not easily be\nreconciled with the aspatial interpretation of ideas supported by\nPlato.\n\nOne gets a picture of the role the notion of \u201cform\u201d plays\nin classical methodology if one considers Aristotle\u2019s\n(384\u2013322 BCE) doctrine of the four causes. In Aristotelian\nmethodology understanding an object implied understanding four\ndifferent aspects of it:\n\n\nMaterial Cause:: that as the result of whose presence\nsomething comes into being\u2014e.g., the bronze of a statue and the\nsilver of a cup, and the classes which contain these\n\nFormal Cause:: the form or pattern; that is, the\nessential formula and the classes which contain it\u2014e.g., the\nratio 2:1 and number in general is the cause of the octave-and the\nparts of the formula.\n\nEfficient Cause:: the source of the first beginning\nof change or rest; e.g., the man who plans is a cause, and the father\nis the cause of the child, and in general that which produces is the\ncause of that which is produced, and that which changes of that which\nis changed.\n\nFinal Cause:: the same as \u201cend\u201d; i.e.,\nthe final cause; e.g., as the \u201cend\u201d of walking is health.\nFor why does a man walk? \u201cTo be healthy\u201d, we say, and by\nsaying this we consider that we have supplied the cause. (Aristotle,\nMetaphysics 1013a)\n\n\nNote that Aristotle, who rejects Plato\u2019s theory of forms as\natemporal aspatial entities, still uses \u201cform\u201d as a\ntechnical concept. This passage states that knowing the form or\nstructure of an object, i.e., the information, is a necessary\ncondition for understanding it. In this sense information is a crucial\naspect of classical epistemology.\n\nThe fact that the ratio 2:1 is cited as an example also illustrates\nthe deep connection between the notion of forms and the idea that the\nworld was governed by mathematical principles. Plato believed under\ninfluence of an older Pythagorean (Pythagoras 572\u2013ca. 500 BCE)\ntradition that \u201ceverything that emerges and happens in the\nworld\u201d could be measured by means of numbers (Politicus\n285a). On various occasions Aristotle mentions the fact that Plato\nassociated ideas with numbers (Vogel 1968: 139). Although formal\nmathematical theories about information only emerged in the twentieth\ncentury, and one has to be careful not to interpret the Greek notion\nof a number in any modern sense, the idea that information was\nessentially a mathematical notion, dates back to classical philosophy:\nthe form of an entity was conceived as a structure or pattern that\ncould be described in terms of numbers. Such a form had both an\nontological and an epistemological aspect: it explains the essence as\nwell as the understandability of the object. The concept of\ninformation thus from the very start of philosophical reflection was\nalready associated with epistemology, ontology and mathematics.\n\nTwo fundamental problems that are not explained by the classical\ntheory of ideas or forms are 1) the actual act of knowing an object\n(i.e., if I see a horse in what way is the idea of a horse activated\nin my mind) and 2) the process of thinking as manipulation of ideas.\nAristotle treats these issues in De Anime, invoking the\nsignet-ring-impression-in-wax analogy:\n\n\nBy a \u201csense\u201d is meant what has the power of receiving into\nitself the sensible forms of things without the matter. This must be\nconceived of as taking place in the way in which a piece of wax takes\non the impress of a signet-ring without the iron or gold; we say that\nwhat produces the impression is a signet of bronze or gold, but its\nparticular metallic constitution makes no difference: in a similar way\nthe sense is affected by what is coloured or flavoured or sounding,\nbut it is indifferent what in each case the substance is; what alone\nmatters is what quality it has, i.e., in what ratio its constituents\nare combined. (De Anime, Book II, Chp. 12)\n\nHave not we already disposed of the difficulty about interaction\ninvolving a common element, when we said that mind is in a sense\npotentially whatever is thinkable, though actually it is nothing until\nit has thought? What it thinks must be in it just as characters may be\nsaid to be on a writing-tablet on which as yet nothing actually stands\nwritten: this is exactly what happens with mind. (De Anime,\nBook III, Chp. 4)\n\n\nThese passages are rich in influential ideas and can with hindsight be\nread as programmatic for a philosophy of information: the process of\ninformatio can be conceived as the imprint of characters on a\nwax tablet (tabula rasa), thinking can be analyzed in terms\nof manipulation of symbols.\n2.2 Medieval Philosophy\n\nThroughout the Middle Ages the reflection on the concept of\ninformatio is taken up by successive thinkers. Illustrative\nfor the Aristotelian influence is the passage of Augustine in De\nTrinitate book XI. Here he analyzes vision as an analogy for the\nunderstanding of the Trinity. There are three aspects: the corporeal\nform in the outside world, the informatio by the sense of\nvision, and the resulting form in the mind. For this process of\ninformation Augustine uses the image of a signet ring making an\nimpression in wax (De Trinitate, XI Cap 2 par 3). Capurro\n(2009) observes that this analysis can be interpreted as an early\nversion of the technical concept of \u201csending a message\u201d in\nmodern information theory, but the idea is older and is a common topic\nin Greek thought (Plato Theaetetus 191c,d; Aristotle De\nAnime, Book II, Chp. 12, Book III, Chp. 4; Theophrastus De\nSensibus 50).\n\nThe tabula rasa notion was later further developed in the\ntheory of knowledge of Avicenna (c. 980\u20131037 CE): \n\n\nThe human intellect at birth is rather like a tabula rasa, a\npure potentiality that is actualized through education and comes to\nknow. Knowledge is attained through empirical familiarity with objects\nin this world from which one abstracts universal concepts. (Sajjad\n2006\n [Other Internet Resources [hereafter OIR]])\n \n\n\nThe idea of a tabula rasa development of the human mind was\nthe topic of a novel Hayy ibn Yaqdhan by the Arabic Andalusian\nphilosopher Ibn Tufail (1105\u20131185 CE, known as\n\u201cAbubacer\u201d or \u201cEbn Tophail\u201d in the West). This\nnovel describes the development of an isolated child on a deserted\nisland. A later translation in Latin under the title Philosophus\nAutodidactus (1761) influenced the empiricist John Locke in the\nformulation of his tabula rasa doctrine.\n\nApart from the permanent creative tension between theology and\nphilosophy, medieval thought, after the rediscovery of\nAristotle\u2019s Metaphysics in the twelfth century inspired\nby Arabic scholars, can be characterized as an elaborate and subtle\ninterpretation and development of, mainly Aristotelian, classical\ntheory. Reflection on the notion of informatio is taken up,\nunder influence of Avicenna, by thinkers like Aquinas (1225\u20131274\nCE) and Duns Scotus (1265/66\u20131308 CE). When Aquinas discusses\nthe question whether angels can interact with matter he refers to the\nAristotelian doctrine of hylomorphism (i.e., the theory that substance\nconsists of matter (hylo (wood), matter) and form\n(morph\u00e8)). Here Aquinas translates this as the\nin-formation of matter (informatio materiae) (Summa\nTheologiae, 1a 110 2; Capurro 2009). Duns Scotus refers to\ninformatio in the technical sense when he discusses\nAugustine\u2019s theory of vision in De Trinitate, XI Cap 2\npar 3 (Duns Scotus, 1639, \u201cDe imagine\u201d,\nOrdinatio, I, d.3, p.3).\n\nThe tension that already existed in classical philosophy between\nPlatonic idealism(universalia ante res) and Aristotelian\nrealism (universalia in rebus) is recaptured as the problem\nof universals: do universal qualities like \u201chumanity\u201d or\nthe idea of a horse exist apart from the individual entities that\ninstantiate them? It is in the context of his rejection of universals\nthat Ockham (c. 1287\u20131347 CE) introduces his well-known razor:\nentities should not be multiplied beyond necessity. Throughout their\nwritings Aquinas and Scotus use the Latin terms informatio\nand informare in a technical sense, although this terminology\nis not used by Ockham.\n2.3 Modern Philosophy\n\nThe history of the concept of information in modern philosophy is\ncomplicated. Probably starting in the fourteenth century the term\n\u201cinformation\u201d emerged in various developing European\nlanguages in the general meaning of \u201ceducation\u201d and\n\u201cinquiry\u201d. The French historical dictionary by Godefroy\n(1881) gives action de former, instruction, enqu\u00eate,\nscience, talent as early meanings of \u201cinformation\u201d.\nThe term was also used explicitly for legal inquiries\n(Dictionnaire du Moyen Fran\u00e7ais (1330\u20131500)\n2015). Because of this colloquial use the term\n\u201cinformation\u201d loses its association with the concept of\n\u201cform\u201d gradually and appears less and less in a formal\nsense in philosophical texts.\n\nAt the end of the Middle Ages society and science are changing\nfundamentally (Hazard 1935; Ong 1958; Dijksterhuis 1986). In a long\ncomplex process the Aristotelian methodology of the four causes was\ntransformed to serve the needs of experimental science:\n\nThe Material Cause developed in to the modern notion of\nmatter.\nThe Formal Cause was reinterpreted as geometric form in\nspace.\nThe Efficient Cause was redefined as direct mechanical interaction\nbetween material bodies.\nThe Final Cause was dismissed as unscientific. Because of this,\nNewton\u2019s contemporaries had difficulty with the concept of the\nforce of gravity in his theory. Gravity as action at a distance seemed\nto be a reintroduction of final causes.\n\n\nIn this changing context the analogy of the wax-impression is\nreinterpreted. A proto-version of the modern concept of information as\nthe structure of a set or sequence of simple ideas is developed by the\nempiricists, but since the technical meaning of the term\n\u201cinformation\u201d is lost, this theory of knowledge is never\nidentified as a new \u201ctheory of information\u201d.\n\nThe consequence of this shift in methodology is that only phenomena\nthat can be explained in terms of mechanical interaction between\nmaterial bodies can be studied scientifically. This implies in a\nmodern sense: the reduction of intensive properties to measurable\nextensive properties. For Galileo this insight is programmatic: \n\n\nTo excite in us tastes, odors, and sounds I believe that nothing is\nrequired in external bodies except shapes, numbers, and slow or rapid\nmovements. (Galileo 1623 [1960: 276) \n\n\nThese insights later led to the doctrine of the difference between\nprimary qualities (space, shape, velocity) and secondary qualities\n(heat, taste, color etc.). In the context of philosophy of information\nGalileo\u2019s observations on the secondary quality of\n\u201cheat\u201d is of particular importance since they lay the\nfoundations for the study of thermodynamics in the nineteenth century:\n\n\n\nHaving shown that many sensations which are supposed to be qualities\nresiding in external objects have no real existence save in us, and\noutside ourselves are mere names, I now say that I am inclined to\nbelieve heat to be of this character. Those materials which produce\nheat in us and make us feel warmth, which are known by the general\nname of \u201cfire,\u201d would then be a multitude of minute\nparticles having certain shapes and moving with certain velocities.\n(Galileo 1623 [1960: 277)\n\n\nA pivotal thinker in this transformation is Ren\u00e9 Descartes\n(1596\u20131650 CE). In his Meditationes, after\n\u201cproving\u201d that the matter (res extensa) and mind\n(res cogitans) are different substances (i.e., forms of being\nexisting independently), the question of the interaction between these\nsubstances becomes an issue. The malleability of wax is for Descartes\nan explicit argument against influence of the res extensa on\nthe res cogitans (Meditationes II, 15). The fact\nthat a piece of wax loses its form and other qualities easily when\nheated, implies that the senses are not adequate for the\nidentification of objects in the world. True knowledge thus can only\nbe reached via \u201cinspection of the mind\u201d. Here the wax\nmetaphor that for more than 1500 years was used to explain\nsensory impression is used to argue against the possibility\nto reach knowledge via the senses. Since the essence of the res\nextensa is extension, thinking fundamentally can not be\nunderstood as a spatial process. Descartes still uses the terms\n\u201cform\u201d and \u201cidea\u201d in the original scholastic\nnon-geometric (atemporal, aspatial) sense. An example is the short\nformal proof of God\u2019s existence in the second answer to Mersenne\nin the Meditationes de Prima Philosophia\n\n\nI use the term idea to refer to the form of any given\nthought, immediate perception of which makes me aware of the thought.\n\n(Idea nomine intelligo cujuslibet cogitationis formam\nillam, per cujus immediatam perceptionem ipsius ejusdem cogitationis\nconscious sum) \n\n\nI call them \u201cideas\u201d says Descartes \n\n\nonly in so far as they make a difference to the mind itself when they\ninform that part of the brain.\n\n(sed tantum quatenus mentem ipsam in illam cerebri partem\nconversam informant). (Descartes, 1641, Ad\nSecundas Objections, Rationes, Dei existentiam & anime\ndistinctionem probantes, more Geometrico dispositae.)\n\n\nBecause the res extensa and the res cogitans are\ndifferent substances, the act of thinking can never be emulated in\nspace: machines can not have the universal faculty of reason.\nDescartes gives two separate motivations:\n\n\nOf these the first is that they could never use words or other signs\narranged in such a manner as is competent to us in order to declare\nour thoughts to others: (\u2026) The second test is, that although\nsuch machines might execute many things with equal or perhaps greater\nperfection than any of us, they would, without doubt, fail in certain\nothers from which it could be discovered that they did not act from\nknowledge, but solely from the disposition of their organs: for while\nreason is an universal instrument that is alike available on every\noccasion, these organs, on the contrary, need a particular arrangement\nfor each particular action; whence it must be morally impossible that\nthere should exist in any machine a diversity of organs sufficient to\nenable it to act in all the occurrences of life, in the way in which\nour reason enables us to act. (Discourse de la\nm\u00e9thode, 1647)\n\n\nThe passage is relevant since it directly argues against the\npossibility of artificial intelligence and it even might be\ninterpreted as arguing against the possibility of a universal Turing\nmachine: reason as a universal instrument can never be emulated in\nspace. This conception is in opposition to the modern concept of\ninformation which as a measurable quantity is essentially spatial,\ni.e., extensive (but in a sense different from that of Descartes).\n\nDescartes does not present a new interpretation of the notions of form\nand idea, but he sets the stage for a debate about the nature of ideas\nthat evolves around two opposite positions:\n\n\nRationalism: The Cartesian notion that ideas are\ninnate and thus a priori. This form of rationalism implies an\ninterpretation of the notion of ideas and forms as atemporal,\naspatial, but complex structures i.e., the idea of \u201ca\nhorse\u201d (i.e., with a head, body and legs). It also matches well\nwith the interpretation of the knowing subject as a created being\n(ens creatu). God created man after his own image and thus\nprovided the human mind with an adequate set of ideas to understand\nhis creation. In this theory growth, of knowledge is a priori\nlimited. Creation of new ideas ex nihilo is impossible. This\nview is difficult to reconcile with the concept of experimental\nscience.\n\nEmpiricism: Concepts are constructed in the mind\na posteriori on the basis of ideas associated with sensory\nimpressions. This doctrine implies a new interpretation of the concept\nof idea as:\n\n\nwhatsoever is the object of understanding when a man thinks \u2026\nwhatever is meant by phantasm, notion, species, or whatever it is\nwhich the mind can be employed about when thinking. (Locke 1689, bk I,\nch 1, para 8)\n\n\nHere ideas are conceived as elementary building blocks of human\nknowledge and reflection. This fits well with the demands of\nexperimental science. The downside is that the mind can never\nformulate apodeictic truths about cause and effects and the essence of\nobserved entities, including its own identity. Human knowledge becomes\nessentially probabilistic (Locke 1689: bk I, ch. 4, para 25).\n\n\nLocke\u2019s reinterpretation of the notion of idea as a\n\u201cstructural placeholder\u201d for any entity present in the\nmind is an essential step in the emergence of the modern concept of\ninformation. Since these ideas are not involved in the justification\nof apodeictic knowledge, the necessity to stress the atemporal and\naspatial nature of ideas vanishes. The construction of concepts on the\nbasis of a collection of elementary ideas based in sensorial\nexperience opens the gate to a reconstruction of knowledge as an\nextensive property of an agent: more ideas implies more probable\nknowledge.\n\nIn the second half of the seventeenth century formal theory of\nprobability is developed by researchers like Pascal (1623\u20131662),\nFermat (1601 or 1606\u20131665) and Christiaan Huygens\n(1629\u20131695). The work De ratiociniis in ludo aleae of\nHuygens was translated in to English by John Arbuthnot (1692). For\nthese authors, the world was essentially mechanistic and thus\ndeterministic, probability was a quality of human knowledge caused by\nits imperfection:\n\n\nIt is impossible for a Die, with such determin\u2019d force and\ndirection, not to fall on such determin\u2019d side, only I\ndon\u2019t know the force and direction which makes it fall on such\ndetermin\u2019d side, and therefore I call it Chance, wich is nothing\nbut the want of art;\u2026 (John Arbuthnot Of the Laws of\nChance (1692), preface) \n\n\nThis text probably influenced Hume, who was the first to marry formal\nprobability theory with theory of knowledge:\n\n\nThough there be no such thing as Chance in the world; our ignorance of\nthe real cause of any event has the same influence on the\nunderstanding, and begets a like species of belief or opinion.\n(\u2026) If a dye were marked with one figure or number of spots on\nfour sides, and with another figure or number of spots on the two\nremaining sides, it would be more probable, that the former would turn\nup than the latter; though, if it had a thousand sides marked in the\nsame manner, and only one side different, the probability would be\nmuch higher, and our belief or expectation of the event more steady\nand secure. This process of the thought or reasoning may seem trivial\nand obvious; but to those who consider it more narrowly, it may,\nperhaps, afford matter for curious speculation. (Hume 1748: Section\nVI, \u201cOn probability\u201d 1)\n\n\nHere knowledge about the future as a degree of belief is measured in\nterms of probability, which in its turn is explained in terms of the\nnumber of configurations a deterministic system in the world can have.\nThe basic building blocks of a modern theory of information are in\nplace. With this new concept of knowledge empiricists laid the\nfoundation for the later development of thermodynamics as a reduction\nof the secondary quality of heat to the primary qualities of\nbodies.\n\nAt the same time the term \u201cinformation\u201d seems to have lost\nmuch of its technical meaning in the writings of the empiricists so\nthis new development is not designated as a new interpretation of the\nnotion of \u201cinformation\u201d. Locke sometimes uses the phrase\nthat our senses \u201cinform\u201d us about the world and\noccasionally uses the word \u201cinformation\u201d. \n\n\nFor what information, what knowledge, carries this proposition in it,\nviz. \u201cLead is a metal\u201d to a man who knows the complex idea\nthe name lead stands for? (Locke 1689: bk IV, ch 8, para 4) \n\n\nHume seems to use information in the same casual way when he observes:\n\n\n\nTwo objects, though perfectly resembling each other, and even\nappearing in the same place at different times, may be numerically\ndifferent: And as the power, by which one object produces another, is\nnever discoverable merely from their idea, it is evident cause and\neffect are relations, of which we receive information from experience,\nand not from any abstract reasoning or reflection. (Hume 1739: Part\nIII, section 1)\n\n\nThe empiricists methodology is not without problems. The biggest issue\nis that all knowledge becomes probabilistic and a posteriori.\nImmanuel Kant (1724\u20131804) was one of the first to point out that\nthe human mind has a grasp of the meta-concepts of space, time and\ncausality that itself can never be understood as the result of a mere\ncombination of \u201cideas\u201d. What is more, these intuitions\nallow us to formulate scientific insights with certainty: i.e., the\nfact that the sum of the angles of a triangle in Euclidean space is\n180 degrees. This issue cannot be explained in the empirical\nframework. If knowledge is created by means of combination of ideas\nthen there must exist an a priori synthesis of ideas in the\nhuman mind. According to Kant, this implies that the human mind can\nevaluate its own capability to formulate scientific judgments. In his\nKritik der reinen Vernunft (1781) Kant developed\ntranscendental philosophy as an investigation of the necessary\nconditions of human knowledge. Although Kant\u2019s transcendental\nprogram did not contribute directly to the development of the concept\nof information, he did influence research in to the foundations of\nmathematics and knowledge relevant for this subject in the nineteenth\nand twentieth century: e.g., the work of Frege, Husserl, Russell,\nBrouwer, L. Wittgenstein, G\u00f6del, Carnap, Popper and Quine.\n2.4 Historical Development of the Meaning of the Term \u201cInformation\u201d\n\nThe history of the term \u201cinformation\u201d is intricately\nrelated to the study of central problems in epistemology and ontology\nin Western philosophy. After a start as a technical term in classical\nand medieval texts the term \u201cinformation\u201d almost vanished\nfrom the philosophical discourse in modern philosophy, but gained\npopularity in colloquial speech. Gradually the term obtained the\nstatus of an abstract mass-noun, a meaning that is orthogonal to the\nclassical process-oriented meaning. In this form it was picked up by\nseveral researchers (Fisher 1925; Shannon 1948) in the twentieth\ncentury who introduced formal methods to measure\n\u201cinformation\u201d. This, in its turn, lead to a revival of the\nphilosophical interest in the concept of information. This complex\nhistory seems to be one of the main reasons for the difficulties in\nformulating a definition of a unified concept of information that\nsatisfies all our intuitions. At least three different meanings of the\nword \u201cinformation\u201d are historically relevant:\n\n\n\u201cInformation\u201d as the process of being\ninformed.\n\nThis is the oldest meaning one finds in the writings of authors like\nCicero (106\u201343 BCE) and Augustine (354\u2013430 CE) and it is\nlost in the modern discourse, although the association of information\nwith processes (i.e., computing, flowing or sending a message) still\nexists. In classical philosophy one could say that when I recognize a\nhorse as such, then the \u201cform\u201d of a horse is planted in my\nmind. This process is my \u201cinformation\u201d of the nature of\nthe horse. Also the act of teaching could be referred to as the\n\u201cinformation\u201d of a pupil. In the same sense one could say\nthat a sculptor creates a sculpture by \u201cinforming\u201d a piece\nof marble. The task of the sculptor is the \u201cinformation\u201d\nof the statue (Capurro & Hj\u00f8rland 2003). This\nprocess-oriented meaning survived quite long in western European\ndiscourse: even in the eighteenth century Robinson Crusoe could refer\nto the education of his servant Friday as his\n\u201cinformation\u201d (Defoe 1719: 261). It is also used in this\nsense by Berkeley: \u201cI love information upon all subjects that\ncome in my way, and especially upon those that are most\nimportant\u201d (Alciphron Dialogue 1, Section 5, Paragraph\n6/10, see Berkeley 1732).\n\n\u201cInformation\u201d as a state of an agent,\n\ni.e., as the result of the process of being informed. If one teaches a\npupil the theorem of Pythagoras then, after this process is completed,\nthe student can be said to \u201chave the information about the\ntheorem of Pythagoras\u201d. In this sense the term\n\u201cinformation\u201d is the result of the same suspect form of\nsubstantiation of a verb (informare \\(\\gt\\)\ninformatio) as many other technical terms in philosophy\n(substance, consciousness, subject, object). This sort of\nterm-formation is notorious for the conceptual difficulties it\ngenerates. Can one derive the fact that I \u201chave\u201d\nconsciousness from the fact that I am conscious? Can one derive the\nfact that I \u201chave\u201d information from the fact that I have\nbeen informed? The transformation to this modern substantiated meaning\nseems to have been gradual and seems to have been general in Western\nEurope at least from the middle of the fifteenth century. In the\nrenaissance a scholar could be referred to as \u201ca man of\ninformation\u201d, much in the same way as we now could say that\nsomeone received an education (Adriaans & van Benthem 2008b;\nCapurro & Hj\u00f8rland 2003). In \u201cEmma\u201d by Jane\nAusten one can read: \u201cMr. Martin, I suppose, is not a man of\ninformation beyond the line of his own business. He does not\nread\u201d (Austen 1815: 21).\n\n\u201cInformation\u201d as the disposition to\ninform,\n\ni.e., as a capacity of an object to inform an agent. When the act of\nteaching me Pythagoras\u2019 theorem leaves me with information about\nthis theorem, it is only natural to assume that a text in which the\ntheorem is explained actually \u201ccontains\u201d this information.\nThe text has the capacity to inform me when I read it. In the same\nsense, when I have received information from a teacher, I am capable\nof transmitting this information to another student. Thus information\nbecomes something that can be stored and measured. This last concept\nof information as an abstract mass-noun has gathered wide acceptance\nin modern society and has found its definitive form in the nineteenth\ncentury, allowing Sherlock Homes to make the following observation:\n\u201c\u2026 friend Lestrade held information in his hands the\nvalue of which he did not himself know\u201d (\u201cThe Adventure of\nthe Noble Bachelor\u201d, Conan Doyle 1892). The association with the\ntechnical philosophical notions like \u201cform\u201d and\n\u201cinforming\u201d has vanished from the general consciousness\nalthough the association between information and processes like\nstoring, gathering, computing and teaching still exist.\n\n3. Building Blocks of Modern Theories of Information\n\nWith hindsight many notions that have to do with optimal code systems,\nideal languages and the association between computing and processing\nlanguage have been recurrent themes in the philosophical reflection\nsince the seventeenth century.\n3.1 Languages\n\nOne of the most elaborate proposals for a universal\n\u201cphilosophical\u201d language was made by bishop John Wilkins\n(Maat 2004): \u201cAn Essay towards a Real Character, and a\nPhilosophical Language\u201d (1668). Wilkins\u2019 project consisted\nof an elaborate system of symbols that supposedly were associated with\nunambiguous concepts in reality. Proposals such as these made\nphilosophers sensitive to the deep connections between language and\nthought. The empiricist methodology made it possible to conceive the\ndevelopment of language as a system of conventional signs in terms of\nassociations between ideas in the human mind. The issue that currently\nis known as the symbol grounding problem (how do arbitrary\nsigns acquire their inter-subjective meaning) was one of the most\nheavily debated questions in the eighteenth century in the context of\nthe problem of the origin of languages. Diverse thinkers as Vico,\nCondillac, Rousseau, Diderot, Herder and Haman made contributions. The\ncentral question was whether language was given a priori (by\nGod) or whether it was constructed and hence an invention of man\nhimself. Typical was the contest issued by the Royal Prussian Academy\nof Sciences in 1769:\n\n\nEn supposant les hommes abandonn\u00e9s \u00e0 leurs\nfacult\u00e9s naturelles, sont-ils en \u00e9tat d\u2019inventer\nle langage? Et par quels moyens parviendront-ils\nd\u2019eux-m\u00eames \u00e0 cette invention? \n\nAssuming men abandoned to their natural faculties, are they able to\ninvent language and by what means will they come to this\n invention?[1]\n\n\nThe controversy raged on for over a century without any conclusion and\nin 1866 the Linguistic Society of Paris (Soci\u00e9t\u00e9 de\nLinguistique de Paris) banished the issue from its arena.\n [2]\n\nPhilosophically more relevant is the work of Leibniz (1646\u20131716)\non a so-called characteristica universalis: the notion of a\nuniversal logical calculus that would be the perfect vehicle for\nscientific reasoning. A central presupposition in Leibniz\u2019\nphilosophy is that such a perfect language of science is in principle\npossible because of the perfect nature of the world as God\u2019s\ncreation (ratio essendi = ration cognoscendi, the\norigin of being is the origin of knowing). This principle was rejected\nby Wolff (1679\u20131754) who suggested more heuristically oriented\ncharacteristica combinatoria (van Peursen 1987). These ideas\nhad to wait for thinkers like Boole (1854, An Investigation of the\nLaws of Thought), Frege (1879, Begriffsschrift), Peirce\n(who in 1886 already suggested that electrical circuits could be used\nto process logical operations) and Whitehead and Russell\n(1910\u20131913, Principia Mathematica) to find a more\nfruitful treatment.\n3.2 Optimal Codes\n\nThe fact that frequencies of letters vary in a language was known\nsince the invention of book printing. Printers needed many more\n\u201ce\u201ds and \u201ct\u201ds than \u201cx\u201ds or\n\u201cq\u201ds to typeset an English text. This knowledge was used\nextensively to decode ciphers since the seventeenth century (Kahn\n1967; Singh 1999). In 1844 an assistant of Samuel Morse, Alfred Vail,\ndetermined the frequency of letters used in a local newspaper in\nMorristown, New Jersey, and used them to optimize Morse code. Thus the\ncore of theory of optimal codes was already established long before\nShannon developed its mathematical foundation (Shannon 1948; Shannon\n& Weaver 1949). Historically important but philosophically less\nrelevant are the efforts of Charles Babbage to construct computing\nmachines (Difference Engine in 1821, and the Analytical Engine\n1834\u20131871) and the attempt of Ada Lovelace (1815\u20131852) to\ndesign what is considered to be the first programming language for the\nAnalytical Engine.\n3.3 Numbers\n\nThe simplest way of representing numbers is via a unary\nsystem. Here the length of the representation of a number is\nequal to the size of the number itself, i.e., the number\n\u201cten\u201d is represented as \u201c\\\\\\\\\\\\\\\\\\\\\u201d. The\nclassical Roman number system is an improvement since it contains\ndifferent symbols for different orders of magnitude (one = I, ten = X,\nhundred = C, thousand = M). This system has enormous drawbacks since\nin principle one needs an infinite amount of symbols to code the\nnatural numbers and because of this the same mathematical operations\n(adding, multiplication etc.) take different forms at different orders\nof magnitude. Around 500 CE the number zero was invented in India.\nUsing zero as a placeholder we can code an infinity of numbers with a\nfinite set of symbols (one = I, ten = 10, hundred = 100, thousand =\n1000 etc.). From a modern perspective an infinite number of position\nsystems is possible as long as we have 0 as a placeholder and a finite\nnumber of other symbols. Our normal decimal number system has ten\ndigits \u201c0, 1, 2, 3, 4, 5, 6, 7, 8, 9\u201d and represents the\nnumber two-hundred-and-fifty-five as \u201c255\u201d. In a binary\nnumber system we only have the symbols \u201c0\u201d and\n\u201c1\u201d. Here two-hundred-and-fifty-five is represented as\n\u201c11111111\u201d. In a hexadecimal system with 16 symbols (0, 1,\n2, 3, 4, 5, 6, 7, 8, 9, a, b, c, d, e, f) the same number can be\nwritten as \u201cff\u201d. Note that the length of these\nrepresentations differs considerable. Using this representation,\nmathematical operations can be standardized irrespective of the order\nof magnitude of numbers we are dealing with, i.e., the possibility of\na uniform algorithmic treatment of mathematical functions (addition,\nsubtraction, multiplication and division etc.) is associated with such\na position system.\n\nThe concept of a positional number system was brought to Europe by the\nPersian mathematician al-Khwarizmi (ca. 780\u2013ca. 850 CE). His\nmain work on numbers (ca. 820 CE) was translated into Latin as\nLiber Algebrae et Almucabola in the twelfth century, which\ngave us amongst other things the term \u201calgebra\u201d. Our word\n\u201calgorithm\u201d is derived from Algoritmi, the Latin\nform of his name. Positional number systems simplified commercial and\nscientific calculations.\n\nIn 1544 Michael Stifel introduced the concept of the exponent of a\nnumber in Arithmetica integra (1544). Thus 8 can be written\nas \\(2^3\\) and 25 as \\(5^2\\). The notion of an exponent immediately\nsuggests the notion of a logarithm as its inverse function: \\(\\log_b\nb^a = a\\). Stifel compared the arithmetic sequence: \n\\[\n-3, -2, -1, 0, 1, 2, 3\n\\]\n\n\nin which the term 1 have a difference of 1 with the geometric\nsequence: \n\\[\n\\frac{1}{8}, \\frac{1}{4}, \\frac{1}{2} , 1, 2, 4, 8\n\\]\n\n\nin which the terms have a ratio of 2. The exponent notation allowed\nhim to rewrite the values of the second table as: \n\\[\n2^{-3}, 2^{-2}, 2^{-1}, 2^0 , 2^1 , 2^2, 2^3\n\\]\n\n\nwhich combines the two tables. This arguably was the first logarithmic\ntable. A more definitive and practical theory of logarithms is\ndeveloped by John Napier (1550\u20131617) in his main work (Napier\n1614). He coined the term logarithm (logos + arithmetic: ratio of\nnumbers). As is clear from the match between arithmetic and geometric\nprogressions, logarithms reduce products to sums: \n\\[\n\\log_b (xy) = \\log_b (x) + \\log_b (y)\n\\]\n\n\nThey also reduce divisions to differences: \n\\[\n\\log_b (x/y) = \\log_b (x) - \\log_b (y)\n\\]\n\n\nand powers to products:  \n\\[\n\\log_b (x^p) = p \\log_b (x)\n\\]\n\n\nAfter publication of the logarithmic tables by Briggs (1624) this new\ntechnique of facilitating complex calculations rapidly gained\npopularity.\n3.4 Physics\n\nGalileo (1623) already had suggested that the analysis of phenomena\nlike heat and pressure could be reduced to the study of movements of\nelementary particles. Within the empirical methodology this could be\nconceived as the question how the sensory experience of the secondary\nquality of heat of an object or a gas could be reduced to movements of\nparticles. Bernoulli (Hydrodynamica published in 1738) was\nthe first to develop a kinetic theory of gases in which\nmacroscopically observable phenomena are described in terms of\nmicrostates of systems of particles that obey the laws of Newtonian\nmechanics, but it was quite an intellectual effort to come up with an\nadequate mathematical treatment. Clausius (1850) made a conclusive\nstep when he introduced the notion of the mean free path of a particle\nbetween two collisions. This opened the way for a statistical\ntreatment by Maxwell who formulated his distribution in 1857, which\nwas the first statistical law in physics. The definitive formula that\ntied all notions together (and that is engraved on his tombstone,\nthough the actual formula is due to Planck) was developed by\nBoltzmann: \n\\[\nS = k \\log W\n\\]\n\n\nIt describes the entropy S of a system in terms of the\nlogarithm of the number of possible microstates W, consistent\nwith the observable macroscopic states of the system, where k\nis the well-known Boltzmann constant. In all its simplicity the value\nof this formula for modern science can hardly be overestimated. The\nexpression \u201c\\(\\log W\\)\u201d can, from the perspective of\ninformation theory, be interpreted in various ways:\n\nAs the amount of entropy in the system.\nAs the length of the number needed to count all possible\nmicrostates consistent with macroscopic observations.\nAs the length of an optimal index we need to identify the\nspecific current unknown microstate of the system, i.e., it is a\nmeasure of our \u201clack of information\u201d.\nAs a measure for the probability of any typical specific\nmicrostate of the system consistent with macroscopic\nobservations.\n\n\nThus it connects the additive nature of logarithm with the extensive\nqualities of entropy, probability, typicality and information and it\nis a fundamental step in the use of mathematics to analyze nature.\nLater Gibbs (1906) refined the formula: \n\\[\nS = -\\sum_i p_i \\ln p_i,\n\\]\n\n\nwhere \\(p_i\\) is the probability that the system is in the\n\\(i^{\\textrm{th}}\\) microstate. This formula was adopted by Shannon\n(1948; Shannon & Weaver 1949) to characterize the communication\nentropy of a system of messages. Although there is a close connection\nbetween the mathematical treatment of entropy and information, the\nexact interpretation of this fact has been a source of controversy\never since (Harremo\u00ebs & Tops\u00f8e 2008; Bais & Farmer\n2008).\n4. Developments in Philosophy of Information\n\nThe modern theories of information emerged in the middle of the\ntwentieth century in a specific intellectual climate in which the\ndistance between the sciences and parts of academic philosophy was\nquite big. Some philosophers displayed a specific anti-scientific\nattitude: Heidegger, \u201cDie Wissenschaft denkt\nnicht.\u201d On the other hand the philosophers from the Wiener\nKreis overtly discredited traditional philosophy as dealing with\nillusionary problems (Carnap 1928). The research program of logical\npositivism was a rigorous reconstruction of philosophy based on a\ncombination of empiricism and the recent advances in logic. It is\nperhaps because of this intellectual climate that early important\ndevelopments in the theory of information took place in isolation from\nmainstream philosophical reflection. A landmark is the work of Dretske\nin the early eighties (Dretske 1981). Since the turn of the century,\ninterest in Philosophy of Information has grown considerably, largely\nunder the influence of the work of Luciano Floridi on semantic\ninformation. Also the rapid theoretical development of quantum\ncomputing and the associated notion of quantum information have had it\nrepercussions on philosophical reflection.\n4.1 Popper: Information as Degree of Falsifiability\n\nThe research program of logical positivism of the Wiener Kreis in the\nfirst half of the twentieth century revitalized the older project of\nempiricism. Its ambition was to reconstruct scientific knowledge on\nthe basis of direct observations and logical relation between\nstatements about those observations. The old criticism of Kant on\nempiricism was revitalized by Quine (1951). Within the framework of\nlogical positivism induction was invalid and causation could never be\nestablished objectively. In his Logik der Forschung (1934)\nPopper formulates his well-known demarcation criterion and he\npositions this explicitly as a solution to Hume\u2019s problem of\ninduction (Popper 1934 [1977: 42]). Scientific theories formulated as\ngeneral laws can never be verified definitively, but they can be\nfalsified by only one observation. This implies that a theory is\n\u201cmore\u201d scientific if it is richer and provides more\nopportunity to be falsified:\n\n\nThus it can be said that the amount of empirical information conveyed\nby a theory, or its empirical content, increases with its\ndegree of falsifiability. (Popper 1934 [1977: 113], emphasis in\noriginal) \n\n\nThis quote, in the context of Popper\u2019s research program, shows\nthat the ambition to measure the amount of empirical information\nin scientific theory conceived as a set of logical statements was\nalready recognized as a philosophical problem more than a decade\nbefore Shannon formulated his theory of information. Popper is aware\nof the fact that the empirical content of a theory is related to its\nfalsifiability and that this in its turn has a relation with the\nprobability of the statements in the theory. Theories with more\nempirical information are less probable. Popper distinguishes\nlogical probability from numerical probability\n(\u201cwhich is employed in the theory of games and chance, and in\nstatistics\u201d; Popper 1934 [1977: 119]). In a passage that is\nprogrammatic for the later development of the concept of information\nhe defines the notion of logical probability:\n\n\nThe logical probability of a statement is complementary to its\nfalsifiability: it increases with decreasing degree of\nfalsifiability. The logical probability 1 corresponds to the degree 0\nof falsifiability and vice versa. (Popper 1934 [1977: 119],\nemphasis in original) \n\nIt is possible to interpret numerical probability as applying to a\nsubsequence (picked out from the logical probability relation) for\nwhich a system of measurement can be defined, on the basis of\nfrequency estimates. (Popper 1934 [1977: 119], emphasis in original)\n\n\n\nPopper never succeeded in formulating a good formal theory to measure\nthis amount of information although in later writings he suggests that\nShannon\u2019s theory of information might be useful (Popper 1934\n[1977], 404 [Appendix IX, from 1954]). These issues were later\ndeveloped in philosophy of science. Theory of conformation studies\ninduction theory and the way in which evidence \u201csupports\u201d\na certain theory (Huber 2007\n [OIR]).\n Although the work of Carnap motivated important developments in both\nphilosophy of science and philosophy of information the connection\nbetween the two disciplines seems to have been lost. There is no\nmention of information theory or any of the more foundational work in\nphilosophy of information in Kuipers (2007a), but the two disciplines\ncertainly have overlapping domains. (See, e.g., the discussion of the\nso-called Black Ravens Paradox by Kuipers (2007b) and Rathmanner &\nHutter (2011).) \n4.2 Shannon: Information Defined in Terms of Probability\n\nIn two landmark papers Shannon (1948; Shannon & Weaver 1949)\ncharacterized the communication entropy of a system of messages\nA: \n\\[\nH(P) = -\\sum_{i\\in A} p_i \\log_2 p_i\n\\]\n\n\nHere \\(p_i\\) is the probability of message i in A. This\nis exactly the formula for Gibb\u2019s entropy in physics. The use of\nbase-2 logarithms ensures that the code length is measured in bits\n(binary digits). It is easily seen that the communication entropy of a\nsystem is maximal when all the messages have equal probability and\nthus are typical. \n\nThe amount of information I in an individual message x\nis given by:  \n\\[\nI(x) = -\\log p_x\n\\]\n\n\nThis formula, that can be interpreted as the inverse of the Boltzmann\nentropy, covers a number of our basic intuitions about\ninformation:\n\nA message x has a certain probability \\(p_x\\) between 0 and\n1 of occurring. \nIf \\(p_x = 1\\) then \\(I(x) = 0\\). If we are certain to get a\nmessage it literally contains no \u201cnews\u201d at al. The lower\nthe probability of the message is, the more information it contains. A\nmessage like \u201cThe sun will rise tomorrow\u201d seems to contain\nless information than the message \u201cJesus was Caesar\u201d\nexactly because the second statement is much less likely to be\ndefended by anyone (although it can be found on the web).\nIf two messages x and y are unrelated then \\(I(x\n\\textrm{ and } y)=I(x) + I(y)\\). Information is extensive.\nThe amount of information in two combined messages is equal to the sum\nof the amount of information in the individual messages. \n\n\nInformation as the negative log of the probability is the only\nmathematical function that exactly fulfills these constraints (Cover\n& Thomas 2006). Shannon offers a theoretical framework in which\nbinary strings can be interpreted as words in a (programming) language\ncontaining a certain amount of information (see\n 3.1 Languages).\n The expression \\(-\\log p_x\\) exactly gives the length of an optimal\ncode for message x and as such formalizes the old intuition\nthat codes are more efficient when frequent letters get shorter\nrepresentations (see\n 3.2 Optimal codes).\n Logarithms as a reduction of multiplication to addition (see\n 3.3 Numbers)\n are a natural representation of extensive properties of systems and\nalready as such had been used by physicists in the nineteenth century\n(see\n 3.4 Physics).\n\nOne aspect of information that Shannon\u2019s definition explicitly\ndoes not cover is the actual content of the messages interpreted as\npropositions. So the statement \u201cJesus was Caesar\u201d and\n\u201cThe moon is made of green cheese\u201d may carry the same\namount of information while their meaning is totally different. A\nlarge part of the effort in philosophy of information has been\ndirected to the formulation of more semantic theories of information\n(Bar-Hillel & Carnap 1953; Floridi 2002, 2003, 2011). Although\nShannon\u2019s proposals at first were almost completely ignored by\nphilosophers it has in the past decennia become apparent that their\nimpact on philosophical issues is big. Dretske (1981) was one of the\nfirst to analyze the philosophical implications of Shannon\u2019s\ntheory, but the exact relation between various systems of logic and\ntheory of information are still unclear (see\n 6.6 Logic and Semantic Information).\n \n4.3 Solomonoff, Kolmogorov, Chaitin: Information as the Length of a Program\n\nThis problem of relating a set of statements to a set of observations\nand defining the corresponding probability was taken up by Carnap\n(1945, 1950). He distinguished two forms of probability:\nProbability\\(_1\\) or \u201cdegree of confirmation\u201d \\(P_1 (h ;\ne)\\) is a logical relation between two sentences, a\nhypothesis h and a sentence e reporting a series of\nobservations. Statements of this type are either analytical or\ncontradictory. The second form, Probability\\(_2\\) or \u201crelative\nfrequency\u201d, is the statistical concept. In the words of his\nstudent Solomonoff (1997):\n\n\nCarnap\u2019s model of probability started with a long sequence of\nsymbols that was a description of the entire universe. Through his own\nformal linguistic analysis, he was able to assign a priori\nprobabilities to any possible string of symbols that might represent\nthe universe.\n\n\nThe method for assigning probabilities Carnap used, was not universal\nand depended heavily on the code systems used. A general theory of\ninduction using Bayes\u2019 rule can only be developed when we can\nassign a universal probability to \u201cany possible string\u201d of\nsymbols. In a paper in 1960 Solomonoff (1960, 1964a,b) was the first\nto sketch an outline of a solution for this problem. He formulated the\nnotion of what is now called a universal probability\ndistribution: consider the set of all possible finite strings to\nbe programs for a universal Turing machine U and define the\nprobability of a string x of symbols in terms of the length of\nthe shortest program p that outputs x on U.\n\nThis notion of Algorithmic Information Theory was invented\nindependently somewhat later separately by Kolmogorov (1965) and\nChaitin (1969). Levin (1974) developed a mathematical expression of\nthe universal a priori probability as a universal (that is,\nmaximal) lower semicomputable semimeasure M, and showed that\nthe negative logarithm of \\(M(x)\\) coincides with the Kolmogorov\ncomplexity of x up to an additive logarithmic term. The actual\ndefinition of the complexity measure is: \n\n\nKolmogorov complexity The algorithmic complexity of a\nstring x is the length \\(\\cal{l}(p)\\) of the smallest program\np that produces x when it runs on a universal Turing\nmachine U, noted as \\(U(p)=x\\):  \n\\[K(x):=\\min_p \\{l(p), U(p)=x\\}\\]\n\n\n\nAlgorithmic Information Theory (a.k.a. Kolmogorov complexity theory)\nhas developed into a rich field of research with a wide range of\ndomains of applications many of which are philosophically relevant (Li\n& Vit\u00e1nyi 2019):\n\nIt provides us with a general theory of induction. The use of\nBayes\u2019 rule allows for a modern reformulation of Ockham\u2019s\nrazor in terms of Minimum Description Length (Rissanen 1978, 1989;\nBarron, Rissanen, & Yu 1998; Gr\u00fcnwald 2007, Long 2019) and\nminimum message length (Wallace 2005). Note that Domingos (1998) has\nargued against the general validity of these principles.\nIt allows us to formulate probabilities and information content\nfor individual objects. Even individual natural numbers.\nIt lays the foundation for a theory of learning as data\ncompression (Adriaans 2007).\nIt gives a definition of randomness of a string in terms of\nincompressibility. This in itself has led to a whole new domain of\nresearch (Niess 2009; Downey & Hirschfeld 2010).\nIt allows us to formulate an objective a priori measure\nof the predictive value of a theory in terms of its randomness\ndeficiency: i.e., the best theory is the shortest theory that makes\nthe data look random conditional to the theory. (Vereshchagin &\nVit\u00e1nyi 2004).\n\n\nThere are also down-sides:\n\nAlgorithmic complexity is uncomputable, although it can in a lot\nof practical cases be approximated and commercial compression programs\nin some cases come close to the theoretical optimum (Cilibrasi &\nVit\u00e1nyi 2005).\nAlgorithmic complexity is an asymptotic measure (i.e., it gives a\nvalue that is correct up to a constant). In some cases the value of\nthis constant is prohibitive for use in practical purposes.\nAlthough the shortest theory is always the best one in terms of\nrandomness deficiency, incremental compression of data-sets is in\ngeneral not a good learning strategy since the randomness deficiency\ndoes not decrease monotonically with the compression rate (Adriaans\n& Vit\u00e1nyi 2009).\nThe generality of the definitions provided by Algorithmic\nInformation Theory depends on the generality of the concept of a\nuniversal Turing machine and thus ultimately on the interpretation of\nthe Church-Turing-Thesis.\nThe Kolmogorov complexity of an object does not take in to account\nthe amount of time it takes to actually compute the object. In this\ncontext Levin proposed a variant of Kolmogorov complexity that\npenalizes the computation time (Levin 1973, 1984):\n\n\n\nLevin complexity The Levin complexity of a string\nx is the sum of the length \\(\\cal{l}(p)\\) and the logarithm of\nthe computation time of the smallest program p that produces\nx when it runs on a universal Turing machine U, noted as\n\\(U(p)=x\\):  \n\\[Kt(x):=\\min_p \\{l(p) + \\log(time(p)), U(p)=x\\}\\]\n\n \n\n\nAlgorithmic Information Theory has gained rapid acceptance as a\nfundamental theory of information. The well-known introduction in\nInformation Theory by Cover and Thomas (2006) states:\n\u201c\u2026 we consider Kolmogorov complexity (i.e., AIT) to be\nmore fundamental than Shannon entropy\u201d (2006: 3).\n\nThe idea that algorithmic complexity theory is a foundation for a\ngeneral theory of artificial intelligence (and theory of knowledge)\nhas already been suggested by Solomonoff (1997) and Chaitin (1987).\nSeveral authors have defended that data compression is a general\nprinciple that governs human cognition (Chater & Vit\u00e1nyi\n2003; Wolff 2006). Hutter (2005, 2007a,b) argues that\nSolomonoff\u2019s formal and complete theory essentially solves the\ninduction problem. Hutter (2007a) and Rathmanner & Hutter (2011)\nenumerate a plethora of classical philosophical and statistical\nproblems around induction and claim that Solomonoff\u2019s theory\nsolves or avoids all these problems. Probably because of its technical\nnature, the theory has been largely ignored by the philosophical\ncommunity. Yet, it stands out as one of the most fundamental\ncontributions to information theory in the twentieth century and it is\nclearly relevant for a number of philosophical issues, such as the\nproblem of induction.\n5. Systematic Considerations\n\nIn a mathematical sense information is associated with measuring\nextensive properties of classes of systems with finite but unlimited\ndimensions (systems of particles, texts, codes, networks, graphs,\ngames etc.). This suggests that a uniform treatment of various\ntheories of information is possible. In the Handbook of Philosophy of\nInformation three different forms of information are distinguished\n(Adriaans & van Benthem 2008b):\n\n\nInformation-A:\n\nKnowledge, logic, what is conveyed in informative answers\n\nInformation-B:\n\nProbabilistic, information-theoretic, measured quantitatively\n\nInformation-C:\n\nAlgorithmic, code compression, measured quantitatively\n\n\nBecause of recent development the connections between Information-B\n(Shannon) and Information-C (Kolmogorov) are reasonably well\nunderstood (Cover & Thomas 2006). The historical material\npresented in this article suggests that reflection on Information-A\n(logic, knowledge) is historically much more interwoven than was\ngenerally known up till now. The research program of logical\npositivism can with hindsight be characterized as the attempt to marry\na possible worlds interpretation of logic with probabilistic reasoning\n(Carnap 1945, 1950; Popper 1934; for a recent approach see Hutter et\nal. 2013). Modern attempt to design a Bayesian epistemology (Bovens\n& Hartmann 2003) do not seem to be aware of the work done in the\nfirst half of the twentieth century. However, an attempt to unify\nInformation-A and Information-B seems a viable exercise (Adriaans\n2020). Also the connection between thermodynamics and information\ntheory have become much closer, amongst others, due to the work of\nGell-Mann & Lloyd (2003) (see also: Bais and Farmer 2008).\nVerlinde (2011, 2017) even presented a reduction of gravity to\ninformation (see the entry on\n information processing and thermodynamic entropy).\n \n5.1 Philosophy of Information as An Extension of Philosophy of Mathematics\n\nWith respect to the main definitions of the concept of information,\nlike Shannon Information, Kolmogorov complexity, semantic information\nand quantum information, a unifying approach to a philosophy of\ninformation is possible, when we interpret it as an extension to the\nphilosophy of mathematics. The answer to questions like \u201cWhat is\ndata?\u201d and \u201cWhat is information?\u201d then evolves from\none\u2019s answer to the related questions like \u201cWhat is a\nset?\u201d and \u201cWhat is a number?\u201d With hindsight one can\nobserve that many open problems in the philosophy of mathematics\nrevolve around the notion of information. \n\nIf we look at the foundations of information and computation there are\ntwo notions that are crucial: the concept of a data set and the\nconcept of an algorithm. Once we accept these notions as fundamental\nthe rest of the theory data and computation unfolds quite naturally.\nOne can \u201cplug in\u201d one\u2019s favorite epistemological or\nmetaphysical stance here, but this does not really affect foundational\nissues in the philosophy of computation and information. One might\nsustain a Formalist, Platonic or intuitionistic view of the\nmathematical universe (see entry on\n philosophy of mathematics)\n and still agree on the basic notion of what effective computation is.\nThe theory of computing, because of its finitistic and constructivist\nnature, seems to live more or less on the common ground in which these\ntheories overlap. \n5.1.1 Information as a natural phenomenon\n\nInformation as a scientific concept emerges naturally in the context\nof our every day dealing with nature when we measure things. Examples\nare ordinary actions like measuring the size of an object with a\nstick, counting using our fingers, drawing a straight line using a\npiece of rope. These processes are the anchor points of abstract\nconcepts like length, distance, number, straight line that form the\nbuilding blocks of science. The fact that these concepts are rooted in\nour concrete experience of reality guarantees their applicability and\nusefulness. The earliest traces of information processing evolved\naround the notions of counting, administration and accountancy.\n\n\nExample: Tally sticks\n\nOne of the most elementary information measuring devices is unary\ncounting using a tally stick. Tally sticks were already used\naround 20,000 years ago. When a hypothetical prehistoric hunter killed\na deer he could have registered this fact by making a scratch\n\u201c|\u201d on a piece of wood. Every stroke on such a stick\nrepresents an object/item/event. The process of unary counting is\nbased on the elementary operation of catenation of symbols\ninto sequences. This measuring method illustrates a primitive\nversion of the concept of extensiveness of information: the\nlength of the sequences is a measure for the amount of items counted.\nNote that such a sequential process of counting is non-commutative and\nnon-associative. If \u201c|\u201d is our basic symbol and \\(\\oplus\\)\nour concatenation operator then a sequence of signs has the form: \n\n\\[((\\dots(| \\oplus |) \\dots) \\oplus |)\\oplus |)\\]\n\n\nA new symbol is always concatenated at the end of the sequence. \n\n\nThis example helps to understand the importance of context in\nthe analysis of information. In itself a scratch on a stick may have\nno meaning at all, but as soon as we decide that such a scratch\nrepresents another object or event it becomes a\nmeaningful symbol. When we manipulate it in such a context we\nprocess information. In principle a simple scratch can represent any\nevent or object we like: symbols are conventional.\n\nDefinition: A symbol is a mark, sign or word\nthat indicates, signifies, or is understood as representing an idea,\nobject, or relationship. \n\nSymbols are the semantic anchors by which symbol manipulating systems\nare tied to the world. Observe that the meta-statement: \n\nThe symbol \u201c|\u201d signifies object y. \n\nif true, specifies semantic information:\n\nIt is wellformed: the statement has a specific syntax.\n\nIt is meaningful: Only in the context where the scratch\n\u201c|\u201d is actually made deliberately on, e.g., a tally stick\nor in a rock to mark a well defined occurrence it has a meaning.\nIt is truthful.\n\n\nSymbol manipulation can take many forms and is not restricted to\nsequences. Many examples of different forms of information processing\ncan be found in prehistoric times. \n\nExample: Counting sheep in Mesopotamia\n\nWith the process of urbanization, early accounting systems emerged in\nMesopotamia around 8000 BCE using clay tokens to administer cattle\n(Schmandt-Besserat 1992). Different shaped tokens were used for\ndifferent types of animals, e.g., sheep and goats. After the\nregistration the tokens were packed in a globular clay container, with\nmarks representing their content on the outside. The container was\nbaked to make the registration permanent. Thus early forms of writing\nemerged. After 4000 BCE the tokens were mounted on a string to\npreserve the order. \n\nThe historical transformation from sets to strings is important. It is\na more sophisticated form of coding of information. Formally we can\ndistinguish several levels of complexity of token combination:\n\nAn unordered collection of similar tokens in a\ncontainer. This represents a set. The tokens can move freely\nin the container. The volume of the tokens is the only relevant\nquality. \nAn unordered collection of tokens of different\ntypes in a container. This represents a so-called\nmultiset. Both volume and frequency are relevant.\nAn ordered collection of typed tokens on a\nstring. This represents a sequence of symbols. In this case\nthe length of the string is a relevant quality. \n\n5.1.2 Symbol manipulation and extensiveness: sets, multisets and strings\n\nSequences of symbols code more information than multisets and\nmultisets are more expressive than sets. Thus the emergence of writing\nitself can be seen as a quest to find the most expressive\nrepresentation of administrative data. When measuring information in\nsequences of messages it is important to distinguish the aspects of\nrepetition, order and grouping. The\nextensive aspects of information can be studied in terms of such\nstructural operations (see entry on\n substructural logics).\n We can study sets of messages in terms of operators defined on\nsequences of symbols. \n\n\nDefinition: Suppose m, n, o,\np, \u2026 are symbols and \\(\\oplus\\) is a tensor or\nconcatentation operator. We define the class of sequences:\n\n\n Any symbol is a sequence\n If \\(\\alpha\\) and \\(\\beta\\) are sequences then \\((\\alpha\n\\oplus\\beta)\\)is a sequence\n For sequences we define the following basic properties on the\nlevel of symbol concatenation:\n\n\n Contraction: \n\\[(m\\ \\oplus m) = m.\\]\n Contraction destroys\ninformation about frequency in the sequence. Physical\ninterpretation: two occurrences of the same symbol can collapse to one\noccurrence when they are concatenated. \n Commutativity: \n\\[(m\\ \\oplus n) = (n\\ \\oplus\\ m)\\]\n Commutativity\ndestroys information about order in the sequence. Physical\ninterpretation: symbols may swap places when they are concatenated.\n\n Associativity: \n\\[ (p\\oplus (q \\oplus r)) = ((p \\oplus q)\\oplus r)\\ \\]\n Associativity\ndestroys information about nesting in the sequence. Physical\ninterpretation: symbols may be regrouped when they are concatenated.\n\n\n\n\n\nObservation: Systems of sequences with contraction,\ncommutativity and associativity behave like sets. Consider the\nequation:  \n\\[\\{p,q\\} \\cup \\{p,r\\} = \\{p,q,r\\}\\]\n\n\nWhen we model the sets as two sequences \\((p \\oplus q)\\) and \\((p\n\\oplus r)\\), the corresponding implication is:  \n\\[(p \\oplus q),(p \\oplus r) \\vdash ((p \\oplus q) \\oplus r)\\]\n\n\nProof: \n\\[\n\\begin{align}\n((p \\oplus q) &\\oplus (p \\oplus r)) & \\tt{Concatenation}\\\\\n((q \\oplus p) & \\oplus (p \\oplus r)) &  \\tt{Commutativity}\\\\\n(((q \\oplus p) \\oplus p) & \\oplus r) & \\tt{Associativity}\\\\\n((q \\oplus (p \\oplus p)) & \\oplus r) & \\tt{Associativity}\\\\\n((q \\oplus p) & \\oplus r) & \\tt{Contraction}\\\\\n((p \\oplus q) & \\oplus r) & \\tt{Commutativity}\n\\end{align}\n\\]\n\n\n\nThe structural aspects of sets, multisets and strings can be\nformulated in terms of these properties: \n\n\nSets: \u00a0 Sequences of messages collapse into sets\nunder contraction, commutativity and\nassociativity. A set is a collection of objects in which each\nelement occurs only once:  \n\\[\\{a,b,c\\} \\cup \\{b,c,d\\} = \\{a,b,c,d\\}\\]\n\n\nand for which order is not relevant: \n\\[\\{a,b,c\\} = \\{b,c,a\\}.\\]\n\n\nSets are associated with our normal everyday naive concept of\ninformation as new, previously unknown, information. We only\nupdate our set if we get a message we have not seen previously. This\nnotion of information is forgetful both with respect to\nsequence and frequency. The set of messages cannot be reconstructed.\nThis behavior is associated with the notion of extensionality\nof sets: we are only interested in equality of elements, not in\nfrequency.\n\nMultisets: \u00a0 Sequences of messages collapse into\nmultisets under commutativity and associativity. A\nmultiset is a collection of objects in which the same element can\noccur multiple times \n\\[\\{a,b,c\\} \\cup \\{b,c,d\\} = \\{a,b,b,c,c,d\\}\\]\n\n\nand for which order is not relevant: \n\\[\\{a,b,a\\} = \\{b,a,a\\}.\\]\n\n\nMultisets are associated with a resource sensitive concept of\ninformation defined in Shannon Information. We are\ninterested in the frequency of the messages. This concept is\nforgetful with regards to sequence. We update our set every\ntime we get a message, but we forget the structure of the sequence.\nThis behavior is associated with the notion of extensiveness\nof information: we are both interested in equality of elements, and in\nfrequency.\n\nSequences: \u00a0 Sequences are associative.\nSequences are ordered multisets: \\(aba \\neq baa\\). The whole structure\nof the sequence of a message is stored. Sequences are associated with\nKolmogorov complexity defined as the length of a sequence of\nsymbols. \n\n\nSets may be interpreted as spaces in which objects can move freely.\nWhen the same objects are in each others vicinity they collapse in to\none object. Multisets can be interpreted as spaces in which objects\ncan move freely, with the constraint that the total number of objects\nstays constant. This is the standard notion of extensiveness: the\ntotal volume of a space stays constant, but the internal structure may\ndiffer. Sequences may be interpreted as spaces in which objects have a\nfixed location. In general a sequence contains more information than\nthe derived multiset, which contains more information than the\nassociated set.\n\nObservation: The interplay between the notion of sequences\nand multisets can be interpreted as a formalisation of the\nmalleability of a piece of wax that pervades history of\nphilosophy as the paradigm of information. Different sequences (forms)\nare representations of the same multiset (matter). The volume of the\npiece of wax (length of the string) is constant and thus a measure for\nthe amount of information that can be represented in the wax (i.e.in\nthe sequence of symbols). In terms of quantum physics the stability of\nthe piece of wax seems to be an emergent property: the statistical\ninstability of objects on an atomic level seem to even out when large\nquantities of them are manipulated. \n5.1.3 Sets and numbers\n\nThe notion of a set in mathematics is considered to be fundamental.\nAny identifiable collection of discrete objects can be considered to\nbe a set. The relation between theory of sets and the concept of\ninformation becomes clear when we analyze the basic statement: \n\n\\[\ne \\in A\n\\]\n\n\nWhich reads the object e is an element of the set A.\nObserve that this statement, if true, represents a piece of semantic\ninformation. It is wellformed, meaningful and truthful. (see entry on\n semantic conceptions of information)\n The concept of information is already at play in the basic building\nblocks of mathematics.The philosophical question \u201cWhat are\nsets?\u201d the answer to the ti esti question, is\ndetermined implicitly by the Zermelo-Fraenkel axioms (see\nentry on\n set theory),\n the first of which is that of extensionality: \n\nTwo sets are equal if they have the same elements. \n\nThe idea that mathematical concepts are defined implicitly by a set of\naxioms was proposed by Hilbert but is not uncontroversial (see entry\non the\n Frege-Hilbert controversy).\n The fact that the definition is implicit entails that we only have\nexamples of what sets are without the possibility to\nformulate any positive predicate that defines them. Elements of a set\nare not necessarily physical, nor abstract, nor spatial or temporal,\nnor simple, nor real. The only prerequisite is the possibility to\nformulate clear judgments about membership. This implicit definition\nof the notion of a set is not unproblematic. We might define objects\nthat at first glance seem to be proper sets, which after scrutiny\nappear to be internally inconsistent. This is the basis for: \n\n\nRussell\u2019s paradox: This paradox, which\nmotivated a lot of research into the foundations of mathematics, is a\nvariant of the liars paradox attributed to the Cretan philosopher\nEpeimenides (ca. 6 BCE) who apparently stated that Cretans always lie.\nThe crux of these paradoxes lies in the combination of the notions of:\nUniversality, Negation, and\nSelf-reference.\n\nAny person who is not Cretan can state that all Cretans always lie.\nFor a Cretan this is not possible because of the universal negative\nself-referential nature of the statement. If the statement is true, he\nis not lying which makes the statement untrue: a real paradox based on\nself contradiction. Along the same lines Russel coined the concept of\nthe set of all sets that are not member of themselves, for\nwhich membership cannot be determined. Apparently the set of all\nsets is an inadmissible object within set theory. In general\nthere is in philosophy and mathematics a limit to the extent in which\na system can verify statements about itself within the system. (For\nfurther discussion, see the entry on\n Russell\u2019s paradox.)\n \n\n\nThe implicit definition of the concepts of sets, entails that the\nclass is essentially open itself. There are mathematical\ndefinitions of objects of which it is unclear or highly controversial\nwhether they define a set or not. \n\nModern philosophy of mathematics starts with the Frege-Russell theory\nof numbers (Frege 1879, 1892, Goodstein 1957, see entry on\n alternative axiomatic set theories)\n in terms of sets. If we accept the notion of a class of objects as\nvalid and fundamental, together with the notion of a one-to-one\ncorrespondence between classes of objects, then we can define numbers\nas sets of equinumerous classes. \n\nDefinition: Two sets Aand B are\nequinumerous, \\(A \\sim B\\), if there exists a one-to-one\ncorrespondence between them, i.e., a function \\(f: A \\rightarrow B\\)\nsuch that for every \\(a \\in A\\) there is exactly one \\(f(a) \\in B\\).\n\n\nAny set of, say four, objects then becomes a representation of the\nnumber 4 and for any other set of objects we can establish membership\nto the equivalence class defining the number 4 by defining a one to\none correspondence to our example set.\n\nDefinition: If A is a finite set, then\n\\(\\mathcal{S}_A = \\{X \\mid X \\sim A \\}\\) is the class of all sets\nequinumerous with A. The associated generalization\noperation is the cardinality function: \\(|A|\n=\\mathcal{S}_A = \\{X \\mid X \\sim A \\} = n\\). This defines a\nnatural number \\(|A|= n \\in \\mathbb{N}\\) associated with the\nset A. \n\nWe can reconstruct large parts of the mathematical universe by\nselecting appropriate mathematical example objects to populate it,\nbeginning with the assumption that there is a single unique empty set\n\\(\\emptyset\\) which represents the number 0. This gives us the\nexistence of a set with only one member \\(\\{\\varnothing\\}\\) to\nrepresent the number 1 and repeating this construction,\n\\(\\{\\varnothing,\\{\\varnothing\\}\\}\\) for 2, the whole set of natural\nnumbers \\(\\mathbb{N}\\) emerges. Elementary arithmetic then is defined\non the basis of Peano\u2019s axioms: \n\nZero is a number.\n If a is a number, the successor of a is a\nnumber.\n Zero is not the successor of a number.\nTwo numbers of which the successors are equal are themselves\nequal.\n(induction axiom.) If a set S of numbers contains zero and\nalso the successor of every number in S, then every number is\nin S.\n\n\nThe fragment of the mathematical universe that emerges is relatively\nuncontroversial and both Platonists and constructivists might agree on\nits basic merits. On the basis of Peano\u2019s axioms we can define\nmore complex functions like addition and multiplication which are\nclosed on \\(\\mathbb{N}\\) and the inverse functions, subtraction and\ndivision, which are not closed and lead to the set of whole numbers\n\\(\\mathbb{Z}\\) and the rational numbers \\(\\mathbb{Q}\\). \n5.1.4 Measuring information in numbers\n\nWe can define the concept of information for a number n by\nmeans of an unspecified function \\(I(n)\\). We observe that addition\nand multiplication specify multisets: both are\nnon-contractive and commutative and\nassociative. Suppose we interpret the tensor operator\n\\(\\oplus\\) as multiplication \\(\\times\\). It is natural to define the\nsemantics for \\(I(m \\times n)\\) in terms of addition. If we\nget both messages m and n, the total amount of\ninformation in the combined messages is the sum of the amount of\ninformation in the individual messages. This leads to the following\nconstraints: \n\n\nDefinition: Additivity Constraint: \n\n\\[ I(m \\times n) = I(m) + I(n) \\]\n\n\n\nFurthermore we want bigger numbers to contain more information than\nsmaller ones, which gives a: \n\n\nDefinition: Monotonicity Constraint: \n\n\\[ I(m) \\leq  I(m + 1) \\]\n\n\n\nWe also want to select a certain number a as our basic unit\nof measurement:\n\n\nDefinition: Normalization Constraint: \n\n\\[ I(a) = 1 \\]\n\n\n\nThe following theorem is due to R\u00e9nyi (1961):\n\n\nTheorem: The Logarithm is the only mathematical\noperation that satisfies Additivity, Monotonicity and Normalisation.\n\n\nObservation: The logarithm \\(\\log_a n\\) of a number n\ncharacterizes our intuitions about the concept of information in a\nnumber n exactly. When we decide that 1) multisets are\nthe right formalisation of the notion of extensiveness, and 2)\nmultiplication is the right operation to express additivity, then the\nlogarithm is the only measurement function that satisfies our\nconstraints. \n\n\nWe define: \n\n\nDefinition: For all natural numbers \\(n \\in\n\\mathbb{N}^{+}\\) \n\\[\nI(n) = \\log_a n.\n\\]\n\n\nFor \\(a = 2\\) our unit of measurement is the bit \nFor \\(a = e\\) (i.e., Euler\u2019s number) our unit of measurement\nis the gnat \nFor \\(a = 10\\) our unit of measurement is the Hartley\n\n\n\n5.1.5 Measuring information and probabilities in sets of numbers\n\nFor finite sets we can now specify the amount of information we get\nwhen we know a certain element of a set conditional to knowing the set\nas a whole.\n\n\nDefinition: Suppose S is a finite set and we\nhave:  \n\\[e \\in S\\]\n\n\nthen,  \n\\[I(e \\mid S) = \\log_a |S| \\]\n\n\ni.e., the log of the cardinality of the set. \n\n\nThe bigger the set, the harder the search is, the more information we\nget when we find what we are looking for. Conversely, without any\nfurther information the probability of selecting a certain\nelement of S is \\(p_S(x) = \\frac{1}{|S|}\\). The associated\nfunction is the so-called Hartley function: \n\n\nDefinition: If a sample from a finite set S uniformly\nat random is picked, the information revealed after the outcome is\nknown is given by the Hartley function (Hartley 1928): \n\n\\[H_0(S)= \\log_a |S|\\]\n\n\n\nThe combination of these definitions gives a theorem that ties\ntogether the notions of conditional information and probability: \n\n\nUnification Theorem: If S is a finite set\nthen,  \n\\[I(x\\mid S) = H_0(S)\\]\n\n\n\nThe information about an element x of a set S\nconditional to the set is equal to the log of the probability that we\nselect this element x under uniform distribution, which is a\nmeasure of our ignorance if we know the set but not which\nelement of the set is to be selected. \n\nObservation: Note that the Hartley function unifies the\nconcepts of entropy defined by Boltzmann \\(S = k \\log W\\),\nwhere W is the cardinality of the set of micro states of system\nS, with the concept of Shannon information \\(I_S(x) =\n- \\log p(x)\\). If we consider S to be a set of messages, then\nthe probability that we select an element x from the set (i.e.,\nget a message from S ) under uniform distribution pis\n\\(\\frac{1}{|S|}\\). \\(H_0(S)\\) is also known as the Hartley\nEntropy of S. \n\nUsing these results we define the conditional amount of\ninformation in a subset of a finite set as:\n\n\nDefinition: If A is a finite set and B\nis an arbitrary subset \\(B \\subset A\\), with \\(|A|=n\\) and \\(|B|=k\\)\nwe have:  \n\\[I(B\\mid A)=\\log_a {n \\choose k}\\]\n\n\n\nThis is just an application of our basic definition of information:\nthe cardinality of the class of subsets of A with size k\nis \\({n \\choose k}\\). \n\nThe formal properties of the concept of probability are specified by\nthe Kolmogorov Axioms of Probability: \n\nDefinition: \\(P(E)\\) is the probability P that\nsome event E occurs. \\((\\Omega, F,P)\\), with \\(P(\\Omega)=1\\),\nis a probability space, with sample space \\(\\Omega\\),\nevent space and probability measure. \n\nLet \\(P(E)\\) be the probability P that some event E\noccurs. Let \\((\\Omega, F,P)\\), with \\(P(\\Omega)=1\\), be a\nprobability space, with sample space \\(\\Omega\\), event\nspace F and probability measure P.\n\nThe probability of an event is a non-negative real\nnumber\nThere is a unit of measure. The probability that one of\nthe events in the event space will occur is 1: \\(P(\\Omega= 1)\\)\nProbability is additive over sets of independent:\n\n\\[P \\left(\\bigcup^{\\infty}_{i=1} E_i \\right) = \\sum^{\\infty}_{i=1} P(E_i)\\]\n \n\n\nOne of the consequences is monotonicity: if \\(A \\subseteq B\\)\nimplies \\(P(A) \\leq P(B)\\). Note that this is the same notion of\nadditivity as defined for the concept of information. At subatomic\nlevel the Kolmogorov Axiom of additivity loses its validity in favor\nof a more subtle notion (see\n section 5.3).\n \n5.1.6 Perspectives for unification\n\nFrom a philosophical point of view the importance of this construction\nlies in the fact that it leads to an ontologically neutral concept of\ninformation based on a very limited robust base of axiomatic\nassumptions: \n\nIt is reductionist in the sense that once one\naccepts the concepts like classes and mappings, the definition of the\nconcept of Information in the context of more complex\nmathematical concepts naturally emerges. \nIt is universal in the sense that the notion of a\nset is universal and open. \nIt is semantic in the sense that the notion of a\nset itself is a semantic concept. \n It unifies a variety of notions (sets,\ncardinality, numbers, probability, extensiveness, entropy and\ninformation) in one coherent conceptual framework. \nIt is ontologically neutral in the sense that the\nnotion of a set or class does not imply any ontological constraint on\nits possible members.\n\n\nThis shows how Shannon\u2019s theory of information and\nBoltzmann\u2019s notion of entropy are rooted in more fundamental\nmathematical concepts. The notions of a set of messages or a\nset of micro states are specializations of the more general\nmathematical concept of a set. The concept of information\nalready exists on this more fundamental level. Although many open\nquestions still remain, specifically in the context of the relation\nbetween information theory and physics, perspectives on a unified\ntheory of information now look better than at the beginning of the\ntwenty-first century. \n5.1.7 Information processing and the flow of information\n\nThe definition of the amount of information in a number in therms of\nlogarithms allows us to classify other mathematical functions in terms\nof their capacity to process information. The Information\nEfficiency of a function is the difference between the amount of\ninformation in the input of a function and the amount of information\nin the output (Adriaans 2021\n [OIR]).\n It allows us to measure how information flows through a set\nof functions. We use the shorthand \\(f(\\overline{x})\\) for\n\\(f(x_1,x_2,\\dots,x_k)\\):\n\n\nDefinition: Information Efficiency of a\nFunction: Let \\(f: \\mathbb{N}^k \\rightarrow \\mathbb{N}\\) be a\nfunction of k variables. We have: \n\n the input information \\(I(\\overline{x})\\) and \n the output information \\(I(f(\\overline{x}))\\). \n The information efficiency of the expression \\( f(\\overline{x})\\)\nis \n\\[\\delta(f(\\overline{x}))= I(f(\\overline{x})) - I(\\overline{x})\\]\n \n A function f is information conserving if\n\\(\\delta(f(\\overline{x}))=0\\) i.e., it contains exactly the amount of\ninformation in its input parameters, \n it is information discarding if\n\\(\\delta(f(\\overline{x}))\\lt 0\\) and \n it has constant information if \\(\\delta(f(\\overline{x}))\n= c\\). \n it is information expanding if\n\\(\\delta(f(\\overline{x}))\\gt 0\\). \n\n\n\nIn general deterministic information processing systems do not\ncreate new information. They only process it. The\nfollowing fundamental theorem about the interaction between\ninformation and computation is due to Adriaans and Van Emde Boas\n(2011):\n\nTheorem: Deterministic programs do not expand\ninformation. \n\nThis is in line with both Shannon\u2019s theory and Kolmogorov\ncomplexity. The outcome of a deterministic program is always the same,\nso the probability of the outcome is 1 which gives under\nShannon\u2019s theory, 0 bits of new information. Likewise\nfor Kolmogorov complexity, the output of a program can never be more\ncomplex than the length of the program itself, plus a constant. This\nis analyzed in depth in Adriaans and Van Emde Boas (2011). In a\ndeterministic world it is the case that if:  \n\\[\\texttt{program(input)=output}\\]\n then\n\n\\[I(\\texttt{output}) \\leq\nI(\\texttt{program}) + I(\\texttt{input})\\]\n\n\nThe essence of information is uncertainty and a message that occurs\nwith probability \u201c1\u201d contains no information. The fact\nthat it might take a long time to compute the number is irrelevant as\nlong as the computation halts. Infinite computations are studied in\nthe theory of Scott domains (Abramsky & Jung 1994). \n\nEstimating the information efficiency of elementary functions is not\ntrivial. The primitive recursive functions (see entry on\n recursive functions)\n have one information expanding operation, the increment\noperation, one information discarding operation,\nchoosing, all the others are information neutral. The\ninformation efficiency of more complex operations is defined by a\ncombination of counting and choosing. From an information efficiency\npoint of view the elementary arithmetical functions are complex\nfamilies of functions that describe computations with the same\noutcome, but with different computational histories.\n\nSome arithmetical operations expand information, some have constant\ninformation and some discard information. During the execution of\ndeterministic programs expansion of information may take place, but,\nif the program is effective, the descriptive complexity of the output\nis limited. The flow of information is determined by the succession of\ntypes of operations, and by the balance between the complexity of the\noperations and the number of variables. \n\nWe briefly discuss the information efficiency of the two basic\nrecursive functions on two variables and their coding\npossibilities:\n\nAddition Addition is associated with information\nstorage in terms of sequences or strings of symbols. It is\ninformation discarding for natural numbers bigger than 1. We\nhave \\(\\delta(a + b) \\lt 0\\) since \\(\\log (a + b) \\lt \\log a + \\log\nb\\). Still, addition has information preserving qualities. If we add\nnumbers with different log units we can reconstruct the frequency of\nthe units from the resulting number: \n\\[\\begin{align}\n232 & = 200 + 30 + 2 \\\\\n& =  (2 \\times 10^2) + (3 \\times 10^1) + (2 \\times 10^0)\\\\\n& = 100 + 100 + 10 + 10 + 10 + 1 + 1\n\\end{align}\n\\]\n \n\nSince the information in the building blocks, 100, 10 and 1, is given\nthe number representation can still be reconstructed. This implies\nthat natural numbers code in terms of addition of powers of \nk in principle two types of information: value and\nfrequency. We can use this insight to code complex typed\ninformation in single natural numbers. Basically it allows us\nto code any natural numbers in a string of symbols of length \\(\\lceil\n\\log_k n \\rceil \\), which specifies a quantitative measure for the\namount of information in a number in terms of the length of its code.\nSee\n section 3.3\n for a historical analysis of the importance of the discovery of\nposition systems for information theory. \n\nMultiplication is by definition information\nconserving. We have: \\(\\delta(a \\times b) = 0\\), since \\(\\log (a\n\\times b) = \\log a + \\log b\\). Still multiplication does not preserve\nall information in its input: the order of the operation is lost. This\nis exactly what we want from an operator that characterizes an\nextensive measure: only the extensive qualities of the\nnumbers are preserved. If we multiply two numbers \\(3 \\times 4\\), then\nthe result, 12, allows us to reconstruct the original computation, in\nso far as we can reduce all its components to their most elementary\nvalues: \\(2 \\times 2 \\times 3 = 12\\). This leads to the observation\nthat some numbers act as information building blocks of other\nnumbers, which gives us the concept of a prime number: \n\nDefinition: A prime number is a number that\nis only divisible by itself or 1. \n\nThe concept of a prime number gives rise to the Fundamental\nTheorem of Arithmetic:\n\nTheorem: Every natural number n greater than 1\nis a product of a multiset \\(A_p\\) of primes, and this multiset is\nunique for n. \n\nThe Fundamental Theorem of Arithmetic can be seen as a theorem about\nconservation of information: for every natural number there is a set\nof natural numbers that contains exactly that same amount of\ninformation. The factors of a number form a so-called\nmultiset: a set that may contain multiple copies of the same\nelement: e.g., the number 12 defines the multiset \\(\\{2,2,3\\}\\) in\nwhich the number 2 occurs twice. This makes multisets a powerful\ndevice for coding information since it codes qualitative information\n(i.e., the numbers 2 and 3) as well as quantitative information (i.e.,\nthe fact that the number 2 occurs twice and the number 3 only once).\nThis implies that natural numbers in terms of multiplication of\nprimes also code two types of information: value and\nfrequency. Again we can use this insight to code complex\ntyped information in single natural numbers. \n5.1.8 Information, primes, and factors\n\nPosition based number representations using addition of powers are\nstraightforward and easy to handle and form the basis of most of our\nmathematical functions. This is not the case for coding systems based\non multiplication. Many of the open questions in the philosophy of\nmathematics and information arise in the context of the concepts of\nthe Fundamental Theorem of Arithmetic and Primes. We give a short\noverview: \n\n\n(Ir)regularity of the set of primes.\n\nSince antiquity it is known that there is an infinite number of\nprimes. The proof is simple. Suppose the set of primes P is\nfinite. Now multiply all elements of P and add 1. The resulting\nnumber cannot be divided by any member of P, so P is\nincomplete. An estimation of the density of the prime numbers given by\nthe Prime Number Theorem (see entry in Encyclopaedia\nBritannica on Prime Number Theorem\n [OIR]).\n It states that the gaps between primes in the set of natural numbers\nof size n is roughly \\( \\ln n\\), where \\(\\ln\\) is the natural\nlogarithm based on Euler\u2019s number e. A refinement of the\ndensity estimation is given by the so-called Riemannn\nhypothesis, formulated by him in 1859 (Goodman and Weisstein 2019\n [OIR]),\n which is commonly regarded as deepest unsolved problems in\nmathematics, although most mathematicians consider the hypothesis to\nbe true.\n\n(In)efficiency of Factorization.\n\nSince multiplication conserves information the function is, to an\nextent, reversible. The process of finding the unique set of primes\nfor a certain natural number n is called\nfactorization. Observe that the use of the term\n\u201conly\u201d in the definition of a prime number implies that\nthis is in fact a negative characterization: a number\nn is prime if there exists no number between 1 and n\nthat divides it. This gives us an effective procedure for\nfactorization of a number n (simply try to divide n by\nall numbers between 1 and \\(n)\\), but such techniques are not\nefficient.\n\nIf we use a position system to represent the number n then the\nprocess of identifying factors of n by trial and error will\ntake a deterministic computer program at most n trials which\ngives a computation time exponential in the length of the\nrepresentation of the number which is \\(\\lceil \\log n \\rceil \\).\nFactorization by trial and error of a relatively simple number, of,\nsay, two hundred digits, which codes a rather small message, could\neasily take a computer of the size of our whole universe longer than\nthe time passed since the big bang. So, although theoretically\nfeasible, such algorithms are completely unpractical. \n\nFactorization is possibly an example of so-called trapdoor\none-to-one function which is easy to compute from one side but very\ndifficult in its inverse. Whether factorization is really difficult,\nremains an open question, although most mathematicians believe the\nproblem to be hard. Note that factorization in this context can be\nseen as the process of decoding a message. If factorization is hard it\ncan be used as an encryption technique. Classical encryption\ntechniques, like RSA, are based on multiplying codes with large prime\nnumbers. Suppose Alice has a message encoded as a large number\nm and she knows Bob has access to a large prime p. She\nsends the number \\(p \\times m = n\\) to Bob. Since Bob knows p\nhe can easily reconstruct m by computing \\(m = n/p\\). Since\nfactorization is difficult any other person that receives the message\nn will have a hard time reconstructing m.\n\nPrimality testing versus Factorization.\n\nAlthough at this moment efficient techniques for factorization on\nclassical computers are not known to exist, there is an efficient\nalgorithm that decides for us whether a number is prime or not: the\nso-called AKS primality test (Agrawal et al. 2004). So, we might know\na number is not prime, while we still do not have access to its set of\nfactors. \n\nClassical- versus Quantum Computing.\n\nTheoretically factorization is efficient on quantum computers using\nShor\u2019s algorithm (Shor 1997). This algorithm has a non-classical\nquantum subroutine, embedded in a deterministic classical program.\nCollections of quantum bits can be modeled in terms of complex higher\ndimensional vector-spaces, that, in principle, allow us to analyze an\nexponential number \\(2^n\\) of correlations between collections of\nn objects. Currently it is not clear whether larger quantum\ncomputers will be stable enough to facilitate practical applications,\nbut that the world at quantum level has relevant computational\npossibilities can not be doubted anymore, e.g., quantum random\ngenerators are available as a commercial product (see\nWikipedia entry on Hardware random number generator\n [OIR]).\n As soon as viable quantum computers become available almost all of\nthe current encryption techniques become useless, although they can be\nreplaced by quantum versions of encryption techniques (see the entry\non\n Quantum Computiong).\n\n\nThere is an infinite number of observations we can make about the set\n\\(\\mathbb{N}\\) that are not implied directly by the axioms, but\ninvolve a considerable amount of computation. \n5.1.9 Incompleteness of arithmetic\n\nIn a landmark paper in 1931 Kurt G\u00f6del proved that any consistent\nformal system that contains elementary arithmetic is fundamentally\nincomplete in the sense that it contains true statements that cannot\nbe proved within the system. In a philosophical context this implies\nthat the semantics of a formal system rich enough to contain\nelementary mathematics cannot be defined in terms of mathematical\nfunctions within the system, i.e., there are statements that contain\nsemantic information about the system in the sense of being\nwell-formed, meaningful and truthful\nwithout being provable. \n\nCentral is the concept of a Recursive Function. (see entry on\n recursive functions).\n Such functions are defined on numbers. G\u00f6del\u2019s notion of a\nrecursive function is closest to what we would associate with\ncomputation in every day life. Basically they are elementary\narithmetical functions operating on natural numbers like addition,\nsubtraction, multiplication and division and all other functions that\ncan be defined on top of these. \n\nWe give the basic structure of the proof. Suppose F is a formal\nsystem, with the following components: \n\n It has a finite set of symbols\n It has a syntax that enables us to combine the symbols in to\nwell-formed formulas\n It has a set of deterministic rules that allows us to derive new\nstatements from given statements\n It contains elementary arithmetic as specified by Peano\u2019s\naxioms (see section\n 5.1.3\n above).\n\n\nAssume furthermore that F is consistent, i.e., it will never\nderive false statements form true ones. In his proof G\u00f6del used\nthe coding possibilities of multiplication to construct an image of\nthe system (see the discussion of\n G\u00f6del numbering\n from the entry on G\u00f6del\u2019s Incompleteness Theorems).\nAccording to the fundamental theorem of arithmetic any number can be\nuniquely factored in to its primes. This defines a one-to-one\nrelationship between multisets of numbers and numbers: the number 12\ncan be constructed on the basis of the multiset \\(\\{2,2,3\\}\\) as\n\\(12=2 \\times 2\\times 3\\) and vice versa. This allows us to code any\nsequence of symbols as a specific individual number in the following\nway:\n\n A unique number is assigned to every symbol\n Prime numbers locate the position of the symbol in a string\n The actual number of the same primes in the set of prime factors\ndefines the symbol\n\n\nOn the basis of this we can code any sequence of symbols as a\nso-called G\u00f6del number, e.g., the number:  \n\\[2 \\times 3 \\times 3 \\times 5 \\times 5 \\times 7 = 3150\\]\n\n\ncodes the multiset \\(\\{2,3,3,5,5,7\\}\\), which represents the string\n\u201cabba\u201d under the assumption \\(a=1\\), \\(b=2\\). With this\nobservation conditions close to those that lead to the paradox of\nRussel are satisfied: elementary arithmetic itself is rich enough to\nexpress: Universality, Negation, and\nSelf-reference.\n\nSince arithmetic is consistent this does not lead to paradoxes, but to\nincompleteness. By a construction related to the liars paradox\nG\u00f6del proved that such a system must contain statements that are\ntrue but not provable: there are true sentences of the form \u201cI\nam not provable\u201d. \n\nTheorem: Any formal system that contains elementary\narithmetic is fundamentally incomplete. It contains\nstatements that are true but not provable. \n\nIn the context of philosophy of information the incompleteness of\nmathematics is a direct consequence of the rich possibilities of the\nnatural numbers to code information. In principle any deterministic\nformal system can be represented in terms of elementary arithmetical\nfunctions. Consequently, If such a system itself contains arithmetic\nas a sub system, it contains a infinite chain of endomorphisms (i.e.,\nimages of itself). Such a system is capable of reasoning about its own\nfunctions and proofs but since it is consistent (and thus the\nconstruction of paradoxes is not possible within the system) it is by\nnecessity incomplete. \n5.2 Information and Symbolic Computation\n\nRecursive functions are abstract relations defined on natural numbers.\nIn principle they can be defined without any reference to space and\ntime. Such functions must be distinguished from the\noperations that we use to compute them. These operations\nmainly depend on the type of symbolic representations that we\nchoose for them. We can represent the number seven as unary number\n\\(|||||||\\), binary number 111, Roman number VII, or Arabic number 7\nand depending on our choice other types of sequential symbol\nmanipulation can be used to compute the addition two plus five is\nseven, which can be represented as: \n\\[\n\\begin{align}\n|| + ||||| & = ||||||| \\\\\n10 + 101 &  = 111 \\\\\n\\textrm{II} + \\textrm{V} & = \\textrm{VII}\\\\\n2 + 5 &= 7 \\\\\n\\end{align}\n\\]\n Consequently we can\nread these four sentences as four statements of the same\nmathematical truth, or as statements specifying the results of four\ndifferent operations. \n\n\nObservation: There are (at least) two different perspectives\nfrom which we can study the notion of computation. The semantics of\nthe symbols is different under these interpretations. \n\nThe Recursive Function Paradigm studies\ncomputation in terms of abstract functions on natural\nnumbers outside space and time. When interpreted as a\nmathematical fact, the \\(+\\) sign in \\(10 + 101 = 111\\) signifies the\nmathematical function called addition and the \\(=\\) sign\nspecifies equality. \nThe Symbol Manipulation Paradigm studies\ncomputation in terms of sequential operations on spatial\nrepresentations of strings of symbols. When interpreted as an\noperation the \\(+\\) sign in \\(10 + 101 = 111\\) signifies the input\nfor a sequential process of symbol manipulation and the \\(=\\)\nsign specifies the result of that operation or\noutput. Such an algorithm could have the following form:\n\n\\[\n\\begin{aligned}\n\\tt{   10}\\\\ \n\\tt{+ 101}\\\\ \\hline\n\\tt{  111}\n\\end{aligned}\\]\n \n\n\n\nThis leads to the following tentative definition:\n\nDefinition: Deterministic Computing on a Macroscopic\nScale can be defined as the local, sequential, manipulation of\ndiscrete objects according to deterministic rules. \n\nIn nature there are many other ways to perform such computations. One\ncould use an abacus, study chemical processes or simply manipulate\nsequences of pebbles on a beach. The fact that the objects we\nmanipulate are discrete together with the observation that the dataset\nis self-referential implies that the data domain is in principle\nDedekind Infinite:\n\nDefinition: A set S is Dedekind Infinite if it\nhas a bijection \\(f: S \\rightarrow S^{\\prime}\\) to a proper subset\n\\(S^{\\prime} \\subset S\\). \n\nSince the data elements are discrete and finite the data domain will\nbe countable infinite and therefore isomorphic to the set of natural\nnumbers. \n\nDefinition: An infinite set S is\ncountable if there exists a bijection with the set of natural\nnumbers \\(\\mathbb{N}\\). \n\nFor infinite countable sets the notion of information is defined as\nfollows: \n\n\nDefinition: Suppose S is countable and\ninfinite and the function \\(f:S \\rightarrow \\mathbb{N}\\) defines a\none-to-one correspondence, then: \n\\[I(a\\mid S,f) = \\log f(a)\\]\n i.e., the amount of\ninformation in an index of a in S given f. \n\n\nNote that the correspondence f is specified explicitly. As soon\nas such an index function is defined for a class of objects in the\nreal world, the manipulation of these objects can be interpreted a\nform of computing.\n5.2.1 Turing machines\n\nOnce we choose a finite set of symbols and our operational rules the\nsystem starts to produce statements about the world.\n\n\nObservation: The meta-sentence:\n\n\nThe sign \u201c0\u201d is the symbol for zero.\n\n\nspecifies semantic information in the same sense as the\nstatement \\(e \\in A\\) does for sets (see\n section 6.6).\n The statement is wellformed, meaningful and\ntruthful. \n\n\nWe can study symbol manipulation in general on an abstract level,\nwithout any semantic implications. Such a theory was published by Alan\nTuring (1912\u20131954). Turing developed a general theory of\ncomputing focusing on the actual operations on symbols a mathematician\nperforms (Turing 1936). For him a computer was an abstraction of a\nreal mathematician sitting behind a desk, receiving problems written\ndown on an in-tray (the inut), solving them according to fixed rules\n(the process) and leaving them to be picked up in an out-tray (the\noutput). \n\nTuring first formulated the notion of a general theory of computing\nalong these lines. He proposed abstract machines that operate on\ninfinite tapes with three symbols: blank \\((b)\\), zero \\((0)\\) and one\n\\((1)\\). Consequently the data domain for Turing machines is the set\nof relevant tape configurations, which can be associated with the set\nof binary strings, consisting of zero\u2019s and one\u2019s. The\nmachines can read and write symbols on the tape and they have a\ntransition function that determines their actions under various\nconditions. On an abstract level Turing machines operate like\nfunctions. \n\nDefinition: If \\(T_i\\) is a Turing machine\nwith index i and x is a string of zero\u2019s and\none\u2019s on the tape that function as the input then\n\\(T_i(x)\\) indicates the tape configuration after the machine has\nstopped, i.e., its output. \n\nThere is an infinite number of Turing machines. Turing discovered that\nthere are so-called universal Turing machines \\(U_j\\) that can emulate\nany other Turing machine \\(T_i\\). \n\nDefinition: The expression \\(U_j(\\overline{T_i}x)\\)\ndenotes the result of the emulation of the computation \\(T_i(x)\\) by\n\\(U_j\\) after reading the self-delimiting description\n\\(\\overline{T_i}\\) of machine \\(T_j\\). \n\nThe self-delimiting code is necessary because the input for \\(U_j\\) is\ncoded as one string \\(\\overline{T_i}x\\). The universal machine \\(U_j\\)\nseparates the input string \\(\\overline{T_i}x\\) in to its two\nconstituent parts: the description of the machine \\(\\overline{T_i}\\)\nand the input for this machine x. \n\nThe self-referential nature of general computational systems allows us\nto construct machines that emulate other machines. This suggests the\npossible existence of a \u2018super machine\u2019 that emulates all\npossible computations on all possible machines and predicts their\noutcome. Using a technique called diagonalization, where one analyzes\nan enumeration of all possible machines running on descriptions of all\npossible machines, Turing proved that such a machine can not exist.\nMore formally: \n\nTheorem: There is no Turing machine that predicts for\nany other Turing machine whether it stops on a certain input or not.\n\n\nThis implies that for a certain universal machine \\(U_i\\) the set of\ninputs on which it stops in finite time, is uncomputable. In recent\nyears the notion of infinite computations on Turing machines has also\nbeen studied (Hamkins and Lewis 2000.) Not every machine will stop on\nevery input, but in some case infinite computations compute useful\noutput (consider the infinite expansion of the number pi). \n\nDefinition: The Halting set is the set of\ncombinations of Turing machines \\(T_i\\) and inputs x such that\nthe computation \\(T_i(x)\\) stops. \n\nThe existence of universal Turing machines indicates that the class\nembodies a notion of universal computing: any computation\nthat can be performed on a specific Turing machine can also be\nperformed on any other universal Turing machine. This is the\nmathematical foundation of the concept of a general programmable\ncomputer. These observations have bearing on the theory of\ninformation: certain measures of information, like Kolmogorov\ncomplexity, are defined, but not computable. \n\nThe proof of the existence uncomputable functions in the class of\nTuring machines is similar to the incompleteness result of G\u00f6del\nfor elementary arithmetic. Since Turing machines were defined to study\nthe notion of computation and thus contain elementary arithmetic. The\nclass of Turing machines is in itself rich enough to express:\nUniversality, Negation and Self-reference.\nConsequently Turing machines can model universal negative statements\nabout themselves. Turing\u2019s uncomputability proof is also\nmotivated by the liars paradox, and the notion of a machine that stops\non a certain input is similar to the notion of a proof that exists for\na certain statement. At the same time Turing machines satisfy the\nconditions of G\u00f6del\u2019s theorem: they can be modeled as a\nformal system F that contains elementary Peano arithmetic.\n\nObservation: Since they can emulate each other, the\nRecursive Function Paradigm and the Symbol Manipulation\nParadigm have the same computational strength. Any\nfunction that can be computed in one paradigm can also by definition\nbe computed in the other. \n\nThis insight can be generalized:\n\nDefinition: An infinite set of computational\nfunctions is Turing complete if it has the same computational\npower as the general class of Turing machines. In this case it is\ncalled Turing equivalent. Such a system is, like the class of Turing\nmachines, universal: it can emulate any computable function. \n\nThe philosophical implications of this observation are strong and\nrich, not only for the theory of computing but also for our\nunderstanding of the concept of information. \n5.2.2 Universality and invariance\n\nThere is an intricate ration between the notion of universal computing\nand that of information. Precisely the fact that Turing Systems are\nuniversal allows us to say that they process information, because\ntheir universality entails invariance:\n\n\nSmall Invariance Theorem: The concept of information\nin a string x measured as the length of the smallest string of\nsymbols s of a program for a universal Turing machine U\nsuch that \\(U(s)= x\\) is invariant, modulo an additive constant, under\nselection of different universal Turing machines \n\nProof: The proof is simple and relevant for\nphilosophy of information. Let \\(l(x)\\) be the length of the string of\nsymbols x. Suppose we have two different universal Turing\nmachines \\(U_j\\) and \\(U_k\\). Since they are universal they can both\nemulate the computation \\(T_i(x)\\) of Turing machine \\(T_i\\) on input\nx: \n\\[U_j(\\overline{T}_i^jx)\\]\n \n\\[U_k(\\overline{T}_i^kx)\\]\n\n\nHere \\(l(\\overline{T}_i^j)\\) is the length of the code for \\(T_i\\) on\n\\(U_j\\) and \\(l(\\overline{T}_i^k)\\) is the length of the code for\n\\(T_i\\) on \\(U_k\\). Suppose \\(l(\\overline{T}_i^jx) \\ll\nl(\\overline{T}_i^kx)\\), i.e., the code for \\(T_i\\) on \\(U_k\\) is much\nless efficient that on \\(U_j\\). Observe that the code for \\(U_j\\) has\nconstant length, i.e., \\(l(\\overline{U}_j^k)=c\\). Since \\(U_k\\) is\nuniversal we can compute:  \n\\[U_k(\\overline{U}_j^k \\ \\overline{T}_i^jx)\\]\n\n\nThe length of the input for this computation is:  \n\\[l(\\overline{U}_j^k \\ \\overline{T}_i^jx) = c + l(\\overline{T}_i^jx)\\]\n\n\nConsequently the specification of the input for the computation\n\\(T_i(x)\\) on the universal machine \\(U_k\\) never needs to longer than\na constant. \\(\\Box\\) \n\n\nThis proof forms the basis of the theory of Kolmogorov complexity and\nis originally due to Solomonoff (1964a,b) and discovered independently\nby Kolmogorov (1965) and Chaitin (1969). Note that this notion of\ninvariance can be generalized over the class of Turing Complete\nSystems:\n\n\nBig Invariance Theorem: The concept of information\nmeasured in terms of the length of the input of a computation is\ninvariant, modulo an additive constant, for for Turing Complete\nsystems. \n\nProof: Suppose we have a Turing Complete system\nF. By Definition any computation \\(T_i(x)\\) on a Turing machine\ncan be emulated in F and vice versa. There will be a special\nuniversal Turing machine \\(U_F\\) that emulates the computation\n\\(T_i(x)\\) in F: \\(U_F(\\overline{T}_i^Fx)\\). In principle\n\\(\\overline{T}_i^F\\) might use a very inefficient way to code programs\nsuch that \\(\\overline{T}_i^F\\) can have any length. Observe that the\ncode for any other universal machine \\(U_j\\) emulated by \\(U_F\\) has\nconstant length, i.e., \\(l(\\overline{U}_j^F)=c\\). Since \\(U_F\\) is\nuniversal we can also compute:  \n\\[U_F(\\overline{U}_j^F \\ \\overline{T}_i^jx)\\]\n\n\nThe length of the input for this computation is: \n\\[l(\\overline{U}_j^F \\ \\overline{T}_i^jx) = c + l(\\overline{T}_i^jx)\\]\n\nConsequently the specification of the input for the computation\n\\(T_i(x)\\) on the universal machine \\(U_F\\) never needs to be longer\nthan a constant. \\(\\Box\\) \n\n\nHow strong this result is becomes clear when we analyze the class of\nTuring complete systems in more detail. In the first half of the\ntwentieth century three fundamentally different proposals for a\ngeneral theory of computation were formulated: G\u00f6del\u2019s\nrecursive functions ( G\u00f6del 1931), Turing\u2019s automata\n(Turing 1937) and Church\u2019s Lambda Calculus (Church 1936). Each\nof these proposals in its own way clarifies aspects of the notion of\ncomputing. Later much more examples followed. The class of Turing\nequivalent systems is diverse. Apart from obvious candidates like all\ngeneral purpose programming languages (C, Fortran, Prolog, etc.) it\nalso contains some unexpected elements like various games (e.g.,\nMagic: The Gathering [Churchill 2012\n OIR]).\n The table below gives an overview of some conceptually interesting\nsystems: \n\n\nAn overview of some Turing Complete systems \n\n\nSystem\nData Domain \n\nGeneral Recursive Functions\nNatural Numbers \n\nTuring machines and their generalizations\nStrings of symbols \n\nDiophantine Equations\nIntegers \n\nLambda calculus\nTerms \n\nType-0 languages\nSentences \n\nBilliard Ball Computing\nIdeal Billiard Balls \n\nCellular automata\nCells in one dimension \n\nConway\u2019s game of life \nCells in two dimensions \n\n\n\nWe make the following: \n\nObservation: The class of Turing equivalent systems is open,\nbecause it is defined in terms of purely operational mappings between\ncomputations. \n\nA direct consequence of this observation is: \n\nObservation: The general theory of computation and\ninformation defined by the class of Complete Turing machines is\nontologically neutral. \n\nIt is not possible to derive any necessary qualities of computational\nsystems and data domains beyond the fact that they are general\nmathematical operations and structures. Data domains on which Turing\nequivalent systems are defined are not necessarily physical, nor\ntemporal, nor spatial, not binary or digital. At any moment a new\nmember for the class can be introduced. We know that there are\ncomputational systems that are weaker than the class of Turing\nmachines (e.g., regular languages). We cannot rule out the possibility\nthat one-day we come across a system that is stronger. The thesis that\nsuch a system does not exist is known as the Church-Turing thesis (see\nentry on\n Church-Turing thesis):\n\nChurch-Turing Thesis: The class of Turing machines\ncharacterizes the notion of algorithmic computing exactly. \n\nWe give an overview of the arguments for and against the thesis:\n\nArguments in favor of the thesis: The theory of Turing\nmachines seems to be the most general theory possible that we can\nformulate since it is based on a very limited set of assumptions about\nwhat computing is. The fact that it is universal also points in the\ndirection of its generality. It is difficult to conceive in what sense\na more powerful system could be \u201cmore\u201d universal. Even if\nwe could think of such a more powerful system, the in- and output for\nsuch a system would have to be finite and discrete and the computation\ntime also finite. So, in the end, any computation would have the form\nof a finite function between finite data sets, and, in principle, all\nsuch relations can be modeled on Turing machines. The fact that all\nknown systems of computation we have defined so far have the same\npower also corroborates the thesis. \n\nArguments against the thesis: The thesis is, in its present\nform, unprovable. The class of Turing Complete systems is open. It is\ndefined on the basis of the existence of equivalence relations between\nknown systems. In this sense it does not define the notion of\ncomputing intrinsically. It doesn\u2019t not provide us with a\nphilosophical theory that defines what computing exactly is.\nConsequently it does not allow us to exclude any system from the class\na priori. At any time a proposal for a notion of computation\nmight emerge that is fundamentally stronger. What is more, nature\nprovides us with stronger notions of computing in the form of quantum\ncomputing. Quantum bits are really a generalization of the normal\nconcept of bits that is associated with symbol manipulation, although\nin the end quantum computing does not seem to necessitate us to\nredefine the notion of computing so far. We can never rule out that\nresearch in physics, biology or chemistry will define systems that\nwill force us to do so. Indeed various authors have suggested such\nsystems but there is currently no consensus on convincing candidates\n(Davis 2006). Dershowitz and Gurevich (2008) claim to have vindicated\nthe hypothesis, but this result is not generally accepted (see the\ndiscussion on \u201cComputability \u2013 What would it mean to\ndisprove the Church-Turing thesis\u201d, in the\n Other Internet Resources [OIR]).\n \n\nBeing Turing complete seems to be quite a natural condition for a\n(formal) system. Any system that is sufficiently rich to represent the\nnatural numbers and elementary arithmetical operations is Turing\ncomplete. What is needed is a finite set of operations defined on a\nset of discrete finite data elements that is rich enough to make the\nsystem self-referential: its operations can be described by its data\nelements. This explains, in part, why we can use mathematics to\ndescribe our world. The abstract notion of computation defined as\nfunctions on numbers in the abstract world mathematics and the\nconcrete notion of computing by manipulation objects in our every day\nworld around us coincide. The concepts of information end computation\nimplied by the Recursive Function Paradigm and the Symbol\nManipulation Paradigm are the same. \n\nObservation: If one accepts the fact that the Church-Turing\nthesis is open, this implies that the question about the existence of\na universal notion of information is also open. At this stage of the\nresearch it is not possible to specify the a priori\nconditions for such a general theory. \n5.3 Quantum Information and Beyond\n\nWe have a reasonable understanding of the concept of classical\ncomputing, but the implications of quantum physics for computing and\ninformation may determine the philosophical research agenda for\ndecades to come if not longer. Still it is already clear that the\nresearch has repercussions for traditional philosophical positions:\nthe Laplacian view (Laplace 1814 [1902]) that the universe is\nessentially deterministic seems to be falsified by empirical\nobservations. Quantum random generators are commercially available\n(see Wikipedia entry on Hardware random number generator\n [OIR])\n and quantum fluctuations do affect neurological, biological and\nphysical processes at a macroscopic scale (Albrecht & Phillips\n2014). Our universe is effectively a process that generates\ninformation permanently. Classical deterministic computing seems to be\ntoo weak a concept to understand its structure. \n\nStandard computing on a macroscopic scale can be defined as local,\nsequential, manipulation of discrete objects according to\ndeterministic rules. Is has a natural interpretation in\noperations on the set of natural numbers N and a natural\nmeasurement function in the log operation \\(\\log: \\mathbb{N}\n\\rightarrow \\mathbb{R}\\) associating a real number to every natural\nnumber. The definition gives us an adequate information measure for\ncountable infinite sets, including number classes like the integers\n\\(\\mathbb{Z}\\), closed under subtraction, and the rational\nnumbers \\(\\mathbb{Q}\\), closed under division.\n\nThe operation of multiplication with the associated\nlogarithmic function characterizes our intuitions about\nadditivity of the concept of information exactly. It leads to a\nnatural bijection between the set of natural numbers \\(\\mathbb{N}\\)\nand the set of multisets of numbers (i.e., sets of prime factors). The\nnotion of a multiset is associated with the properties of\ncommutativity and associativity. This program can be\nextended to other classes of numbers when we study division algebras\nin higher dimensions. The following table gives an overview of some\nrelevant number classes together with the properties of the\noperation of multiplication for these classes: \n\n\nNumber Class\nSymbol\nDimen\u00adsions\nCoun\u00adtable\nLinear\nCommu\u00adtative\nAssoci\u00adative \n\nNatural numbers\n\\(\\mathbb{N}\\)\n1\nYes\nYes\nYes\nYes \n\n Integers\n\\(\\mathbb{Z}\\)\n1\nYes\nYes\nYes\nYes \n\nRational numbers\n\\(\\mathbb{Q}\\)\n1\nYes\nYes\nYes\nYes \n\nReal numbers\n\\(\\mathbb{R}\\)\n1\nNo\nYes\nYes\nYes \n\nComplex numbers\n\\(\\mathbb{C}\\)\n2\nNo\nNo\nYes\nYes \n\n Quaternions \n\\(\\mathbb{H}\\)\n4\nNo\nNo\nNo\nYes \n\n Octonions \n\\(\\mathbb{O}\\)\n8\nNo\nNo\nNo\nNo \n\n\nThe table is ordered in terms of increasing generality. Starting from\nthe set of natural numbers \\(\\mathbb{N}\\), various extensions are\npossible taking into account closure under subtraction,\n\\(\\mathbb{Z}\\), and division, \\(\\mathbb{Q}\\). This are the number\nclasses for which we have adequate finite symbolic representations on\na macroscopic scale. For elements of the real numbers \\(\\mathbb{R}\\)\nsuch a representations are not available. The real numbers\n\\(\\mathbb{R}\\) introduce the aspect of manipulation of infinite\namounts of information in one operation.\n\nObservation: For almost all \\(e \\in \\mathbb{R}\\) we\nhave \\(I(e) = \\infty\\). \n\nMore complex division algebras can be defined when we introduce\nimaginary numbers as negative squares \\(i^2 = -1\\). We can now define\ncomplex numbers: \\(a + bi\\), where a is the real part and\n\\(bi\\) the imaginary part. Complex numbers can be interpreted as\nvectors in a two dimensional plane. Consequently they lack the notion\nof a strict linear order between symbols. Addition is quite\nstraightforward: \n\\[(a + bi) + (c + di) = (a + b) + (c + d)i\\]\n\n\nMultiplication follows the normal distribution rule but the result is\nless intuitive since it involves a negative term generated by\n\\(i^2\\): \n\\[(a + bi) (c + di) = (ac - bd) + (bc + ad)i\\]\n\n\nIn this context multiplication ceases to be a purely extensive\noperation:\n\nMore complicated numbers systems with generalizations of this type of\nmultiplication in 4 and 8 dimensions can be defined. Kervaire (1958)\nand Bott & Milnor (1958) independently proved that the only four\ndivision algebras built on the reals are \\(\\mathbb{R}\\),\n\\(\\mathbb{C}\\), \\(\\mathbb{H}\\) and \\(\\mathbb{O}\\), so the table gives\na comprehensive view of all possible algebra\u2019s that define a\nnotion of extensiveness. For each of the number classes in the table a\nseparate theory of information measurement, based on the properties of\nmultiplication, can be developed. For the countable classes\n\\(\\mathbb{N}\\), \\(\\mathbb{Z}\\) and \\(\\mathbb{Q}\\) these theories ware\nequivalent to the standard concept of information implied by the\nnotion of Turing equivalence. Up to the real numbers these theories\nsatisfy our intuitive notions of extensiveness of information. For\ncomplex numbers the notion of information efficiency of\nmultiplication is destroyed. The quaternions lack the property of\ncommutativity and the octonions that of\nassociativity. These models are not just abstract\nconstructions since the algebras play an important role in our\ndescriptions of nature:\n\nComplex numbers are used to specify the mathematical models of\nquantum physics (Nielsen & Chuang 2000).\nQuaternions do the same for Einstein\u2019s special theory of\nrelativity (De Leo 1996).\nSome physicists believe octonions form a theoretical basis for a\nunified theory of strong and electromagnetic forces (e.g., Furey\n2015).\n\n\nWe briefly discuss the application of vector spaces in quantum\nphysics. Classical information is measured in bits. Implementation of\nbits in nature involves macroscopic physical systems with at least two\ndifferent stable states and a low energy reversible transition process\n(i.e., switches, relays, transistors). The most fundamental way to\nstore information in nature on an atomic level involves qubits. The\nqubit is described by a state vector in a two-level quantum-mechanical\nsystem, which is formally equivalent to a two-dimensional vector space\nover the complex numbers (Von Neumann 1932; Nielsen & Chuang\n2000). Quantum algorithms have, in some cases, a fundamentally lower\ncomplexity (e.g., Shor\u2019s algorithm for factorization of integers\n(Shor 1997)).\n\n\nDefinition: The quantum bit, or\nqubit, is a generalization of the classical bit. The quantum\nstate of qubit is represented as the linear superposition of two\northonormal basis vectors: \n\\[\\ket{0} = \\begin{bmatrix}1 \\\\ 0 \\end{bmatrix}, \\ket{1} =\n\\begin{bmatrix}0 \\\\ 1 \\end{bmatrix} \\]\n\n\nHere the so-called Dirac or \u201cbra-ket\u201d notion is used:\nwhere \\(\\ket{0}\\) and \\(\\ket{1}\\) are pronounced as \u201cket\n0\u201d and \u201cket 1\u201d. The two vectors together form the\ncomputational basis \\(\\{\\ket{0}, \\ket{1}\\}\\), which defines a\nvector in a two-dimensional Hilbert space. A combination of\nn qubits is represented by a superposition vector in a\n\\(2^n\\) dimensional Hilbert space, e.g.: \n\\[\\ket{00} =  \\begin{bmatrix}1\n\\\\\n 0 \\\\\n 0 \\\\\n 0 \n\\end{bmatrix}, \\ket{01} =  \\begin{bmatrix}\n0 \\\\\n 1 \\\\\n 0 \\\\\n 0 \n\\end{bmatrix}, \\ket{10} =  \\begin{bmatrix}0\n\\\\\n 0 \\\\\n 1 \\\\\n 0 \n\\end{bmatrix}, \\ket{11} =  \\begin{bmatrix}\n0 \\\\\n 0 \\\\\n 0 \\\\\n 1 \n\\end{bmatrix} \\]\n\n\nA pure qubit is a coherent superposition of the basis states:\n \n\\[\\ket{\\psi} = \\alpha\\ket{0} + \\beta\\ket{1}\\]\n\n\nwhere \\(\\alpha\\) and \\(\\beta\\) are complex numbers, with the\nconstraint: \n\\[|\\alpha|^2 + |\\beta|^2 = 1\\]\n\n\nIn this way the values can be interpreted as probabilities:\n\\(|\\alpha|^2\\) is the probability that the qubit has value 0 and\n\\(|\\beta|^2\\) is the probability that the qubit has value 1. \n\n\nUnder this mathematical model our intuitions about computing as local,\nsequential, manipulation of discrete objects according to\ndeterministic rules evolve in to a much richer paradigm:\n\n Infinite information  The introduction of\nreal numbers facilitates the manipulation of objects of\ninfinite descriptive complexity, although there is currently no\nindication that this expressivity is actually necessary in quantum\nphysics. \n Non-classical probability Complex\nnumbers facilitate a richer notion of extensiveness in which\nprobabilities cease to be classical. The third axiom of Kolmogorov\nloses its validity in favor of probabilities that enhance or suppress\neach other, consequently extensiveness of information is lost. \n Superposition and Entanglement  The\nrepresentation of qubits in terms of complex high dimensional vector\nspaces implies that qubits cease to be isolated discrete objects.\nQuantum bits can be in superposition, a situation in which they are in\ntwo discrete states at the same time. Quantum bits fluctuate and\nconsequently they generate information. Moreover quantum\nstates of qubits can be correlated even when the information bearers\nare separated by a long distance in space. This phenomenon, known as\nentanglement destroys the property of locality of\nclassical computing (see the entry on\n quantum entanglement and information).\n \n\n\nFrom this analysis it is clear that the description of our universe at\nvery small (and very large) scales involves mathematical models that\nare alien to our experience of reality in everyday life. The\nproperties that allow us to understand the world (the existence of\nstable, discrete objects that preserve their identity in space and\ntime) seem to be emergent aspects of a much more complex\nreality that is incomprehensible to us outside its mathematical\nformulation. Yet, at a macroscopic level, the universe facilitates\nelementary processes, like counting, measuring lengths, and the\nmanipulation of symbols, that allow us to develop a consistent\nhierarchy of mathematical models some of which seems to describe the\ndeeper underlying structure of reality. \n\nIn a sense the same mathematical properties that drove the development\nof elementary accounting systems in Mesopotamia four thousand years\nago, still help us to penetrate in to the world of subatomic\nstructures. In the past decennia information seems to have become a\nvital concept in physics. Seth Lloyd and others (Zuse 1969; Wheeler\n1990; Schmidhuber 1997b; Wolfram 2002; Hutter 2010) have analyzed\ncomputational models of various physical systems. The notion of\ninformation seems to play a major role in the analysis of black holes\n(Lloyd & Ng 2004; Bekenstein 1994\n [OIR]).\n Erik Verlinde (2011, 2017) has proposed a theory in which gravity is\nanalyzed in terms of information. For the moment these models seem to\nbe purely descriptive without any possibility of empirical\nverification. \n6. Anomalies, Paradoxes, and Problems\n\nSome of the fundamental issues in philosophy of Information are\nclosely related to existing philosophical problems, others seem to be\nnew. In this paragraph we discuss a number of observations that may\ndetermine the future research agenda. Some relevant questions are:\n\nAre there uniquely identifying descriptions that do not contain\nall information about the object they refer to?\nDoes computation create new information? \nIs there a difference between construction and systematic search?\n\n\n\nSince Frege most mathematicians seem to believe that the answer to the\nfirst question is positive (Frege 1879, 1892). The descriptions\n\u201cThe morning star\u201d and \u201cThe evening star\u201d are\nassociated with procedures to identify the planet Venus, but\nthey do not give access to all information about the object itself. If\nthis were so the discovery that the evening star is in fact also the\nmorning star would be uninformative. If we want to maintain this\nposition we get into conflict, because in terms of information theory\nthe answer to the second question is negative (see\n section 5.1.7).\n Yet this observation is highly counter intuitive, because it implies\nthat we never can construct new information on the basis of\ndeterministic computation, which leads to the third question. These\nissues cluster around one of the fundamental open problems of\nPhilosophy of Information: \n\nOpen problem What is the interaction between\nInformation and Computation? \n\nWhy would we compute at all, if according to our known information\nmeasures, deterministic computing does not produce new information?\nThe question could be rephrased as: should we use Kolmogorov or Levin\ncomplexity (Levin 1973, 1974, 1984) as our basic information measure?\nIn fact both choices lead to relevant, but fundamentally different,\ntheories of information. When using the Levin measure, computing\ngenerates information and the answer to the three questions above is a\n\u201cyes\u201d, when using Kolmogorov this is not the case. The\nquestions are related to many problems both in mathematics and\ncomputer science. Related issues like approximation, computability and\npartial information are also studied in the context of Scott domains\n(Abramsky & Jung 1994). Below we discuss some relevant\nobservations. \n6.1 The Paradox of Systematic Search\n\nThe essence of information is the fact that it reduces uncertainty.\nThis observation leads to problems in opaque contexts, for instance,\nwhen we search an object. This is illustrated by Meno\u2019s paradox\n(see entry on\n epistemic paradoxes):\n\n\nAnd how will you enquire, Socrates, into that which you do not\nknow? What will you put forth as the subject of enquiry? And if you\nfind what you want, how will you ever know that this is the thing\nwhich you did not know? (Plato, Meno, 80d1-4) \n\n\nThe paradox is related to other open problems in computer science and\nphilosophy. Suppose that John is looking for a unicorn. It is very\nunlikely that unicorns exist, so, in terms of Shannon\u2019s theory,\nJohn gets a lot of information if he finds one. Yet from a descriptive\nKolmogorov point of view, John does not get new information, since he\nalready knows what unicorns are. The related paradox of systematic\nsearch might be formulated as follows:\n\n\nAny information that can be found by means of systematic search has no\nvalue, since we are certain to find it, given enough time.\nConsequently information only has value as long as we are uncertain\nabout its existence, but then, since we already know what we are\nlooking for, we get no new information when we find out that it\nexists. \n\nExample: Goldbach conjectured in 1742 that every even\nnumber bigger than 2 could be written as the sum of two primes. Until\ntoday this conjecture remains unproved. Consider the term \u201cThe\nfirst number that violates Goldbach\u2019s conjecture\u201d. It does\nnot give us all information about the number, since the number might\nnot exist. The prefix \u201cthe first\u201d ensures the description,\nif it exists, is unique, and it gives us an algorithm to find the\nnumber. It is a partial uniquely identifying description.\nThis algorithm is only effective if the number really exists,\notherwise it will run forever. If we find the number this will be\ngreat news, but from the perspective of descriptive complexity the\nnumber itself will be totally uninteresting, since we already know the\nrelevant properties to find it. Observe that, even if we have a number\nn that is a counter example to Goldbach\u2019s conjecture, it\nmight be difficult to verify this: we might have to check almost all\nprimes \\( \\leq n\\). This can be done effectively (we will\nalways get a result) but not, as far as we know, efficiently\n(it might take \u201cclose\u201d to n different computations)\n. \n\n\nA possible solution is to specify the constraint that it is\nillegal to measure the information content of an object in\nterms of partial descriptions, but this would destroy our theory of\ndescriptive complexity. Note that the complexity of an object is the\nlength of the shortest program that produces an object on a universal\nTuring machine. In this sense the phrase \u201cthe first number that\nviolates Goldbach\u2019s conjecture\u201d is a perfect description\nof a program, and it adequately measures the descriptive complexity of\nsuch a number. The short description reflects the fact that the\nnumber, if it exists, is very special, and thus it has a high\npossibility to occur in some mathematical context. \n\nThere are relations which well-studied philosophical problems like the\nAnselm\u2019s ontological argument for God\u2019s existence and the\nKantian counter claim that existence is not a predicate. In order to\navoid similar problems Russell proposed to interpret unique\ndescriptions existentially (Russell 1905): A sentence like \u201cThe\nking of France is bald\u201d would have the following logical\nstructure:  \n\\[\\exists (x) (KF(x) \\wedge \\forall (y)(KF(y) \\rightarrow x=y) \\wedge B(x))\\]\n\n\nThis interpretation does not help us to analyze decision problems that\ndeal with existence. Suppose the predicate L is true of\nx if I\u2019m looking for x, then the logical structure\nof the phrase \u201cI\u2019m looking for the king of France\u201d\nwould be:  \n\\[\\exists (x) (KF(x) \\wedge\n\\forall (y)(KF(y) \\rightarrow x=y) \\wedge L(x)),\\]\n\n\ni.e., if the king of France does not exist it cannot be true that I am\nlooking for him, which is unsatisfactory. Kripke (1971) criticized\nRussell\u2019s solution and proposed his so-called causal theory of\nreference in which a name get its reference by an initial act of\n\u201cbaptism\u201d. It then becomes a rigid designator\n(see entry on\n rigid designators)\n that can be followed back to that original act via causal chains. In\nthis way ad hoc descriptions like \u201cJohn was the fourth\nperson to step out of the elevator this morning\u201d can establish a\nsemantics for a name.\n\nIn the context of mathematics and information theory the corresponding\nconcept is that of names, constructive predicates and ad-hoc\npredicates of numbers. For any number there will be in principle an\ninfinite number of true statements about that number. Since elementary\narithmetic is incomplete there will be statements about numbers that\nare true but unprovable. In the limit a vanishing fragment of numbers\nwill have true predicates that actually compress their description.\nConsider the following statements:\n\n The symbol \u201c8\u201d is the name for the number eight.\n\n The number x is the 1000th Fibonacci number. \n The number x is the first number that violates the\nGoldbach conjecture. \n\n\nThe first statement simply specifies a name for a number. The second\nstatement gives a partial description that is constructive,\ninformation compressing and unique. The 1000th Fibonacci number has\n209 digits, so the description \u201cthe 1000th Fibonacci\nnumber\u201d is much more efficient than the actual name of the\nnumber. Moreover, we have an algorithm to construct the number. This\nmight not be that case for the description in the third statement. We\ndo not know whether the first number that violates the Goldbach\nconjecture exists, but if it does, the description might well be\nad hoc and thus gives us no clue to construct the number.\nThis rise to the conjecture that there are data compressing\neffective ad hoc descriptions:\n\nConjecture: There exist numbers that are compressed\nby non-constructive unique effective descriptions, i.e., the validity\nof the description can be checked effectively given the number, but\nthe number cannot be constructed effectively from the description,\nexcept by means of systematic search. \n\nThe conjecture is a more general variant of the so-called P vs. NP\nthesis (see\n section 6.3).\n If one replaces the term \u201ceffective\u201d with the term\n\u201cefficient\u201d one gets a formulation of the \\(\\textrm{P}\n\\neq \\textrm{NP}\\) thesis.\n6.2 Effective Search in Finite Sets\n\nWhen we restrict ourselves to effective search in finite sets, the\nproblem of partial descriptions, and construction versus search\nremain. It seems natural to assume that when one has a definition of a\nset of numbers, then one also has all the information about the\nmembers of the set and about its subsets, but this is not true. In\ngeneral the computation of the amount of information in a set of\nnumbers is a highly non-trivial issue. We give some results:\n\n\nLemma A subset \\(A \\subset S\\) of a set S can\ncontain more information conditional to the set than the set itself.\n\n\nProof: Consider the set S of all natural\nnumbers smaller than n. The descriptive complexity of this set\nin bits is \\( \\log_2 n + c\\). Now construct A by selecting half\nof the elements of S randomly. Observe that:  \n\\[I(A\\mid S)=\\log_2 {n \\choose {n/2}}\\]\n\n\nWe have: \n\\[\n\\lim_{n \\rightarrow \\infty} \n\\frac{I(A\\mid S)}\n{n}\n = \n\\lim_{n \\rightarrow \\infty} \n\\frac{\\log_2 {n \\choose {n/2}}}\n{n}\n = 1\\]\n\n\nThe conditional descriptive complexity of this set will be: \\(I(A\\mid\nS) \\approx n + c \\gg \\log n + c\\). \\(\\Box\\)\n\n\nA direct consequence is that we can lose information when we merge two\nsets. An even stronger result is: \n\n\nLemma: An element of a set can contain more\ninformation than the set itself. \n\nProof: Consider the set S of natural numbers\nsmaller then \\(2^n\\). The cardinality of S is \\(2^n\\). The\ndescriptive complexity of this set is \\(\\log n + c\\) bits, but for\nhalf of the elements of S we need n bits to describe\nthem. \\(\\Box\\)\n\n\nIn this case the description of the set itself is highly compressible,\nbut it still contains non-compressible elements. When we merge or\nsplit sets of numbers, or add or remove elements, the effects on the\namount of information are in general hard to predict and might even be\nuncomputable: \n\n\nTheorem: Information is not monotone under set\ntheoretical operations \n\nProof: Immediate consequence of the lemmas above.\n\\(\\Box\\)\n\n\nThis shows how the notion of information pervades our everyday life.\nWhen John has two apples in his pocket it seems that he can do\nwhatever he wants with them, but, in fact, as soon as he chooses one\nof the two, he has created (new) information. The consequences for\nsearch problems are clear: we can always effectively perform bounded\nsearch on the elements and the set of subsets of a set. Consequently\nwhen we search for such a set of subsets by means of partial\ndescriptions then the result generates (new) information. This\nanalysis prima facie appears to force us to accept that in mathematics\nthere are simple descriptions that allow us to identify complex\nobjects by means of systematic search. When we look for the object we\nhave only little information about it, when we finally find it our\ninformation increases to the set of full facts about the object\nsearched. This is in conflict with our current theories of information\n(Shannon and Kolmogorov): any description that allows us to identify\nan object effectively by deterministic search contains all relevant\ninformation about the object. The time complexity of the search\nprocess then is irrelevant. \n6.3 The P versus NP Problem, Descriptive Complexity Versus Time Complexity\n\nIn the past decennia mathematicians have been pondering about a\nrelated question: suppose it would be easy to check whether I\nhave found what I\u2019m looking for, how hard can it be to find such\nan object? In mathematics and computer science there seems to be a\nconsiderable class of decision problems that cannot be solved\nconstructively in polynomial time, \\(t(x)=x^c\\), where c is a\nconstant and x is the length of the input), but only through\nsystematic search of a large part of the solution space, which might\ntake exponential time, \\(t(x)=c^x\\). This difference roughly coincides\nwith the separation of problems that are computationally feasible from\nthose that are not.\n\nThe issue of the existence of such problems has been framed as the\npossible equivalence of the class P of decision problems that can be\nsolved in time polynomial to the input to the class NP of problems for\nwhich the solution can be checked in time polynomial to the input.\n(Garey & Johnson 1979; see also Cook 2000\n [OIR]\n for a good introduction.) \n\nExample: A well-known example in the class NP is the\nso-called subset sum problem: given a finite set of natural\nnumbers S, is there a subset \\(S^{\\prime}\\subseteq S\\) that\nsums up to some number k? It is clear that when someone\nproposes a solution \\(X \\subseteq S\\) to this problem we can easily\ncheck whether the elements of X add up to k, but we\nmight have to check almost all subsets of S in order to find\nsuch a solution ourselves. \n\nThis is an example of a so-called decision problem. The answer is a\nsimple \u201cyes\u201d or \u201cno\u201d, but it might be hard to\nfind the answer. Observe that the formulation of the question\nconditional to S has descriptive complexity \\(\\log k + c\\),\nwhereas most random subsets of S have a conditional descriptive\ncomplexity of \\(|S|\\). So any subset \\(S^{\\prime}\\) that adds up to\nk might have a descriptive complexity that is bigger then the\nformulation of the search problem. In this sense search seems to\ngenerate information. The problem is that if such a set exists the\nsearch process is bounded, and thus effective, which means that the\nphrase \u201cthe first subset of S that adds up to\nk\u201d is an adequate description. If \\(\\textrm{P} =\n\\textrm{NP}\\) then the Kolmogorov complexity and the Levin complexity\nof the set \\(S^{\\prime}\\) we find roughly coincide, if \\(P \\neq\n\\textit{NP}\\) then in some cases \\(Kt(S^{\\prime}) \\gg K(S^{\\prime})\\).\nBoth positions, the theory that search generates new information and\nthe theory that it does not, are counterintuitive from different\nperspectives.\n\nThe P vs. NP problem, that appears to be very hard, has been a rich\nsource of research in computer science and mathematics although\nrelatively little has been published on its philosophical relevance.\nThat a solution might have profound philosophical impact is\nillustrated by a quote from Scott Aaronson:\n\n\nIf P = NP, then the world would be a profoundly different place than\nwe usually assume it to be. There would be no special value in\n\u201ccreative leaps,\u201d no fundamental gap between solving a\nproblem and recognizing the solution once it\u2019s found. Everyone\nwho could appreciate a symphony would be Mozart; everyone who could\nfollow a step-by-step argument would be Gauss\u2026. (Aaronson 2006\n\u2013 in the Other Internet Resources)\n\n\nIn fact, if \\(\\textrm{P}=\\textrm{NP}\\) then every object that has a\ndescription that is not too large and easy to check is also easy to\nfind. \n6.4 Model Selection and Data Compression\n\nIn current scientific methodology the sequential aspects of the\nscientific process are formalized in terms of the empirical cycle,\nwhich according to de Groot (1969) has the following stages:\n\nObservation: The observation of a phenomenon and inquiry\nconcerning its causes.\nInduction: The formulation of hypotheses\u2014generalized\nexplanations for the phenomenon.\n Deduction: The formulation of experiments that will test the\nhypotheses (i.e., confirm them if true, refute them if false).\n Testing: The procedures by which the hypotheses are tested and\ndata are collected.\n Evaluation: The interpretation of the data and the formulation of\na theory\u2014an abductive argument that presents the results of the\nexperiment as the most reasonable explanation for the phenomenon.\n\n\nIn the context of information theory the set of observations will be a\ndata set and we can construct models by observing regularities in this\ndata set. Science aims at the construction of true models of our\nreality. It is in this sense a semantical venture. In the 21-st\ncentury the process of theory formation and testing will for the\nlargest part be done automatically by computers working on large\ndatabases with observations. Turing award winner Jim Grey framed the\nemerging discipline of e-science as the fourth data-driven paradigm of\nscience. The others are empirical, theoretical and computational. As\nsuch the process of automatic theory construction on the basis of data\nis part of the methodology of science and consequently of philosophy\nof information (Adriaans & Zantinge 1996; Bell, Hey, & Szalay\n2009; Hey, Tansley, and Tolle 2009). Many well-known learning\nalgorithms, like decision tree induction, support vector machines,\nnormalized information distance and neural networks, use entropy based\ninformation measures to extract meaningful and useful models out of\nlarge data bases. The very name of the discipline Knowledge Discovery\nin Databases (KDD) is witness to the ambition of the Big Data research\nprogram. We quote:\n\n\nAt an abstract level, the KDD field is concerned with the development\nof methods and techniques for making sense of data. The basic problem\naddressed by the KDD process is one of mapping low-level data (which\nare typically too voluminous to understand and digest easily) into\nother forms that might be more compact (for example, a short report),\nmore abstract (for example, a descriptive approximation or model of\nthe process that generated the data), or more useful (for example, a\npredictive model for estimating the value of future cases). At the\ncore of the process is the application of specific data-mining methods\nfor pattern discovery and extraction. (Fayyad, Piatetsky-Shapiro,\n& Smyth 1996: 37)\n\n\nMuch of the current research focuses on the issue of selecting an\noptimal computational model for a data set. The theory of Kolmogorov\ncomplexity is an interesting methodological foundation to study\nlearning and theory construction as a form of data compression. The\nintuition is that the shortest theory that still explains the data is\nalso the best model for generalization of the observations. A crucial\ndistinction in this context is the one between one- and two-part\ncode optimization: \n\n\n\nOne-part Code Optimization: The methodological\naspects of the theory of Kolmogorov complexity become clear if we\nfollow its definition. We begin with a well-formed dataset y\nand select an appropriate universal machine \\(U_j\\). The expression\n\\(U_j(\\overline{T_i}x)= y\\) is a true sentence that gives us\ninformation about y. The first move in the development of a\ntheory of measurement is to force all expressiveness to the\ninstructional or procedural part of the sentence by a restriction to\nsentences that describe computations on empty input:  \n\\[U_j(\\overline{T_i}\\emptyset)= y\\]\n\n\nThis restriction is vital for the proof of invariance. From this, in\nprinciple infinite, class of sentences we can measure the length when\nrepresented as a program. We select the ones (there might be more than\none) of the form \\(\\overline{T_i}\\) that are shortest. The length\n\\(\\mathit{l}(\\overline{T_i})\\) of such a shortest description is a\nmeasure for the information content of y. It is asymptotic in\nthe sense that, when the data set y grows to an infinite\nlength, the information content assigned by the choice of another\nTuring machine will never vary by more than a constant in the limit.\nKolmogorov complexity measures the information content of a data set\nin terms of the shortest description of the set of instructions that\nproduces the data set on a universal computing device.\n\n\nTwo-part Code Optimization: Note that by restricting\nourselves to programs with empty input and the focus on the length\n of programs instead of their content we gain the\nquality of invariance for our measure, but we also lose a lot of\nexpressiveness. The information in the actual program that produces\nthe data set is neglected. Subsequent research therefore has focused\non techniques to make the explanatory power, hidden in the Kolmogorov\ncomplexity measure, explicit. \n\n\nA possible approach is suggested by an interpretation of Bayes\u2019\nlaw. If we combine Shannon\u2019s notion of an optimal code with\nBayes\u2019 law, we get a rough theory about optimal model selection.\nLet \\(\\mathcal{H}\\) be a set of hypotheses and let x be a data\nset. Using Bayes\u2019 law, the optimal computational model under\nthis distribution would be:  \n\\[\\begin{equation}\nM_{\\textit{map}}(x) = \\textit{argmax}_{M \\in \\mathcal{H}} \\frac{P(M) P(x\\mid M)}{P(x)} \n\\end{equation} \\]\n\n\nThis is equivalent to optimizing:  \n\\[\n\\begin{equation}\\label{OptimalIbE} \\textit{argmin}_{M \\in \\mathcal{H}} - \\log P(M) - \\log P(x\\mid M) \\end{equation}\n\\]\n\n\nHere \\(-\\log P(M)\\) can be interpreted as the length of the optimal\nmodel code in Shannon\u2019s sense and \\(- \\log P(x\\mid M)\\)\nas the length of the optimal data-to-model code; i.e., the\ndata interpreted with help of the model. This insight is canonized in\nthe so-called:\n\nMinimum Description Length (MDL) Principle: The best\ntheory to explain a data set is the one that minimizes the sum in bits\nof a description of the theory (model code) and of the data set\nencoded with the theory (the data to model code). \n\nThe MDL principle is often referred to as a modern version of\nOckham\u2019s razor (see entry on\n William of Ockham),\n although in its original form Ockham\u2019s razor is an ontological\nprinciple and has little to do with data compression (Long 2019). In\nmany cases MDL is a valid heuristic tool and the mathematical\nproperties of the theory have been studied extensively (Gr\u00fcnwald\n2007). Still MDL, Ockham\u2019s razor and two-part code optimization\nhave been the subject of considerable debate in the past decennia\n(e.g., Domingos 1998; McAllister 2003). \n\nThe philosophical implications of the work initiated by Solomonoff,\nKolmogorov and Chaitin in the sixties of the 20-th century are\nfundamental and diverse. The universal distribution m proposed\nby Solomonoff, for instance, codifies all possible mathematical\nknowledge and when updated on the basis of empirical observations\nwould in principle converge to an optimal scientific model of our\nworld. In this sense the choice of a universal Turing machine as basis\nfor our theory of information measurement has philosophical\nimportance, specifically for methodology of science. A choice for a\nuniversal Turing machine can be seen as a choice of a set of\nbias for our methodology. There are roughly two schools:\n\nPoor machine: choose a small universal Turing\nmachine. If the machine is small it is also general and universal,\nsince there is no room to encode any bias in to the machine. Moreover\na restriction to small machines gives small overhead when emulating\none machine on the other so the version of Kolmogorov complexity you\nget gives a measurement with a smaller asymptotic margin. Hutter\nexplicitly defends the choice of \u201cnatural\u201d small machines\n(Hutter 2005; Rathmanner & Hutter 2011), but also Li and\nVit\u00e1nyi (2019) seem to suggest the use of small models. \nRich machine: choose a big machine that\nexplicitly reflects what you already know about the world. For\nSolomonoff, the inventor of algorithmic complexity, the choice of a\nuniversal Turing machine is the choice for a universal prior. He\ndefends an evolutionary approach to learning in which an agent\nconstantly adapts the prior to what he already has discovered. The\nselection of your reference Turing machine uniquely characterizes your\na priori information (Solomonoff 1997). \n\n\nBoth approaches have their value. For rigid mathematical proofs the\npoor machine approach is often best. For practical applications on\nfinite data sets the rich model strategy often gets much better\nresults, since a poor machine would have to \u201cre-invent the\nwheel\u201d every time it compresses a data set. This leads to the\nconclusion that Kolmogorov complexity inherently contains a theory\nabout scientific bias and as such implies a methodology in which the\nclass of admissible universal models should be explicitly formulated\nand motivated a priori. In the past decennia there have been\na number of proposals to define a formal unit of measurement of the\namount of structural (or model-) information in a data set.\n\nAesthetic measure (Birkhoff 1950)\nSophistication (Koppel 1987; Antunes et al. 2006; Antunes &\nFortnow 2003)\nLogical Depth (Bennet 1988)\nEffective complexity (Gell-Mann, Lloyd 2003)\nMeaningful Information (Vit\u00e1nyi 2006)\nSelf-dissimilarity (Wolpert & Macready 2007)\nComputational Depth (Antunes et al. 2006)\nFacticity (Adriaans 2008)\n\n\nThree intuitions dominate the research. A string is\n\u201cinteresting\u201d when \u2026 \n\na certain amount of computation is involved in its creation\n(Sophistication, Computational Depth);\nthere is a balance between the model-code and the data-code under\ntwo-part code optimization (effective complexity, facticity);\nit has internal phase transitions (self-dissimilarity).\n\n\nSuch models penalize both maximal entropy and low information content.\nThe exact relationship between these intuitions is unclear. The\nproblem of meaningful information has been researched extensively in\nthe past years, but the ambition to formulate a universal method for\nmodel selection based on compression techniques seems to be misguided:\n\n\nObservation: A measure of meaningful information based on\ntwo-part code optimization can never be invariant in the\nsense of Kolmogorov complexity (Bloem et al. 2015, Adriaans 2020).\n\n\nThis appears to be the case even if we restrict ourselves to weaker\ncomputational models like total functions, but more research is\nnecessary. There seems to be no a priori mathematical\njustification for the approach, although two-part code optimization\ncontinues to be a valid approach in an empirical setting of data sets\nthat have been created on the basis of repeated observations.\nPhenomena that might be related to a theory of structural information\nand that currently are ill-understood are: phase transitions in the\nhardness of satisfiability problems related to their complexity (Simon\n& Dubois 1989; Crawford & Auton 1993) and phase transitions in\nthe expressiveness of Turing machines related to their complexity\n(Crutchfield & Young 1989, 1990; Langton 1990; Dufort &\nLumsden 1994). \n6.5 Determinism and Thermodynamics\n\nMany basic concepts of information theory were developed in the\nnineteenth century in the context of the emerging science of\nthermodynamics. There is a reasonable understanding of the\nrelationship between Kolmogorov Complexity and Shannon information (Li\n& Vit\u00e1nyi 2008; Gr\u00fcnwald & Vit\u00e1nyi 2008;\nCover & Thomas 2006), but the unification between the notion of\nentropy in thermodynamics and Shannon-Kolmogorov information is very\nincomplete apart from some very ad hoc insights\n(Harremo\u00ebs & Tops\u00f8e 2008; Bais & Farmer 2008).\nFredkin and Toffoli (1982) have proposed so-called billiard ball\ncomputers to study reversible systems in thermodynamics (Durand-Lose\n2002) (see the entry on\n information processing and thermodynamic entropy).\n Possible theoretical models could with high probability be\ncorroborated with feasible experiments (e.g., Joule\u2019s adiabatic\nexpansion, see Adriaans 2008). \n\nQuestions that emerge are:\n\nWhat is a computational process from a thermodynamical point of\nview? \n Can a thermodynamic theory of computing serve as a theory of\nnon-equilibrium dynamics? \nIs the expressiveness of real numbers necessary for a physical\ndescription of our universe? \n\n\nThese problems seem to be hard because 150 years of research in\nthermodynamics still leaves us with a lot of conceptual unclarities in\nthe heart of the theory of thermodynamics itself (see entry on\n thermodynamic asymmetry in time).\n\nReal numbers are not accessible to us in finite computational\nprocesses yet they do play a role in our analysis of thermodynamic\nprocesses. The most elegant models of physical systems are based on\nfunctions in continuous spaces. In such models almost all points in\nspace carry an infinite amount of information. Yet, the cornerstone of\nthermodynamics is that a finite amount of space has finite entropy.\nThere is, on the basis of the theory of quantum information, no\nfundamental reason to assume that the expressiveness of real numbers\nis never used in nature itself on this level. This problem is related\nto questions studied in philosophy of mathematics (an intuitionistic\nversus a more platonic view). The issue is central in some of the more\nphilosophical discussions on the nature of computation and information\n(Putnam 1988; Searle 1990). The problem is also related to the notion\nof phase transitions in the description of nature (e.g.,\nthermodynamics versus statistical mechanics) and to the idea of levels\nof abstraction (Floridi 2002, 2019). \n\nIn the past decade some progress has been made in the analysis of\nthese questions. A basic insight is that the interaction between time\nand computational processes can be understood at an abstract\nmathematical level, without the burden of some intended physical\napplication (Adriaans & van Emde Boas 2011). Central is the\ninsight that deterministic programs do not generate new information.\nConsequently deterministic computational models of physical systems\ncan never give an account of the growth of information or entropy in\nnature:\n\nObservation: The Laplacian assumption that the universe can\nbe described as a deterministic computer is, given the fundamental\ntheorem of Adriaans and van Emde Boas (2011) and the assumption that\nquantum physics as a essentially stochastic description of the\nstructure of our reality, incorrect. \n\nA statistical reduction of thermodynamics to a deterministic theory\nlike Newtonian physics leads to a notion of entropy that is\nfundamentally different from the information processed by\ndeterministic computers. From this perspective the mathematical models\nof thermodynamics, which are basically differential equations on\nspaces of real numbers, seem to operate on a level that is not\nexpressive enough. More advanced mathematical models, taking in to\naccount quantum effects, might resolve some of the conceptual\ndifficulties. At a subatomic level nature seems to be inherently\nprobabilistic. If probabilistic quantum effects play a role in the\nbehavior of real billiard balls, then the debate whether entropy\nincreases in an abstract gas, made out of ideal balls, seems a bit\nacademic. There is reason to assume that stochastic phenomena at\nquantum level are a source of probability at a macroscopic scale\n(Albrecht & Phillips 2014). From this perspective the universe is\na constant source of, literally, astronomical amounts of information\nat any scale. \n6.6 Logic and Semantic Information\n\nLogical and computational approaches to the understanding of\ninformation both have their roots in the \u201clinguistic turn\u201d\nthat characterized the philosophical research in the beginning of the\ntwentieth century and the elementary research questions originate from\nthe work of Frege (1879, 1892, see the entry on\n logic and information).\n The ambition to quantify information in sets of true\nsentences, as apparent in the work of researchers like Popper,\nCarnap, Solomonoff, Kolmogorov, Chaitin, Rissanen, Koppel,\nSchmidthuber, Li, Vit\u00e1nyi and Hutter is an inherently semantic\nresearch program. In fact, Shannon\u2019s theory of information is\nthe only modern approach that explicitly claims to be non-semantic.\nMore recent quantitative information measures like Kolmogorov\ncomplexity (with its ambition to codify all scientific knowledge in\nterms of a universal distribution) and quantum information (with its\nconcept of observation of physical systems) inherently assume\na semantic component. At the same time it is possible to develop\nquantitative versions of semantic theories (see entry on\n semantic conceptions of information).\n \n\nThe central intuition of algorithmic complexity theory that an\nintension or meaning of an object can be a computation, was originally\nformulated by Frege (1879, 1892). The expressions \u201c1 + 4\u201d\nand \u201c2 + 3\u201d have the same extension (Bedeutung)\n\u201c5\u201d, but a different intension (Sinn). In this\nsense one mathematical object can have an infinity of different\nmeanings. There are opaque contexts in which such a distinction is\nnecessary. Consider the sentence \u201cJohn knows that \\(\\log_2 2^2 =\n2\\)\u201d. Clearly the fact that \\(\\log_2 2^2\\) represents a specific\ncomputation is relevant here. The sentence \u201cJohn knows that \\(2\n= 2\\)\u201d seems to have a different meaning.\n\nDunn (2001, 2008) has pointed out that the analysis of information in\nlogic is intricately related to the notions of intension and\nextension. The distinction between intension and extension is already\nanticipated in the\n Port Royal Logic\n (1662) and the writings of Mill (1843), Boole (1847) and Peirce\n(1868) but was systematically introduced in logic by Frege (1879,\n1892). In a modern sense the extension of a predicate, say\n\u201cX is a bachelor\u201d, is simply the set of bachelors\nin our domain. The intension is associated with the meaning of the\npredicate and allows us to derive from the fact that \u201cJohn is a\nbachelor\u201d the facts that \u201cJohn is male\u201d and\n\u201cJohn is unmarried\u201d. It is clear that this phenomenon has\na relation with both the possible world interpretation of modal\noperators and the notion of information. A bachelor is by necessity\nalso male, i.e., in every possible world in which John is a bachelor\nhe is also male, consequently: If someone gives me the information\nthat John is a bachelor I get the information that he is male and\nunmarried for free. \n\nThe possible world interpretation of modal operators (Kripke 1959) is\nrelated to the notion of \u201cstate description\u201d introduced by\nCarnap (1947). A state description is a conjunction that contains\nexactly one of each atomic sentence or its negation (see\n section 4.3).\n The ambition to define a good probability measure for state\ndescriptions was one of the motivations for Solomonoff (1960, 1997) to\ndevelop algorithmic information theory. From this perspective\nKolmogorov complexity, with its separation of data types (programs,\ndata, machines) and its focus on true sentences describing effects of\nprocesses is basically a semantic theory (Adriaans 2020). This is\nimmediately clear if we evaluate the expression: \n\\[U_j(\\overline{T_i}x)= y\\]\n\n\nAs is explained in\n section 5.2.1\n the expression \\(U_j(\\overline{T_i}x)\\) denotes the result of the\nemulation of the computation \\(T_i(x)\\) by \\(U_j\\) after reading the\nself-delimiting description \\(\\overline{T_i}\\) of machine \\(T_j\\).\nThis expression can be interpreted as a piece of semantic\ninformation in the context of the informational map (See\nentry on\n semantic conceptions of information)\n as follows: \n\nThe universal Turing machine \\(U_j\\) is a context\nis which the computation takes place. It can be interpreted as a\npossible computational world in a modal\ninterpretation of computational semantics.\nThe sequences of symbols \\(\\overline{T_i}x\\) and y are\nwell-formed data. \nThe sequence \\(\\overline{T_i}\\) is a self-delimiting\ndescription of a program and it can be interpreted as\na piece of well-formed instructional data.\nThe sequence \\(\\overline{T_i}x\\) is an intension.\nThe sequence y is the corresponding extension.\n\nThe expression \\(U_j(\\overline{T_i}x)= y\\) states the result of\nthe program \\(\\overline{T_i}x\\) in world \\(U_j\\) is y. It is a\ntrue sentence.\n\n\nThe logical structure of the sentence \\(U_j(\\overline{T_i}x)= y\\) is\ncomparable to a true sentence like: \n\nIn the context of empirical observations on planet earth, the bright\nstar you can see in the morning in the eastern sky is Venus \n\nMutatis mutandis one could develop the following\ninterpretation: \\(U_j\\) can be seen as a context that, for instance,\ncodifies a bias for scientific observations on earth,\ny is the extension Venus, \\(\\overline{T_i}x\\) is the\nintension \u201cthe bright star you can see in the morning\nin the eastern sky\u201d. The intension consists of \\(T_i\\), which\ncan be interpreted as some general astronomical observation routine\n(e.g., instructional data), and x provides the well-formed data\nthat tells one where to look (bright star in the morning in the\neastern sky). \n\nThis suggests a possible unification between more truth oriented\ntheories of information and computational approaches in terms of the\ninformational map presented in the entry of\n semantic conceptions of information.\n We delineate some research questions:\n\nWhat is a good logical system (or set of systems) that formalizes\nour intuitions of the relation between concepts like\n\u201cknowing\u201d, \u201cbelieving\u201d and \u201cbeing\ninformed of\u201d. There are proposals by: Dretske (1981), van\nBenthem (2006; van Benthem & de Rooij 2003), Floridi (2003, 2011)\nand others. A careful mapping of these concepts onto our current\nlandscape of known logics (structural, modal) might clarify the\nstrengths and weaknesses of different proposals. \nIt is unclear what the specific difference (in the\nAristotelian sense) is that separates environmental data from\nother data, e.g., if one uses pebbles on a beach to count the number\nof dolphins one has observed, then it might be impossible for the\nuninformed passer by to judge whether this collection of stones is\nenvironmental data or not.\nThe category of instructional data seems to be too narrow\nsince it pins us down on a specific interpretation of what computing\nis. For the most part Turing equivalent computational paradigms are\nnot instructional, although one might defend the view that programs\nfor Turing machines are such data.\nIt is unclear how we can cope with the ontological\nduality that is inherent to the self referential aspects of\nTuring complete systems: Turing machines operate on data that at\nthe same time act as representations of programs, i.e.,\ninstructional and non-instructional. \nIt is unclear how a theory that defines information exclusively in\nterms of true statements can deal with fundamental issues in quantum\nphysics. How can an inconsistent logical model in which\nSchr\u00f6dinger\u2019s cat is at the same time dead and alive\ncontain any information in such a theory? \n\n6.7 Meaning and Computation\n\nEver since Descartes, the idea that the meaningful world, we perceive\naround us, can be reduced to physical processes has been a predominant\ntheme in western philosophy. The corresponding philosophical\nself-reflection in history neatly follows the technical developments\nfrom: Is the human mind an automaton, to is the mind a Turing machine\nand, eventually, is the mind a quantum computer? It is not the place\nhere to discuss these matters extensively, but the corresponding\nproblem in philosophy of information is relevant:\n\nOpen problem: Can meaning be reduced to computation?\n\n\nThe question is interwoven with more general issues in philosophy and\nits answer directly forces a choice between a more\npositivistic or a more hermeneutical approach to\nphilosophy, with consequences for theory of knowledge, metaphysics,\naesthetics and ethics. It also effects direct practical decisions we\ntake on a daily basis. Should the actions of a medical doctor be\nguided by evidence based medicine or by the notion of\ncaritas? Is a patient a conscious human being that wants to\nlead a meaningful life, or is he ultimately just a system that needs\nto be repaired? \n\nThe idea that meaning is essentially a computational phenomenon may\nseem extreme, but here are many discussions and theories in science,\nphilosophy and culture that implicitly assume such a view. In popular\nculture, e.g., there is a remarkable collection of movies and books in\nwhich we find evil computers that are conscious of themselves (2001,\nA Space Odyssey), individuals that upload their consciousness\nto a computer (1992, The Lawnmower Man), and fight battles in\nvirtual realities (1999, The Matrix). In philosophy the\nposition of Bostrom (2003), who defends the view that it is very\nlikely that we already live in a computer simulation, is illustrative.\nThere are many ways to argue the pros and cons of the reduction of\nmeaning to computation. We give an overview of possible arguments for\nthe two extreme positions: \n\n\n\nMeaning is an emergent aspect of computation: Science is our\nbest effort to develop a valid objective theoretical description of\nthe universe based on intersubjectively verifiable repeated\nobservations. Science tells us that our reality at a small scale\nconsists of elementary particles whose behavior is described by exact\nmathematical models. At an elementary level these particles interact\nand exchange information. These processes are essentially\ncomputational. At this most basic level of description there is no\nroom for a subjective notion of meaning. There is no reason to deny\nthat we as human being experience a meaningful world, but as such this\nmust be an emergent aspect of nature. At a fundamental level it does\nnot exist. We can describe our universe as a big quantum computer. We\ncan estimate the information storage content of our universe to be\n\\(10^{92}\\) bits and the number of computational steps it made since\nthe big bang as \\(10^{123}\\) (Lloyd 2000; Lloyd & Ng 2004). As\nhuman beings we are just subsystems of the universe with an estimated\ncomplexity of roughly \\(10^{30}\\) bits. It might be technically\nimpossible, but there seems to be no theoretical objection against the\nidea that we can in principle construct an exact copy of a human\nbeing, either as a direct physical copy or as a simulation in a\ncomputer. Such an \u201cartificial\u201d person will experience a\nmeaningful world, but the experience will be emergent.  \n\n\nMeaning is ontologically rooted in our individual experience of\nthe world and thus irreducible: The reason scientific theories\neliminate most semantic aspects of our world, is caused by the very\nnature of methodology of science itself. The essence of meaning and\nthe associated emotions is that they are rooted in our individual\nexperience of the world. By focusing on repeated observations of\nsimilar events by different observers scientific methodology excludes\nthe possibility of an analysis of the concept of meaning a\npriori. Empirical scientific methodology is valuable in the sense\nthat it allows us to abstract from the individual differences of\nconscious observers, but there is no reason to reduce our ontology to\nthe phenomena studied by empirical science. Isolated individual events\nand observations are by definition not open to experimental analysis\nand this seems to be the point of demarcation between science and the\nhumanities. In disciplines like history, literature, visual art and\nethics we predominantly analyze individual events and individual\nobjects. The closer these are to our individual existence, the more\nmeaning they have for us. There is no reason to doubt the fact that\nsentences like \u201cGuernica is a masterpiece that shows the\natrocities of war\u201d or \u201cMcEnroe played such an inspired\nmatch that he deserved to win\u201d uttered in the right context\nconvey meaningful information. The view that this information content\nultimately should be understood in terms of computational processes\nseems too extreme to be viable. \n\n\nApart from that, a discipline like physics, that until recently\noverlooked about 68% of the energy in the universe and 27% of the\nmatter, that has no unified theory of elementary forces and only\nexplains the fundamental aspects of our world in terms of mathematical\nmodels that lack any intuitive foundation, for the moment does not\nseem to converge to a model that could be an adequate basis for a\nreductionistic metaphysics. \n\nAs soon as one defines information in terms of true statements, some\nmeanings become computational and others lack that feature. In the\ncontext of empirical science we can study groups of researchers that\naim at the construction of theories generalizing structural\ninformation in data sets of repeated observations. Such processes of\ntheory construction and intersubjective verification and\nfalsification have an inherent computational component. In fact,\nthis notion of intersubjective verification seems an essential element\nof mathematics. This is the main cause of the fact that central\nquestions of humanities are not open for quantitative analysis: We can\ndisagree on the question whether one painting is more beautiful than\nthe other, but not on the fact that there are two paintings. \n\nIt is clear that computation as a conceptual model pays a role in many\nscientific disciplines varying from cognition (Chater &\nVit\u00e1nyi 2003), to biology (see entry on\n biological information)\n and physics (Lloyd & Ng 2004; Verlinde 2011, 2017). Extracting\nmeaningful models out of data sets by means of computation is the\ndriving force behind the Big Data revolution (Adriaans & Zantinge\n1996; Bell, Hey, & Szalay 2009; Hey, Tansley, & Tolle 2009).\nEverything that multinationals like Google and Facebook\n\u201cknow\u201d about individuals is extracted from large data\nbases by means of computational processes, and it cannot be denied\nthat this kind of \u201cknowledge\u201d has a considerable amount of\nimpact on society. The research question \u201cHow can we construct\nmeaningful data out of large data sets by means of computation?\u201d\nis a fundamental meta-problem of science in the twenty-first century\nand as such part of philosophy of information, but there is no strict\nnecessity for a reductionistic view. \n7. Conclusion\n\nThe first domain that could benefit from philosophy of information is\nof course philosophy itself. The concept of information potentially\nhas an impact on almost all philosophical main disciplines, ranging\nfrom logic, theory of knowledge, to ontology and even ethics and\nesthetics (see introduction above). Philosophy of science and\nphilosophy of information, with their interest in the problem of\ninduction and theory formation, probably both could benefit from\ncloser cooperation (see\n 4.1 Popper: Information as degree of falsifiability).\n The concept of information plays an important role in the history of\nphilosophy that is not completely understood (see\n 2. History of the term and the concept of information).\n \n\nAs information has become a central issue in almost all of the\nsciences and humanities this development will also impact\nphilosophical reflection in these areas. Archaeologists, linguists,\nphysicists, astronomers all deal with information. The first thing a\nscientist has to do before he can formulate a theory is gathering\ninformation. The application possibilities are abundant. Datamining\nand the handling of extremely large data sets seems to be an essential\nfor almost every empirical discipline in the twenty-first century.\n\nIn biology we have found out that information is essential for the\norganization of life itself and for the propagation of complex\norganisms (see entry on\n biological information).\n One of the main problems is that current models do not explain the\ncomplexity of life well. Valiant has started a research program that\nstudies evolution as a form of computational learning (Valiant 2009)\nin order to explain this discrepancy. Aaronson (2013) has argued\nexplicitly for a closer cooperation between complexity theory and\nphilosophy.\n\nUntil recently the general opinion was that the various notions of\ninformation were more or less isolated but in recent years\nconsiderable progress has been made in the understanding of the\nrelationship between these concepts. Cover and Thomas (2006), for\ninstance, see a perfect match between Kolmogorov complexity and\nShannon information. Similar observations have been made by\nGr\u00fcnwald and Vit\u00e1nyi (2008). Also the connections that\nexist between the theory of thermodynamics and information theory have\nbeen studied (Bais & Farmer 2008; Harremo\u00ebs &\nTops\u00f8e 2008) and it is clear that the connections between\nphysics and information theory are much more elaborate than a mere\nad hoc similarity between the formal treatment of entropy and\ninformation suggests (Gell-Mann & Lloyd 2003; Verlinde (2011,\n2017). Quantum computing is at this moment not developed to a point\nwhere it is effectively more powerful than classical computing, but\nthis threshold might be passed in the coming years. From the point of\nview of philosophy many conceptual problems of quantum physics and\ninformation theory seem to merge into one field of related questions:\n\n\nWhat is the relation between information and computation?\nIs computation in the real world fundamentally\nnon-deterministic?\n What is the relation between symbol manipulation on a macroscopic\nscale and the world of quantum physics? \n What is a good model of quantum computing and how do we control\nits power? \n Is there information beyond the world of quanta?\n\n\nThe notion of information has become central in both our society and\nin the sciences. Information technology plays a pivotal role in the\nway we organize our lives. It also has become a basic category in the\nsciences and the humanities. Philosophy of information, both as a\nhistorical and a systematic discipline, offers a new perspective on\nold philosophical problems and also suggests new research domains.\n",
    "bibliography": {
        "categories": [],
        "cat_ref_text": {
            "ref_list": [
                "Aaronson, Scott, 2013, \u201cWhy Philosophers Should Care About\nComputational Complexity\u201d, in <em>Computability: Turing,\nG\u00f6del, Church, and Beyond</em>, Brian Jack Copeland, Carl J.\nPosy, and Oron Shagrir (eds.), Cambridge, MA: The MIT Press.\n [<a href=\"http://philsci-archive.pitt.edu/8748/\" target=\"other\">Aaronson 2013 preprint available online</a>]",
                "Abramsky, Samson and Achim Jung, 1994, \u201cDomain\ntheory\u201d, in <em>Handbook of Logic in Computer Science (vol. 3):\nSemantic Structure</em>, Samson Abramsky, Dov M. Gabbay, and Thomas S.\nE. Maibaum (eds.),. Oxford University Press. pp. 1\u2013168.",
                "Adams, Fred and Jo\u00e3o Antonio de Moraes, 2016, \u201cIs\nThere a Philosophy of Information?\u201d, <em>Topoi</em>, 35(1):\n161\u2013171. doi:10.1007/s11245-014-9252-9",
                "Adriaans, Pieter, 2007, \u201cLearning as Data\nCompression\u201d, in <em>Computation and Logic in the Real\nWorld</em>, S. Barry Cooper, Benedikt L\u00f6we, and Andrea Sorbi\n(eds.), (Lecture Notes in Computer Science: Volume 4497), Berlin,\nHeidelberg: Springer Berlin Heidelberg, 11\u201324.\ndoi:10.1007/978-3-540-73001-9_2",
                "\u2013\u2013\u2013, 2008, \u201cBetween Order and Chaos: The\nQuest for Meaningful Information\u201d, <em>Theory of Computing\nSystems</em> (Special Issue: Computation and Logic in the Real World;\nGuest Editors: S. Barry Cooper, Elvira Mayordomo and Andrea Sorbi),\n45(4): 650\u2013674. doi:10.1007/s00224-009-9173-y",
                "\u2013\u2013\u2013, 2020, \u201cA computational theory of\nmeaning\u201d, in <em>Advances in Info-Metrics: Information and\nInformation Processing across Disciplines</em>, Min Chen, Michael\nDunn, Amos Golan and Aman Ullah (eds.), New York: Oxford University\nPress, 32\u201378. doi:10.1093/oso/9780190636685.003.0002",
                "Adriaans, Pieter and Peter van Emde Boas, 2011,\n\u201cComputation, Information, and the Arrow of Time\u201d, in\n<em>Computability in Context: Computation and Logic in the Real\nWorld</em>, by S Barry Cooper and Andrea Sorbi (eds), London: Imperial\nCollege Press, 1\u201317. doi:10.1142/9781848162778_0001",
                "Adriaans, Pieter and Johan van Benthem, 2008a,\n\u201cIntroduction: Information Is What Information Does\u201d, in\nAdriaans &amp; van Benthem 2008b: 3\u201326.\ndoi:10.1016/B978-0-444-51726-5.50006-6",
                "\u2013\u2013\u2013 (eds.), 2008b, <em>Philosophy of\nInformation</em>, (Handbook of the Philosophy of Science 8),\nAmsterdam: Elsevier. doi:10.1016/C2009-0-16481-4",
                "Adriaans, Pieter and Paul M.B. Vit\u00e1nyi, 2009,\n\u201cApproximation of the Two-Part MDL Code\u201d, <em>IEEE\nTransactions on Information Theory</em>, 55(1): 444\u2013457.\ndoi:10.1109/TIT.2008.2008152",
                "Adriaans, Pieter and Dolf Zantinge, 1996, <em>Data Mining</em>,\nHarlow, England: Addison-Wesley.",
                "Agrawal, Manindra, Neeraj Kayal, and Nitin Saxena, 2004,\n\u201cPRIMES Is in P\u201d, <em>Annals of Mathematics</em>, 160(2):\n781\u2013793. doi:10.4007/annals.2004.160.781",
                "Albrecht, Andreas and Daniel Phillips, 2014, \u201cOrigin of\nProbabilities and Their Application to the Multiverse\u201d,\n<em>Physical Review D</em>, 90(12): 123514. doi:10.1103/PhysRevD.90.\n123514",
                "Antunes, Lu\u00eds and Lance Fortnow, 2003,\n\u201cSophistication Revisited\u201d, in <em>Proceedings of the 30th\nInternational Colloquium on Automata, Languages and Programming</em>\n(Lecture Notes in Computer Science: Volume 2719), Jos C. M. Baeten,\nJan Karel Lenstra, Joachim Parrow, and Gerhard J. Woeginger (eds.),\nBerlin: Springer, pp. 267\u2013277. doi:10.1007/3-540-45061-0_23",
                "Antunes, Luis, Lance Fortnow, Dieter van Melkebeek, and N.V.\nVinodchandran, 2006, \u201cComputational Depth: Concept and\nApplications\u201d, <em>Theoretical Computer Science</em>, 354(3):\n391\u2013404. doi:10.1016/j.tcs.2005.11.033",
                "Aquinas, St. Thomas, 1265\u20131274, <em>Summa\nTheologiae</em>.",
                "Arbuthnot, John, 1692, <em>Of the Laws of Chance, or, a method of\nCalculation of the Hazards of Game, Plainly demonstrated, And applied\nto Games as present most in Use</em>, translation of Huygens\u2019\n<em>De Ratiociniis in Ludo Aleae</em>, 1657.",
                "Aristotle. <em>Aristotle in 23 Volumes</em>, Vols. 17, 18,\ntranslated by Hugh Tredennick, Cambridge, MA: Harvard University\nPress; London, William Heinemann Ltd. 1933, 1989.",
                "Austen, Jane, 1815, <em>Emma</em>, London: Richard Bentley and\nSon.",
                "Bar-Hillel, Yehoshua and Rudolf Carnap, 1953, \u201cSemantic\nInformation\u201d, <em>The British Journal for the Philosophy of\nScience</em>, 4(14): 147\u2013157. doi:10.1093/bjps/IV.14.147",
                "Bais, F. Alexander and J. Doyne Farmer, 2008, \u201cThe Physics\nof Information\u201d, Adriaans and van Benthem 2008b: 609\u2013683.\ndoi:10.1016/B978-0-444-51726-5.50020-0",
                "Barron, Andrew, Jorma Rissanen, and Bin Yu, 1998, \u201cThe\nMinimum Description Length Principle in Coding and Modeling\u201d,\n<em>IEEE Transactions on Information Theory</em>, 44(6):\n2743\u20132760. doi:10.1109/18.720554",
                "Barwise, Jon and John Perry, 1983, <em>Situations and\nAttitudes</em>, Cambridge, MA: MIT Press.",
                "Bell, Gordon, Tony Hey, and Alex Szalay, 2009, \u201cComputer\nScience: Beyond the Data Deluge\u201d, <em>Science</em>, 323(5919):\n1297\u20131298. doi:10.1126/science.1170411",
                "Bennett, C. H., 1988, \u201cLogical Depth and Physical\nComplexity\u201d, in Rolf Herken (ed.), <em>The Universal Turing\nMachine: A Half-Century Survey</em>, Oxford: Oxford University Press,\npp. 227\u2013257.",
                "Berkeley, George, 1732, <em>Alciphron: Or the Minute\nPhilosopher</em>, Edinburgh: Thomas Nelson, 1948\u201357.",
                "Bernoulli, Danielis, 1738, <em>Hydrodynamica</em>, Argentorati:\nsumptibus Johannis Reinholdi Dulseckeri.\n [<a href=\"http://doi.org/10.3931/e-rara-3911\" target=\"other\">Bernoulli 1738 available online</a>]",
                "Birkhoff, George David, 1950, <em>Collected Mathematical\nPapers</em>, New York: American Mathematical Society.",
                "Bloem, Peter, Steven de Rooij, and Pieter Adriaans, 2015,\n\u201cTwo Problems for Sophistication\u201d, in <em>Algorithmic\nLearning Theory</em>, (Lecture Notes in Computer Science 9355),\nKamalika Chaudhuri, Claudio Gentile, and Sandra Zilles (eds.), Cham:\nSpringer International Publishing, 379\u2013394.\ndoi:10.1007/978-3-319-24486-0_25",
                "Boltzmann, Ludwig, 1866, \u201c\u00dcber die Mechanische\nBedeutung des Zweiten Hauptsatzes der W\u00e4rmetheorie\u201d,\n<em>Wiener Berichte</em>, 53: 195\u2013220.",
                "Boole, George, 1847, <em>Mathematical Analysis of Logic: Being an\nEssay towards a Calculus of Deductive Reasoning</em>, Cambridge:\nMacmillan, Barclay, &amp; Macmillan.\n [<a href=\"http://archive.org/details/mathematicalanal00booluoft\" target=\"other\">Boole 1847 available online</a>].",
                "\u2013\u2013\u2013, 1854, <em>An Investigation of the Laws of\nThought: On which are Founded the Mathematical Theories of Logic and\nProbabilities</em>, London: Walton and Maberly.",
                "Bostrom, Nick, 2003, \u201cAre We Living in a Computer\nSimulation?\u201d, <em>The Philosophical Quarterly</em>, 53(211):\n243\u2013255. doi:10.1111/1467-9213.00309",
                "Bott, R. and J. Milnor, 1958, \u201cOn the Parallelizability of\nthe Spheres\u201d, <em>Bulletin of the American Mathematical\nSociety</em>, 64(3): 87\u201389.\ndoi:10.1090/S0002-9904-1958-10166-4",
                "Bovens, Luc and Stephan Hartmann, 2003, <em>Bayesian\nEpistemology</em>, Oxford: Oxford University Press.\ndoi:10.1093/0199269750.001.0001",
                "Brenner, Joseph E., 2008, <em>Logic in Reality</em>, Dordrecht:\nSpringer Netherlands. doi:10.1007/978-1-4020-8375-4",
                "Briggs, Henry, 1624, <em>Arithmetica Logarithmica</em>, London:\nGulielmus Iones. ",
                "Capurro, Rafael, 1978, <em>Information. Ein Beitrag zur\netymologischen und ideengeschichtlichen Begr\u00fcndung des\nInformationsbegriffs</em> (Information: A contribution to the\nfoundation of the concept of information based on its etymology and in\nthe history of ideas), Munich, Germany: Saur.\n [<a href=\"http://www.capurro.de/info.html\" target=\"other\">Capurro 1978 available online</a>].",
                "\u2013\u2013\u2013, 2009, \u201cPast, Present, and Future of\nthe Concept of Information\u201d, <em>TripleC: Communication,\nCapitalism &amp; Critique</em>, 7(2): 125\u2013141.\ndoi:10.31269/triplec.v7i2.113",
                "Capurro, Rafael and Birger Hj\u00f8rland, 2003, \u201cThe\nConcept of Information\u201d, in Blaise Cronin (ed.), <em>Annual\nReview of Information Science and Technology (ARIST)</em>, 37:\n343\u2013411 (Chapter 8). doi:10.1002/aris.1440370109",
                "Capurro, Rafael and John Holgate (eds.), 2011, <em>Messages and\nMessengers: Angeletics as an Approach to the Phenomenology of\nCommunication</em> (<em>Von Boten Und Botschaften</em>,\n(Schriftenreihe Des International Center for Information Ethics 5),\nM\u00fcnchen: Fink.",
                "Carnap, Rudolf, 1928, <em>Scheinprobleme in der Philosophie</em>\n(Pseudoproblems of Philosophy), Berlin: Weltkreis-Verlag.",
                "\u2013\u2013\u2013, 1945, \u201cThe Two Concepts of\nProbability: The Problem of Probability\u201d, <em>Philosophy and\nPhenomenological Research</em>, 5(4): 513\u2013532.\ndoi:10.2307/2102817",
                "\u2013\u2013\u2013, 1947, <em>Meaning and Necessity</em>,\nChicago: The University of Chicago Press.",
                "\u2013\u2013\u2013, 1950, <em>Logical Foundations of\nProbability</em>, Chicago: The University of Chicago Press.",
                "Chaitin, Gregory J., 1969, \u201cOn the Length of Programs for\nComputing Finite Binary Sequences: Statistical Considerations\u201d,\n<em>Journal of the ACM</em>, 16(1): 145\u2013159.\ndoi:10.1145/321495.321506",
                "\u2013\u2013\u2013, 1987, <em>Algorithmic Information\nTheory</em>, Cambridge: Cambridge University Press.\ndoi:10.1017/CBO9780511608858",
                "Chater, Nick and Paul Vit\u00e1nyi, 2003, \u201cSimplicity: A\nUnifying Principle in Cognitive Science?\u201d, <em>Trends in\nCognitive Sciences</em>, 7(1): 19\u201322.\ndoi:10.1016/S1364-6613(02)00005-0",
                "Church, Alonzo, 1936, \u201cAn Unsolvable Problem of Elementary\nNumber Theory\u201d, <em>American Journal of Mathematics</em> 58(2):\n345\u2013363. doi:10.2307/2371045 ",
                "Cilibrasi, Rudi and Paul M.B. Vitanyi, 2005, \u201cClustering by\nCompression\u201d, <em>IEEE Transactions on Information Theory</em>,\n51(4): 1523\u20131545. doi:10.1109/TIT.2005.844059",
                "Clausius, R., 1850, \u201cUeber die bewegende Kraft der\nW\u00e4rme und die Gesetze, welche sich daraus f\u00fcr die\nW\u00e4rmelehre selbst ableiten lassen\u201d, <em>Annalen der Physik\nund Chemie</em>, 155(3): 368\u2013397.\ndoi:10.1002/andp.18501550306",
                "Conan Doyle, Arthur, 1892, \u201cThe Adventure of the Noble\nBachelor\u201d, in <em>The Adventures of Sherlock Holmes</em>,\nLondon: George Newnes Ltd, story 10.",
                "Cover, Thomas M. and Joy A. Thomas, 2006, <em>Elements of\nInformation Theory</em>, second edition, New York: John Wiley &amp;\nSons. ",
                "Crawford, James M. and Larry D. Auton, 1993, \u201cExperimental\nResults on the Crossover Point in Satisfiability Problems\u201d,\n<em>Proceedings of the Eleventh National Conference on Artificial\nIntelligence</em>, AAAI Press, pp. 21\u201327.\n [<a href=\"https://www.aaai.org/Library/AAAI/1993/aaai93-004.php\" target=\"other\">Crawford &amp; Auton 1993 available online</a>]",
                "Crutchfield, James P. and Karl Young, 1989, \u201cInferring\nStatistical Complexity\u201d, <em>Physical Review Letters</em>,\n63(2): 105\u2013108. doi:10.1103/PhysRevLett.63.105",
                "\u2013\u2013\u2013, 1990, \u201cComputation at the Onset of\nChaos\u201d, in <em>Entropy, Complexity, and the Physics of\nInformation</em>, W. Zurek, editor, SFI Studies in the Sciences of\nComplexity, VIII, Reading, MA: Addison-Wesley, pp. 223\u2013269.\n [<a href=\"http://csc.ucdavis.edu/~cmg/compmech/pubs/CompOnsetTitlePage.htm\" target=\"other\">Crutchfield &amp; Young 1990 available online</a>]",
                "D\u2019Alfonso, Simon, 2012, \u201cTowards a Framework for\nSemantic Information\u201d, Ph.D. Thesis, Department of Philosophy,\nSchool of Historical and Philosophical Studies, The University of\nMelbourne.\n <a href=\"http://minerva-access.unimelb.edu.au/handle/11343/37818\" target=\"other\">D\u2019Alfonso 2012 available online</a>",
                "Davis, Martin, 2006, \u201cWhy There Is No Such Discipline as\nHypercomputation\u201d, <em>Applied Mathematics and Computation</em>,\n178(1): 4\u20137. doi:10.1016/j.amc.2005.09.066",
                "Defoe, Daniel, 1719, <em>The Life and Strange Surprising\nAdventures of Robinson Crusoe of York, Mariner: who lived Eight and\nTwenty Years, all alone in an uninhabited Island on the coast of\nAmerica, near the Mouth of the Great River of Oroonoque; Having been\ncast on Shore by Shipwreck, wherein all the Men perished but himself.\nWith An Account how he was at last as strangely deliver\u2019d by\nPirates. Written by Himself</em>, London: W. Taylor.\n [<a href=\"http://www.pierre-marteau.com/editions/1719-robinson-crusoe.html\" target=\"other\">Defoe 1719 available online</a>]",
                "De Leo, Stefano, 1996, \u201cQuaternions and Special\nRelativity\u201d, <em>Journal of Mathematical Physics</em>, 37(6):\n2955\u20132968. doi:10.1063/1.531548",
                "Dershowitz, Nachum and Yuri Gurevich, 2008, \u201cA Natural\nAxiomatization of Computability and Proof of Church\u2019s\nThesis\u201d, <em>Bulletin of Symbolic Logic</em>, 14(3):\n299\u2013350. doi:10.2178/bsl/1231081370",
                "Descartes, Ren\u00e9, 1641, <em>Meditationes de Prima\nPhilosophia</em> (Meditations on First Philosophy), Paris. ",
                "\u2013\u2013\u2013, 1647, <em>Discours de la\nM\u00e9thode</em> (Discourse on Method), Leiden.",
                "Devlin, Keith and Duska Rosenberg, 2008, \u201cInformation in the\nStudy of Human Interaction\u201d, Adriaans and van Benthem 2008b:\n685\u2013709. doi:10.1016/B978-0-444-51726-5.50021-2",
                "Dictionnaire du Moyen Fran\u00e7ais (1330\u20131500), 2015,\n\u201cInformation\u201d, in <em>Dictionnaire du Moyen\nFran\u00e7ais (1330\u20131500)</em>, volume 16, 313\u2013315.\n [<a href=\"http://www.atilf.fr/dmf/\" target=\"other\">Dictionnaire du Moyen Fran\u00e7ais available online</a>]",
                "Domingos, Pedro, 1998, \u201cOccam\u2019s Two Razors: The Sharp\nand the Blunt\u201d, in <em>Proceedings of the Fourth International\nConference on Knowledge Discovery and Data Mining</em> (KDD\u201398),\nNew York: AAAI Press, pp. 37\u201343.\n [<a href=\"https://aaai.org/Library/KDD/1998/kdd98-006.php\" target=\"other\">Domingos 1998 available online</a>]",
                "Downey, Rodney G. and Denis R. Hirschfeldt, 2010, <em>Algorithmic\nRandomness and Complexity</em>, (Theory and Applications of\nComputability), New York: Springer New York.\ndoi:10.1007/978-0-387-68441-3",
                "Dretske, Fred, 1981, <em>Knowledge and the Flow of\nInformation</em>, Cambridge, MA: The MIT Press.",
                "Dufort, Paul A. and Charles J. Lumsden, 1994, \u201cThe\nComplexity and Entropy of Turing Machines\u201d, in <em>Proceedings\nWorkshop on Physics and Computation</em>. PhysComp \u201994, Dallas,\nTX: IEEE Computer Society Press, 227\u2013232.\ndoi:10.1109/PHYCMP.1994.363677",
                "Dunn, Jon Michael, 2001, \u201cThe Concept of Information and the\nDevelopment of Modern Logic\u201d, in <em>Zwischen traditioneller und\nmoderner Logik: Nichtklassiche Ansatze</em> (<em>Non-classical\nApproaches in the Transition from Traditional to Modern Logic</em>),\nWerner Stelzner and Manfred St\u00f6ckler (eds.), Paderborn: Mentis,\n423\u2013447.",
                "\u2013\u2013\u2013, 2008, \u201cInformation in Computer\nScience\u201d, in Adriaans and van Benthem 2008b: 581\u2013608.\ndoi:10.1016/B978-0-444-51726-5.50019-4",
                "Dijksterhuis, E. J., 1986, <em>The Mechanization of the World\nPicture: Pythagoras to Newton</em>, Princeton, NJ: Princeton\nUniversity Press.",
                "Duns Scotus, John [1265/66\u20131308 CE], <em>Opera Omnia</em>\n(The Wadding edition), Luke Wadding (ed.), Lyon, 1639; reprinted\nHildesheim: Georg Olms Verlagsbuchhandlung, 1968.",
                "Durand-Lose, J\u00e9r\u00f4me, 2002, \u201cComputing Inside\nthe Billiard Ball Model\u201d, in <em>Collision-Based Computing</em>,\nAndrew Adamatzky (ed.), London: Springer London, 135\u2013160.\ndoi:10.1007/978-1-4471-0129-1_6",
                "Edwards, Paul, 1967, <em>The Encyclopedia of Philosophy</em>, 8\nvolumes, New York: Macmillan Publishing Company.",
                "Fayyad, Usama, Gregory Piatetsky-Shapiro, and Padhraic Smyth,\n1996, \u201cFrom Data Mining to Knowledge Discovery in\nDatabases\u201d, <em>AI Magazine</em>, 17(3): 37\u201337.",
                "Fisher, R. A., 1925, \u201cTheory of Statistical\nEstimation\u201d, <em>Mathematical Proceedings of the Cambridge\nPhilosophical Society</em>, 22(05): 700\u2013725.\ndoi:10.1017/S0305004100009580",
                "Floridi, Luciano, 1999, \u201cInformation Ethics: On the\nPhilosophical Foundation of Computer Ethics\u201d, <em>Ethics and\nInformation Technology</em>, 1(1): 33\u201352.\ndoi:10.1023/A:1010018611096",
                "\u2013\u2013\u2013, 2002, \u201cWhat Is the Philosophy of\nInformation?\u201d <em>Metaphilosophy</em>, 33(1\u20132):\n123\u2013145. doi:10.1111/1467-9973.00221",
                "\u2013\u2013\u2013 (ed.), 2003, <em>The Blackwell Guide to the\nPhilosophy of Computing and Information</em>, Oxford: Blackwell.\ndoi:10.1002/9780470757017",
                "\u2013\u2013\u2013, 2010, \u201cThe Philosophy of Information\nas a Conceptual Framework\u201d, <em>Knowledge, Technology &amp;\nPolicy</em>, 23(1\u20132): 253\u2013281.\ndoi:10.1007/s12130-010-9112-x",
                "\u2013\u2013\u2013, 2011, <em>The Philosophy of\nInformation</em>, Oxford: Oxford University Press.\ndoi:10.1093/acprof:oso/9780199232383.001.0001",
                "\u2013\u2013\u2013, 2019, <em>The logic of information: a\ntheory of philosophy as conceptual design</em>, Oxford: Oxford\nUniversity Press. doi:10.1093/oso/9780198833635.001.0001",
                "Fredkin, Edward and Tommaso Toffoli, 1982, \u201cConservative\nLogic\u201d, <em>International Journal of Theoretical Physics</em>,\n21(3\u20134): 219\u2013253. doi:10.1007/BF01857727",
                "Frege, Gottlob, 1879, <em>Begriffsschrift: eine der arithmetischen\nnachgebildete Formelsprache des reinen Denkens</em>, Halle.",
                "\u2013\u2013\u2013, 1892, \u201c\u00dcber Sinn und\nBedeutung\u201d, <em>Zeitschrift f\u00fcr Philosophie und\nphilosophische Kritik</em>, NF 100.",
                "Furey, C., 2015, \u201cCharge Quantization from a Number\nOperator\u201d, <em>Physics Letters B</em>, 742(March):\n195\u2013199. doi:10.1016/j.physletb.2015.01.023",
                "Galileo Galilei, 1623 [1960], <em>Il Saggiatore</em> (in Italian),\nRome; translated as <em>The Assayer</em>, by Stillman Drake and C. D.\nO\u2019Malley, in <em>The Controversy on the Comets of 1618</em>,\nPhiladelphia: University of Pennsylvania Press, 1960,\n151\u2013336.",
                "Garey, Michael R. and David S. Johnson, 1979, <em>Computers and\nIntractability: A Guide to the Theory of NP-Completeness</em>, (A\nSeries of Books in the Mathematical Sciences), San Francisco: W. H.\nFreeman.",
                "Gell-Mann, Murray and Seth Lloyd, 2003, \u201cEffective\nComputing\u201d. SFI Working Paper 03-12-068, Santa Fe, NM: Santa Fe\nInstitute.\n [<a href=\"https://www.santafe.edu/research/results/working-papers/effective-complexity\" target=\"other\">Gell-Mann &amp; Lloyd 2003 available online</a>]",
                "Gibbs, J. Willard, 1906, <em>The Scientific Papers of J. Willard\nGibbs in Two Volumes</em>, 1. Longmans, Green, and Co.",
                "Godefroy, Fr\u00e9d\u00e9ric G., 1881, <em>Dictionnaire de\nl\u2019ancienne langue fran\u00e7aise et de tous ses dialectes du\n9e au 15e si\u00e8cle</em>, Paris: F. Vieweg.",
                "G\u00f6del, Kurt, 1931, \u201c\u00dcber formal unentscheidbare\nS\u00e4tze der Principia Mathematica und verwandter Systeme I\u201d,\n<em>Monatshefte f\u00fcr Mathematik und Physik</em>, 38\u201338(1):\n173\u2013198. doi:10.1007/BF01700692",
                "Goodstein, R. L., 1957, \u201cThe Definition of Number\u201d,\n<em>The Mathematical Gazette</em>, 41(337): 180\u2013186.\ndoi:10.2307/3609188",
                "Gr\u00fcnwald, Peter D., 2007, <em>The Minimum Description Length\nPrinciple</em>, Cambridge, MA: MIT Press.",
                "Gr\u00fcnwald, Peter D. and Paul M.B. Vit\u00e1nyi, 2008,\n\u201cAlgorithmic Information Theory\u201d, in Adriaans and van\nBenthem 2008b: 281\u2013317.\ndoi:10.1016/B978-0-444-51726-5.50013-3",
                "Groot, Adrianus Dingeman de, 1961 [1969], <em>Methodology:\nFoundations of Inference and Research in the Behavioral Sciences</em>\n(<em>Methodologie: grondslagen van onderzoek en denken in de\ngedragswetenschappen</em>), The Hague: Mouton. ",
                "Hamkins, J., and Lewis, A., 2000, <em>Infinite time Turing\nmachines. Journal of Symbolic Logic</em>, 65(2), 567-604.\ndoi:10.2307/2586556 ",
                "Harremo\u00ebs, Peter and Flemming Tops\u00f8e, 2008, \u201cThe\nQuantitative Theory of Information\u201d, in Adriaans and van Benthem\n2008b: 171\u2013216. doi:10.1016/B978-0-444-51726-5.50011-X",
                "Hartley, R.V.L., 1928, \u201cTransmission of Information\u201d,\n<em>Bell System Technical Journal</em>, 7(3): 535\u2013563.\ndoi:10.1002/j.1538-7305.1928.tb01236.x",
                "Hazard, Paul, 1935, <em>La Crise de La Conscience\nEurop\u00e9enne (1680\u20131715)</em>, Paris: Boivin.",
                "Hey, Anthony J. G., Stewart Tansley, and Kristin Tolle (eds.),\n2009, <em>The Fourth Paradigm: Data-Intensive Scientific\nDiscovery</em>, Redmond, WA: Microsoft Research.\n [<a href=\"https://www.microsoft.com/en-us/research/publication/fourth-paradigm-data-intensive-scientific-discovery/\" target=\"other\">Hey et al. 2009 available online</a>]",
                "Hintikka, Jaakko, 1962, <em>Knowledge and Belief: An Introduction\nto the Logic of the Two Notions</em>, (Contemporary Philosophy),\nIthaca, NY: Cornell University Press.",
                "\u2013\u2013\u2013, 1973, <em>Logic, Language Games and\nInformation: Kantian Themes in the Philosophy of Logic</em>, Oxford:\nClarendon Press.",
                "Hume, David, 1739\u201340, <em>A Treatise of Human Nature</em>.\nReprinted, L.A. Selby-Bigge (ed.), Oxford: Clarendon Press, 1896.\n [<a href=\"http://oll.libertyfund.org/titles/hume-a-treatise-of-human-nature\" target=\"other\">Hume 1739\u201340 [1896] available online</a>]",
                "\u2013\u2013\u2013, 1748, <em>An Enquiry concerning Human\nUnderstanding</em>. Reprinted in <em>Enquiries Concerning the Human\nUnderstanding and Concerning the Principles of Morals</em>, 1777 which\nwas reprinted, L.A. Selby-Bigge (ed.), Oxford: Clarendon Press, 1888\n(second edition 1902).\n [<a href=\"http://oll.libertyfund.org/titles/hume-enquiries-concerning-the-human-understanding-and-concerning-the-principles-of-morals\" target=\"other\">Hume 1748 [1902] available online</a>]",
                "Hutter, Marcus, 2005, <em>Universal Artificial Intellegence:\nSequential Decisions Based on Algorithmic Probability</em>, (Texts in\nTheoretical Computer Science, an EATCS Series), Berlin, Heidelberg:\nSpringer Berlin Heidelberg. doi:10.1007/b138233",
                "\u2013\u2013\u2013, 2007a, \u201cOn Universal Prediction and\nBayesian Confirmation\u201d, <em>Theoretical Computer Science</em>,\n384(1): 33\u201348. doi:10.1016/j.tcs.2007.05.016",
                "\u2013\u2013\u2013, 2007b, \u201cAlgorithmic Information\nTheory: a brief non-technical guide to the field\u201d,\n<em>Scholarpedia</em>, 2(3): art. 2519.\ndoi:10.4249/scholarpedia.2519",
                "\u2013\u2013\u2013, 2010, \u201cA Complete Theory of\nEverything (will be subjective)\u201d, <em>Algorithms</em>, 3(4):\n329\u2013350. doi:10.3390/a3040329",
                "Hutter, Marcus, John W. Lloyd, Kee Siong Ng, and William T.B.\nUther, 2013, \u201cProbabilities on Sentences in an Expressive\nLogic\u201d, <em>Journal of Applied Logic</em>, special issue:\n<em>Combining Probability and Logic: Papers from Progic 2011</em>,\nJeffrey Helzner (ed.), 11(4): 386\u2013420.\ndoi:10.1016/j.jal.2013.03.003. ",
                "Ibn Tufail, <em>Hayy ibn Yaqdhan</em>, translated as\n<em>Philosophus Autodidactus</em>, published by Edward Pococke the\nYounger in 1671.",
                "Kahn, David, 1967, <em>The Code-Breakers, The Comprehensive\nHistory of Secret Communication from Ancient Times to the\nInternet</em>, New York: Scribner.",
                "Kant, Immanuel, 1781, <em>Kritik der reinen Vernunft</em>\n(Critique of Pure Reason), Germany. ",
                "Kervaire, Michel A., 1958, \u201cNon-Parallelizability of the\nn-Sphere for n &gt; 7\u201d, <em>Proceedings of the National Academy\nof Sciences of the United States of America</em>, 44(3):\n280\u2013283. doi:10.1073/pnas.44.3.280",
                "al-Khw\u0101rizm\u012b, Mu\u1e25ammad ibn M\u016bs\u0101, ca. 820\nCE, <em>Hisab al-jabr w\u2019al-muqabala, Kitab al-Jabr\nwa-l-Muqabala</em> (<em>The Compendious Book on Calculation by\nCompletion and Balancing</em>), Translated by Frederic Rosen, London:\nMurray, 1831.\n [<a href=\"http://www.wilbourhall.org/index.html#algebra\" target=\"other\">al-Khwarizmi translation available online</a>]",
                "Kolmogorov, A.N., 1965, \u201cThree Approaches to the\nQuantitative Definition of Information\u201d, <em>Problems of\nInformation Transmission</em>, 1(1): 1\u20137. Reprinted 1968 in\n<em>International Journal of Computer Mathematics</em>, 2(1\u20134):\n157\u2013168. doi:10.1080/00207166808803030",
                "Koppel, Moshe, 1987, \u201cComplexity, Depth, and\nSophistication\u201d, <em>Complex Systems</em>, 1(6):\n1087\u20131091.\n [<a href=\"https://www.complex-systems.com/abstracts/v01_i06_a04/\" target=\"other\">Koppel 1987 available online</a>]",
                "Kripke, Saul A., 1959, \u201cA Completeness Theorem in Modal\nLogic\u201d, <em>The Journal of Symbolic Logic</em>, 24(1):\n1\u201314. doi:10.2307/2964568",
                "\u2013\u2013\u2013, 1971, \u201cIdentity and Necessity\u201d,\nin Milton K. Munitz (ed.), <em>Identity and Individuation</em>, New\nYork: New York University Press, pp. 135-164.",
                "Kuipers, Theo A.F. (ed.), 2007a, <em>General Philosophy of\nScience: Focal Issues</em>, Amsterdam: Elsevier Science\nPublishers.",
                "\u2013\u2013\u2013, 2007b, \u201cExplanation in Philosophy of\nScience\u201d, in Kuipers 2007a.",
                "Landauer, Rolf, 1961, \u201cIrreversibility and heat generation\nin the computing process\u201d, <em>IBM Journal of Research and\nDevelopment</em>, 5(3): 183\u2013191. doi:10.1147/rd.53.0183",
                "\u2013\u2013\u2013, 1991, \u201cInformation is\nPhysical\u201d, <em>Physics Today</em>, 44(5): 23\u201329. doi:\n10.1063/1.881299",
                "Langton, Chris G., 1990, \u201cComputation at the Edge of Chaos:\nPhase Transitions and Emergent Computation\u201d, <em>Physica D:\nNonlinear Phenomena</em>, 42(1\u20133): 12\u201337.\ndoi:10.1016/0167-2789(90)90064-V",
                "Laplace, Pierre Simon, Marquis de, 1814 [1902], <em>A\nPhilosophical Essay on Probabilities</em>, F.W. Truscott and F.L.\nEmory (trans.), New York: J. Wiley; London: Chapman &amp; Hall.",
                "Lenski, Wolfgang, 2010, \u201cInformation: A Conceptual\nInvestigation\u201d, <em>Information 2010</em>, 1(2): 74\u2013118.\ndoi:10.3390/info1020074",
                "Levin, Leonid A., 1973, \u201cUniversal Sequential Search\nProblems\u201d, <em>Problems of Information Transmission</em>, 9(3):\n265\u2013266.",
                "\u2013\u2013\u2013,1974, \u201cLaws of Information\nConservation (Non-Growth) and Aspects of the Foundation of Probability\nTheory\u201d, <em>Problems of Information Transmission</em>, 10(3):\n206\u2013210.",
                "\u2013\u2013\u2013, 1984, \u201cRandomness Conservation\nInequalities; Information and Independence in Mathematical\nTheories\u201d, <em>Information and Control</em>, 61(1): 15\u201337.\ndoi:10.1016/S0019-9958(84)80060-1",
                "Li, Ming and Paul Vit\u00e1nyi, 2019, <em>An Introduction to\nKolmogorov Complexity and Its Applications</em>, (Texts in Computer\nScience), New York: Springer New York.\ndoi:10.1007/978-0-387-49820-1",
                "Lloyd, Seth, 2000, \u201cUltimate Physical Limits to\nComputation\u201d, <em>Nature</em>, 406(6799): 1047\u20131054.\ndoi:10.1038/35023282",
                "Lloyd, Seth and Y. Jack Ng, 2004, \u201cBlack Hole\nComputers\u201d, <em>Scientific American</em>, 291(5): 52\u201361.\ndoi:10.1038/scientificamerican1104-52",
                "Locke, John, 1689, <em>An Essay Concerning Human\nUnderstanding</em>, J. W. Yolton (ed.), London: Dent; New York:\nDutton, 1961.\n [<a href=\"https://oll.libertyfund.org/titles/locke-the-works-of-john-locke-in-nine-volumes\" target=\"other\">Locke 1689 available online</a>]",
                "Long, B.R., 2014, \u201cInformation is intrinsically semantic but\nalethically neutral\u201d, <em>Synthese</em>, 191: 3447\u20133467.\ndoi:10.1007/s11229-014-0457-7",
                "\u2013\u2013\u2013, 2019, \u201cA Scientific Metaphysics and\nOckham\u2019s Razor\u201d, <em>Axiomathes</em>, 5: 1\u201331.\ndoi:10.1007/s10516-019-09430-5",
                "Lundgren, B., 2019, \u201cDoes semantic information need to be\ntruthful?\u201d, <em>Synthese</em>, 196: 2885\u20132906.\ndoi:10.1007/s11229-017-1587-5",
                "Maat, Jaap, 2004, <em>Philosophical Languages in the Seventeenth\nCentury: Dalgarno, Wilkins, Leibniz</em>, The New Synthese Historical\nLibrary (Book 54), Springer. ",
                "McAllister, James W., 2003, \u201cEffective Complexity as a\nMeasure of Information Content\u201d, <em>Philosophy of Science</em>,\n70(2): 302\u2013307. doi:10.1086/375469",
                "Mill, John Stuart, 1843, <em>A System of Logic</em>, London.",
                "Montague, Richard, 2008, \u201cUniversal Grammar\u201d,\n<em>Theoria</em>, 36(3): 373\u2013398.\ndoi:10.1111/j.1755-2567.1970.tb00434.x",
                "Mugur-Sch\u00e4chter, Mioara, 2003, \u201cQuantum Mechanics\nVersus a Method of Relativized Conceptualization\u201d, in\n<em>Quantum Mechanics, Mathematics, Cognition and Action</em>, Mioara\nMugur-Sch\u00e4chter and Alwyn van der Merwe (eds.), Dordrecht:\nSpringer Netherlands, 109\u2013307. doi:10.1007/0-306-48144-8_7",
                "Napier, John, 1614, <em>Mirifici Logarithmorum Canonis\nDescriptio</em> (<em>The Description of the Wonderful Canon of\nLogarithms</em>), Edinburgh: Andre Hart. Translated and annotated by\nIan Bruce, www.17centurymaths.com.\n [<a href=\"http://www.17centurymaths.com/contents/napiercontents.html\" target=\"other\">Napier 1614 [Bruce translation] available online</a>].",
                "Nielsen, Michael A. and Isaac L. Chuang, 2000, <em>Quantum\nComputation and Quantum Information</em>, Cambridge: Cambridge\nUniversity Press.",
                "Nies, Andr\u00e9, 2009, <em>Computability and Randomness</em>,\nOxford: Oxford University Press.\ndoi:10.1093/acprof:oso/9780199230761.001.0001",
                "Nyquist, H., 1924, \u201cCertain Factors Affecting Telegraph\nSpeed\u201d, <em>Bell System Technical Journal</em>, 3(2):\n324\u2013346. doi:10.1002/j.1538-7305.1924.tb01361.x",
                "Ong, Walter J., 1958, <em>Ramus, Method, and the Decay of\nDialogue, From the Art of Discourse to the Art of Reason</em>,\nCambridge MA: Harvard University Press.",
                "Parikh, Rohit and Ramaswamy Ramanujam, 2003, \u201cA Knowledge\nBased Semantics of Messages\u201d, <em>Journal of Logic, Language and\nInformation</em>, 12(4): 453\u2013467.\ndoi:10.1023/A:1025007018583",
                "Peirce, Charles S., 1868, \u201cUpon Logical Comprehension and\nExtension\u201d, <em>Proceedings of the American Academy of Arts and\nSciences</em>, 7: 416\u2013432. doi:10.2307/20179572",
                "\u2013\u2013\u2013, 1886 [1993], \u201c Letter Peirce to A.\nMarquand\u201d, Reprinted in <em>Writings of Charles S. Peirce: A\nChronological Edition, Volume 5: 1884\u20131886</em>, Indianapolis:\nIndiana University Press, pp. 424\u2013427. See also Arthur W. Burks,\n1978, \u201cBook Review: \u2018The New Elements of\nMathematics\u2019 by Charles S. Peirce, Carolyn Eisele\n(editor)\u201d, <em>Bulletin of the American Mathematical\nSociety</em>, 84(5): 913\u2013919.\ndoi:10.1090/S0002-9904-1978-14533-9",
                "Popper, Karl, 1934, <em>The Logic of Scientific Discovery</em>,\n(<em>Logik der Forschung</em>), English translation 1959, London:\nHutchison. Reprinted 1977.",
                "Putnam, Hilary, 1988, <em>Representation and reality</em>,\nCambridge, MA: The MIT Press.",
                "Quine, W.V.O., 1951, \u201cMain Trends in Recent Philosophy: Two\nDogmas of Empiricism\u201d, <em>The Philosophical Review</em>, 60(1):\n20\u201343. Reprinted in his 1953 <em>From a Logical Point of\nView</em>, Cambridge, MA: Harvard University Press.\ndoi:10.2307/2181906",
                "Rathmanner, Samuel and Marcus Hutter, 2011, \u201cA Philosophical\nTreatise of Universal Induction\u201d, <em>Entropy</em>, 13(6):\n1076\u20131136. doi:10.3390/e13061076",
                "R\u00e9dei, Mikl\u00f3s and Michael St\u00f6ltzner (eds.),\n2001, <em>John von Neumann and the Foundations of Quantum\nPhysics</em>, (Vienna Circle Institute Yearbook, 8), Dordrecht:\nKluwer.",
                "R\u00e9nyi, Alfr\u00e9d, 1961, \u201cOn Measures of Entropy\nand Information\u201d, in <em>Proceedings of the Fourth Berkeley\nSymposium on Mathematical Statistics and Probability, Volume 1:\nContributions to the Theory of Statistics</em>, Berkeley, CA: The\nRegents of the University of California, pp. 547\u2013561.\n [<a href=\"https://projecteuclid.org/euclid.bsmsp/1200512181\" target=\"other\">R\u00e9nyi 1961 available online</a>]",
                "Rissanen, J., 1978, \u201cModeling by Shortest Data\nDescription\u201d, <em>Automatica</em>, 14(5): 465\u2013471.\ndoi:10.1016/0005-1098(78)90005-5",
                "\u2013\u2013\u2013, 1989, <em>Stochastic Complexity in\nStatistical Inquiry</em>, (World Scientific Series in Computer\nScience, 15), Singapore: World Scientific.",
                "Rooy, Robert van, 2004, \u201cSignalling Games Select Horn\nStrategies\u201d, <em>Linguistics and Philosophy</em>, 27(4):\n493\u2013527. doi:10.1023/B:LING.0000024403.88733.3f",
                "Russell, Bertrand, 1905, \u201cOn Denoting\u201d, <em>Mind</em>,\nnew series, 14(4): 479\u2013493. doi:10.1093/mind/XIV.4.479",
                "Schmandt-Besserat, Denise, 1992, <em>Before Writing</em> (Volume\nI: From Counting to Cuneiform), Austin, TX: University of Texas\nPress.",
                "Schmidhuber, J\u00fcurgen, 1997a, \u201cLow-Complexity\nArt\u201d, <em>Leonardo</em>, 30(2): 97\u2013103.\ndoi:10.2307/1576418",
                "\u2013\u2013\u2013, 1997b, \u201cA Computer Scientist\u2019s\nView of Life, the Universe, and Everything\u201d, in <em>Foundations\nof Computer Science</em>, (Lecture Notes in Computer Science, 1337),\nChristian Freksa, Matthias Jantzen, and R\u00fcdiger Valk (eds.),\nBerlin, Heidelberg: Springer Berlin Heidelberg, 201\u2013208.\ndoi:10.1007/BFb0052088",
                "Schnelle, H., 1976, \u201cInformation\u201d, in Joachim Ritter\n(ed.), <em>Historisches W\u00f6rterbuch der Philosophie</em>, IV\n[Historical dictionary of philosophy, IV] (pp. 116\u2013117).\nStuttgart, Germany: Schwabe.",
                "Searle, John R., 1990, \u201cIs the Brain a Digital\nComputer?\u201d, <em>Proceedings and Addresses of the American\nPhilosophical Association</em>, 64(3): 21\u201337.\ndoi:10.2307/3130074",
                "Seiffert, Helmut, 1968, <em>Information \u00fcber die\nInformation</em> [Information about information] Munich: Beck.",
                "Shannon, Claude E., 1948, \u201cA Mathematical Theory of\nCommunication\u201d, <em>Bell System Technical Journal</em>, 27(3):\n379\u2013423 &amp; 27(4): 623\u2013656.\ndoi:10.1002/j.1538-7305.1948.tb01338.x &amp;\ndoi:10.1002/j.1538-7305.1948.tb00917.x",
                "Shannon, Claude E. and Warren Weaver, 1949, <em>The Mathematical\nTheory of Communication</em>, Urbana, IL: University of Illinois\nPress.",
                "Shor, Peter W., 1997, \u201cPolynomial-Time Algorithms for Prime\nFactorization and Discrete Logarithms on a Quantum Computer\u201d,\n<em>SIAM Journal on Computing</em>, 26(5): 1484\u20131509.\ndoi:10.1137/S0097539795293172",
                "Simon, J.C. and Olivier Dubois, 1989, \u201cNumber of Solutions\nof Satisfiability Instances \u2013 Applications to Knowledge\nBases\u201d, <em>International Journal of Pattern Recognition and\nArtificial Intelligence</em>, 3(1): 53\u201365.\ndoi:10.1142/S0218001489000061",
                "Simondon, Gilbert, 1989, <em>L\u2019individuation Psychique et\nCollective: \u00c0 La Lumi\u00e8re des Notions de Forme,\nInformation, Potentiel et M\u00e9tastabilit\u00e9\n(L\u2019Invention Philosophique)</em>, Paris: Aubier.",
                "Singh, Simon, 1999, <em>The Code Book: The Science of Secrecy from\nAncient Egypt to Quantum Cryptography</em>, New York: Anchor\nBooks.",
                "Solomonoff, R. J., 1960, \u201cA Preliminary Report on a General\nTheory of Inductive Inference\u201d. Report ZTB-138, Cambridge, MA:\nZator.\n [<a href=\"http://raysolomonoff.com/publications/pubs.html\" target=\"other\">Solomonoff 1960 available online</a>]",
                "\u2013\u2013\u2013, 1964a, \u201cA Formal Theory of Inductive\nInference. Part I\u201d, <em>Information and Control</em>, 7(1):\n1\u201322. doi:10.1016/S0019-9958(64)90223-2",
                "\u2013\u2013\u2013, 1964b, \u201cA Formal Theory of Inductive\nInference. Part II\u201d, <em>Information and Control</em>, 7(2):\n224\u2013254. doi:10.1016/S0019-9958(64)90131-7",
                "\u2013\u2013\u2013, 1997, \u201cThe Discovery of Algorithmic\nProbability\u201d, <em>Journal of Computer and System Sciences</em>,\n55(1): 73\u201388. doi:10.1006/jcss.1997.1500",
                "Stalnaker, Richard, 1984, <em>Inquiry</em>, Cambridge, MA: MIT\nPress.",
                "Stifel, Michael, 1544, <em>Arithmetica integra</em>, Nuremberg:\nJohan Petreium.",
                "Tarski, Alfred, 1944, \u201cThe Semantic Conception of Truth: And\nthe Foundations of Semantics\u201d, <em>Philosophy and\nPhenomenological Research</em>, 4(3): 341\u2013376.\ndoi:10.2307/2102968",
                "Tsallis, Constantino, 1988, \u201cPossible Generalization of\nBoltzmann-Gibbs Statistics\u201d, <em>Journal of Statistical\nPhysics</em>, 52(1\u20132): 479\u2013487.\ndoi:10.1007/BF01016429",
                "Turing, A. M., 1937, \u201cOn Computable Numbers, with an\nApplication to the Entscheidungsproblem\u201d, <em>Proceedings of the\nLondon Mathematical Society</em>, s2-42(1): 230\u2013265.\ndoi:10.1112/plms/s2-42.1.230",
                "Valiant, Leslie G., 2009, \u201cEvolvability\u201d, <em>Journal\nof the ACM</em>, 56(1): Article 3. doi:10.1145/1462153.1462156",
                "van Benthem, Johan F.A.K., 1990, \u201cKunstmatige Intelligentie:\nEen Voortzetting van de Filosofie met Andere Middelen\u201d,\n<em>Algemeen Nederlands Tijdschrift voor Wijsbegeerte</em>, 82:\n83\u2013100.",
                "\u2013\u2013\u2013, 2006, \u201cEpistemic Logic and\nEpistemology: The State of Their Affairs\u201d, <em>Philosophical\nStudies</em>, 128(1): 49\u201376. doi:10.1007/s11098-005-4052-0",
                "van Benthem, Johan and Robert van Rooy, 2003, \u201cConnecting\nthe Different Faces of Information\u201d, <em>Journal of Logic,\nLanguage and Information</em>, 12(4): 375\u2013379.\ndoi:10.1023/A:1025026116766",
                "van Peursen, Cornelis Anthonie, 1987, \u201cChristian\nWolff\u2019s Philosophy of Contingent Reality\u201d, <em>Journal of\nthe History of Philosophy</em>, 25(1): 69\u201382.\ndoi:10.1353/hph.1987.0005",
                "van Rooij, Robert, 2003, \u201cQuestioning to resolve decision\nproblems\u201d, <em>Linguistics and Philosophy</em>, 26:\n727\u2013763.",
                "Vereshchagin, Nikolai K. and Paul M.B. Vit\u00e1nyi, 2004,\n\u201cKolmogorov\u2019s Structure Functions and Model\nSelection\u201d, <em>IEEE Transactions on Information Theory</em>,\n50(12): 3265\u20133290. doi:10.1109/TIT.2004.838346",
                "Verlinde, Erik, 2011, \u201cOn the Origin of Gravity and the Laws\nof Newton\u201d, <em>Journal of High Energy Physics</em>, 2011(4).\ndoi:10.1007/JHEP04(2011)029 ",
                "\u2013\u2013\u2013, 2017, \u201cEmergent Gravity and the Dark\nUniverse\u201d, <em>SciPost Physics</em>, 2(3): 016.\ndoi:10.21468/SciPostPhys.2.3.016",
                "Vigo, Ronaldo, 2011, \u201cRepresentational Information: A New\nGeneral Notion and Measure of Information\u201d, <em>Information\nSciences</em>, 181(21): 4847\u20134859.\ndoi:10.1016/j.ins.2011.05.020",
                "\u2013\u2013\u2013, 2012, \u201cComplexity over Uncertainty in\nGeneralized Representational Information Theory (GRIT): A\nStructure-Sensitive General Theory of Information\u201d,\n<em>Information</em>, 4(1): 1\u201330. doi:10.3390/info4010001",
                "Vit\u00e1nyi, Paul M., 2006, \u201cMeaningful\nInformation\u201d, <em>IEEE Transactions on Information Theory</em>,\n52(10): 4617\u20134626. doi:10.1109/TIT.2006.881729\n [<a href=\"http://arxiv.org/abs/cs.CC/0111053\" target=\"other\">Vit\u00e1nyi 2006 available online</a>].",
                "Vogel, Cornelia Johanna de, 1968, <em>Plato: De filosoof van het\ntranscendente</em>, Baarn: Het Wereldvenster.",
                "Von Neumann, John, 1932, <em>Mathematische Grundlagen der\nQuantenmechanik</em>, Berlin: Springer.",
                "Wallace, C. S., 2005, <em>Statistical and Inductive Inference by\nMinimum Message Length</em>, Berlin: Springer.\ndoi:10.1007/0-387-27656-4",
                "Wheeler, John Archibald, 1990, \u201cInformation, Physics,\nQuantum: The Search for Links\u201d, in <em>Complexity, Entropy and\nthe Physics of Information</em>, Wojciech H. Zurek (ed.), Boulder, CO:\nWestview Press, 309\u2013336.\n [<a href=\"https://philpapers.org/go.pl?id=WHEIPQ&amp;u=https%3A%2F%2Fphilpapers.org%2Farchive%2FWHEIPQ.pdf\" target=\"other\">Wheeler 1990 available online</a>]",
                "Whitehead, Alfred and Bertrand Russell, 1910, 1912, 1913,\n<em>Principia Mathematica</em>, 3 vols, Cambridge: Cambridge\nUniversity Press; 2nd edn, 1925 (Vol. 1), 1927 (Vols 2, 3).",
                "Wilkins, John, 1668, \u201cAn Essay towards a Real Character, and\na Philosophical Language\u201d, London.\n [<a href=\"https://archive.org/details/AnEssayTowardsARealCharacterAndAPhilosophicalLanguage/page/n7\" target=\"other\">Wilkins 1668 available online</a>]",
                "Windelband, Wilhelm, 1903, <em>Lehrbuch der Geschichte der\nPhilosophie</em>, T\u00fcbingen: J.C.B. Mohr. ",
                "Wolff, J. Gerard, 2006, <em>Unifying Computing and Cognition</em>,\nMenai Bridge: CognitionResearch.org.uk.",
                "Wolfram, Stephen, 2002, <em>A New Kind of Science</em>, Champaign,\nIL: Wolfram Media.",
                "Wolpert, David H. and William Macready, 2007, \u201cUsing\nSelf-Dissimilarity to Quantify Complexity\u201d, <em>Complexity</em>,\n12(3): 77\u201385. doi:10.1002/cplx.20165",
                "Wu, Kun, 2010, \u201cThe Basic Theory of the Philosophy of\nInformation\u201d, in <em>Proceedings of the 4th International\nConference on the Foundations of Information Science</em>, Beijing,\nChina, Pp. 21\u201324.",
                "\u2013\u2013\u2013, 2016, \u201cThe Interaction and\nConvergence of the Philosophy and Science of Information\u201d,\n<em>Philosophies</em>, 1(3): 228\u2013244.\ndoi:10.3390/philosophies1030228",
                "Zuse, Konrad, 1969, <em>Rechnender Raum</em>, Braunschweig:\nFriedrich Vieweg &amp; Sohn. Translated as <em>Calculating Space</em>,\nMIT Technical Translation AZT-70-164-GEMIT, MIT (Proj. MAC),\nCambridge, MA, Feb. 1970. English revised by A. German and H. Zenil\n2012.\n [<a href=\"https://philpapers.org/archive/ZUSRR.pdf\" target=\"other\">Zuse 1969 [2012] available online</a>]"
            ]
        },
        "raw_text": "<div id=\"bibliography\">\n<h2 id=\"Bib\">Bibliography</h2>\n<ul class=\"hanging\">\n<li>Aaronson, Scott, 2013, \u201cWhy Philosophers Should Care About\nComputational Complexity\u201d, in <em>Computability: Turing,\nG\u00f6del, Church, and Beyond</em>, Brian Jack Copeland, Carl J.\nPosy, and Oron Shagrir (eds.), Cambridge, MA: The MIT Press.\n [<a href=\"http://philsci-archive.pitt.edu/8748/\" target=\"other\">Aaronson 2013 preprint available online</a>]</li>\n<li>Abramsky, Samson and Achim Jung, 1994, \u201cDomain\ntheory\u201d, in <em>Handbook of Logic in Computer Science (vol. 3):\nSemantic Structure</em>, Samson Abramsky, Dov M. Gabbay, and Thomas S.\nE. Maibaum (eds.),. Oxford University Press. pp. 1\u2013168.</li>\n<li>Adams, Fred and Jo\u00e3o Antonio de Moraes, 2016, \u201cIs\nThere a Philosophy of Information?\u201d, <em>Topoi</em>, 35(1):\n161\u2013171. doi:10.1007/s11245-014-9252-9</li>\n<li>Adriaans, Pieter, 2007, \u201cLearning as Data\nCompression\u201d, in <em>Computation and Logic in the Real\nWorld</em>, S. Barry Cooper, Benedikt L\u00f6we, and Andrea Sorbi\n(eds.), (Lecture Notes in Computer Science: Volume 4497), Berlin,\nHeidelberg: Springer Berlin Heidelberg, 11\u201324.\ndoi:10.1007/978-3-540-73001-9_2</li>\n<li>\u2013\u2013\u2013, 2008, \u201cBetween Order and Chaos: The\nQuest for Meaningful Information\u201d, <em>Theory of Computing\nSystems</em> (Special Issue: Computation and Logic in the Real World;\nGuest Editors: S. Barry Cooper, Elvira Mayordomo and Andrea Sorbi),\n45(4): 650\u2013674. doi:10.1007/s00224-009-9173-y</li>\n<li>\u2013\u2013\u2013, 2020, \u201cA computational theory of\nmeaning\u201d, in <em>Advances in Info-Metrics: Information and\nInformation Processing across Disciplines</em>, Min Chen, Michael\nDunn, Amos Golan and Aman Ullah (eds.), New York: Oxford University\nPress, 32\u201378. doi:10.1093/oso/9780190636685.003.0002</li>\n<li>Adriaans, Pieter and Peter van Emde Boas, 2011,\n\u201cComputation, Information, and the Arrow of Time\u201d, in\n<em>Computability in Context: Computation and Logic in the Real\nWorld</em>, by S Barry Cooper and Andrea Sorbi (eds), London: Imperial\nCollege Press, 1\u201317. doi:10.1142/9781848162778_0001</li>\n<li>Adriaans, Pieter and Johan van Benthem, 2008a,\n\u201cIntroduction: Information Is What Information Does\u201d, in\nAdriaans &amp; van Benthem 2008b: 3\u201326.\ndoi:10.1016/B978-0-444-51726-5.50006-6</li>\n<li>\u2013\u2013\u2013 (eds.), 2008b, <em>Philosophy of\nInformation</em>, (Handbook of the Philosophy of Science 8),\nAmsterdam: Elsevier. doi:10.1016/C2009-0-16481-4</li>\n<li>Adriaans, Pieter and Paul M.B. Vit\u00e1nyi, 2009,\n\u201cApproximation of the Two-Part MDL Code\u201d, <em>IEEE\nTransactions on Information Theory</em>, 55(1): 444\u2013457.\ndoi:10.1109/TIT.2008.2008152</li>\n<li>Adriaans, Pieter and Dolf Zantinge, 1996, <em>Data Mining</em>,\nHarlow, England: Addison-Wesley.</li>\n<li>Agrawal, Manindra, Neeraj Kayal, and Nitin Saxena, 2004,\n\u201cPRIMES Is in P\u201d, <em>Annals of Mathematics</em>, 160(2):\n781\u2013793. doi:10.4007/annals.2004.160.781</li>\n<li>Albrecht, Andreas and Daniel Phillips, 2014, \u201cOrigin of\nProbabilities and Their Application to the Multiverse\u201d,\n<em>Physical Review D</em>, 90(12): 123514. doi:10.1103/PhysRevD.90.\n123514</li>\n<li>Antunes, Lu\u00eds and Lance Fortnow, 2003,\n\u201cSophistication Revisited\u201d, in <em>Proceedings of the 30th\nInternational Colloquium on Automata, Languages and Programming</em>\n(Lecture Notes in Computer Science: Volume 2719), Jos C. M. Baeten,\nJan Karel Lenstra, Joachim Parrow, and Gerhard J. Woeginger (eds.),\nBerlin: Springer, pp. 267\u2013277. doi:10.1007/3-540-45061-0_23</li>\n<li>Antunes, Luis, Lance Fortnow, Dieter van Melkebeek, and N.V.\nVinodchandran, 2006, \u201cComputational Depth: Concept and\nApplications\u201d, <em>Theoretical Computer Science</em>, 354(3):\n391\u2013404. doi:10.1016/j.tcs.2005.11.033</li>\n<li>Aquinas, St. Thomas, 1265\u20131274, <em>Summa\nTheologiae</em>.</li>\n<li>Arbuthnot, John, 1692, <em>Of the Laws of Chance, or, a method of\nCalculation of the Hazards of Game, Plainly demonstrated, And applied\nto Games as present most in Use</em>, translation of Huygens\u2019\n<em>De Ratiociniis in Ludo Aleae</em>, 1657.</li>\n<li>Aristotle. <em>Aristotle in 23 Volumes</em>, Vols. 17, 18,\ntranslated by Hugh Tredennick, Cambridge, MA: Harvard University\nPress; London, William Heinemann Ltd. 1933, 1989.</li>\n<li>Austen, Jane, 1815, <em>Emma</em>, London: Richard Bentley and\nSon.</li>\n<li>Bar-Hillel, Yehoshua and Rudolf Carnap, 1953, \u201cSemantic\nInformation\u201d, <em>The British Journal for the Philosophy of\nScience</em>, 4(14): 147\u2013157. doi:10.1093/bjps/IV.14.147</li>\n<li>Bais, F. Alexander and J. Doyne Farmer, 2008, \u201cThe Physics\nof Information\u201d, Adriaans and van Benthem 2008b: 609\u2013683.\ndoi:10.1016/B978-0-444-51726-5.50020-0</li>\n<li>Barron, Andrew, Jorma Rissanen, and Bin Yu, 1998, \u201cThe\nMinimum Description Length Principle in Coding and Modeling\u201d,\n<em>IEEE Transactions on Information Theory</em>, 44(6):\n2743\u20132760. doi:10.1109/18.720554</li>\n<li>Barwise, Jon and John Perry, 1983, <em>Situations and\nAttitudes</em>, Cambridge, MA: MIT Press.</li>\n<li>Bell, Gordon, Tony Hey, and Alex Szalay, 2009, \u201cComputer\nScience: Beyond the Data Deluge\u201d, <em>Science</em>, 323(5919):\n1297\u20131298. doi:10.1126/science.1170411</li>\n<li>Bennett, C. H., 1988, \u201cLogical Depth and Physical\nComplexity\u201d, in Rolf Herken (ed.), <em>The Universal Turing\nMachine: A Half-Century Survey</em>, Oxford: Oxford University Press,\npp. 227\u2013257.</li>\n<li>Berkeley, George, 1732, <em>Alciphron: Or the Minute\nPhilosopher</em>, Edinburgh: Thomas Nelson, 1948\u201357.</li>\n<li>Bernoulli, Danielis, 1738, <em>Hydrodynamica</em>, Argentorati:\nsumptibus Johannis Reinholdi Dulseckeri.\n [<a href=\"http://doi.org/10.3931/e-rara-3911\" target=\"other\">Bernoulli 1738 available online</a>]</li>\n<li>Birkhoff, George David, 1950, <em>Collected Mathematical\nPapers</em>, New York: American Mathematical Society.</li>\n<li>Bloem, Peter, Steven de Rooij, and Pieter Adriaans, 2015,\n\u201cTwo Problems for Sophistication\u201d, in <em>Algorithmic\nLearning Theory</em>, (Lecture Notes in Computer Science 9355),\nKamalika Chaudhuri, Claudio Gentile, and Sandra Zilles (eds.), Cham:\nSpringer International Publishing, 379\u2013394.\ndoi:10.1007/978-3-319-24486-0_25</li>\n<li>Boltzmann, Ludwig, 1866, \u201c\u00dcber die Mechanische\nBedeutung des Zweiten Hauptsatzes der W\u00e4rmetheorie\u201d,\n<em>Wiener Berichte</em>, 53: 195\u2013220.</li>\n<li>Boole, George, 1847, <em>Mathematical Analysis of Logic: Being an\nEssay towards a Calculus of Deductive Reasoning</em>, Cambridge:\nMacmillan, Barclay, &amp; Macmillan.\n [<a href=\"http://archive.org/details/mathematicalanal00booluoft\" target=\"other\">Boole 1847 available online</a>].</li>\n<li>\u2013\u2013\u2013, 1854, <em>An Investigation of the Laws of\nThought: On which are Founded the Mathematical Theories of Logic and\nProbabilities</em>, London: Walton and Maberly.</li>\n<li>Bostrom, Nick, 2003, \u201cAre We Living in a Computer\nSimulation?\u201d, <em>The Philosophical Quarterly</em>, 53(211):\n243\u2013255. doi:10.1111/1467-9213.00309</li>\n<li>Bott, R. and J. Milnor, 1958, \u201cOn the Parallelizability of\nthe Spheres\u201d, <em>Bulletin of the American Mathematical\nSociety</em>, 64(3): 87\u201389.\ndoi:10.1090/S0002-9904-1958-10166-4</li>\n<li>Bovens, Luc and Stephan Hartmann, 2003, <em>Bayesian\nEpistemology</em>, Oxford: Oxford University Press.\ndoi:10.1093/0199269750.001.0001</li>\n<li>Brenner, Joseph E., 2008, <em>Logic in Reality</em>, Dordrecht:\nSpringer Netherlands. doi:10.1007/978-1-4020-8375-4</li>\n<li>Briggs, Henry, 1624, <em>Arithmetica Logarithmica</em>, London:\nGulielmus Iones. </li>\n<li>Capurro, Rafael, 1978, <em>Information. Ein Beitrag zur\netymologischen und ideengeschichtlichen Begr\u00fcndung des\nInformationsbegriffs</em> (Information: A contribution to the\nfoundation of the concept of information based on its etymology and in\nthe history of ideas), Munich, Germany: Saur.\n [<a href=\"http://www.capurro.de/info.html\" target=\"other\">Capurro 1978 available online</a>].</li>\n<li>\u2013\u2013\u2013, 2009, \u201cPast, Present, and Future of\nthe Concept of Information\u201d, <em>TripleC: Communication,\nCapitalism &amp; Critique</em>, 7(2): 125\u2013141.\ndoi:10.31269/triplec.v7i2.113</li>\n<li>Capurro, Rafael and Birger Hj\u00f8rland, 2003, \u201cThe\nConcept of Information\u201d, in Blaise Cronin (ed.), <em>Annual\nReview of Information Science and Technology (ARIST)</em>, 37:\n343\u2013411 (Chapter 8). doi:10.1002/aris.1440370109</li>\n<li>Capurro, Rafael and John Holgate (eds.), 2011, <em>Messages and\nMessengers: Angeletics as an Approach to the Phenomenology of\nCommunication</em> (<em>Von Boten Und Botschaften</em>,\n(Schriftenreihe Des International Center for Information Ethics 5),\nM\u00fcnchen: Fink.</li>\n<li>Carnap, Rudolf, 1928, <em>Scheinprobleme in der Philosophie</em>\n(Pseudoproblems of Philosophy), Berlin: Weltkreis-Verlag.</li>\n<li>\u2013\u2013\u2013, 1945, \u201cThe Two Concepts of\nProbability: The Problem of Probability\u201d, <em>Philosophy and\nPhenomenological Research</em>, 5(4): 513\u2013532.\ndoi:10.2307/2102817</li>\n<li>\u2013\u2013\u2013, 1947, <em>Meaning and Necessity</em>,\nChicago: The University of Chicago Press.</li>\n<li>\u2013\u2013\u2013, 1950, <em>Logical Foundations of\nProbability</em>, Chicago: The University of Chicago Press.</li>\n<li>Chaitin, Gregory J., 1969, \u201cOn the Length of Programs for\nComputing Finite Binary Sequences: Statistical Considerations\u201d,\n<em>Journal of the ACM</em>, 16(1): 145\u2013159.\ndoi:10.1145/321495.321506</li>\n<li>\u2013\u2013\u2013, 1987, <em>Algorithmic Information\nTheory</em>, Cambridge: Cambridge University Press.\ndoi:10.1017/CBO9780511608858</li>\n<li>Chater, Nick and Paul Vit\u00e1nyi, 2003, \u201cSimplicity: A\nUnifying Principle in Cognitive Science?\u201d, <em>Trends in\nCognitive Sciences</em>, 7(1): 19\u201322.\ndoi:10.1016/S1364-6613(02)00005-0</li>\n<li>Church, Alonzo, 1936, \u201cAn Unsolvable Problem of Elementary\nNumber Theory\u201d, <em>American Journal of Mathematics</em> 58(2):\n345\u2013363. doi:10.2307/2371045 </li>\n<li>Cilibrasi, Rudi and Paul M.B. Vitanyi, 2005, \u201cClustering by\nCompression\u201d, <em>IEEE Transactions on Information Theory</em>,\n51(4): 1523\u20131545. doi:10.1109/TIT.2005.844059</li>\n<li>Clausius, R., 1850, \u201cUeber die bewegende Kraft der\nW\u00e4rme und die Gesetze, welche sich daraus f\u00fcr die\nW\u00e4rmelehre selbst ableiten lassen\u201d, <em>Annalen der Physik\nund Chemie</em>, 155(3): 368\u2013397.\ndoi:10.1002/andp.18501550306</li>\n<li>Conan Doyle, Arthur, 1892, \u201cThe Adventure of the Noble\nBachelor\u201d, in <em>The Adventures of Sherlock Holmes</em>,\nLondon: George Newnes Ltd, story 10.</li>\n<li>Cover, Thomas M. and Joy A. Thomas, 2006, <em>Elements of\nInformation Theory</em>, second edition, New York: John Wiley &amp;\nSons. </li>\n<li>Crawford, James M. and Larry D. Auton, 1993, \u201cExperimental\nResults on the Crossover Point in Satisfiability Problems\u201d,\n<em>Proceedings of the Eleventh National Conference on Artificial\nIntelligence</em>, AAAI Press, pp. 21\u201327.\n [<a href=\"https://www.aaai.org/Library/AAAI/1993/aaai93-004.php\" target=\"other\">Crawford &amp; Auton 1993 available online</a>]</li>\n<li>Crutchfield, James P. and Karl Young, 1989, \u201cInferring\nStatistical Complexity\u201d, <em>Physical Review Letters</em>,\n63(2): 105\u2013108. doi:10.1103/PhysRevLett.63.105</li>\n<li>\u2013\u2013\u2013, 1990, \u201cComputation at the Onset of\nChaos\u201d, in <em>Entropy, Complexity, and the Physics of\nInformation</em>, W. Zurek, editor, SFI Studies in the Sciences of\nComplexity, VIII, Reading, MA: Addison-Wesley, pp. 223\u2013269.\n [<a href=\"http://csc.ucdavis.edu/~cmg/compmech/pubs/CompOnsetTitlePage.htm\" target=\"other\">Crutchfield &amp; Young 1990 available online</a>]</li>\n<li>D\u2019Alfonso, Simon, 2012, \u201cTowards a Framework for\nSemantic Information\u201d, Ph.D. Thesis, Department of Philosophy,\nSchool of Historical and Philosophical Studies, The University of\nMelbourne.\n <a href=\"http://minerva-access.unimelb.edu.au/handle/11343/37818\" target=\"other\">D\u2019Alfonso 2012 available online</a></li>\n<li>Davis, Martin, 2006, \u201cWhy There Is No Such Discipline as\nHypercomputation\u201d, <em>Applied Mathematics and Computation</em>,\n178(1): 4\u20137. doi:10.1016/j.amc.2005.09.066</li>\n<li>Defoe, Daniel, 1719, <em>The Life and Strange Surprising\nAdventures of Robinson Crusoe of York, Mariner: who lived Eight and\nTwenty Years, all alone in an uninhabited Island on the coast of\nAmerica, near the Mouth of the Great River of Oroonoque; Having been\ncast on Shore by Shipwreck, wherein all the Men perished but himself.\nWith An Account how he was at last as strangely deliver\u2019d by\nPirates. Written by Himself</em>, London: W. Taylor.\n [<a href=\"http://www.pierre-marteau.com/editions/1719-robinson-crusoe.html\" target=\"other\">Defoe 1719 available online</a>]</li>\n<li>De Leo, Stefano, 1996, \u201cQuaternions and Special\nRelativity\u201d, <em>Journal of Mathematical Physics</em>, 37(6):\n2955\u20132968. doi:10.1063/1.531548</li>\n<li>Dershowitz, Nachum and Yuri Gurevich, 2008, \u201cA Natural\nAxiomatization of Computability and Proof of Church\u2019s\nThesis\u201d, <em>Bulletin of Symbolic Logic</em>, 14(3):\n299\u2013350. doi:10.2178/bsl/1231081370</li>\n<li>Descartes, Ren\u00e9, 1641, <em>Meditationes de Prima\nPhilosophia</em> (Meditations on First Philosophy), Paris. </li>\n<li>\u2013\u2013\u2013, 1647, <em>Discours de la\nM\u00e9thode</em> (Discourse on Method), Leiden.</li>\n<li>Devlin, Keith and Duska Rosenberg, 2008, \u201cInformation in the\nStudy of Human Interaction\u201d, Adriaans and van Benthem 2008b:\n685\u2013709. doi:10.1016/B978-0-444-51726-5.50021-2</li>\n<li>Dictionnaire du Moyen Fran\u00e7ais (1330\u20131500), 2015,\n\u201cInformation\u201d, in <em>Dictionnaire du Moyen\nFran\u00e7ais (1330\u20131500)</em>, volume 16, 313\u2013315.\n [<a href=\"http://www.atilf.fr/dmf/\" target=\"other\">Dictionnaire du Moyen Fran\u00e7ais available online</a>]</li>\n<li>Domingos, Pedro, 1998, \u201cOccam\u2019s Two Razors: The Sharp\nand the Blunt\u201d, in <em>Proceedings of the Fourth International\nConference on Knowledge Discovery and Data Mining</em> (KDD\u201398),\nNew York: AAAI Press, pp. 37\u201343.\n [<a href=\"https://aaai.org/Library/KDD/1998/kdd98-006.php\" target=\"other\">Domingos 1998 available online</a>]</li>\n<li>Downey, Rodney G. and Denis R. Hirschfeldt, 2010, <em>Algorithmic\nRandomness and Complexity</em>, (Theory and Applications of\nComputability), New York: Springer New York.\ndoi:10.1007/978-0-387-68441-3</li>\n<li>Dretske, Fred, 1981, <em>Knowledge and the Flow of\nInformation</em>, Cambridge, MA: The MIT Press.</li>\n<li>Dufort, Paul A. and Charles J. Lumsden, 1994, \u201cThe\nComplexity and Entropy of Turing Machines\u201d, in <em>Proceedings\nWorkshop on Physics and Computation</em>. PhysComp \u201994, Dallas,\nTX: IEEE Computer Society Press, 227\u2013232.\ndoi:10.1109/PHYCMP.1994.363677</li>\n<li>Dunn, Jon Michael, 2001, \u201cThe Concept of Information and the\nDevelopment of Modern Logic\u201d, in <em>Zwischen traditioneller und\nmoderner Logik: Nichtklassiche Ansatze</em> (<em>Non-classical\nApproaches in the Transition from Traditional to Modern Logic</em>),\nWerner Stelzner and Manfred St\u00f6ckler (eds.), Paderborn: Mentis,\n423\u2013447.</li>\n<li>\u2013\u2013\u2013, 2008, \u201cInformation in Computer\nScience\u201d, in Adriaans and van Benthem 2008b: 581\u2013608.\ndoi:10.1016/B978-0-444-51726-5.50019-4</li>\n<li>Dijksterhuis, E. J., 1986, <em>The Mechanization of the World\nPicture: Pythagoras to Newton</em>, Princeton, NJ: Princeton\nUniversity Press.</li>\n<li>Duns Scotus, John [1265/66\u20131308 CE], <em>Opera Omnia</em>\n(The Wadding edition), Luke Wadding (ed.), Lyon, 1639; reprinted\nHildesheim: Georg Olms Verlagsbuchhandlung, 1968.</li>\n<li>Durand-Lose, J\u00e9r\u00f4me, 2002, \u201cComputing Inside\nthe Billiard Ball Model\u201d, in <em>Collision-Based Computing</em>,\nAndrew Adamatzky (ed.), London: Springer London, 135\u2013160.\ndoi:10.1007/978-1-4471-0129-1_6</li>\n<li>Edwards, Paul, 1967, <em>The Encyclopedia of Philosophy</em>, 8\nvolumes, New York: Macmillan Publishing Company.</li>\n<li>Fayyad, Usama, Gregory Piatetsky-Shapiro, and Padhraic Smyth,\n1996, \u201cFrom Data Mining to Knowledge Discovery in\nDatabases\u201d, <em>AI Magazine</em>, 17(3): 37\u201337.</li>\n<li>Fisher, R. A., 1925, \u201cTheory of Statistical\nEstimation\u201d, <em>Mathematical Proceedings of the Cambridge\nPhilosophical Society</em>, 22(05): 700\u2013725.\ndoi:10.1017/S0305004100009580</li>\n<li>Floridi, Luciano, 1999, \u201cInformation Ethics: On the\nPhilosophical Foundation of Computer Ethics\u201d, <em>Ethics and\nInformation Technology</em>, 1(1): 33\u201352.\ndoi:10.1023/A:1010018611096</li>\n<li>\u2013\u2013\u2013, 2002, \u201cWhat Is the Philosophy of\nInformation?\u201d <em>Metaphilosophy</em>, 33(1\u20132):\n123\u2013145. doi:10.1111/1467-9973.00221</li>\n<li>\u2013\u2013\u2013 (ed.), 2003, <em>The Blackwell Guide to the\nPhilosophy of Computing and Information</em>, Oxford: Blackwell.\ndoi:10.1002/9780470757017</li>\n<li>\u2013\u2013\u2013, 2010, \u201cThe Philosophy of Information\nas a Conceptual Framework\u201d, <em>Knowledge, Technology &amp;\nPolicy</em>, 23(1\u20132): 253\u2013281.\ndoi:10.1007/s12130-010-9112-x</li>\n<li>\u2013\u2013\u2013, 2011, <em>The Philosophy of\nInformation</em>, Oxford: Oxford University Press.\ndoi:10.1093/acprof:oso/9780199232383.001.0001</li>\n<li>\u2013\u2013\u2013, 2019, <em>The logic of information: a\ntheory of philosophy as conceptual design</em>, Oxford: Oxford\nUniversity Press. doi:10.1093/oso/9780198833635.001.0001</li>\n<li>Fredkin, Edward and Tommaso Toffoli, 1982, \u201cConservative\nLogic\u201d, <em>International Journal of Theoretical Physics</em>,\n21(3\u20134): 219\u2013253. doi:10.1007/BF01857727</li>\n<li>Frege, Gottlob, 1879, <em>Begriffsschrift: eine der arithmetischen\nnachgebildete Formelsprache des reinen Denkens</em>, Halle.</li>\n<li>\u2013\u2013\u2013, 1892, \u201c\u00dcber Sinn und\nBedeutung\u201d, <em>Zeitschrift f\u00fcr Philosophie und\nphilosophische Kritik</em>, NF 100.</li>\n<li>Furey, C., 2015, \u201cCharge Quantization from a Number\nOperator\u201d, <em>Physics Letters B</em>, 742(March):\n195\u2013199. doi:10.1016/j.physletb.2015.01.023</li>\n<li>Galileo Galilei, 1623 [1960], <em>Il Saggiatore</em> (in Italian),\nRome; translated as <em>The Assayer</em>, by Stillman Drake and C. D.\nO\u2019Malley, in <em>The Controversy on the Comets of 1618</em>,\nPhiladelphia: University of Pennsylvania Press, 1960,\n151\u2013336.</li>\n<li>Garey, Michael R. and David S. Johnson, 1979, <em>Computers and\nIntractability: A Guide to the Theory of NP-Completeness</em>, (A\nSeries of Books in the Mathematical Sciences), San Francisco: W. H.\nFreeman.</li>\n<li>Gell-Mann, Murray and Seth Lloyd, 2003, \u201cEffective\nComputing\u201d. SFI Working Paper 03-12-068, Santa Fe, NM: Santa Fe\nInstitute.\n [<a href=\"https://www.santafe.edu/research/results/working-papers/effective-complexity\" target=\"other\">Gell-Mann &amp; Lloyd 2003 available online</a>]</li>\n<li>Gibbs, J. Willard, 1906, <em>The Scientific Papers of J. Willard\nGibbs in Two Volumes</em>, 1. Longmans, Green, and Co.</li>\n<li>Godefroy, Fr\u00e9d\u00e9ric G., 1881, <em>Dictionnaire de\nl\u2019ancienne langue fran\u00e7aise et de tous ses dialectes du\n9e au 15e si\u00e8cle</em>, Paris: F. Vieweg.</li>\n<li>G\u00f6del, Kurt, 1931, \u201c\u00dcber formal unentscheidbare\nS\u00e4tze der Principia Mathematica und verwandter Systeme I\u201d,\n<em>Monatshefte f\u00fcr Mathematik und Physik</em>, 38\u201338(1):\n173\u2013198. doi:10.1007/BF01700692</li>\n<li>Goodstein, R. L., 1957, \u201cThe Definition of Number\u201d,\n<em>The Mathematical Gazette</em>, 41(337): 180\u2013186.\ndoi:10.2307/3609188</li>\n<li>Gr\u00fcnwald, Peter D., 2007, <em>The Minimum Description Length\nPrinciple</em>, Cambridge, MA: MIT Press.</li>\n<li>Gr\u00fcnwald, Peter D. and Paul M.B. Vit\u00e1nyi, 2008,\n\u201cAlgorithmic Information Theory\u201d, in Adriaans and van\nBenthem 2008b: 281\u2013317.\ndoi:10.1016/B978-0-444-51726-5.50013-3</li>\n<li>Groot, Adrianus Dingeman de, 1961 [1969], <em>Methodology:\nFoundations of Inference and Research in the Behavioral Sciences</em>\n(<em>Methodologie: grondslagen van onderzoek en denken in de\ngedragswetenschappen</em>), The Hague: Mouton. </li>\n<li>Hamkins, J., and Lewis, A., 2000, <em>Infinite time Turing\nmachines. Journal of Symbolic Logic</em>, 65(2), 567-604.\ndoi:10.2307/2586556 </li>\n<li>Harremo\u00ebs, Peter and Flemming Tops\u00f8e, 2008, \u201cThe\nQuantitative Theory of Information\u201d, in Adriaans and van Benthem\n2008b: 171\u2013216. doi:10.1016/B978-0-444-51726-5.50011-X</li>\n<li>Hartley, R.V.L., 1928, \u201cTransmission of Information\u201d,\n<em>Bell System Technical Journal</em>, 7(3): 535\u2013563.\ndoi:10.1002/j.1538-7305.1928.tb01236.x</li>\n<li>Hazard, Paul, 1935, <em>La Crise de La Conscience\nEurop\u00e9enne (1680\u20131715)</em>, Paris: Boivin.</li>\n<li>Hey, Anthony J. G., Stewart Tansley, and Kristin Tolle (eds.),\n2009, <em>The Fourth Paradigm: Data-Intensive Scientific\nDiscovery</em>, Redmond, WA: Microsoft Research.\n [<a href=\"https://www.microsoft.com/en-us/research/publication/fourth-paradigm-data-intensive-scientific-discovery/\" target=\"other\">Hey et al. 2009 available online</a>]</li>\n<li>Hintikka, Jaakko, 1962, <em>Knowledge and Belief: An Introduction\nto the Logic of the Two Notions</em>, (Contemporary Philosophy),\nIthaca, NY: Cornell University Press.</li>\n<li>\u2013\u2013\u2013, 1973, <em>Logic, Language Games and\nInformation: Kantian Themes in the Philosophy of Logic</em>, Oxford:\nClarendon Press.</li>\n<li>Hume, David, 1739\u201340, <em>A Treatise of Human Nature</em>.\nReprinted, L.A. Selby-Bigge (ed.), Oxford: Clarendon Press, 1896.\n [<a href=\"http://oll.libertyfund.org/titles/hume-a-treatise-of-human-nature\" target=\"other\">Hume 1739\u201340 [1896] available online</a>]</li>\n<li>\u2013\u2013\u2013, 1748, <em>An Enquiry concerning Human\nUnderstanding</em>. Reprinted in <em>Enquiries Concerning the Human\nUnderstanding and Concerning the Principles of Morals</em>, 1777 which\nwas reprinted, L.A. Selby-Bigge (ed.), Oxford: Clarendon Press, 1888\n(second edition 1902).\n [<a href=\"http://oll.libertyfund.org/titles/hume-enquiries-concerning-the-human-understanding-and-concerning-the-principles-of-morals\" target=\"other\">Hume 1748 [1902] available online</a>]</li>\n<li>Hutter, Marcus, 2005, <em>Universal Artificial Intellegence:\nSequential Decisions Based on Algorithmic Probability</em>, (Texts in\nTheoretical Computer Science, an EATCS Series), Berlin, Heidelberg:\nSpringer Berlin Heidelberg. doi:10.1007/b138233</li>\n<li>\u2013\u2013\u2013, 2007a, \u201cOn Universal Prediction and\nBayesian Confirmation\u201d, <em>Theoretical Computer Science</em>,\n384(1): 33\u201348. doi:10.1016/j.tcs.2007.05.016</li>\n<li>\u2013\u2013\u2013, 2007b, \u201cAlgorithmic Information\nTheory: a brief non-technical guide to the field\u201d,\n<em>Scholarpedia</em>, 2(3): art. 2519.\ndoi:10.4249/scholarpedia.2519</li>\n<li>\u2013\u2013\u2013, 2010, \u201cA Complete Theory of\nEverything (will be subjective)\u201d, <em>Algorithms</em>, 3(4):\n329\u2013350. doi:10.3390/a3040329</li>\n<li>Hutter, Marcus, John W. Lloyd, Kee Siong Ng, and William T.B.\nUther, 2013, \u201cProbabilities on Sentences in an Expressive\nLogic\u201d, <em>Journal of Applied Logic</em>, special issue:\n<em>Combining Probability and Logic: Papers from Progic 2011</em>,\nJeffrey Helzner (ed.), 11(4): 386\u2013420.\ndoi:10.1016/j.jal.2013.03.003. </li>\n<li>Ibn Tufail, <em>Hayy ibn Yaqdhan</em>, translated as\n<em>Philosophus Autodidactus</em>, published by Edward Pococke the\nYounger in 1671.</li>\n<li>Kahn, David, 1967, <em>The Code-Breakers, The Comprehensive\nHistory of Secret Communication from Ancient Times to the\nInternet</em>, New York: Scribner.</li>\n<li>Kant, Immanuel, 1781, <em>Kritik der reinen Vernunft</em>\n(Critique of Pure Reason), Germany. </li>\n<li>Kervaire, Michel A., 1958, \u201cNon-Parallelizability of the\nn-Sphere for n &gt; 7\u201d, <em>Proceedings of the National Academy\nof Sciences of the United States of America</em>, 44(3):\n280\u2013283. doi:10.1073/pnas.44.3.280</li>\n<li>al-Khw\u0101rizm\u012b, Mu\u1e25ammad ibn M\u016bs\u0101, ca. 820\nCE, <em>Hisab al-jabr w\u2019al-muqabala, Kitab al-Jabr\nwa-l-Muqabala</em> (<em>The Compendious Book on Calculation by\nCompletion and Balancing</em>), Translated by Frederic Rosen, London:\nMurray, 1831.\n [<a href=\"http://www.wilbourhall.org/index.html#algebra\" target=\"other\">al-Khwarizmi translation available online</a>]</li>\n<li>Kolmogorov, A.N., 1965, \u201cThree Approaches to the\nQuantitative Definition of Information\u201d, <em>Problems of\nInformation Transmission</em>, 1(1): 1\u20137. Reprinted 1968 in\n<em>International Journal of Computer Mathematics</em>, 2(1\u20134):\n157\u2013168. doi:10.1080/00207166808803030</li>\n<li>Koppel, Moshe, 1987, \u201cComplexity, Depth, and\nSophistication\u201d, <em>Complex Systems</em>, 1(6):\n1087\u20131091.\n [<a href=\"https://www.complex-systems.com/abstracts/v01_i06_a04/\" target=\"other\">Koppel 1987 available online</a>]</li>\n<li>Kripke, Saul A., 1959, \u201cA Completeness Theorem in Modal\nLogic\u201d, <em>The Journal of Symbolic Logic</em>, 24(1):\n1\u201314. doi:10.2307/2964568</li>\n<li>\u2013\u2013\u2013, 1971, \u201cIdentity and Necessity\u201d,\nin Milton K. Munitz (ed.), <em>Identity and Individuation</em>, New\nYork: New York University Press, pp. 135-164.</li>\n<li>Kuipers, Theo A.F. (ed.), 2007a, <em>General Philosophy of\nScience: Focal Issues</em>, Amsterdam: Elsevier Science\nPublishers.</li>\n<li>\u2013\u2013\u2013, 2007b, \u201cExplanation in Philosophy of\nScience\u201d, in Kuipers 2007a.</li>\n<li>Landauer, Rolf, 1961, \u201cIrreversibility and heat generation\nin the computing process\u201d, <em>IBM Journal of Research and\nDevelopment</em>, 5(3): 183\u2013191. doi:10.1147/rd.53.0183</li>\n<li>\u2013\u2013\u2013, 1991, \u201cInformation is\nPhysical\u201d, <em>Physics Today</em>, 44(5): 23\u201329. doi:\n10.1063/1.881299</li>\n<li>Langton, Chris G., 1990, \u201cComputation at the Edge of Chaos:\nPhase Transitions and Emergent Computation\u201d, <em>Physica D:\nNonlinear Phenomena</em>, 42(1\u20133): 12\u201337.\ndoi:10.1016/0167-2789(90)90064-V</li>\n<li>Laplace, Pierre Simon, Marquis de, 1814 [1902], <em>A\nPhilosophical Essay on Probabilities</em>, F.W. Truscott and F.L.\nEmory (trans.), New York: J. Wiley; London: Chapman &amp; Hall.</li>\n<li>Lenski, Wolfgang, 2010, \u201cInformation: A Conceptual\nInvestigation\u201d, <em>Information 2010</em>, 1(2): 74\u2013118.\ndoi:10.3390/info1020074</li>\n<li>Levin, Leonid A., 1973, \u201cUniversal Sequential Search\nProblems\u201d, <em>Problems of Information Transmission</em>, 9(3):\n265\u2013266.</li>\n<li>\u2013\u2013\u2013,1974, \u201cLaws of Information\nConservation (Non-Growth) and Aspects of the Foundation of Probability\nTheory\u201d, <em>Problems of Information Transmission</em>, 10(3):\n206\u2013210.</li>\n<li>\u2013\u2013\u2013, 1984, \u201cRandomness Conservation\nInequalities; Information and Independence in Mathematical\nTheories\u201d, <em>Information and Control</em>, 61(1): 15\u201337.\ndoi:10.1016/S0019-9958(84)80060-1</li>\n<li>Li, Ming and Paul Vit\u00e1nyi, 2019, <em>An Introduction to\nKolmogorov Complexity and Its Applications</em>, (Texts in Computer\nScience), New York: Springer New York.\ndoi:10.1007/978-0-387-49820-1</li>\n<li>Lloyd, Seth, 2000, \u201cUltimate Physical Limits to\nComputation\u201d, <em>Nature</em>, 406(6799): 1047\u20131054.\ndoi:10.1038/35023282</li>\n<li>Lloyd, Seth and Y. Jack Ng, 2004, \u201cBlack Hole\nComputers\u201d, <em>Scientific American</em>, 291(5): 52\u201361.\ndoi:10.1038/scientificamerican1104-52</li>\n<li>Locke, John, 1689, <em>An Essay Concerning Human\nUnderstanding</em>, J. W. Yolton (ed.), London: Dent; New York:\nDutton, 1961.\n [<a href=\"https://oll.libertyfund.org/titles/locke-the-works-of-john-locke-in-nine-volumes\" target=\"other\">Locke 1689 available online</a>]</li>\n<li>Long, B.R., 2014, \u201cInformation is intrinsically semantic but\nalethically neutral\u201d, <em>Synthese</em>, 191: 3447\u20133467.\ndoi:10.1007/s11229-014-0457-7</li>\n<li>\u2013\u2013\u2013, 2019, \u201cA Scientific Metaphysics and\nOckham\u2019s Razor\u201d, <em>Axiomathes</em>, 5: 1\u201331.\ndoi:10.1007/s10516-019-09430-5</li>\n<li>Lundgren, B., 2019, \u201cDoes semantic information need to be\ntruthful?\u201d, <em>Synthese</em>, 196: 2885\u20132906.\ndoi:10.1007/s11229-017-1587-5</li>\n<li>Maat, Jaap, 2004, <em>Philosophical Languages in the Seventeenth\nCentury: Dalgarno, Wilkins, Leibniz</em>, The New Synthese Historical\nLibrary (Book 54), Springer. </li>\n<li>McAllister, James W., 2003, \u201cEffective Complexity as a\nMeasure of Information Content\u201d, <em>Philosophy of Science</em>,\n70(2): 302\u2013307. doi:10.1086/375469</li>\n<li>Mill, John Stuart, 1843, <em>A System of Logic</em>, London.</li>\n<li>Montague, Richard, 2008, \u201cUniversal Grammar\u201d,\n<em>Theoria</em>, 36(3): 373\u2013398.\ndoi:10.1111/j.1755-2567.1970.tb00434.x</li>\n<li>Mugur-Sch\u00e4chter, Mioara, 2003, \u201cQuantum Mechanics\nVersus a Method of Relativized Conceptualization\u201d, in\n<em>Quantum Mechanics, Mathematics, Cognition and Action</em>, Mioara\nMugur-Sch\u00e4chter and Alwyn van der Merwe (eds.), Dordrecht:\nSpringer Netherlands, 109\u2013307. doi:10.1007/0-306-48144-8_7</li>\n<li>Napier, John, 1614, <em>Mirifici Logarithmorum Canonis\nDescriptio</em> (<em>The Description of the Wonderful Canon of\nLogarithms</em>), Edinburgh: Andre Hart. Translated and annotated by\nIan Bruce, www.17centurymaths.com.\n [<a href=\"http://www.17centurymaths.com/contents/napiercontents.html\" target=\"other\">Napier 1614 [Bruce translation] available online</a>].</li>\n<li>Nielsen, Michael A. and Isaac L. Chuang, 2000, <em>Quantum\nComputation and Quantum Information</em>, Cambridge: Cambridge\nUniversity Press.</li>\n<li>Nies, Andr\u00e9, 2009, <em>Computability and Randomness</em>,\nOxford: Oxford University Press.\ndoi:10.1093/acprof:oso/9780199230761.001.0001</li>\n<li>Nyquist, H., 1924, \u201cCertain Factors Affecting Telegraph\nSpeed\u201d, <em>Bell System Technical Journal</em>, 3(2):\n324\u2013346. doi:10.1002/j.1538-7305.1924.tb01361.x</li>\n<li>Ong, Walter J., 1958, <em>Ramus, Method, and the Decay of\nDialogue, From the Art of Discourse to the Art of Reason</em>,\nCambridge MA: Harvard University Press.</li>\n<li>Parikh, Rohit and Ramaswamy Ramanujam, 2003, \u201cA Knowledge\nBased Semantics of Messages\u201d, <em>Journal of Logic, Language and\nInformation</em>, 12(4): 453\u2013467.\ndoi:10.1023/A:1025007018583</li>\n<li>Peirce, Charles S., 1868, \u201cUpon Logical Comprehension and\nExtension\u201d, <em>Proceedings of the American Academy of Arts and\nSciences</em>, 7: 416\u2013432. doi:10.2307/20179572</li>\n<li>\u2013\u2013\u2013, 1886 [1993], \u201c Letter Peirce to A.\nMarquand\u201d, Reprinted in <em>Writings of Charles S. Peirce: A\nChronological Edition, Volume 5: 1884\u20131886</em>, Indianapolis:\nIndiana University Press, pp. 424\u2013427. See also Arthur W. Burks,\n1978, \u201cBook Review: \u2018The New Elements of\nMathematics\u2019 by Charles S. Peirce, Carolyn Eisele\n(editor)\u201d, <em>Bulletin of the American Mathematical\nSociety</em>, 84(5): 913\u2013919.\ndoi:10.1090/S0002-9904-1978-14533-9</li>\n<li>Popper, Karl, 1934, <em>The Logic of Scientific Discovery</em>,\n(<em>Logik der Forschung</em>), English translation 1959, London:\nHutchison. Reprinted 1977.</li>\n<li>Putnam, Hilary, 1988, <em>Representation and reality</em>,\nCambridge, MA: The MIT Press.</li>\n<li>Quine, W.V.O., 1951, \u201cMain Trends in Recent Philosophy: Two\nDogmas of Empiricism\u201d, <em>The Philosophical Review</em>, 60(1):\n20\u201343. Reprinted in his 1953 <em>From a Logical Point of\nView</em>, Cambridge, MA: Harvard University Press.\ndoi:10.2307/2181906</li>\n<li>Rathmanner, Samuel and Marcus Hutter, 2011, \u201cA Philosophical\nTreatise of Universal Induction\u201d, <em>Entropy</em>, 13(6):\n1076\u20131136. doi:10.3390/e13061076</li>\n<li>R\u00e9dei, Mikl\u00f3s and Michael St\u00f6ltzner (eds.),\n2001, <em>John von Neumann and the Foundations of Quantum\nPhysics</em>, (Vienna Circle Institute Yearbook, 8), Dordrecht:\nKluwer.</li>\n<li>R\u00e9nyi, Alfr\u00e9d, 1961, \u201cOn Measures of Entropy\nand Information\u201d, in <em>Proceedings of the Fourth Berkeley\nSymposium on Mathematical Statistics and Probability, Volume 1:\nContributions to the Theory of Statistics</em>, Berkeley, CA: The\nRegents of the University of California, pp. 547\u2013561.\n [<a href=\"https://projecteuclid.org/euclid.bsmsp/1200512181\" target=\"other\">R\u00e9nyi 1961 available online</a>]</li>\n<li>Rissanen, J., 1978, \u201cModeling by Shortest Data\nDescription\u201d, <em>Automatica</em>, 14(5): 465\u2013471.\ndoi:10.1016/0005-1098(78)90005-5</li>\n<li>\u2013\u2013\u2013, 1989, <em>Stochastic Complexity in\nStatistical Inquiry</em>, (World Scientific Series in Computer\nScience, 15), Singapore: World Scientific.</li>\n<li>Rooy, Robert van, 2004, \u201cSignalling Games Select Horn\nStrategies\u201d, <em>Linguistics and Philosophy</em>, 27(4):\n493\u2013527. doi:10.1023/B:LING.0000024403.88733.3f</li>\n<li>Russell, Bertrand, 1905, \u201cOn Denoting\u201d, <em>Mind</em>,\nnew series, 14(4): 479\u2013493. doi:10.1093/mind/XIV.4.479</li>\n<li>Schmandt-Besserat, Denise, 1992, <em>Before Writing</em> (Volume\nI: From Counting to Cuneiform), Austin, TX: University of Texas\nPress.</li>\n<li>Schmidhuber, J\u00fcurgen, 1997a, \u201cLow-Complexity\nArt\u201d, <em>Leonardo</em>, 30(2): 97\u2013103.\ndoi:10.2307/1576418</li>\n<li>\u2013\u2013\u2013, 1997b, \u201cA Computer Scientist\u2019s\nView of Life, the Universe, and Everything\u201d, in <em>Foundations\nof Computer Science</em>, (Lecture Notes in Computer Science, 1337),\nChristian Freksa, Matthias Jantzen, and R\u00fcdiger Valk (eds.),\nBerlin, Heidelberg: Springer Berlin Heidelberg, 201\u2013208.\ndoi:10.1007/BFb0052088</li>\n<li>Schnelle, H., 1976, \u201cInformation\u201d, in Joachim Ritter\n(ed.), <em>Historisches W\u00f6rterbuch der Philosophie</em>, IV\n[Historical dictionary of philosophy, IV] (pp. 116\u2013117).\nStuttgart, Germany: Schwabe.</li>\n<li>Searle, John R., 1990, \u201cIs the Brain a Digital\nComputer?\u201d, <em>Proceedings and Addresses of the American\nPhilosophical Association</em>, 64(3): 21\u201337.\ndoi:10.2307/3130074</li>\n<li>Seiffert, Helmut, 1968, <em>Information \u00fcber die\nInformation</em> [Information about information] Munich: Beck.</li>\n<li>Shannon, Claude E., 1948, \u201cA Mathematical Theory of\nCommunication\u201d, <em>Bell System Technical Journal</em>, 27(3):\n379\u2013423 &amp; 27(4): 623\u2013656.\ndoi:10.1002/j.1538-7305.1948.tb01338.x &amp;\ndoi:10.1002/j.1538-7305.1948.tb00917.x</li>\n<li>Shannon, Claude E. and Warren Weaver, 1949, <em>The Mathematical\nTheory of Communication</em>, Urbana, IL: University of Illinois\nPress.</li>\n<li>Shor, Peter W., 1997, \u201cPolynomial-Time Algorithms for Prime\nFactorization and Discrete Logarithms on a Quantum Computer\u201d,\n<em>SIAM Journal on Computing</em>, 26(5): 1484\u20131509.\ndoi:10.1137/S0097539795293172</li>\n<li>Simon, J.C. and Olivier Dubois, 1989, \u201cNumber of Solutions\nof Satisfiability Instances \u2013 Applications to Knowledge\nBases\u201d, <em>International Journal of Pattern Recognition and\nArtificial Intelligence</em>, 3(1): 53\u201365.\ndoi:10.1142/S0218001489000061</li>\n<li>Simondon, Gilbert, 1989, <em>L\u2019individuation Psychique et\nCollective: \u00c0 La Lumi\u00e8re des Notions de Forme,\nInformation, Potentiel et M\u00e9tastabilit\u00e9\n(L\u2019Invention Philosophique)</em>, Paris: Aubier.</li>\n<li>Singh, Simon, 1999, <em>The Code Book: The Science of Secrecy from\nAncient Egypt to Quantum Cryptography</em>, New York: Anchor\nBooks.</li>\n<li>Solomonoff, R. J., 1960, \u201cA Preliminary Report on a General\nTheory of Inductive Inference\u201d. Report ZTB-138, Cambridge, MA:\nZator.\n [<a href=\"http://raysolomonoff.com/publications/pubs.html\" target=\"other\">Solomonoff 1960 available online</a>]</li>\n<li>\u2013\u2013\u2013, 1964a, \u201cA Formal Theory of Inductive\nInference. Part I\u201d, <em>Information and Control</em>, 7(1):\n1\u201322. doi:10.1016/S0019-9958(64)90223-2</li>\n<li>\u2013\u2013\u2013, 1964b, \u201cA Formal Theory of Inductive\nInference. Part II\u201d, <em>Information and Control</em>, 7(2):\n224\u2013254. doi:10.1016/S0019-9958(64)90131-7</li>\n<li>\u2013\u2013\u2013, 1997, \u201cThe Discovery of Algorithmic\nProbability\u201d, <em>Journal of Computer and System Sciences</em>,\n55(1): 73\u201388. doi:10.1006/jcss.1997.1500</li>\n<li>Stalnaker, Richard, 1984, <em>Inquiry</em>, Cambridge, MA: MIT\nPress.</li>\n<li>Stifel, Michael, 1544, <em>Arithmetica integra</em>, Nuremberg:\nJohan Petreium.</li>\n<li>Tarski, Alfred, 1944, \u201cThe Semantic Conception of Truth: And\nthe Foundations of Semantics\u201d, <em>Philosophy and\nPhenomenological Research</em>, 4(3): 341\u2013376.\ndoi:10.2307/2102968</li>\n<li>Tsallis, Constantino, 1988, \u201cPossible Generalization of\nBoltzmann-Gibbs Statistics\u201d, <em>Journal of Statistical\nPhysics</em>, 52(1\u20132): 479\u2013487.\ndoi:10.1007/BF01016429</li>\n<li>Turing, A. M., 1937, \u201cOn Computable Numbers, with an\nApplication to the Entscheidungsproblem\u201d, <em>Proceedings of the\nLondon Mathematical Society</em>, s2-42(1): 230\u2013265.\ndoi:10.1112/plms/s2-42.1.230</li>\n<li>Valiant, Leslie G., 2009, \u201cEvolvability\u201d, <em>Journal\nof the ACM</em>, 56(1): Article 3. doi:10.1145/1462153.1462156</li>\n<li>van Benthem, Johan F.A.K., 1990, \u201cKunstmatige Intelligentie:\nEen Voortzetting van de Filosofie met Andere Middelen\u201d,\n<em>Algemeen Nederlands Tijdschrift voor Wijsbegeerte</em>, 82:\n83\u2013100.</li>\n<li>\u2013\u2013\u2013, 2006, \u201cEpistemic Logic and\nEpistemology: The State of Their Affairs\u201d, <em>Philosophical\nStudies</em>, 128(1): 49\u201376. doi:10.1007/s11098-005-4052-0</li>\n<li>van Benthem, Johan and Robert van Rooy, 2003, \u201cConnecting\nthe Different Faces of Information\u201d, <em>Journal of Logic,\nLanguage and Information</em>, 12(4): 375\u2013379.\ndoi:10.1023/A:1025026116766</li>\n<li>van Peursen, Cornelis Anthonie, 1987, \u201cChristian\nWolff\u2019s Philosophy of Contingent Reality\u201d, <em>Journal of\nthe History of Philosophy</em>, 25(1): 69\u201382.\ndoi:10.1353/hph.1987.0005</li>\n<li>van Rooij, Robert, 2003, \u201cQuestioning to resolve decision\nproblems\u201d, <em>Linguistics and Philosophy</em>, 26:\n727\u2013763.</li>\n<li>Vereshchagin, Nikolai K. and Paul M.B. Vit\u00e1nyi, 2004,\n\u201cKolmogorov\u2019s Structure Functions and Model\nSelection\u201d, <em>IEEE Transactions on Information Theory</em>,\n50(12): 3265\u20133290. doi:10.1109/TIT.2004.838346</li>\n<li>Verlinde, Erik, 2011, \u201cOn the Origin of Gravity and the Laws\nof Newton\u201d, <em>Journal of High Energy Physics</em>, 2011(4).\ndoi:10.1007/JHEP04(2011)029 </li>\n<li>\u2013\u2013\u2013, 2017, \u201cEmergent Gravity and the Dark\nUniverse\u201d, <em>SciPost Physics</em>, 2(3): 016.\ndoi:10.21468/SciPostPhys.2.3.016</li>\n<li>Vigo, Ronaldo, 2011, \u201cRepresentational Information: A New\nGeneral Notion and Measure of Information\u201d, <em>Information\nSciences</em>, 181(21): 4847\u20134859.\ndoi:10.1016/j.ins.2011.05.020</li>\n<li>\u2013\u2013\u2013, 2012, \u201cComplexity over Uncertainty in\nGeneralized Representational Information Theory (GRIT): A\nStructure-Sensitive General Theory of Information\u201d,\n<em>Information</em>, 4(1): 1\u201330. doi:10.3390/info4010001</li>\n<li>Vit\u00e1nyi, Paul M., 2006, \u201cMeaningful\nInformation\u201d, <em>IEEE Transactions on Information Theory</em>,\n52(10): 4617\u20134626. doi:10.1109/TIT.2006.881729\n [<a href=\"http://arxiv.org/abs/cs.CC/0111053\" target=\"other\">Vit\u00e1nyi 2006 available online</a>].</li>\n<li>Vogel, Cornelia Johanna de, 1968, <em>Plato: De filosoof van het\ntranscendente</em>, Baarn: Het Wereldvenster.</li>\n<li>Von Neumann, John, 1932, <em>Mathematische Grundlagen der\nQuantenmechanik</em>, Berlin: Springer.</li>\n<li>Wallace, C. S., 2005, <em>Statistical and Inductive Inference by\nMinimum Message Length</em>, Berlin: Springer.\ndoi:10.1007/0-387-27656-4</li>\n<li>Wheeler, John Archibald, 1990, \u201cInformation, Physics,\nQuantum: The Search for Links\u201d, in <em>Complexity, Entropy and\nthe Physics of Information</em>, Wojciech H. Zurek (ed.), Boulder, CO:\nWestview Press, 309\u2013336.\n [<a href=\"https://philpapers.org/go.pl?id=WHEIPQ&amp;u=https%3A%2F%2Fphilpapers.org%2Farchive%2FWHEIPQ.pdf\" target=\"other\">Wheeler 1990 available online</a>]</li>\n<li>Whitehead, Alfred and Bertrand Russell, 1910, 1912, 1913,\n<em>Principia Mathematica</em>, 3 vols, Cambridge: Cambridge\nUniversity Press; 2nd edn, 1925 (Vol. 1), 1927 (Vols 2, 3).</li>\n<li>Wilkins, John, 1668, \u201cAn Essay towards a Real Character, and\na Philosophical Language\u201d, London.\n [<a href=\"https://archive.org/details/AnEssayTowardsARealCharacterAndAPhilosophicalLanguage/page/n7\" target=\"other\">Wilkins 1668 available online</a>]</li>\n<li>Windelband, Wilhelm, 1903, <em>Lehrbuch der Geschichte der\nPhilosophie</em>, T\u00fcbingen: J.C.B. Mohr. </li>\n<li>Wolff, J. Gerard, 2006, <em>Unifying Computing and Cognition</em>,\nMenai Bridge: CognitionResearch.org.uk.</li>\n<li>Wolfram, Stephen, 2002, <em>A New Kind of Science</em>, Champaign,\nIL: Wolfram Media.</li>\n<li>Wolpert, David H. and William Macready, 2007, \u201cUsing\nSelf-Dissimilarity to Quantify Complexity\u201d, <em>Complexity</em>,\n12(3): 77\u201385. doi:10.1002/cplx.20165</li>\n<li>Wu, Kun, 2010, \u201cThe Basic Theory of the Philosophy of\nInformation\u201d, in <em>Proceedings of the 4th International\nConference on the Foundations of Information Science</em>, Beijing,\nChina, Pp. 21\u201324.</li>\n<li>\u2013\u2013\u2013, 2016, \u201cThe Interaction and\nConvergence of the Philosophy and Science of Information\u201d,\n<em>Philosophies</em>, 1(3): 228\u2013244.\ndoi:10.3390/philosophies1030228</li>\n<li>Zuse, Konrad, 1969, <em>Rechnender Raum</em>, Braunschweig:\nFriedrich Vieweg &amp; Sohn. Translated as <em>Calculating Space</em>,\nMIT Technical Translation AZT-70-164-GEMIT, MIT (Proj. MAC),\nCambridge, MA, Feb. 1970. English revised by A. German and H. Zenil\n2012.\n [<a href=\"https://philpapers.org/archive/ZUSRR.pdf\" target=\"other\">Zuse 1969 [2012] available online</a>]</li>\n</ul>\n</div>"
    },
    "related_entries": {
        "entry_list": [
            "Aristotle, Special Topics: causality",
            "Church-Turing Thesis",
            "epistemic paradoxes",
            "Frege, Gottlob: controversy with Hilbert",
            "Frege, Gottlob: theorem and foundations for arithmetic",
            "G\u00f6del, Kurt: incompleteness theorems",
            "information: biological",
            "information: semantic conceptions of",
            "information processing: and thermodynamic entropy",
            "logic: and information",
            "logic: substructural",
            "mathematics, philosophy of",
            "Ockham [Occam], William",
            "Plato: middle period metaphysics and epistemology",
            "Port Royal Logic",
            "properties",
            "quantum theory: quantum entanglement and information",
            "rationalism vs. empiricism",
            "recursive functions",
            "rigid designators",
            "Russell\u2019s paradox",
            "set theory",
            "set theory: alternative axiomatic theories",
            "set theory: continuum hypothesis",
            "time: thermodynamic asymmetry in"
        ],
        "entry_link": [
            {
                "../aristotle-causality/": "Aristotle, Special Topics: causality"
            },
            {
                "../church-turing/": "Church-Turing Thesis"
            },
            {
                "../epistemic-paradoxes/": "epistemic paradoxes"
            },
            {
                "../frege-hilbert/": "Frege, Gottlob: controversy with Hilbert"
            },
            {
                "../frege-theorem/": "Frege, Gottlob: theorem and foundations for arithmetic"
            },
            {
                "../goedel-incompleteness/": "G\u00f6del, Kurt: incompleteness theorems"
            },
            {
                "../information-biological/": "information: biological"
            },
            {
                "../information-semantic/": "information: semantic conceptions of"
            },
            {
                "../information-entropy/": "information processing: and thermodynamic entropy"
            },
            {
                "../logic-information/": "logic: and information"
            },
            {
                "../logic-substructural/": "logic: substructural"
            },
            {
                "../philosophy-mathematics/": "mathematics, philosophy of"
            },
            {
                "../ockham/": "Ockham [Occam], William"
            },
            {
                "../plato-metaphysics/": "Plato: middle period metaphysics and epistemology"
            },
            {
                "../port-royal-logic/": "Port Royal Logic"
            },
            {
                "../properties/": "properties"
            },
            {
                "../qt-entangle/": "quantum theory: quantum entanglement and information"
            },
            {
                "../rationalism-empiricism/": "rationalism vs. empiricism"
            },
            {
                "../recursive-functions/": "recursive functions"
            },
            {
                "../rigid-designators/": "rigid designators"
            },
            {
                "../russell-paradox/": "Russell\u2019s paradox"
            },
            {
                "../set-theory/": "set theory"
            },
            {
                "../settheory-alternative/": "set theory: alternative axiomatic theories"
            },
            {
                "../continuum-hypothesis/": "set theory: continuum hypothesis"
            },
            {
                "../time-thermo/": "time: thermodynamic asymmetry in"
            }
        ]
    },
    "academic_tools": {
        "listed_text": [
            "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=information\" target=\"other\">How to cite this entry</a>.",
            "<img alt=\"sep man icon\" src=\"../../symbols/sepman-icon.jpg\"/>",
            "<a href=\"https://leibniz.stanford.edu/friends/preview/information/\" target=\"other\">Preview the PDF version of this entry</a> at the\n <a href=\"https://leibniz.stanford.edu/friends/\" target=\"other\">Friends of the SEP Society</a>.",
            "<img alt=\"inpho icon\" src=\"../../symbols/inpho.png\"/>",
            "<a href=\"https://www.inphoproject.org/entity?sep=information&amp;redirect=True\" target=\"other\">Look up topics and thinkers related to this entry</a>\n at the Internet Philosophy Ontology Project (InPhO).",
            "<img alt=\"phil papers icon\" src=\"../../symbols/pp.gif\"/>",
            "<a href=\"https://philpapers.org/sep/information/\" target=\"other\">Enhanced bibliography for this entry</a>\nat <a href=\"https://philpapers.org/\" target=\"other\">PhilPapers</a>, with links to its database."
        ],
        "listed_links": [
            {
                "https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=information": "How to cite this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/preview/information/": "Preview the PDF version of this entry"
            },
            {
                "https://leibniz.stanford.edu/friends/": "Friends of the SEP Society"
            },
            {
                "https://www.inphoproject.org/entity?sep=information&redirect=True": "Look up topics and thinkers related to this entry"
            },
            {
                "https://philpapers.org/sep/information/": "Enhanced bibliography for this entry"
            },
            {
                "https://philpapers.org/": "PhilPapers"
            }
        ]
    },
    "other_internet_resources": {
        "listed_text": [
            "Aaronson, Scott, 2006,\n <a href=\"http://www.scottaaronson.com/blog/?p=122\" target=\"other\">Reasons to Believe</a>,\n <em>Shtetl-Optimized</em> blog post, September 4, 2006.",
            "Adriaans, Pieter W., 2021,\n <a href=\"https://arxiv.org/abs/2111.04335\" target=\"other\">\u201cDifferential Information Theory\u201d</a>,\n unpublished manuscript, November 2021, arXiv:2111.04335. ",
            "Bekenstein, Jacob D., 1994,\n \u201c<a href=\"http://arxiv.org/abs/gr-qc/9409015\" target=\"other\">Do We Understand Black Hole Entropy?</a>\u201d,\n Plenary talk at Seventh Marcel Grossman meeting at Stanford\nUniversity., arXiv:gr-qc/9409015.",
            "Churchill, Alex, 2012,\n <a href=\"https://www.toothycat.net/~hologram/Turing/index.html\" target=\"other\">Magic: the Gathering is Turing Complete</a>.",
            "Cook, Stephen, 2000,\n <a href=\"https://www.claymath.org/wp-content/uploads/2022/06/pvsnp.pdf\" target=\"other\">The P versus NP Problem</a>,\n Clay Mathematical Institute; The Millennium Prize Problem.",
            "Huber, Franz, 2007,\n <a href=\"http://www.iep.utm.edu/conf-ind/\" target=\"other\">Confirmation and Induction</a>,\n entry in the <em>Internet Encyclopedia of Philosophy</em>.",
            "Sajjad, H. Rizvi, 2006,\n \u201c<a href=\"http://www.iep.utm.edu/avicenna/\" target=\"other\">Avicenna/Ibn Sina</a>\u201d,\n entry in the <em>Internet Encyclopedia of Philosophy</em>.",
            "Goodman, L. and Weisstein, E.W., 2019,\n \u201c<a href=\"http://mathworld.wolfram.com/RiemannHypothesis.html\" target=\"other\">The Riemann Hypothesis</a>\u201d,\n <em>From MathWorld--A Wolfram Web Resource</em>.",
            "<a href=\"http://cstheory.stackexchange.com/questions/88/what-would-it-mean-to-disprove-church-turing-thesis\" target=\"other\">Computability \u2013 What would it mean to disprove Church-Turing thesis?</a>,\n discussion on Theoretical Computer Science StackExchange.",
            "<a href=\"https://www.britannica.com/science/prime-number-theorem\" target=\"other\">Prime Number Theorem</a>,\n <em>Encyclopedia Britannica</em>, December 20, 2010.",
            "<a href=\"https://en.wikipedia.org/w/index.php?title=Hardware_random_number_generator&amp;oldid=867411555\" target=\"other\">Hardware random number generator</a>,\n <em>Wikipedia</em> entry, November 2018."
        ],
        "listed_links": [
            {
                "http://www.scottaaronson.com/blog/?p=122": "Reasons to Believe"
            },
            {
                "https://arxiv.org/abs/2111.04335": "\u201cDifferential Information Theory\u201d"
            },
            {
                "http://arxiv.org/abs/gr-qc/9409015": "Do We Understand Black Hole Entropy?"
            },
            {
                "https://www.toothycat.net/~hologram/Turing/index.html": "Magic: the Gathering is Turing Complete"
            },
            {
                "https://www.claymath.org/wp-content/uploads/2022/06/pvsnp.pdf": "The P versus NP Problem"
            },
            {
                "http://www.iep.utm.edu/conf-ind/": "Confirmation and Induction"
            },
            {
                "http://www.iep.utm.edu/avicenna/": "Avicenna/Ibn Sina"
            },
            {
                "http://mathworld.wolfram.com/RiemannHypothesis.html": "The Riemann Hypothesis"
            },
            {
                "http://cstheory.stackexchange.com/questions/88/what-would-it-mean-to-disprove-church-turing-thesis": "Computability \u2013 What would it mean to disprove Church-Turing thesis?"
            },
            {
                "https://www.britannica.com/science/prime-number-theorem": "Prime Number Theorem"
            },
            {
                "https://en.wikipedia.org/w/index.php?title=Hardware_random_number_generator&oldid=867411555": "Hardware random number generator"
            }
        ]
    }
}